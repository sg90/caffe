nohup: ignoring input
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:09:20.288328  1998 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:09:20.288465  1998 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:09:20.288473  1998 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:09:20.288619  1998 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:09:20.288671  1998 net.cpp:292] Input 0 -> data
I0909 13:09:20.288697  1998 net.cpp:66] Creating Layer conv1
I0909 13:09:20.288703  1998 net.cpp:329] conv1 <- data
I0909 13:09:20.288712  1998 net.cpp:290] conv1 -> conv1
I0909 13:09:20.290091  1998 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:09:20.290112  1998 net.cpp:125] conv1 needs backward computation.
I0909 13:09:20.290130  1998 net.cpp:66] Creating Layer relu1
I0909 13:09:20.290137  1998 net.cpp:329] relu1 <- conv1
I0909 13:09:20.290143  1998 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:09:20.290153  1998 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:09:20.290158  1998 net.cpp:125] relu1 needs backward computation.
I0909 13:09:20.290165  1998 net.cpp:66] Creating Layer pool1
I0909 13:09:20.290170  1998 net.cpp:329] pool1 <- conv1
I0909 13:09:20.290177  1998 net.cpp:290] pool1 -> pool1
I0909 13:09:20.290189  1998 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:09:20.290194  1998 net.cpp:125] pool1 needs backward computation.
I0909 13:09:20.290201  1998 net.cpp:66] Creating Layer norm1
I0909 13:09:20.290206  1998 net.cpp:329] norm1 <- pool1
I0909 13:09:20.290213  1998 net.cpp:290] norm1 -> norm1
I0909 13:09:20.290223  1998 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:09:20.290230  1998 net.cpp:125] norm1 needs backward computation.
I0909 13:09:20.290236  1998 net.cpp:66] Creating Layer conv2
I0909 13:09:20.290242  1998 net.cpp:329] conv2 <- norm1
I0909 13:09:20.290249  1998 net.cpp:290] conv2 -> conv2
I0909 13:09:20.299338  1998 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:09:20.299353  1998 net.cpp:125] conv2 needs backward computation.
I0909 13:09:20.299360  1998 net.cpp:66] Creating Layer relu2
I0909 13:09:20.299366  1998 net.cpp:329] relu2 <- conv2
I0909 13:09:20.299372  1998 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:09:20.299381  1998 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:09:20.299386  1998 net.cpp:125] relu2 needs backward computation.
I0909 13:09:20.299392  1998 net.cpp:66] Creating Layer pool2
I0909 13:09:20.299397  1998 net.cpp:329] pool2 <- conv2
I0909 13:09:20.299404  1998 net.cpp:290] pool2 -> pool2
I0909 13:09:20.299412  1998 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:09:20.299418  1998 net.cpp:125] pool2 needs backward computation.
I0909 13:09:20.299425  1998 net.cpp:66] Creating Layer fc7
I0909 13:09:20.299430  1998 net.cpp:329] fc7 <- pool2
I0909 13:09:20.299438  1998 net.cpp:290] fc7 -> fc7
I0909 13:09:20.936529  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.936575  1998 net.cpp:125] fc7 needs backward computation.
I0909 13:09:20.936588  1998 net.cpp:66] Creating Layer relu7
I0909 13:09:20.936594  1998 net.cpp:329] relu7 <- fc7
I0909 13:09:20.936602  1998 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:09:20.936612  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.936619  1998 net.cpp:125] relu7 needs backward computation.
I0909 13:09:20.936625  1998 net.cpp:66] Creating Layer drop7
I0909 13:09:20.936631  1998 net.cpp:329] drop7 <- fc7
I0909 13:09:20.936638  1998 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:09:20.936650  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.936655  1998 net.cpp:125] drop7 needs backward computation.
I0909 13:09:20.936664  1998 net.cpp:66] Creating Layer fc8
I0909 13:09:20.936669  1998 net.cpp:329] fc8 <- fc7
I0909 13:09:20.936678  1998 net.cpp:290] fc8 -> fc8
I0909 13:09:20.944442  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.944454  1998 net.cpp:125] fc8 needs backward computation.
I0909 13:09:20.944461  1998 net.cpp:66] Creating Layer relu8
I0909 13:09:20.944468  1998 net.cpp:329] relu8 <- fc8
I0909 13:09:20.944475  1998 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:09:20.944483  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.944489  1998 net.cpp:125] relu8 needs backward computation.
I0909 13:09:20.944494  1998 net.cpp:66] Creating Layer drop8
I0909 13:09:20.944500  1998 net.cpp:329] drop8 <- fc8
I0909 13:09:20.944506  1998 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:09:20.944514  1998 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:09:20.944519  1998 net.cpp:125] drop8 needs backward computation.
I0909 13:09:20.944526  1998 net.cpp:66] Creating Layer fc9
I0909 13:09:20.944531  1998 net.cpp:329] fc9 <- fc8
I0909 13:09:20.944540  1998 net.cpp:290] fc9 -> fc9
I0909 13:09:20.944911  1998 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:09:20.944933  1998 net.cpp:125] fc9 needs backward computation.
I0909 13:09:20.944942  1998 net.cpp:66] Creating Layer fc10
I0909 13:09:20.944948  1998 net.cpp:329] fc10 <- fc9
I0909 13:09:20.944955  1998 net.cpp:290] fc10 -> fc10
I0909 13:09:20.944968  1998 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:09:20.944977  1998 net.cpp:125] fc10 needs backward computation.
I0909 13:09:20.944983  1998 net.cpp:66] Creating Layer prob
I0909 13:09:20.944989  1998 net.cpp:329] prob <- fc10
I0909 13:09:20.944996  1998 net.cpp:290] prob -> prob
I0909 13:09:20.945006  1998 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:09:20.945013  1998 net.cpp:125] prob needs backward computation.
I0909 13:09:20.945018  1998 net.cpp:156] This network produces output prob
I0909 13:09:20.945029  1998 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:09:20.945036  1998 net.cpp:167] Network initialization done.
I0909 13:09:20.945041  1998 net.cpp:168] Memory required for data: 6183480
Classifying 232 inputs.
Done in 138.90 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:11:41.489476  2199 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:11:41.489648  2199 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:11:41.489660  2199 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:11:41.489807  2199 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:11:41.489872  2199 net.cpp:292] Input 0 -> data
I0909 13:11:41.489900  2199 net.cpp:66] Creating Layer conv1
I0909 13:11:41.489908  2199 net.cpp:329] conv1 <- data
I0909 13:11:41.489917  2199 net.cpp:290] conv1 -> conv1
I0909 13:11:41.491261  2199 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:11:41.491281  2199 net.cpp:125] conv1 needs backward computation.
I0909 13:11:41.491291  2199 net.cpp:66] Creating Layer relu1
I0909 13:11:41.491297  2199 net.cpp:329] relu1 <- conv1
I0909 13:11:41.491304  2199 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:11:41.491314  2199 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:11:41.491320  2199 net.cpp:125] relu1 needs backward computation.
I0909 13:11:41.491328  2199 net.cpp:66] Creating Layer pool1
I0909 13:11:41.491334  2199 net.cpp:329] pool1 <- conv1
I0909 13:11:41.491343  2199 net.cpp:290] pool1 -> pool1
I0909 13:11:41.491353  2199 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:11:41.491360  2199 net.cpp:125] pool1 needs backward computation.
I0909 13:11:41.491368  2199 net.cpp:66] Creating Layer norm1
I0909 13:11:41.491374  2199 net.cpp:329] norm1 <- pool1
I0909 13:11:41.491381  2199 net.cpp:290] norm1 -> norm1
I0909 13:11:41.491391  2199 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:11:41.491397  2199 net.cpp:125] norm1 needs backward computation.
I0909 13:11:41.491405  2199 net.cpp:66] Creating Layer conv2
I0909 13:11:41.491412  2199 net.cpp:329] conv2 <- norm1
I0909 13:11:41.491420  2199 net.cpp:290] conv2 -> conv2
I0909 13:11:41.500313  2199 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:11:41.500337  2199 net.cpp:125] conv2 needs backward computation.
I0909 13:11:41.500345  2199 net.cpp:66] Creating Layer relu2
I0909 13:11:41.500352  2199 net.cpp:329] relu2 <- conv2
I0909 13:11:41.500360  2199 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:11:41.500368  2199 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:11:41.500375  2199 net.cpp:125] relu2 needs backward computation.
I0909 13:11:41.500382  2199 net.cpp:66] Creating Layer pool2
I0909 13:11:41.500396  2199 net.cpp:329] pool2 <- conv2
I0909 13:11:41.500402  2199 net.cpp:290] pool2 -> pool2
I0909 13:11:41.500411  2199 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:11:41.500417  2199 net.cpp:125] pool2 needs backward computation.
I0909 13:11:41.500426  2199 net.cpp:66] Creating Layer fc7
I0909 13:11:41.500432  2199 net.cpp:329] fc7 <- pool2
I0909 13:11:41.500439  2199 net.cpp:290] fc7 -> fc7
I0909 13:11:42.146947  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.146991  2199 net.cpp:125] fc7 needs backward computation.
I0909 13:11:42.147004  2199 net.cpp:66] Creating Layer relu7
I0909 13:11:42.147013  2199 net.cpp:329] relu7 <- fc7
I0909 13:11:42.147022  2199 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:11:42.147032  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.147039  2199 net.cpp:125] relu7 needs backward computation.
I0909 13:11:42.147047  2199 net.cpp:66] Creating Layer drop7
I0909 13:11:42.147053  2199 net.cpp:329] drop7 <- fc7
I0909 13:11:42.147063  2199 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:11:42.147073  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.147080  2199 net.cpp:125] drop7 needs backward computation.
I0909 13:11:42.147089  2199 net.cpp:66] Creating Layer fc8
I0909 13:11:42.147096  2199 net.cpp:329] fc8 <- fc7
I0909 13:11:42.147114  2199 net.cpp:290] fc8 -> fc8
I0909 13:11:42.154903  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.154918  2199 net.cpp:125] fc8 needs backward computation.
I0909 13:11:42.154927  2199 net.cpp:66] Creating Layer relu8
I0909 13:11:42.154933  2199 net.cpp:329] relu8 <- fc8
I0909 13:11:42.154940  2199 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:11:42.154950  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.154956  2199 net.cpp:125] relu8 needs backward computation.
I0909 13:11:42.154964  2199 net.cpp:66] Creating Layer drop8
I0909 13:11:42.154970  2199 net.cpp:329] drop8 <- fc8
I0909 13:11:42.154978  2199 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:11:42.154985  2199 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:11:42.154991  2199 net.cpp:125] drop8 needs backward computation.
I0909 13:11:42.155000  2199 net.cpp:66] Creating Layer fc9
I0909 13:11:42.155006  2199 net.cpp:329] fc9 <- fc8
I0909 13:11:42.155015  2199 net.cpp:290] fc9 -> fc9
I0909 13:11:42.155388  2199 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:11:42.155401  2199 net.cpp:125] fc9 needs backward computation.
I0909 13:11:42.155411  2199 net.cpp:66] Creating Layer fc10
I0909 13:11:42.155417  2199 net.cpp:329] fc10 <- fc9
I0909 13:11:42.155426  2199 net.cpp:290] fc10 -> fc10
I0909 13:11:42.155439  2199 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:11:42.155448  2199 net.cpp:125] fc10 needs backward computation.
I0909 13:11:42.155455  2199 net.cpp:66] Creating Layer prob
I0909 13:11:42.155462  2199 net.cpp:329] prob <- fc10
I0909 13:11:42.155469  2199 net.cpp:290] prob -> prob
I0909 13:11:42.155479  2199 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:11:42.155488  2199 net.cpp:125] prob needs backward computation.
I0909 13:11:42.155493  2199 net.cpp:156] This network produces output prob
I0909 13:11:42.155505  2199 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:11:42.155514  2199 net.cpp:167] Network initialization done.
I0909 13:11:42.155520  2199 net.cpp:168] Memory required for data: 6183480
Classifying 541 inputs.
Done in 359.63 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:18:21.657548  2451 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:18:21.657714  2451 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:18:21.657724  2451 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:18:21.657870  2451 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:18:21.657927  2451 net.cpp:292] Input 0 -> data
I0909 13:18:21.657953  2451 net.cpp:66] Creating Layer conv1
I0909 13:18:21.657960  2451 net.cpp:329] conv1 <- data
I0909 13:18:21.657969  2451 net.cpp:290] conv1 -> conv1
I0909 13:18:21.666332  2451 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:18:21.666358  2451 net.cpp:125] conv1 needs backward computation.
I0909 13:18:21.666368  2451 net.cpp:66] Creating Layer relu1
I0909 13:18:21.666375  2451 net.cpp:329] relu1 <- conv1
I0909 13:18:21.666383  2451 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:18:21.666393  2451 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:18:21.666399  2451 net.cpp:125] relu1 needs backward computation.
I0909 13:18:21.666406  2451 net.cpp:66] Creating Layer pool1
I0909 13:18:21.666412  2451 net.cpp:329] pool1 <- conv1
I0909 13:18:21.666419  2451 net.cpp:290] pool1 -> pool1
I0909 13:18:21.666430  2451 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:18:21.666437  2451 net.cpp:125] pool1 needs backward computation.
I0909 13:18:21.666445  2451 net.cpp:66] Creating Layer norm1
I0909 13:18:21.666450  2451 net.cpp:329] norm1 <- pool1
I0909 13:18:21.666457  2451 net.cpp:290] norm1 -> norm1
I0909 13:18:21.666467  2451 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:18:21.666473  2451 net.cpp:125] norm1 needs backward computation.
I0909 13:18:21.666481  2451 net.cpp:66] Creating Layer conv2
I0909 13:18:21.666488  2451 net.cpp:329] conv2 <- norm1
I0909 13:18:21.666496  2451 net.cpp:290] conv2 -> conv2
I0909 13:18:21.675444  2451 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:18:21.675462  2451 net.cpp:125] conv2 needs backward computation.
I0909 13:18:21.675477  2451 net.cpp:66] Creating Layer relu2
I0909 13:18:21.675483  2451 net.cpp:329] relu2 <- conv2
I0909 13:18:21.675490  2451 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:18:21.675498  2451 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:18:21.675504  2451 net.cpp:125] relu2 needs backward computation.
I0909 13:18:21.675511  2451 net.cpp:66] Creating Layer pool2
I0909 13:18:21.675524  2451 net.cpp:329] pool2 <- conv2
I0909 13:18:21.675531  2451 net.cpp:290] pool2 -> pool2
I0909 13:18:21.675540  2451 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:18:21.675546  2451 net.cpp:125] pool2 needs backward computation.
I0909 13:18:21.675554  2451 net.cpp:66] Creating Layer fc7
I0909 13:18:21.675565  2451 net.cpp:329] fc7 <- pool2
I0909 13:18:21.675573  2451 net.cpp:290] fc7 -> fc7
I0909 13:18:22.313563  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.313623  2451 net.cpp:125] fc7 needs backward computation.
I0909 13:18:22.314251  2451 net.cpp:66] Creating Layer relu7
I0909 13:18:22.314292  2451 net.cpp:329] relu7 <- fc7
I0909 13:18:22.314301  2451 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:18:22.314312  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.314319  2451 net.cpp:125] relu7 needs backward computation.
I0909 13:18:22.314327  2451 net.cpp:66] Creating Layer drop7
I0909 13:18:22.314334  2451 net.cpp:329] drop7 <- fc7
I0909 13:18:22.314344  2451 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:18:22.314357  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.314363  2451 net.cpp:125] drop7 needs backward computation.
I0909 13:18:22.314373  2451 net.cpp:66] Creating Layer fc8
I0909 13:18:22.314378  2451 net.cpp:329] fc8 <- fc7
I0909 13:18:22.314388  2451 net.cpp:290] fc8 -> fc8
I0909 13:18:22.322180  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.322196  2451 net.cpp:125] fc8 needs backward computation.
I0909 13:18:22.322203  2451 net.cpp:66] Creating Layer relu8
I0909 13:18:22.322209  2451 net.cpp:329] relu8 <- fc8
I0909 13:18:22.322216  2451 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:18:22.322227  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.322232  2451 net.cpp:125] relu8 needs backward computation.
I0909 13:18:22.322239  2451 net.cpp:66] Creating Layer drop8
I0909 13:18:22.322247  2451 net.cpp:329] drop8 <- fc8
I0909 13:18:22.322253  2451 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:18:22.322260  2451 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:18:22.322268  2451 net.cpp:125] drop8 needs backward computation.
I0909 13:18:22.322275  2451 net.cpp:66] Creating Layer fc9
I0909 13:18:22.322281  2451 net.cpp:329] fc9 <- fc8
I0909 13:18:22.322290  2451 net.cpp:290] fc9 -> fc9
I0909 13:18:22.322664  2451 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:18:22.322679  2451 net.cpp:125] fc9 needs backward computation.
I0909 13:18:22.322687  2451 net.cpp:66] Creating Layer fc10
I0909 13:18:22.322695  2451 net.cpp:329] fc10 <- fc9
I0909 13:18:22.322701  2451 net.cpp:290] fc10 -> fc10
I0909 13:18:22.322715  2451 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:18:22.322724  2451 net.cpp:125] fc10 needs backward computation.
I0909 13:18:22.322732  2451 net.cpp:66] Creating Layer prob
I0909 13:18:22.322738  2451 net.cpp:329] prob <- fc10
I0909 13:18:22.322746  2451 net.cpp:290] prob -> prob
I0909 13:18:22.322756  2451 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:18:22.322764  2451 net.cpp:125] prob needs backward computation.
I0909 13:18:22.322770  2451 net.cpp:156] This network produces output prob
I0909 13:18:22.322782  2451 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:18:22.322790  2451 net.cpp:167] Network initialization done.
I0909 13:18:22.322796  2451 net.cpp:168] Memory required for data: 6183480
Classifying 78 inputs.
Done in 48.66 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:19:16.437808  2525 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:19:16.437954  2525 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:19:16.437964  2525 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:19:16.438113  2525 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:19:16.438175  2525 net.cpp:292] Input 0 -> data
I0909 13:19:16.438202  2525 net.cpp:66] Creating Layer conv1
I0909 13:19:16.438210  2525 net.cpp:329] conv1 <- data
I0909 13:19:16.438218  2525 net.cpp:290] conv1 -> conv1
I0909 13:19:16.439584  2525 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:19:16.439602  2525 net.cpp:125] conv1 needs backward computation.
I0909 13:19:16.439612  2525 net.cpp:66] Creating Layer relu1
I0909 13:19:16.439620  2525 net.cpp:329] relu1 <- conv1
I0909 13:19:16.439626  2525 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:19:16.439636  2525 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:19:16.439642  2525 net.cpp:125] relu1 needs backward computation.
I0909 13:19:16.439651  2525 net.cpp:66] Creating Layer pool1
I0909 13:19:16.439656  2525 net.cpp:329] pool1 <- conv1
I0909 13:19:16.439663  2525 net.cpp:290] pool1 -> pool1
I0909 13:19:16.439676  2525 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:19:16.439682  2525 net.cpp:125] pool1 needs backward computation.
I0909 13:19:16.439689  2525 net.cpp:66] Creating Layer norm1
I0909 13:19:16.439697  2525 net.cpp:329] norm1 <- pool1
I0909 13:19:16.439703  2525 net.cpp:290] norm1 -> norm1
I0909 13:19:16.439714  2525 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:19:16.439720  2525 net.cpp:125] norm1 needs backward computation.
I0909 13:19:16.439733  2525 net.cpp:66] Creating Layer conv2
I0909 13:19:16.439740  2525 net.cpp:329] conv2 <- norm1
I0909 13:19:16.439749  2525 net.cpp:290] conv2 -> conv2
I0909 13:19:16.448866  2525 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:19:16.448884  2525 net.cpp:125] conv2 needs backward computation.
I0909 13:19:16.448892  2525 net.cpp:66] Creating Layer relu2
I0909 13:19:16.448899  2525 net.cpp:329] relu2 <- conv2
I0909 13:19:16.448906  2525 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:19:16.448915  2525 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:19:16.448920  2525 net.cpp:125] relu2 needs backward computation.
I0909 13:19:16.448927  2525 net.cpp:66] Creating Layer pool2
I0909 13:19:16.448933  2525 net.cpp:329] pool2 <- conv2
I0909 13:19:16.448940  2525 net.cpp:290] pool2 -> pool2
I0909 13:19:16.448950  2525 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:19:16.448956  2525 net.cpp:125] pool2 needs backward computation.
I0909 13:19:16.448963  2525 net.cpp:66] Creating Layer fc7
I0909 13:19:16.448969  2525 net.cpp:329] fc7 <- pool2
I0909 13:19:16.448977  2525 net.cpp:290] fc7 -> fc7
I0909 13:19:17.088624  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.088680  2525 net.cpp:125] fc7 needs backward computation.
I0909 13:19:17.088696  2525 net.cpp:66] Creating Layer relu7
I0909 13:19:17.088706  2525 net.cpp:329] relu7 <- fc7
I0909 13:19:17.088714  2525 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:19:17.088726  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.088732  2525 net.cpp:125] relu7 needs backward computation.
I0909 13:19:17.088747  2525 net.cpp:66] Creating Layer drop7
I0909 13:19:17.088753  2525 net.cpp:329] drop7 <- fc7
I0909 13:19:17.088762  2525 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:19:17.088773  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.088780  2525 net.cpp:125] drop7 needs backward computation.
I0909 13:19:17.088789  2525 net.cpp:66] Creating Layer fc8
I0909 13:19:17.088795  2525 net.cpp:329] fc8 <- fc7
I0909 13:19:17.088805  2525 net.cpp:290] fc8 -> fc8
I0909 13:19:17.096597  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.096611  2525 net.cpp:125] fc8 needs backward computation.
I0909 13:19:17.096619  2525 net.cpp:66] Creating Layer relu8
I0909 13:19:17.096626  2525 net.cpp:329] relu8 <- fc8
I0909 13:19:17.096632  2525 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:19:17.096642  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.096648  2525 net.cpp:125] relu8 needs backward computation.
I0909 13:19:17.096655  2525 net.cpp:66] Creating Layer drop8
I0909 13:19:17.096662  2525 net.cpp:329] drop8 <- fc8
I0909 13:19:17.096668  2525 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:19:17.096676  2525 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:19:17.096683  2525 net.cpp:125] drop8 needs backward computation.
I0909 13:19:17.096690  2525 net.cpp:66] Creating Layer fc9
I0909 13:19:17.096696  2525 net.cpp:329] fc9 <- fc8
I0909 13:19:17.096707  2525 net.cpp:290] fc9 -> fc9
I0909 13:19:17.097081  2525 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:19:17.097095  2525 net.cpp:125] fc9 needs backward computation.
I0909 13:19:17.097105  2525 net.cpp:66] Creating Layer fc10
I0909 13:19:17.097110  2525 net.cpp:329] fc10 <- fc9
I0909 13:19:17.097118  2525 net.cpp:290] fc10 -> fc10
I0909 13:19:17.097131  2525 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:19:17.097141  2525 net.cpp:125] fc10 needs backward computation.
I0909 13:19:17.097148  2525 net.cpp:66] Creating Layer prob
I0909 13:19:17.097154  2525 net.cpp:329] prob <- fc10
I0909 13:19:17.097162  2525 net.cpp:290] prob -> prob
I0909 13:19:17.097172  2525 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:19:17.097179  2525 net.cpp:125] prob needs backward computation.
I0909 13:19:17.097185  2525 net.cpp:156] This network produces output prob
I0909 13:19:17.097196  2525 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:19:17.097205  2525 net.cpp:167] Network initialization done.
I0909 13:19:17.097211  2525 net.cpp:168] Memory required for data: 6183480
Classifying 238 inputs.
Done in 141.53 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:21:44.883002  2534 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:21:44.883148  2534 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:21:44.883158  2534 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:21:44.883308  2534 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:21:44.883363  2534 net.cpp:292] Input 0 -> data
I0909 13:21:44.883388  2534 net.cpp:66] Creating Layer conv1
I0909 13:21:44.883396  2534 net.cpp:329] conv1 <- data
I0909 13:21:44.883405  2534 net.cpp:290] conv1 -> conv1
I0909 13:21:44.884770  2534 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:21:44.884799  2534 net.cpp:125] conv1 needs backward computation.
I0909 13:21:44.884809  2534 net.cpp:66] Creating Layer relu1
I0909 13:21:44.884816  2534 net.cpp:329] relu1 <- conv1
I0909 13:21:44.884824  2534 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:21:44.884834  2534 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:21:44.884840  2534 net.cpp:125] relu1 needs backward computation.
I0909 13:21:44.884848  2534 net.cpp:66] Creating Layer pool1
I0909 13:21:44.884855  2534 net.cpp:329] pool1 <- conv1
I0909 13:21:44.884861  2534 net.cpp:290] pool1 -> pool1
I0909 13:21:44.884873  2534 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:21:44.884881  2534 net.cpp:125] pool1 needs backward computation.
I0909 13:21:44.884887  2534 net.cpp:66] Creating Layer norm1
I0909 13:21:44.884893  2534 net.cpp:329] norm1 <- pool1
I0909 13:21:44.884901  2534 net.cpp:290] norm1 -> norm1
I0909 13:21:44.884912  2534 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:21:44.884918  2534 net.cpp:125] norm1 needs backward computation.
I0909 13:21:44.884927  2534 net.cpp:66] Creating Layer conv2
I0909 13:21:44.884932  2534 net.cpp:329] conv2 <- norm1
I0909 13:21:44.884940  2534 net.cpp:290] conv2 -> conv2
I0909 13:21:44.894093  2534 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:21:44.894111  2534 net.cpp:125] conv2 needs backward computation.
I0909 13:21:44.894119  2534 net.cpp:66] Creating Layer relu2
I0909 13:21:44.894126  2534 net.cpp:329] relu2 <- conv2
I0909 13:21:44.894134  2534 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:21:44.894141  2534 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:21:44.894148  2534 net.cpp:125] relu2 needs backward computation.
I0909 13:21:44.894155  2534 net.cpp:66] Creating Layer pool2
I0909 13:21:44.894160  2534 net.cpp:329] pool2 <- conv2
I0909 13:21:44.894168  2534 net.cpp:290] pool2 -> pool2
I0909 13:21:44.894177  2534 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:21:44.894183  2534 net.cpp:125] pool2 needs backward computation.
I0909 13:21:44.894191  2534 net.cpp:66] Creating Layer fc7
I0909 13:21:44.894197  2534 net.cpp:329] fc7 <- pool2
I0909 13:21:44.894206  2534 net.cpp:290] fc7 -> fc7
I0909 13:21:45.536487  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.536536  2534 net.cpp:125] fc7 needs backward computation.
I0909 13:21:45.536550  2534 net.cpp:66] Creating Layer relu7
I0909 13:21:45.536558  2534 net.cpp:329] relu7 <- fc7
I0909 13:21:45.536567  2534 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:21:45.536577  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.536583  2534 net.cpp:125] relu7 needs backward computation.
I0909 13:21:45.536592  2534 net.cpp:66] Creating Layer drop7
I0909 13:21:45.536598  2534 net.cpp:329] drop7 <- fc7
I0909 13:21:45.536607  2534 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:21:45.536618  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.536625  2534 net.cpp:125] drop7 needs backward computation.
I0909 13:21:45.536634  2534 net.cpp:66] Creating Layer fc8
I0909 13:21:45.536640  2534 net.cpp:329] fc8 <- fc7
I0909 13:21:45.536650  2534 net.cpp:290] fc8 -> fc8
I0909 13:21:45.544253  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.544267  2534 net.cpp:125] fc8 needs backward computation.
I0909 13:21:45.544275  2534 net.cpp:66] Creating Layer relu8
I0909 13:21:45.544281  2534 net.cpp:329] relu8 <- fc8
I0909 13:21:45.544288  2534 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:21:45.544297  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.544303  2534 net.cpp:125] relu8 needs backward computation.
I0909 13:21:45.544311  2534 net.cpp:66] Creating Layer drop8
I0909 13:21:45.544317  2534 net.cpp:329] drop8 <- fc8
I0909 13:21:45.544323  2534 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:21:45.544330  2534 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:21:45.544337  2534 net.cpp:125] drop8 needs backward computation.
I0909 13:21:45.544344  2534 net.cpp:66] Creating Layer fc9
I0909 13:21:45.544350  2534 net.cpp:329] fc9 <- fc8
I0909 13:21:45.544368  2534 net.cpp:290] fc9 -> fc9
I0909 13:21:45.544731  2534 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:21:45.544744  2534 net.cpp:125] fc9 needs backward computation.
I0909 13:21:45.544754  2534 net.cpp:66] Creating Layer fc10
I0909 13:21:45.544759  2534 net.cpp:329] fc10 <- fc9
I0909 13:21:45.544766  2534 net.cpp:290] fc10 -> fc10
I0909 13:21:45.544780  2534 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:21:45.544788  2534 net.cpp:125] fc10 needs backward computation.
I0909 13:21:45.544796  2534 net.cpp:66] Creating Layer prob
I0909 13:21:45.544802  2534 net.cpp:329] prob <- fc10
I0909 13:21:45.544809  2534 net.cpp:290] prob -> prob
I0909 13:21:45.544819  2534 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:21:45.544827  2534 net.cpp:125] prob needs backward computation.
I0909 13:21:45.544832  2534 net.cpp:156] This network produces output prob
I0909 13:21:45.544843  2534 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:21:45.544852  2534 net.cpp:167] Network initialization done.
I0909 13:21:45.544857  2534 net.cpp:168] Memory required for data: 6183480
Classifying 352 inputs.
Done in 224.69 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:25:55.701241  2587 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:25:55.701416  2587 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:25:55.701426  2587 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:25:55.701606  2587 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:25:55.701669  2587 net.cpp:292] Input 0 -> data
I0909 13:25:55.701697  2587 net.cpp:66] Creating Layer conv1
I0909 13:25:55.701704  2587 net.cpp:329] conv1 <- data
I0909 13:25:55.701714  2587 net.cpp:290] conv1 -> conv1
I0909 13:25:55.710232  2587 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:25:55.710258  2587 net.cpp:125] conv1 needs backward computation.
I0909 13:25:55.710268  2587 net.cpp:66] Creating Layer relu1
I0909 13:25:55.710275  2587 net.cpp:329] relu1 <- conv1
I0909 13:25:55.710283  2587 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:25:55.710293  2587 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:25:55.710299  2587 net.cpp:125] relu1 needs backward computation.
I0909 13:25:55.710306  2587 net.cpp:66] Creating Layer pool1
I0909 13:25:55.710312  2587 net.cpp:329] pool1 <- conv1
I0909 13:25:55.710320  2587 net.cpp:290] pool1 -> pool1
I0909 13:25:55.710331  2587 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:25:55.710338  2587 net.cpp:125] pool1 needs backward computation.
I0909 13:25:55.710345  2587 net.cpp:66] Creating Layer norm1
I0909 13:25:55.710351  2587 net.cpp:329] norm1 <- pool1
I0909 13:25:55.710360  2587 net.cpp:290] norm1 -> norm1
I0909 13:25:55.710369  2587 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:25:55.710376  2587 net.cpp:125] norm1 needs backward computation.
I0909 13:25:55.710384  2587 net.cpp:66] Creating Layer conv2
I0909 13:25:55.710391  2587 net.cpp:329] conv2 <- norm1
I0909 13:25:55.710398  2587 net.cpp:290] conv2 -> conv2
I0909 13:25:55.719521  2587 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:25:55.719547  2587 net.cpp:125] conv2 needs backward computation.
I0909 13:25:55.719555  2587 net.cpp:66] Creating Layer relu2
I0909 13:25:55.719563  2587 net.cpp:329] relu2 <- conv2
I0909 13:25:55.719570  2587 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:25:55.719578  2587 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:25:55.719593  2587 net.cpp:125] relu2 needs backward computation.
I0909 13:25:55.719599  2587 net.cpp:66] Creating Layer pool2
I0909 13:25:55.719605  2587 net.cpp:329] pool2 <- conv2
I0909 13:25:55.719614  2587 net.cpp:290] pool2 -> pool2
I0909 13:25:55.719621  2587 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:25:55.719629  2587 net.cpp:125] pool2 needs backward computation.
I0909 13:25:55.719636  2587 net.cpp:66] Creating Layer fc7
I0909 13:25:55.719642  2587 net.cpp:329] fc7 <- pool2
I0909 13:25:55.719650  2587 net.cpp:290] fc7 -> fc7
I0909 13:25:56.361449  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.361498  2587 net.cpp:125] fc7 needs backward computation.
I0909 13:25:56.361516  2587 net.cpp:66] Creating Layer relu7
I0909 13:25:56.361526  2587 net.cpp:329] relu7 <- fc7
I0909 13:25:56.361534  2587 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:25:56.361546  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.361552  2587 net.cpp:125] relu7 needs backward computation.
I0909 13:25:56.361559  2587 net.cpp:66] Creating Layer drop7
I0909 13:25:56.361567  2587 net.cpp:329] drop7 <- fc7
I0909 13:25:56.361574  2587 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:25:56.361587  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.361593  2587 net.cpp:125] drop7 needs backward computation.
I0909 13:25:56.361603  2587 net.cpp:66] Creating Layer fc8
I0909 13:25:56.361618  2587 net.cpp:329] fc8 <- fc7
I0909 13:25:56.361631  2587 net.cpp:290] fc8 -> fc8
I0909 13:25:56.369405  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.369417  2587 net.cpp:125] fc8 needs backward computation.
I0909 13:25:56.369426  2587 net.cpp:66] Creating Layer relu8
I0909 13:25:56.369431  2587 net.cpp:329] relu8 <- fc8
I0909 13:25:56.369441  2587 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:25:56.369448  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.369454  2587 net.cpp:125] relu8 needs backward computation.
I0909 13:25:56.369462  2587 net.cpp:66] Creating Layer drop8
I0909 13:25:56.369468  2587 net.cpp:329] drop8 <- fc8
I0909 13:25:56.369475  2587 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:25:56.369482  2587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:25:56.369488  2587 net.cpp:125] drop8 needs backward computation.
I0909 13:25:56.369496  2587 net.cpp:66] Creating Layer fc9
I0909 13:25:56.369503  2587 net.cpp:329] fc9 <- fc8
I0909 13:25:56.369519  2587 net.cpp:290] fc9 -> fc9
I0909 13:25:56.369895  2587 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:25:56.369909  2587 net.cpp:125] fc9 needs backward computation.
I0909 13:25:56.369917  2587 net.cpp:66] Creating Layer fc10
I0909 13:25:56.369925  2587 net.cpp:329] fc10 <- fc9
I0909 13:25:56.369932  2587 net.cpp:290] fc10 -> fc10
I0909 13:25:56.369946  2587 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:25:56.369956  2587 net.cpp:125] fc10 needs backward computation.
I0909 13:25:56.369962  2587 net.cpp:66] Creating Layer prob
I0909 13:25:56.369968  2587 net.cpp:329] prob <- fc10
I0909 13:25:56.369977  2587 net.cpp:290] prob -> prob
I0909 13:25:56.369987  2587 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:25:56.369994  2587 net.cpp:125] prob needs backward computation.
I0909 13:25:56.370000  2587 net.cpp:156] This network produces output prob
I0909 13:25:56.370012  2587 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:25:56.370021  2587 net.cpp:167] Network initialization done.
I0909 13:25:56.370026  2587 net.cpp:168] Memory required for data: 6183480
Classifying 22 inputs.
Done in 14.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:26:14.464550  2626 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:26:14.464709  2626 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:26:14.464720  2626 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:26:14.464874  2626 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:26:14.464937  2626 net.cpp:292] Input 0 -> data
I0909 13:26:14.464967  2626 net.cpp:66] Creating Layer conv1
I0909 13:26:14.464974  2626 net.cpp:329] conv1 <- data
I0909 13:26:14.464983  2626 net.cpp:290] conv1 -> conv1
I0909 13:26:14.466377  2626 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:26:14.466398  2626 net.cpp:125] conv1 needs backward computation.
I0909 13:26:14.466408  2626 net.cpp:66] Creating Layer relu1
I0909 13:26:14.466414  2626 net.cpp:329] relu1 <- conv1
I0909 13:26:14.466423  2626 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:26:14.466431  2626 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:26:14.466439  2626 net.cpp:125] relu1 needs backward computation.
I0909 13:26:14.466446  2626 net.cpp:66] Creating Layer pool1
I0909 13:26:14.466452  2626 net.cpp:329] pool1 <- conv1
I0909 13:26:14.466459  2626 net.cpp:290] pool1 -> pool1
I0909 13:26:14.466472  2626 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:26:14.466480  2626 net.cpp:125] pool1 needs backward computation.
I0909 13:26:14.466486  2626 net.cpp:66] Creating Layer norm1
I0909 13:26:14.466492  2626 net.cpp:329] norm1 <- pool1
I0909 13:26:14.466500  2626 net.cpp:290] norm1 -> norm1
I0909 13:26:14.466511  2626 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:26:14.466518  2626 net.cpp:125] norm1 needs backward computation.
I0909 13:26:14.466526  2626 net.cpp:66] Creating Layer conv2
I0909 13:26:14.466532  2626 net.cpp:329] conv2 <- norm1
I0909 13:26:14.466541  2626 net.cpp:290] conv2 -> conv2
I0909 13:26:14.475519  2626 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:26:14.475540  2626 net.cpp:125] conv2 needs backward computation.
I0909 13:26:14.475556  2626 net.cpp:66] Creating Layer relu2
I0909 13:26:14.475564  2626 net.cpp:329] relu2 <- conv2
I0909 13:26:14.475571  2626 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:26:14.475579  2626 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:26:14.475586  2626 net.cpp:125] relu2 needs backward computation.
I0909 13:26:14.475600  2626 net.cpp:66] Creating Layer pool2
I0909 13:26:14.475606  2626 net.cpp:329] pool2 <- conv2
I0909 13:26:14.475613  2626 net.cpp:290] pool2 -> pool2
I0909 13:26:14.475622  2626 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:26:14.475636  2626 net.cpp:125] pool2 needs backward computation.
I0909 13:26:14.475643  2626 net.cpp:66] Creating Layer fc7
I0909 13:26:14.475649  2626 net.cpp:329] fc7 <- pool2
I0909 13:26:14.475657  2626 net.cpp:290] fc7 -> fc7
I0909 13:26:15.124037  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.124088  2626 net.cpp:125] fc7 needs backward computation.
I0909 13:26:15.124101  2626 net.cpp:66] Creating Layer relu7
I0909 13:26:15.124109  2626 net.cpp:329] relu7 <- fc7
I0909 13:26:15.124119  2626 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:26:15.124130  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.124135  2626 net.cpp:125] relu7 needs backward computation.
I0909 13:26:15.124143  2626 net.cpp:66] Creating Layer drop7
I0909 13:26:15.124150  2626 net.cpp:329] drop7 <- fc7
I0909 13:26:15.124158  2626 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:26:15.124171  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.124177  2626 net.cpp:125] drop7 needs backward computation.
I0909 13:26:15.124186  2626 net.cpp:66] Creating Layer fc8
I0909 13:26:15.124192  2626 net.cpp:329] fc8 <- fc7
I0909 13:26:15.124202  2626 net.cpp:290] fc8 -> fc8
I0909 13:26:15.132019  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.132033  2626 net.cpp:125] fc8 needs backward computation.
I0909 13:26:15.132041  2626 net.cpp:66] Creating Layer relu8
I0909 13:26:15.132047  2626 net.cpp:329] relu8 <- fc8
I0909 13:26:15.132056  2626 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:26:15.132064  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.132071  2626 net.cpp:125] relu8 needs backward computation.
I0909 13:26:15.132077  2626 net.cpp:66] Creating Layer drop8
I0909 13:26:15.132083  2626 net.cpp:329] drop8 <- fc8
I0909 13:26:15.132091  2626 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:26:15.132097  2626 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:26:15.132103  2626 net.cpp:125] drop8 needs backward computation.
I0909 13:26:15.132112  2626 net.cpp:66] Creating Layer fc9
I0909 13:26:15.132117  2626 net.cpp:329] fc9 <- fc8
I0909 13:26:15.132127  2626 net.cpp:290] fc9 -> fc9
I0909 13:26:15.132489  2626 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:26:15.132503  2626 net.cpp:125] fc9 needs backward computation.
I0909 13:26:15.132511  2626 net.cpp:66] Creating Layer fc10
I0909 13:26:15.132518  2626 net.cpp:329] fc10 <- fc9
I0909 13:26:15.132525  2626 net.cpp:290] fc10 -> fc10
I0909 13:26:15.132539  2626 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:26:15.132547  2626 net.cpp:125] fc10 needs backward computation.
I0909 13:26:15.132555  2626 net.cpp:66] Creating Layer prob
I0909 13:26:15.132561  2626 net.cpp:329] prob <- fc10
I0909 13:26:15.132570  2626 net.cpp:290] prob -> prob
I0909 13:26:15.132580  2626 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:26:15.132586  2626 net.cpp:125] prob needs backward computation.
I0909 13:26:15.132592  2626 net.cpp:156] This network produces output prob
I0909 13:26:15.132603  2626 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:26:15.132612  2626 net.cpp:167] Network initialization done.
I0909 13:26:15.132617  2626 net.cpp:168] Memory required for data: 6183480
Classifying 302 inputs.
Done in 206.39 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:30:06.219104  2809 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:30:06.219254  2809 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:30:06.219264  2809 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:30:06.220155  2809 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:30:06.224982  2809 net.cpp:292] Input 0 -> data
I0909 13:30:06.225015  2809 net.cpp:66] Creating Layer conv1
I0909 13:30:06.225023  2809 net.cpp:329] conv1 <- data
I0909 13:30:06.225033  2809 net.cpp:290] conv1 -> conv1
I0909 13:30:06.236487  2809 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:30:06.236515  2809 net.cpp:125] conv1 needs backward computation.
I0909 13:30:06.236526  2809 net.cpp:66] Creating Layer relu1
I0909 13:30:06.236533  2809 net.cpp:329] relu1 <- conv1
I0909 13:30:06.236541  2809 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:30:06.236552  2809 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:30:06.236558  2809 net.cpp:125] relu1 needs backward computation.
I0909 13:30:06.236565  2809 net.cpp:66] Creating Layer pool1
I0909 13:30:06.236572  2809 net.cpp:329] pool1 <- conv1
I0909 13:30:06.236579  2809 net.cpp:290] pool1 -> pool1
I0909 13:30:06.236593  2809 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:30:06.236598  2809 net.cpp:125] pool1 needs backward computation.
I0909 13:30:06.236606  2809 net.cpp:66] Creating Layer norm1
I0909 13:30:06.236613  2809 net.cpp:329] norm1 <- pool1
I0909 13:30:06.236620  2809 net.cpp:290] norm1 -> norm1
I0909 13:30:06.236636  2809 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:30:06.236644  2809 net.cpp:125] norm1 needs backward computation.
I0909 13:30:06.236651  2809 net.cpp:66] Creating Layer conv2
I0909 13:30:06.236659  2809 net.cpp:329] conv2 <- norm1
I0909 13:30:06.236666  2809 net.cpp:290] conv2 -> conv2
I0909 13:30:06.245854  2809 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:30:06.245872  2809 net.cpp:125] conv2 needs backward computation.
I0909 13:30:06.245880  2809 net.cpp:66] Creating Layer relu2
I0909 13:30:06.245887  2809 net.cpp:329] relu2 <- conv2
I0909 13:30:06.245894  2809 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:30:06.245903  2809 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:30:06.245908  2809 net.cpp:125] relu2 needs backward computation.
I0909 13:30:06.245916  2809 net.cpp:66] Creating Layer pool2
I0909 13:30:06.245923  2809 net.cpp:329] pool2 <- conv2
I0909 13:30:06.245929  2809 net.cpp:290] pool2 -> pool2
I0909 13:30:06.245939  2809 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:30:06.245945  2809 net.cpp:125] pool2 needs backward computation.
I0909 13:30:06.245952  2809 net.cpp:66] Creating Layer fc7
I0909 13:30:06.245959  2809 net.cpp:329] fc7 <- pool2
I0909 13:30:06.245967  2809 net.cpp:290] fc7 -> fc7
I0909 13:30:06.884528  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.884577  2809 net.cpp:125] fc7 needs backward computation.
I0909 13:30:06.884591  2809 net.cpp:66] Creating Layer relu7
I0909 13:30:06.884598  2809 net.cpp:329] relu7 <- fc7
I0909 13:30:06.884608  2809 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:30:06.884619  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.884626  2809 net.cpp:125] relu7 needs backward computation.
I0909 13:30:06.884634  2809 net.cpp:66] Creating Layer drop7
I0909 13:30:06.884640  2809 net.cpp:329] drop7 <- fc7
I0909 13:30:06.884649  2809 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:30:06.884661  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.884668  2809 net.cpp:125] drop7 needs backward computation.
I0909 13:30:06.884677  2809 net.cpp:66] Creating Layer fc8
I0909 13:30:06.884685  2809 net.cpp:329] fc8 <- fc7
I0909 13:30:06.884695  2809 net.cpp:290] fc8 -> fc8
I0909 13:30:06.892477  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.892491  2809 net.cpp:125] fc8 needs backward computation.
I0909 13:30:06.892499  2809 net.cpp:66] Creating Layer relu8
I0909 13:30:06.892505  2809 net.cpp:329] relu8 <- fc8
I0909 13:30:06.892513  2809 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:30:06.892523  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.892529  2809 net.cpp:125] relu8 needs backward computation.
I0909 13:30:06.892536  2809 net.cpp:66] Creating Layer drop8
I0909 13:30:06.892542  2809 net.cpp:329] drop8 <- fc8
I0909 13:30:06.892550  2809 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:30:06.892557  2809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:06.892565  2809 net.cpp:125] drop8 needs backward computation.
I0909 13:30:06.892572  2809 net.cpp:66] Creating Layer fc9
I0909 13:30:06.892580  2809 net.cpp:329] fc9 <- fc8
I0909 13:30:06.892588  2809 net.cpp:290] fc9 -> fc9
I0909 13:30:06.892962  2809 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:30:06.892976  2809 net.cpp:125] fc9 needs backward computation.
I0909 13:30:06.892984  2809 net.cpp:66] Creating Layer fc10
I0909 13:30:06.892992  2809 net.cpp:329] fc10 <- fc9
I0909 13:30:06.892999  2809 net.cpp:290] fc10 -> fc10
I0909 13:30:06.893013  2809 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:30:06.893021  2809 net.cpp:125] fc10 needs backward computation.
I0909 13:30:06.893029  2809 net.cpp:66] Creating Layer prob
I0909 13:30:06.893035  2809 net.cpp:329] prob <- fc10
I0909 13:30:06.893043  2809 net.cpp:290] prob -> prob
I0909 13:30:06.893054  2809 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:30:06.893061  2809 net.cpp:125] prob needs backward computation.
I0909 13:30:06.893067  2809 net.cpp:156] This network produces output prob
I0909 13:30:06.893079  2809 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:30:06.893100  2809 net.cpp:167] Network initialization done.
I0909 13:30:06.893105  2809 net.cpp:168] Memory required for data: 6183480
Classifying 16 inputs.
Done in 9.88 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:30:19.063524  2817 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:30:19.063680  2817 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:30:19.063691  2817 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:30:19.063840  2817 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:30:19.063894  2817 net.cpp:292] Input 0 -> data
I0909 13:30:19.063920  2817 net.cpp:66] Creating Layer conv1
I0909 13:30:19.063927  2817 net.cpp:329] conv1 <- data
I0909 13:30:19.063946  2817 net.cpp:290] conv1 -> conv1
I0909 13:30:19.065310  2817 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:30:19.065330  2817 net.cpp:125] conv1 needs backward computation.
I0909 13:30:19.065340  2817 net.cpp:66] Creating Layer relu1
I0909 13:30:19.065346  2817 net.cpp:329] relu1 <- conv1
I0909 13:30:19.065353  2817 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:30:19.065363  2817 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:30:19.065369  2817 net.cpp:125] relu1 needs backward computation.
I0909 13:30:19.065377  2817 net.cpp:66] Creating Layer pool1
I0909 13:30:19.065383  2817 net.cpp:329] pool1 <- conv1
I0909 13:30:19.065390  2817 net.cpp:290] pool1 -> pool1
I0909 13:30:19.065402  2817 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:30:19.065409  2817 net.cpp:125] pool1 needs backward computation.
I0909 13:30:19.065417  2817 net.cpp:66] Creating Layer norm1
I0909 13:30:19.065423  2817 net.cpp:329] norm1 <- pool1
I0909 13:30:19.065430  2817 net.cpp:290] norm1 -> norm1
I0909 13:30:19.065441  2817 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:30:19.065448  2817 net.cpp:125] norm1 needs backward computation.
I0909 13:30:19.065456  2817 net.cpp:66] Creating Layer conv2
I0909 13:30:19.065462  2817 net.cpp:329] conv2 <- norm1
I0909 13:30:19.065470  2817 net.cpp:290] conv2 -> conv2
I0909 13:30:19.074924  2817 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:30:19.074944  2817 net.cpp:125] conv2 needs backward computation.
I0909 13:30:19.074952  2817 net.cpp:66] Creating Layer relu2
I0909 13:30:19.074959  2817 net.cpp:329] relu2 <- conv2
I0909 13:30:19.074966  2817 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:30:19.074975  2817 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:30:19.074981  2817 net.cpp:125] relu2 needs backward computation.
I0909 13:30:19.074988  2817 net.cpp:66] Creating Layer pool2
I0909 13:30:19.074995  2817 net.cpp:329] pool2 <- conv2
I0909 13:30:19.075001  2817 net.cpp:290] pool2 -> pool2
I0909 13:30:19.075011  2817 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:30:19.075016  2817 net.cpp:125] pool2 needs backward computation.
I0909 13:30:19.075024  2817 net.cpp:66] Creating Layer fc7
I0909 13:30:19.075031  2817 net.cpp:329] fc7 <- pool2
I0909 13:30:19.075038  2817 net.cpp:290] fc7 -> fc7
I0909 13:30:19.715070  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.715119  2817 net.cpp:125] fc7 needs backward computation.
I0909 13:30:19.715133  2817 net.cpp:66] Creating Layer relu7
I0909 13:30:19.715142  2817 net.cpp:329] relu7 <- fc7
I0909 13:30:19.715150  2817 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:30:19.715160  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.715167  2817 net.cpp:125] relu7 needs backward computation.
I0909 13:30:19.715174  2817 net.cpp:66] Creating Layer drop7
I0909 13:30:19.715181  2817 net.cpp:329] drop7 <- fc7
I0909 13:30:19.715189  2817 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:30:19.715201  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.715209  2817 net.cpp:125] drop7 needs backward computation.
I0909 13:30:19.715217  2817 net.cpp:66] Creating Layer fc8
I0909 13:30:19.715224  2817 net.cpp:329] fc8 <- fc7
I0909 13:30:19.715234  2817 net.cpp:290] fc8 -> fc8
I0909 13:30:19.723017  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.723031  2817 net.cpp:125] fc8 needs backward computation.
I0909 13:30:19.723039  2817 net.cpp:66] Creating Layer relu8
I0909 13:30:19.723045  2817 net.cpp:329] relu8 <- fc8
I0909 13:30:19.723054  2817 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:30:19.723062  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.723068  2817 net.cpp:125] relu8 needs backward computation.
I0909 13:30:19.723075  2817 net.cpp:66] Creating Layer drop8
I0909 13:30:19.723083  2817 net.cpp:329] drop8 <- fc8
I0909 13:30:19.723089  2817 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:30:19.723096  2817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:30:19.723104  2817 net.cpp:125] drop8 needs backward computation.
I0909 13:30:19.723111  2817 net.cpp:66] Creating Layer fc9
I0909 13:30:19.723129  2817 net.cpp:329] fc9 <- fc8
I0909 13:30:19.723139  2817 net.cpp:290] fc9 -> fc9
I0909 13:30:19.723515  2817 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:30:19.723527  2817 net.cpp:125] fc9 needs backward computation.
I0909 13:30:19.723536  2817 net.cpp:66] Creating Layer fc10
I0909 13:30:19.723543  2817 net.cpp:329] fc10 <- fc9
I0909 13:30:19.723551  2817 net.cpp:290] fc10 -> fc10
I0909 13:30:19.723564  2817 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:30:19.723573  2817 net.cpp:125] fc10 needs backward computation.
I0909 13:30:19.723582  2817 net.cpp:66] Creating Layer prob
I0909 13:30:19.723587  2817 net.cpp:329] prob <- fc10
I0909 13:30:19.723595  2817 net.cpp:290] prob -> prob
I0909 13:30:19.723605  2817 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:30:19.723613  2817 net.cpp:125] prob needs backward computation.
I0909 13:30:19.723618  2817 net.cpp:156] This network produces output prob
I0909 13:30:19.723630  2817 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:30:19.723639  2817 net.cpp:167] Network initialization done.
I0909 13:30:19.723644  2817 net.cpp:168] Memory required for data: 6183480
Classifying 292 inputs.
Done in 177.00 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:33:23.965443  3081 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:33:23.965605  3081 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:33:23.965617  3081 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:33:23.965766  3081 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:33:23.965829  3081 net.cpp:292] Input 0 -> data
I0909 13:33:23.965857  3081 net.cpp:66] Creating Layer conv1
I0909 13:33:23.965865  3081 net.cpp:329] conv1 <- data
I0909 13:33:23.965874  3081 net.cpp:290] conv1 -> conv1
I0909 13:33:23.967238  3081 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:33:23.967258  3081 net.cpp:125] conv1 needs backward computation.
I0909 13:33:23.967267  3081 net.cpp:66] Creating Layer relu1
I0909 13:33:23.967274  3081 net.cpp:329] relu1 <- conv1
I0909 13:33:23.967283  3081 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:33:23.967291  3081 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:33:23.967298  3081 net.cpp:125] relu1 needs backward computation.
I0909 13:33:23.967306  3081 net.cpp:66] Creating Layer pool1
I0909 13:33:23.967313  3081 net.cpp:329] pool1 <- conv1
I0909 13:33:23.967319  3081 net.cpp:290] pool1 -> pool1
I0909 13:33:23.967332  3081 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:33:23.967339  3081 net.cpp:125] pool1 needs backward computation.
I0909 13:33:23.967346  3081 net.cpp:66] Creating Layer norm1
I0909 13:33:23.967352  3081 net.cpp:329] norm1 <- pool1
I0909 13:33:23.967360  3081 net.cpp:290] norm1 -> norm1
I0909 13:33:23.967370  3081 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:33:23.967377  3081 net.cpp:125] norm1 needs backward computation.
I0909 13:33:23.967386  3081 net.cpp:66] Creating Layer conv2
I0909 13:33:23.967391  3081 net.cpp:329] conv2 <- norm1
I0909 13:33:23.967399  3081 net.cpp:290] conv2 -> conv2
I0909 13:33:23.976337  3081 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:33:23.976354  3081 net.cpp:125] conv2 needs backward computation.
I0909 13:33:23.976369  3081 net.cpp:66] Creating Layer relu2
I0909 13:33:23.976377  3081 net.cpp:329] relu2 <- conv2
I0909 13:33:23.976383  3081 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:33:23.976392  3081 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:33:23.976397  3081 net.cpp:125] relu2 needs backward computation.
I0909 13:33:23.976404  3081 net.cpp:66] Creating Layer pool2
I0909 13:33:23.976418  3081 net.cpp:329] pool2 <- conv2
I0909 13:33:23.976424  3081 net.cpp:290] pool2 -> pool2
I0909 13:33:23.976433  3081 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:33:23.976439  3081 net.cpp:125] pool2 needs backward computation.
I0909 13:33:23.976447  3081 net.cpp:66] Creating Layer fc7
I0909 13:33:23.976454  3081 net.cpp:329] fc7 <- pool2
I0909 13:33:23.976460  3081 net.cpp:290] fc7 -> fc7
I0909 13:33:24.613854  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.613911  3081 net.cpp:125] fc7 needs backward computation.
I0909 13:33:24.613927  3081 net.cpp:66] Creating Layer relu7
I0909 13:33:24.613935  3081 net.cpp:329] relu7 <- fc7
I0909 13:33:24.613945  3081 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:33:24.613955  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.613961  3081 net.cpp:125] relu7 needs backward computation.
I0909 13:33:24.613976  3081 net.cpp:66] Creating Layer drop7
I0909 13:33:24.613982  3081 net.cpp:329] drop7 <- fc7
I0909 13:33:24.613991  3081 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:33:24.614003  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.614020  3081 net.cpp:125] drop7 needs backward computation.
I0909 13:33:24.614030  3081 net.cpp:66] Creating Layer fc8
I0909 13:33:24.614037  3081 net.cpp:329] fc8 <- fc7
I0909 13:33:24.614047  3081 net.cpp:290] fc8 -> fc8
I0909 13:33:24.621728  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.621742  3081 net.cpp:125] fc8 needs backward computation.
I0909 13:33:24.621749  3081 net.cpp:66] Creating Layer relu8
I0909 13:33:24.621755  3081 net.cpp:329] relu8 <- fc8
I0909 13:33:24.621762  3081 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:33:24.621773  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.621778  3081 net.cpp:125] relu8 needs backward computation.
I0909 13:33:24.621785  3081 net.cpp:66] Creating Layer drop8
I0909 13:33:24.621791  3081 net.cpp:329] drop8 <- fc8
I0909 13:33:24.621798  3081 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:33:24.621805  3081 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:33:24.621811  3081 net.cpp:125] drop8 needs backward computation.
I0909 13:33:24.621819  3081 net.cpp:66] Creating Layer fc9
I0909 13:33:24.621825  3081 net.cpp:329] fc9 <- fc8
I0909 13:33:24.621834  3081 net.cpp:290] fc9 -> fc9
I0909 13:33:24.622196  3081 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:33:24.622210  3081 net.cpp:125] fc9 needs backward computation.
I0909 13:33:24.622218  3081 net.cpp:66] Creating Layer fc10
I0909 13:33:24.622225  3081 net.cpp:329] fc10 <- fc9
I0909 13:33:24.622232  3081 net.cpp:290] fc10 -> fc10
I0909 13:33:24.622246  3081 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:33:24.622254  3081 net.cpp:125] fc10 needs backward computation.
I0909 13:33:24.622262  3081 net.cpp:66] Creating Layer prob
I0909 13:33:24.622267  3081 net.cpp:329] prob <- fc10
I0909 13:33:24.622274  3081 net.cpp:290] prob -> prob
I0909 13:33:24.622284  3081 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:33:24.622292  3081 net.cpp:125] prob needs backward computation.
I0909 13:33:24.622298  3081 net.cpp:156] This network produces output prob
I0909 13:33:24.622309  3081 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:33:24.622318  3081 net.cpp:167] Network initialization done.
I0909 13:33:24.622323  3081 net.cpp:168] Memory required for data: 6183480
Classifying 134 inputs.
Done in 91.00 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:34:59.094362  3155 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:34:59.094509  3155 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:34:59.094521  3155 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:34:59.094668  3155 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:34:59.094733  3155 net.cpp:292] Input 0 -> data
I0909 13:34:59.094761  3155 net.cpp:66] Creating Layer conv1
I0909 13:34:59.094769  3155 net.cpp:329] conv1 <- data
I0909 13:34:59.094779  3155 net.cpp:290] conv1 -> conv1
I0909 13:34:59.096145  3155 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:34:59.096164  3155 net.cpp:125] conv1 needs backward computation.
I0909 13:34:59.096174  3155 net.cpp:66] Creating Layer relu1
I0909 13:34:59.096181  3155 net.cpp:329] relu1 <- conv1
I0909 13:34:59.096189  3155 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:34:59.096199  3155 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:34:59.096204  3155 net.cpp:125] relu1 needs backward computation.
I0909 13:34:59.096212  3155 net.cpp:66] Creating Layer pool1
I0909 13:34:59.096218  3155 net.cpp:329] pool1 <- conv1
I0909 13:34:59.096226  3155 net.cpp:290] pool1 -> pool1
I0909 13:34:59.096237  3155 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:34:59.096245  3155 net.cpp:125] pool1 needs backward computation.
I0909 13:34:59.096252  3155 net.cpp:66] Creating Layer norm1
I0909 13:34:59.096258  3155 net.cpp:329] norm1 <- pool1
I0909 13:34:59.096266  3155 net.cpp:290] norm1 -> norm1
I0909 13:34:59.096276  3155 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:34:59.096282  3155 net.cpp:125] norm1 needs backward computation.
I0909 13:34:59.096290  3155 net.cpp:66] Creating Layer conv2
I0909 13:34:59.096297  3155 net.cpp:329] conv2 <- norm1
I0909 13:34:59.096304  3155 net.cpp:290] conv2 -> conv2
I0909 13:34:59.105411  3155 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:34:59.105427  3155 net.cpp:125] conv2 needs backward computation.
I0909 13:34:59.105435  3155 net.cpp:66] Creating Layer relu2
I0909 13:34:59.105442  3155 net.cpp:329] relu2 <- conv2
I0909 13:34:59.105449  3155 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:34:59.105458  3155 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:34:59.105463  3155 net.cpp:125] relu2 needs backward computation.
I0909 13:34:59.105470  3155 net.cpp:66] Creating Layer pool2
I0909 13:34:59.105476  3155 net.cpp:329] pool2 <- conv2
I0909 13:34:59.105484  3155 net.cpp:290] pool2 -> pool2
I0909 13:34:59.105497  3155 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:34:59.105504  3155 net.cpp:125] pool2 needs backward computation.
I0909 13:34:59.105520  3155 net.cpp:66] Creating Layer fc7
I0909 13:34:59.105528  3155 net.cpp:329] fc7 <- pool2
I0909 13:34:59.105537  3155 net.cpp:290] fc7 -> fc7
I0909 13:34:59.744281  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.744331  3155 net.cpp:125] fc7 needs backward computation.
I0909 13:34:59.744346  3155 net.cpp:66] Creating Layer relu7
I0909 13:34:59.744354  3155 net.cpp:329] relu7 <- fc7
I0909 13:34:59.744362  3155 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:34:59.744374  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.744380  3155 net.cpp:125] relu7 needs backward computation.
I0909 13:34:59.744388  3155 net.cpp:66] Creating Layer drop7
I0909 13:34:59.744395  3155 net.cpp:329] drop7 <- fc7
I0909 13:34:59.744403  3155 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:34:59.744415  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.744421  3155 net.cpp:125] drop7 needs backward computation.
I0909 13:34:59.744431  3155 net.cpp:66] Creating Layer fc8
I0909 13:34:59.744436  3155 net.cpp:329] fc8 <- fc7
I0909 13:34:59.744446  3155 net.cpp:290] fc8 -> fc8
I0909 13:34:59.752238  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.752250  3155 net.cpp:125] fc8 needs backward computation.
I0909 13:34:59.752259  3155 net.cpp:66] Creating Layer relu8
I0909 13:34:59.752264  3155 net.cpp:329] relu8 <- fc8
I0909 13:34:59.752271  3155 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:34:59.752280  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.752287  3155 net.cpp:125] relu8 needs backward computation.
I0909 13:34:59.752295  3155 net.cpp:66] Creating Layer drop8
I0909 13:34:59.752300  3155 net.cpp:329] drop8 <- fc8
I0909 13:34:59.752307  3155 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:34:59.752315  3155 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:34:59.752321  3155 net.cpp:125] drop8 needs backward computation.
I0909 13:34:59.752329  3155 net.cpp:66] Creating Layer fc9
I0909 13:34:59.752336  3155 net.cpp:329] fc9 <- fc8
I0909 13:34:59.752344  3155 net.cpp:290] fc9 -> fc9
I0909 13:34:59.752717  3155 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:34:59.752730  3155 net.cpp:125] fc9 needs backward computation.
I0909 13:34:59.752739  3155 net.cpp:66] Creating Layer fc10
I0909 13:34:59.752746  3155 net.cpp:329] fc10 <- fc9
I0909 13:34:59.752753  3155 net.cpp:290] fc10 -> fc10
I0909 13:34:59.752768  3155 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:34:59.752776  3155 net.cpp:125] fc10 needs backward computation.
I0909 13:34:59.752784  3155 net.cpp:66] Creating Layer prob
I0909 13:34:59.752789  3155 net.cpp:329] prob <- fc10
I0909 13:34:59.752796  3155 net.cpp:290] prob -> prob
I0909 13:34:59.752806  3155 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:34:59.752815  3155 net.cpp:125] prob needs backward computation.
I0909 13:34:59.752820  3155 net.cpp:156] This network produces output prob
I0909 13:34:59.752832  3155 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:34:59.752841  3155 net.cpp:167] Network initialization done.
I0909 13:34:59.752847  3155 net.cpp:168] Memory required for data: 6183480
Classifying 349 inputs.
Done in 216.11 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:38:46.556599  3223 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:38:46.556752  3223 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:38:46.556762  3223 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:38:46.556915  3223 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:38:46.556979  3223 net.cpp:292] Input 0 -> data
I0909 13:38:46.557008  3223 net.cpp:66] Creating Layer conv1
I0909 13:38:46.557015  3223 net.cpp:329] conv1 <- data
I0909 13:38:46.557024  3223 net.cpp:290] conv1 -> conv1
I0909 13:38:46.558434  3223 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:38:46.558454  3223 net.cpp:125] conv1 needs backward computation.
I0909 13:38:46.558464  3223 net.cpp:66] Creating Layer relu1
I0909 13:38:46.558470  3223 net.cpp:329] relu1 <- conv1
I0909 13:38:46.558477  3223 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:38:46.558487  3223 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:38:46.558493  3223 net.cpp:125] relu1 needs backward computation.
I0909 13:38:46.558501  3223 net.cpp:66] Creating Layer pool1
I0909 13:38:46.558507  3223 net.cpp:329] pool1 <- conv1
I0909 13:38:46.558514  3223 net.cpp:290] pool1 -> pool1
I0909 13:38:46.558526  3223 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:38:46.558533  3223 net.cpp:125] pool1 needs backward computation.
I0909 13:38:46.558540  3223 net.cpp:66] Creating Layer norm1
I0909 13:38:46.558547  3223 net.cpp:329] norm1 <- pool1
I0909 13:38:46.558559  3223 net.cpp:290] norm1 -> norm1
I0909 13:38:46.558572  3223 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:38:46.558578  3223 net.cpp:125] norm1 needs backward computation.
I0909 13:38:46.558585  3223 net.cpp:66] Creating Layer conv2
I0909 13:38:46.558593  3223 net.cpp:329] conv2 <- norm1
I0909 13:38:46.558600  3223 net.cpp:290] conv2 -> conv2
I0909 13:38:46.567598  3223 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:38:46.567615  3223 net.cpp:125] conv2 needs backward computation.
I0909 13:38:46.567631  3223 net.cpp:66] Creating Layer relu2
I0909 13:38:46.567637  3223 net.cpp:329] relu2 <- conv2
I0909 13:38:46.567644  3223 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:38:46.567652  3223 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:38:46.567658  3223 net.cpp:125] relu2 needs backward computation.
I0909 13:38:46.567666  3223 net.cpp:66] Creating Layer pool2
I0909 13:38:46.567678  3223 net.cpp:329] pool2 <- conv2
I0909 13:38:46.567685  3223 net.cpp:290] pool2 -> pool2
I0909 13:38:46.567693  3223 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:38:46.567699  3223 net.cpp:125] pool2 needs backward computation.
I0909 13:38:46.567708  3223 net.cpp:66] Creating Layer fc7
I0909 13:38:46.567713  3223 net.cpp:329] fc7 <- pool2
I0909 13:38:46.567720  3223 net.cpp:290] fc7 -> fc7
I0909 13:38:47.206105  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.206156  3223 net.cpp:125] fc7 needs backward computation.
I0909 13:38:47.206171  3223 net.cpp:66] Creating Layer relu7
I0909 13:38:47.206179  3223 net.cpp:329] relu7 <- fc7
I0909 13:38:47.206188  3223 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:38:47.206198  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.206205  3223 net.cpp:125] relu7 needs backward computation.
I0909 13:38:47.206213  3223 net.cpp:66] Creating Layer drop7
I0909 13:38:47.206219  3223 net.cpp:329] drop7 <- fc7
I0909 13:38:47.206228  3223 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:38:47.206239  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.206246  3223 net.cpp:125] drop7 needs backward computation.
I0909 13:38:47.206255  3223 net.cpp:66] Creating Layer fc8
I0909 13:38:47.206261  3223 net.cpp:329] fc8 <- fc7
I0909 13:38:47.206271  3223 net.cpp:290] fc8 -> fc8
I0909 13:38:47.214061  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.214074  3223 net.cpp:125] fc8 needs backward computation.
I0909 13:38:47.214082  3223 net.cpp:66] Creating Layer relu8
I0909 13:38:47.214088  3223 net.cpp:329] relu8 <- fc8
I0909 13:38:47.214095  3223 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:38:47.214105  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.214112  3223 net.cpp:125] relu8 needs backward computation.
I0909 13:38:47.214118  3223 net.cpp:66] Creating Layer drop8
I0909 13:38:47.214124  3223 net.cpp:329] drop8 <- fc8
I0909 13:38:47.214131  3223 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:38:47.214139  3223 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:38:47.214145  3223 net.cpp:125] drop8 needs backward computation.
I0909 13:38:47.214154  3223 net.cpp:66] Creating Layer fc9
I0909 13:38:47.214159  3223 net.cpp:329] fc9 <- fc8
I0909 13:38:47.214169  3223 net.cpp:290] fc9 -> fc9
I0909 13:38:47.214545  3223 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:38:47.214558  3223 net.cpp:125] fc9 needs backward computation.
I0909 13:38:47.214567  3223 net.cpp:66] Creating Layer fc10
I0909 13:38:47.214573  3223 net.cpp:329] fc10 <- fc9
I0909 13:38:47.214581  3223 net.cpp:290] fc10 -> fc10
I0909 13:38:47.214596  3223 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:38:47.214604  3223 net.cpp:125] fc10 needs backward computation.
I0909 13:38:47.214612  3223 net.cpp:66] Creating Layer prob
I0909 13:38:47.214617  3223 net.cpp:329] prob <- fc10
I0909 13:38:47.214624  3223 net.cpp:290] prob -> prob
I0909 13:38:47.214634  3223 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:38:47.214643  3223 net.cpp:125] prob needs backward computation.
I0909 13:38:47.214648  3223 net.cpp:156] This network produces output prob
I0909 13:38:47.214670  3223 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:38:47.214679  3223 net.cpp:167] Network initialization done.
I0909 13:38:47.214684  3223 net.cpp:168] Memory required for data: 6183480
Classifying 367 inputs.
Done in 230.00 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:42:53.841897  3371 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:42:53.842046  3371 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:42:53.842057  3371 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:42:53.842207  3371 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:42:53.842262  3371 net.cpp:292] Input 0 -> data
I0909 13:42:53.842299  3371 net.cpp:66] Creating Layer conv1
I0909 13:42:53.842308  3371 net.cpp:329] conv1 <- data
I0909 13:42:53.842316  3371 net.cpp:290] conv1 -> conv1
I0909 13:42:53.843683  3371 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:42:53.843703  3371 net.cpp:125] conv1 needs backward computation.
I0909 13:42:53.843713  3371 net.cpp:66] Creating Layer relu1
I0909 13:42:53.843720  3371 net.cpp:329] relu1 <- conv1
I0909 13:42:53.843727  3371 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:42:53.843737  3371 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:42:53.843744  3371 net.cpp:125] relu1 needs backward computation.
I0909 13:42:53.843751  3371 net.cpp:66] Creating Layer pool1
I0909 13:42:53.843757  3371 net.cpp:329] pool1 <- conv1
I0909 13:42:53.843765  3371 net.cpp:290] pool1 -> pool1
I0909 13:42:53.843776  3371 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:42:53.843783  3371 net.cpp:125] pool1 needs backward computation.
I0909 13:42:53.843791  3371 net.cpp:66] Creating Layer norm1
I0909 13:42:53.843797  3371 net.cpp:329] norm1 <- pool1
I0909 13:42:53.843804  3371 net.cpp:290] norm1 -> norm1
I0909 13:42:53.843816  3371 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:42:53.843822  3371 net.cpp:125] norm1 needs backward computation.
I0909 13:42:53.843829  3371 net.cpp:66] Creating Layer conv2
I0909 13:42:53.843837  3371 net.cpp:329] conv2 <- norm1
I0909 13:42:53.843844  3371 net.cpp:290] conv2 -> conv2
I0909 13:42:53.852880  3371 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:42:53.852897  3371 net.cpp:125] conv2 needs backward computation.
I0909 13:42:53.852912  3371 net.cpp:66] Creating Layer relu2
I0909 13:42:53.852920  3371 net.cpp:329] relu2 <- conv2
I0909 13:42:53.852926  3371 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:42:53.852934  3371 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:42:53.852941  3371 net.cpp:125] relu2 needs backward computation.
I0909 13:42:53.852955  3371 net.cpp:66] Creating Layer pool2
I0909 13:42:53.852962  3371 net.cpp:329] pool2 <- conv2
I0909 13:42:53.852968  3371 net.cpp:290] pool2 -> pool2
I0909 13:42:53.852977  3371 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:42:53.852983  3371 net.cpp:125] pool2 needs backward computation.
I0909 13:42:53.852990  3371 net.cpp:66] Creating Layer fc7
I0909 13:42:53.852996  3371 net.cpp:329] fc7 <- pool2
I0909 13:42:53.853004  3371 net.cpp:290] fc7 -> fc7
I0909 13:42:54.498283  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.498337  3371 net.cpp:125] fc7 needs backward computation.
I0909 13:42:54.498353  3371 net.cpp:66] Creating Layer relu7
I0909 13:42:54.498360  3371 net.cpp:329] relu7 <- fc7
I0909 13:42:54.498369  3371 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:42:54.498380  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.498386  3371 net.cpp:125] relu7 needs backward computation.
I0909 13:42:54.498395  3371 net.cpp:66] Creating Layer drop7
I0909 13:42:54.498401  3371 net.cpp:329] drop7 <- fc7
I0909 13:42:54.498409  3371 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:42:54.498422  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.498428  3371 net.cpp:125] drop7 needs backward computation.
I0909 13:42:54.498437  3371 net.cpp:66] Creating Layer fc8
I0909 13:42:54.498443  3371 net.cpp:329] fc8 <- fc7
I0909 13:42:54.498453  3371 net.cpp:290] fc8 -> fc8
I0909 13:42:54.506237  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.506252  3371 net.cpp:125] fc8 needs backward computation.
I0909 13:42:54.506259  3371 net.cpp:66] Creating Layer relu8
I0909 13:42:54.506265  3371 net.cpp:329] relu8 <- fc8
I0909 13:42:54.506273  3371 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:42:54.506283  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.506289  3371 net.cpp:125] relu8 needs backward computation.
I0909 13:42:54.506295  3371 net.cpp:66] Creating Layer drop8
I0909 13:42:54.506301  3371 net.cpp:329] drop8 <- fc8
I0909 13:42:54.506309  3371 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:42:54.506316  3371 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:42:54.506335  3371 net.cpp:125] drop8 needs backward computation.
I0909 13:42:54.506343  3371 net.cpp:66] Creating Layer fc9
I0909 13:42:54.506350  3371 net.cpp:329] fc9 <- fc8
I0909 13:42:54.506360  3371 net.cpp:290] fc9 -> fc9
I0909 13:42:54.506733  3371 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:42:54.506747  3371 net.cpp:125] fc9 needs backward computation.
I0909 13:42:54.506755  3371 net.cpp:66] Creating Layer fc10
I0909 13:42:54.506762  3371 net.cpp:329] fc10 <- fc9
I0909 13:42:54.506769  3371 net.cpp:290] fc10 -> fc10
I0909 13:42:54.506783  3371 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:42:54.506793  3371 net.cpp:125] fc10 needs backward computation.
I0909 13:42:54.506799  3371 net.cpp:66] Creating Layer prob
I0909 13:42:54.506806  3371 net.cpp:329] prob <- fc10
I0909 13:42:54.506814  3371 net.cpp:290] prob -> prob
I0909 13:42:54.506824  3371 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:42:54.506831  3371 net.cpp:125] prob needs backward computation.
I0909 13:42:54.506837  3371 net.cpp:156] This network produces output prob
I0909 13:42:54.506850  3371 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:42:54.506858  3371 net.cpp:167] Network initialization done.
I0909 13:42:54.506865  3371 net.cpp:168] Memory required for data: 6183480
Classifying 18 inputs.
Done in 11.54 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:43:09.526928  3380 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:43:09.527076  3380 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:43:09.527086  3380 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:43:09.527235  3380 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:43:09.527298  3380 net.cpp:292] Input 0 -> data
I0909 13:43:09.527325  3380 net.cpp:66] Creating Layer conv1
I0909 13:43:09.527333  3380 net.cpp:329] conv1 <- data
I0909 13:43:09.527343  3380 net.cpp:290] conv1 -> conv1
I0909 13:43:09.528723  3380 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:43:09.528744  3380 net.cpp:125] conv1 needs backward computation.
I0909 13:43:09.528754  3380 net.cpp:66] Creating Layer relu1
I0909 13:43:09.528761  3380 net.cpp:329] relu1 <- conv1
I0909 13:43:09.528769  3380 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:43:09.528779  3380 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:43:09.528784  3380 net.cpp:125] relu1 needs backward computation.
I0909 13:43:09.528792  3380 net.cpp:66] Creating Layer pool1
I0909 13:43:09.528798  3380 net.cpp:329] pool1 <- conv1
I0909 13:43:09.528806  3380 net.cpp:290] pool1 -> pool1
I0909 13:43:09.528817  3380 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:43:09.528825  3380 net.cpp:125] pool1 needs backward computation.
I0909 13:43:09.528831  3380 net.cpp:66] Creating Layer norm1
I0909 13:43:09.528838  3380 net.cpp:329] norm1 <- pool1
I0909 13:43:09.528846  3380 net.cpp:290] norm1 -> norm1
I0909 13:43:09.528856  3380 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:43:09.528862  3380 net.cpp:125] norm1 needs backward computation.
I0909 13:43:09.528870  3380 net.cpp:66] Creating Layer conv2
I0909 13:43:09.528877  3380 net.cpp:329] conv2 <- norm1
I0909 13:43:09.528884  3380 net.cpp:290] conv2 -> conv2
I0909 13:43:09.538390  3380 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:43:09.538431  3380 net.cpp:125] conv2 needs backward computation.
I0909 13:43:09.538444  3380 net.cpp:66] Creating Layer relu2
I0909 13:43:09.538450  3380 net.cpp:329] relu2 <- conv2
I0909 13:43:09.538458  3380 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:43:09.538468  3380 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:43:09.538475  3380 net.cpp:125] relu2 needs backward computation.
I0909 13:43:09.538482  3380 net.cpp:66] Creating Layer pool2
I0909 13:43:09.538488  3380 net.cpp:329] pool2 <- conv2
I0909 13:43:09.538496  3380 net.cpp:290] pool2 -> pool2
I0909 13:43:09.538506  3380 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:43:09.538512  3380 net.cpp:125] pool2 needs backward computation.
I0909 13:43:09.538522  3380 net.cpp:66] Creating Layer fc7
I0909 13:43:09.538527  3380 net.cpp:329] fc7 <- pool2
I0909 13:43:09.538535  3380 net.cpp:290] fc7 -> fc7
I0909 13:43:10.185847  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.185901  3380 net.cpp:125] fc7 needs backward computation.
I0909 13:43:10.185917  3380 net.cpp:66] Creating Layer relu7
I0909 13:43:10.185925  3380 net.cpp:329] relu7 <- fc7
I0909 13:43:10.185933  3380 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:43:10.185945  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.185957  3380 net.cpp:125] relu7 needs backward computation.
I0909 13:43:10.185966  3380 net.cpp:66] Creating Layer drop7
I0909 13:43:10.185971  3380 net.cpp:329] drop7 <- fc7
I0909 13:43:10.185981  3380 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:43:10.186002  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.186009  3380 net.cpp:125] drop7 needs backward computation.
I0909 13:43:10.186019  3380 net.cpp:66] Creating Layer fc8
I0909 13:43:10.186025  3380 net.cpp:329] fc8 <- fc7
I0909 13:43:10.186035  3380 net.cpp:290] fc8 -> fc8
I0909 13:43:10.193824  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.193836  3380 net.cpp:125] fc8 needs backward computation.
I0909 13:43:10.193845  3380 net.cpp:66] Creating Layer relu8
I0909 13:43:10.193850  3380 net.cpp:329] relu8 <- fc8
I0909 13:43:10.193857  3380 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:43:10.193867  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.193873  3380 net.cpp:125] relu8 needs backward computation.
I0909 13:43:10.193881  3380 net.cpp:66] Creating Layer drop8
I0909 13:43:10.193886  3380 net.cpp:329] drop8 <- fc8
I0909 13:43:10.193893  3380 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:43:10.193902  3380 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:43:10.193907  3380 net.cpp:125] drop8 needs backward computation.
I0909 13:43:10.193915  3380 net.cpp:66] Creating Layer fc9
I0909 13:43:10.193922  3380 net.cpp:329] fc9 <- fc8
I0909 13:43:10.193930  3380 net.cpp:290] fc9 -> fc9
I0909 13:43:10.194303  3380 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:43:10.194317  3380 net.cpp:125] fc9 needs backward computation.
I0909 13:43:10.194327  3380 net.cpp:66] Creating Layer fc10
I0909 13:43:10.194334  3380 net.cpp:329] fc10 <- fc9
I0909 13:43:10.194341  3380 net.cpp:290] fc10 -> fc10
I0909 13:43:10.194355  3380 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:43:10.194363  3380 net.cpp:125] fc10 needs backward computation.
I0909 13:43:10.194371  3380 net.cpp:66] Creating Layer prob
I0909 13:43:10.194377  3380 net.cpp:329] prob <- fc10
I0909 13:43:10.194385  3380 net.cpp:290] prob -> prob
I0909 13:43:10.194393  3380 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:43:10.194402  3380 net.cpp:125] prob needs backward computation.
I0909 13:43:10.194408  3380 net.cpp:156] This network produces output prob
I0909 13:43:10.194419  3380 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:43:10.194428  3380 net.cpp:167] Network initialization done.
I0909 13:43:10.194434  3380 net.cpp:168] Memory required for data: 6183480
Classifying 202 inputs.
Done in 122.03 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:45:20.029886  3394 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:45:20.030032  3394 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:45:20.030042  3394 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:45:20.030190  3394 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:45:20.030254  3394 net.cpp:292] Input 0 -> data
I0909 13:45:20.030282  3394 net.cpp:66] Creating Layer conv1
I0909 13:45:20.030288  3394 net.cpp:329] conv1 <- data
I0909 13:45:20.030297  3394 net.cpp:290] conv1 -> conv1
I0909 13:45:20.031661  3394 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:45:20.031680  3394 net.cpp:125] conv1 needs backward computation.
I0909 13:45:20.031690  3394 net.cpp:66] Creating Layer relu1
I0909 13:45:20.031697  3394 net.cpp:329] relu1 <- conv1
I0909 13:45:20.031704  3394 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:45:20.031713  3394 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:45:20.031720  3394 net.cpp:125] relu1 needs backward computation.
I0909 13:45:20.031728  3394 net.cpp:66] Creating Layer pool1
I0909 13:45:20.031734  3394 net.cpp:329] pool1 <- conv1
I0909 13:45:20.031741  3394 net.cpp:290] pool1 -> pool1
I0909 13:45:20.031754  3394 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:45:20.031760  3394 net.cpp:125] pool1 needs backward computation.
I0909 13:45:20.031767  3394 net.cpp:66] Creating Layer norm1
I0909 13:45:20.031774  3394 net.cpp:329] norm1 <- pool1
I0909 13:45:20.031781  3394 net.cpp:290] norm1 -> norm1
I0909 13:45:20.031792  3394 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:45:20.031798  3394 net.cpp:125] norm1 needs backward computation.
I0909 13:45:20.031806  3394 net.cpp:66] Creating Layer conv2
I0909 13:45:20.031813  3394 net.cpp:329] conv2 <- norm1
I0909 13:45:20.031821  3394 net.cpp:290] conv2 -> conv2
I0909 13:45:20.040875  3394 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:45:20.040892  3394 net.cpp:125] conv2 needs backward computation.
I0909 13:45:20.040909  3394 net.cpp:66] Creating Layer relu2
I0909 13:45:20.040915  3394 net.cpp:329] relu2 <- conv2
I0909 13:45:20.040922  3394 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:45:20.040930  3394 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:45:20.040936  3394 net.cpp:125] relu2 needs backward computation.
I0909 13:45:20.040951  3394 net.cpp:66] Creating Layer pool2
I0909 13:45:20.040961  3394 net.cpp:329] pool2 <- conv2
I0909 13:45:20.040968  3394 net.cpp:290] pool2 -> pool2
I0909 13:45:20.040977  3394 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:45:20.040983  3394 net.cpp:125] pool2 needs backward computation.
I0909 13:45:20.040990  3394 net.cpp:66] Creating Layer fc7
I0909 13:45:20.040997  3394 net.cpp:329] fc7 <- pool2
I0909 13:45:20.041004  3394 net.cpp:290] fc7 -> fc7
I0909 13:45:20.681927  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.681983  3394 net.cpp:125] fc7 needs backward computation.
I0909 13:45:20.681999  3394 net.cpp:66] Creating Layer relu7
I0909 13:45:20.682008  3394 net.cpp:329] relu7 <- fc7
I0909 13:45:20.682016  3394 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:45:20.682028  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.682034  3394 net.cpp:125] relu7 needs backward computation.
I0909 13:45:20.682042  3394 net.cpp:66] Creating Layer drop7
I0909 13:45:20.682049  3394 net.cpp:329] drop7 <- fc7
I0909 13:45:20.682056  3394 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:45:20.682068  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.682075  3394 net.cpp:125] drop7 needs backward computation.
I0909 13:45:20.682085  3394 net.cpp:66] Creating Layer fc8
I0909 13:45:20.682091  3394 net.cpp:329] fc8 <- fc7
I0909 13:45:20.682101  3394 net.cpp:290] fc8 -> fc8
I0909 13:45:20.689877  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.689889  3394 net.cpp:125] fc8 needs backward computation.
I0909 13:45:20.689898  3394 net.cpp:66] Creating Layer relu8
I0909 13:45:20.689903  3394 net.cpp:329] relu8 <- fc8
I0909 13:45:20.689913  3394 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:45:20.689920  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.689926  3394 net.cpp:125] relu8 needs backward computation.
I0909 13:45:20.689934  3394 net.cpp:66] Creating Layer drop8
I0909 13:45:20.689940  3394 net.cpp:329] drop8 <- fc8
I0909 13:45:20.689947  3394 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:45:20.689954  3394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:45:20.689961  3394 net.cpp:125] drop8 needs backward computation.
I0909 13:45:20.689970  3394 net.cpp:66] Creating Layer fc9
I0909 13:45:20.689975  3394 net.cpp:329] fc9 <- fc8
I0909 13:45:20.689985  3394 net.cpp:290] fc9 -> fc9
I0909 13:45:20.690357  3394 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:45:20.690371  3394 net.cpp:125] fc9 needs backward computation.
I0909 13:45:20.690379  3394 net.cpp:66] Creating Layer fc10
I0909 13:45:20.690387  3394 net.cpp:329] fc10 <- fc9
I0909 13:45:20.690393  3394 net.cpp:290] fc10 -> fc10
I0909 13:45:20.690407  3394 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:45:20.690417  3394 net.cpp:125] fc10 needs backward computation.
I0909 13:45:20.690423  3394 net.cpp:66] Creating Layer prob
I0909 13:45:20.690429  3394 net.cpp:329] prob <- fc10
I0909 13:45:20.690438  3394 net.cpp:290] prob -> prob
I0909 13:45:20.690448  3394 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:45:20.690455  3394 net.cpp:125] prob needs backward computation.
I0909 13:45:20.690461  3394 net.cpp:156] This network produces output prob
I0909 13:45:20.690472  3394 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:45:20.690481  3394 net.cpp:167] Network initialization done.
I0909 13:45:20.690487  3394 net.cpp:168] Memory required for data: 6183480
Classifying 83 inputs.
Done in 53.15 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:46:16.035302  3403 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:46:16.035452  3403 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:46:16.035464  3403 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:46:16.035611  3403 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:46:16.035676  3403 net.cpp:292] Input 0 -> data
I0909 13:46:16.035703  3403 net.cpp:66] Creating Layer conv1
I0909 13:46:16.035712  3403 net.cpp:329] conv1 <- data
I0909 13:46:16.035720  3403 net.cpp:290] conv1 -> conv1
I0909 13:46:16.037082  3403 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:46:16.037101  3403 net.cpp:125] conv1 needs backward computation.
I0909 13:46:16.037111  3403 net.cpp:66] Creating Layer relu1
I0909 13:46:16.037118  3403 net.cpp:329] relu1 <- conv1
I0909 13:46:16.037125  3403 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:46:16.037135  3403 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:46:16.037142  3403 net.cpp:125] relu1 needs backward computation.
I0909 13:46:16.037149  3403 net.cpp:66] Creating Layer pool1
I0909 13:46:16.037155  3403 net.cpp:329] pool1 <- conv1
I0909 13:46:16.037163  3403 net.cpp:290] pool1 -> pool1
I0909 13:46:16.037175  3403 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:46:16.037181  3403 net.cpp:125] pool1 needs backward computation.
I0909 13:46:16.037194  3403 net.cpp:66] Creating Layer norm1
I0909 13:46:16.037200  3403 net.cpp:329] norm1 <- pool1
I0909 13:46:16.037207  3403 net.cpp:290] norm1 -> norm1
I0909 13:46:16.037219  3403 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:46:16.037225  3403 net.cpp:125] norm1 needs backward computation.
I0909 13:46:16.037233  3403 net.cpp:66] Creating Layer conv2
I0909 13:46:16.037240  3403 net.cpp:329] conv2 <- norm1
I0909 13:46:16.037247  3403 net.cpp:290] conv2 -> conv2
I0909 13:46:16.046514  3403 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:46:16.046541  3403 net.cpp:125] conv2 needs backward computation.
I0909 13:46:16.046550  3403 net.cpp:66] Creating Layer relu2
I0909 13:46:16.046557  3403 net.cpp:329] relu2 <- conv2
I0909 13:46:16.046566  3403 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:46:16.046574  3403 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:46:16.046581  3403 net.cpp:125] relu2 needs backward computation.
I0909 13:46:16.046588  3403 net.cpp:66] Creating Layer pool2
I0909 13:46:16.046594  3403 net.cpp:329] pool2 <- conv2
I0909 13:46:16.046602  3403 net.cpp:290] pool2 -> pool2
I0909 13:46:16.046610  3403 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:46:16.046617  3403 net.cpp:125] pool2 needs backward computation.
I0909 13:46:16.046625  3403 net.cpp:66] Creating Layer fc7
I0909 13:46:16.046632  3403 net.cpp:329] fc7 <- pool2
I0909 13:46:16.046639  3403 net.cpp:290] fc7 -> fc7
I0909 13:46:16.695905  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.695953  3403 net.cpp:125] fc7 needs backward computation.
I0909 13:46:16.695967  3403 net.cpp:66] Creating Layer relu7
I0909 13:46:16.695976  3403 net.cpp:329] relu7 <- fc7
I0909 13:46:16.695991  3403 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:46:16.696001  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.696007  3403 net.cpp:125] relu7 needs backward computation.
I0909 13:46:16.696015  3403 net.cpp:66] Creating Layer drop7
I0909 13:46:16.696022  3403 net.cpp:329] drop7 <- fc7
I0909 13:46:16.696029  3403 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:46:16.696040  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.696046  3403 net.cpp:125] drop7 needs backward computation.
I0909 13:46:16.696055  3403 net.cpp:66] Creating Layer fc8
I0909 13:46:16.696061  3403 net.cpp:329] fc8 <- fc7
I0909 13:46:16.696070  3403 net.cpp:290] fc8 -> fc8
I0909 13:46:16.703609  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.703624  3403 net.cpp:125] fc8 needs backward computation.
I0909 13:46:16.703630  3403 net.cpp:66] Creating Layer relu8
I0909 13:46:16.703636  3403 net.cpp:329] relu8 <- fc8
I0909 13:46:16.703644  3403 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:46:16.703652  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.703660  3403 net.cpp:125] relu8 needs backward computation.
I0909 13:46:16.703666  3403 net.cpp:66] Creating Layer drop8
I0909 13:46:16.703672  3403 net.cpp:329] drop8 <- fc8
I0909 13:46:16.703678  3403 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:46:16.703686  3403 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:16.703692  3403 net.cpp:125] drop8 needs backward computation.
I0909 13:46:16.703701  3403 net.cpp:66] Creating Layer fc9
I0909 13:46:16.703706  3403 net.cpp:329] fc9 <- fc8
I0909 13:46:16.703714  3403 net.cpp:290] fc9 -> fc9
I0909 13:46:16.704077  3403 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:46:16.704089  3403 net.cpp:125] fc9 needs backward computation.
I0909 13:46:16.704097  3403 net.cpp:66] Creating Layer fc10
I0909 13:46:16.704104  3403 net.cpp:329] fc10 <- fc9
I0909 13:46:16.704112  3403 net.cpp:290] fc10 -> fc10
I0909 13:46:16.704124  3403 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:46:16.704133  3403 net.cpp:125] fc10 needs backward computation.
I0909 13:46:16.704140  3403 net.cpp:66] Creating Layer prob
I0909 13:46:16.704146  3403 net.cpp:329] prob <- fc10
I0909 13:46:16.704154  3403 net.cpp:290] prob -> prob
I0909 13:46:16.704164  3403 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:46:16.704174  3403 net.cpp:125] prob needs backward computation.
I0909 13:46:16.704187  3403 net.cpp:156] This network produces output prob
I0909 13:46:16.704200  3403 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:46:16.704207  3403 net.cpp:167] Network initialization done.
I0909 13:46:16.704213  3403 net.cpp:168] Memory required for data: 6183480
Classifying 49 inputs.
Done in 31.68 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:46:50.227694  3412 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:46:50.227854  3412 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:46:50.227864  3412 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:46:50.228009  3412 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:46:50.228072  3412 net.cpp:292] Input 0 -> data
I0909 13:46:50.228099  3412 net.cpp:66] Creating Layer conv1
I0909 13:46:50.228106  3412 net.cpp:329] conv1 <- data
I0909 13:46:50.228116  3412 net.cpp:290] conv1 -> conv1
I0909 13:46:50.229435  3412 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:46:50.229454  3412 net.cpp:125] conv1 needs backward computation.
I0909 13:46:50.229465  3412 net.cpp:66] Creating Layer relu1
I0909 13:46:50.229470  3412 net.cpp:329] relu1 <- conv1
I0909 13:46:50.229478  3412 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:46:50.229487  3412 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:46:50.229493  3412 net.cpp:125] relu1 needs backward computation.
I0909 13:46:50.229501  3412 net.cpp:66] Creating Layer pool1
I0909 13:46:50.229506  3412 net.cpp:329] pool1 <- conv1
I0909 13:46:50.229527  3412 net.cpp:290] pool1 -> pool1
I0909 13:46:50.229549  3412 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:46:50.229557  3412 net.cpp:125] pool1 needs backward computation.
I0909 13:46:50.229564  3412 net.cpp:66] Creating Layer norm1
I0909 13:46:50.229570  3412 net.cpp:329] norm1 <- pool1
I0909 13:46:50.229580  3412 net.cpp:290] norm1 -> norm1
I0909 13:46:50.229599  3412 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:46:50.229605  3412 net.cpp:125] norm1 needs backward computation.
I0909 13:46:50.229614  3412 net.cpp:66] Creating Layer conv2
I0909 13:46:50.229619  3412 net.cpp:329] conv2 <- norm1
I0909 13:46:50.229626  3412 net.cpp:290] conv2 -> conv2
I0909 13:46:50.238646  3412 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:46:50.238669  3412 net.cpp:125] conv2 needs backward computation.
I0909 13:46:50.238678  3412 net.cpp:66] Creating Layer relu2
I0909 13:46:50.238685  3412 net.cpp:329] relu2 <- conv2
I0909 13:46:50.238693  3412 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:46:50.238701  3412 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:46:50.238708  3412 net.cpp:125] relu2 needs backward computation.
I0909 13:46:50.238714  3412 net.cpp:66] Creating Layer pool2
I0909 13:46:50.238720  3412 net.cpp:329] pool2 <- conv2
I0909 13:46:50.238729  3412 net.cpp:290] pool2 -> pool2
I0909 13:46:50.238736  3412 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:46:50.238744  3412 net.cpp:125] pool2 needs backward computation.
I0909 13:46:50.238751  3412 net.cpp:66] Creating Layer fc7
I0909 13:46:50.238757  3412 net.cpp:329] fc7 <- pool2
I0909 13:46:50.238765  3412 net.cpp:290] fc7 -> fc7
I0909 13:46:50.878217  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.878273  3412 net.cpp:125] fc7 needs backward computation.
I0909 13:46:50.878288  3412 net.cpp:66] Creating Layer relu7
I0909 13:46:50.878296  3412 net.cpp:329] relu7 <- fc7
I0909 13:46:50.878304  3412 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:46:50.878314  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.878329  3412 net.cpp:125] relu7 needs backward computation.
I0909 13:46:50.878336  3412 net.cpp:66] Creating Layer drop7
I0909 13:46:50.878342  3412 net.cpp:329] drop7 <- fc7
I0909 13:46:50.878350  3412 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:46:50.878362  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.878368  3412 net.cpp:125] drop7 needs backward computation.
I0909 13:46:50.878377  3412 net.cpp:66] Creating Layer fc8
I0909 13:46:50.878383  3412 net.cpp:329] fc8 <- fc7
I0909 13:46:50.878392  3412 net.cpp:290] fc8 -> fc8
I0909 13:46:50.886075  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.886096  3412 net.cpp:125] fc8 needs backward computation.
I0909 13:46:50.886104  3412 net.cpp:66] Creating Layer relu8
I0909 13:46:50.886111  3412 net.cpp:329] relu8 <- fc8
I0909 13:46:50.886118  3412 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:46:50.886128  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.886134  3412 net.cpp:125] relu8 needs backward computation.
I0909 13:46:50.886142  3412 net.cpp:66] Creating Layer drop8
I0909 13:46:50.886148  3412 net.cpp:329] drop8 <- fc8
I0909 13:46:50.886155  3412 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:46:50.886173  3412 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:46:50.886180  3412 net.cpp:125] drop8 needs backward computation.
I0909 13:46:50.886188  3412 net.cpp:66] Creating Layer fc9
I0909 13:46:50.886194  3412 net.cpp:329] fc9 <- fc8
I0909 13:46:50.886204  3412 net.cpp:290] fc9 -> fc9
I0909 13:46:50.886596  3412 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:46:50.886610  3412 net.cpp:125] fc9 needs backward computation.
I0909 13:46:50.886618  3412 net.cpp:66] Creating Layer fc10
I0909 13:46:50.886625  3412 net.cpp:329] fc10 <- fc9
I0909 13:46:50.886632  3412 net.cpp:290] fc10 -> fc10
I0909 13:46:50.886646  3412 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:46:50.886654  3412 net.cpp:125] fc10 needs backward computation.
I0909 13:46:50.886662  3412 net.cpp:66] Creating Layer prob
I0909 13:46:50.886668  3412 net.cpp:329] prob <- fc10
I0909 13:46:50.886675  3412 net.cpp:290] prob -> prob
I0909 13:46:50.886685  3412 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:46:50.886694  3412 net.cpp:125] prob needs backward computation.
I0909 13:46:50.886699  3412 net.cpp:156] This network produces output prob
I0909 13:46:50.886711  3412 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:46:50.886720  3412 net.cpp:167] Network initialization done.
I0909 13:46:50.886726  3412 net.cpp:168] Memory required for data: 6183480
Classifying 155 inputs.
Done in 100.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:48:34.403203  3461 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:48:34.403370  3461 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:48:34.403380  3461 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:48:34.403530  3461 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:48:34.403594  3461 net.cpp:292] Input 0 -> data
I0909 13:48:34.403621  3461 net.cpp:66] Creating Layer conv1
I0909 13:48:34.403630  3461 net.cpp:329] conv1 <- data
I0909 13:48:34.403637  3461 net.cpp:290] conv1 -> conv1
I0909 13:48:34.405002  3461 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:48:34.405022  3461 net.cpp:125] conv1 needs backward computation.
I0909 13:48:34.405032  3461 net.cpp:66] Creating Layer relu1
I0909 13:48:34.405038  3461 net.cpp:329] relu1 <- conv1
I0909 13:48:34.405046  3461 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:48:34.405055  3461 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:48:34.405062  3461 net.cpp:125] relu1 needs backward computation.
I0909 13:48:34.405071  3461 net.cpp:66] Creating Layer pool1
I0909 13:48:34.405076  3461 net.cpp:329] pool1 <- conv1
I0909 13:48:34.405083  3461 net.cpp:290] pool1 -> pool1
I0909 13:48:34.405096  3461 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:48:34.405102  3461 net.cpp:125] pool1 needs backward computation.
I0909 13:48:34.405109  3461 net.cpp:66] Creating Layer norm1
I0909 13:48:34.405115  3461 net.cpp:329] norm1 <- pool1
I0909 13:48:34.405122  3461 net.cpp:290] norm1 -> norm1
I0909 13:48:34.405133  3461 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:48:34.405139  3461 net.cpp:125] norm1 needs backward computation.
I0909 13:48:34.405148  3461 net.cpp:66] Creating Layer conv2
I0909 13:48:34.405154  3461 net.cpp:329] conv2 <- norm1
I0909 13:48:34.405161  3461 net.cpp:290] conv2 -> conv2
I0909 13:48:34.414353  3461 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:48:34.414371  3461 net.cpp:125] conv2 needs backward computation.
I0909 13:48:34.414387  3461 net.cpp:66] Creating Layer relu2
I0909 13:48:34.414394  3461 net.cpp:329] relu2 <- conv2
I0909 13:48:34.414402  3461 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:48:34.414410  3461 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:48:34.414417  3461 net.cpp:125] relu2 needs backward computation.
I0909 13:48:34.414432  3461 net.cpp:66] Creating Layer pool2
I0909 13:48:34.414438  3461 net.cpp:329] pool2 <- conv2
I0909 13:48:34.414444  3461 net.cpp:290] pool2 -> pool2
I0909 13:48:34.414453  3461 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:48:34.414459  3461 net.cpp:125] pool2 needs backward computation.
I0909 13:48:34.414468  3461 net.cpp:66] Creating Layer fc7
I0909 13:48:34.414474  3461 net.cpp:329] fc7 <- pool2
I0909 13:48:34.414481  3461 net.cpp:290] fc7 -> fc7
I0909 13:48:35.050860  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.050915  3461 net.cpp:125] fc7 needs backward computation.
I0909 13:48:35.050928  3461 net.cpp:66] Creating Layer relu7
I0909 13:48:35.050937  3461 net.cpp:329] relu7 <- fc7
I0909 13:48:35.050946  3461 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:48:35.050957  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.050963  3461 net.cpp:125] relu7 needs backward computation.
I0909 13:48:35.050972  3461 net.cpp:66] Creating Layer drop7
I0909 13:48:35.050992  3461 net.cpp:329] drop7 <- fc7
I0909 13:48:35.051002  3461 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:48:35.051013  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.051020  3461 net.cpp:125] drop7 needs backward computation.
I0909 13:48:35.051029  3461 net.cpp:66] Creating Layer fc8
I0909 13:48:35.051035  3461 net.cpp:329] fc8 <- fc7
I0909 13:48:35.051045  3461 net.cpp:290] fc8 -> fc8
I0909 13:48:35.058789  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.058801  3461 net.cpp:125] fc8 needs backward computation.
I0909 13:48:35.058809  3461 net.cpp:66] Creating Layer relu8
I0909 13:48:35.058815  3461 net.cpp:329] relu8 <- fc8
I0909 13:48:35.058823  3461 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:48:35.058831  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.058837  3461 net.cpp:125] relu8 needs backward computation.
I0909 13:48:35.058845  3461 net.cpp:66] Creating Layer drop8
I0909 13:48:35.058851  3461 net.cpp:329] drop8 <- fc8
I0909 13:48:35.058857  3461 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:48:35.058864  3461 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:35.058871  3461 net.cpp:125] drop8 needs backward computation.
I0909 13:48:35.058878  3461 net.cpp:66] Creating Layer fc9
I0909 13:48:35.058884  3461 net.cpp:329] fc9 <- fc8
I0909 13:48:35.058893  3461 net.cpp:290] fc9 -> fc9
I0909 13:48:35.059254  3461 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:48:35.059268  3461 net.cpp:125] fc9 needs backward computation.
I0909 13:48:35.059278  3461 net.cpp:66] Creating Layer fc10
I0909 13:48:35.059283  3461 net.cpp:329] fc10 <- fc9
I0909 13:48:35.059291  3461 net.cpp:290] fc10 -> fc10
I0909 13:48:35.059304  3461 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:48:35.059314  3461 net.cpp:125] fc10 needs backward computation.
I0909 13:48:35.059320  3461 net.cpp:66] Creating Layer prob
I0909 13:48:35.059326  3461 net.cpp:329] prob <- fc10
I0909 13:48:35.059334  3461 net.cpp:290] prob -> prob
I0909 13:48:35.059345  3461 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:48:35.059351  3461 net.cpp:125] prob needs backward computation.
I0909 13:48:35.059356  3461 net.cpp:156] This network produces output prob
I0909 13:48:35.059368  3461 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:48:35.059376  3461 net.cpp:167] Network initialization done.
I0909 13:48:35.059381  3461 net.cpp:168] Memory required for data: 6183480
Classifying 23 inputs.
Done in 14.38 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:48:50.654388  3608 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:48:50.654538  3608 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:48:50.654548  3608 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:48:50.654697  3608 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:48:50.654762  3608 net.cpp:292] Input 0 -> data
I0909 13:48:50.654789  3608 net.cpp:66] Creating Layer conv1
I0909 13:48:50.654798  3608 net.cpp:329] conv1 <- data
I0909 13:48:50.654808  3608 net.cpp:290] conv1 -> conv1
I0909 13:48:50.656170  3608 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:48:50.656190  3608 net.cpp:125] conv1 needs backward computation.
I0909 13:48:50.656200  3608 net.cpp:66] Creating Layer relu1
I0909 13:48:50.656208  3608 net.cpp:329] relu1 <- conv1
I0909 13:48:50.656215  3608 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:48:50.656225  3608 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:48:50.656232  3608 net.cpp:125] relu1 needs backward computation.
I0909 13:48:50.656240  3608 net.cpp:66] Creating Layer pool1
I0909 13:48:50.656246  3608 net.cpp:329] pool1 <- conv1
I0909 13:48:50.656255  3608 net.cpp:290] pool1 -> pool1
I0909 13:48:50.656266  3608 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:48:50.656273  3608 net.cpp:125] pool1 needs backward computation.
I0909 13:48:50.656281  3608 net.cpp:66] Creating Layer norm1
I0909 13:48:50.656287  3608 net.cpp:329] norm1 <- pool1
I0909 13:48:50.656296  3608 net.cpp:290] norm1 -> norm1
I0909 13:48:50.656306  3608 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:48:50.656313  3608 net.cpp:125] norm1 needs backward computation.
I0909 13:48:50.656322  3608 net.cpp:66] Creating Layer conv2
I0909 13:48:50.656328  3608 net.cpp:329] conv2 <- norm1
I0909 13:48:50.656337  3608 net.cpp:290] conv2 -> conv2
I0909 13:48:50.665421  3608 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:48:50.665438  3608 net.cpp:125] conv2 needs backward computation.
I0909 13:48:50.665454  3608 net.cpp:66] Creating Layer relu2
I0909 13:48:50.665460  3608 net.cpp:329] relu2 <- conv2
I0909 13:48:50.665468  3608 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:48:50.665477  3608 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:48:50.665483  3608 net.cpp:125] relu2 needs backward computation.
I0909 13:48:50.665503  3608 net.cpp:66] Creating Layer pool2
I0909 13:48:50.665513  3608 net.cpp:329] pool2 <- conv2
I0909 13:48:50.665523  3608 net.cpp:290] pool2 -> pool2
I0909 13:48:50.665531  3608 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:48:50.665539  3608 net.cpp:125] pool2 needs backward computation.
I0909 13:48:50.665546  3608 net.cpp:66] Creating Layer fc7
I0909 13:48:50.665552  3608 net.cpp:329] fc7 <- pool2
I0909 13:48:50.665560  3608 net.cpp:290] fc7 -> fc7
I0909 13:48:51.306776  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.306828  3608 net.cpp:125] fc7 needs backward computation.
I0909 13:48:51.306843  3608 net.cpp:66] Creating Layer relu7
I0909 13:48:51.306850  3608 net.cpp:329] relu7 <- fc7
I0909 13:48:51.306859  3608 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:48:51.306870  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.306884  3608 net.cpp:125] relu7 needs backward computation.
I0909 13:48:51.306892  3608 net.cpp:66] Creating Layer drop7
I0909 13:48:51.306898  3608 net.cpp:329] drop7 <- fc7
I0909 13:48:51.306906  3608 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:48:51.306918  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.306926  3608 net.cpp:125] drop7 needs backward computation.
I0909 13:48:51.306933  3608 net.cpp:66] Creating Layer fc8
I0909 13:48:51.306941  3608 net.cpp:329] fc8 <- fc7
I0909 13:48:51.306949  3608 net.cpp:290] fc8 -> fc8
I0909 13:48:51.314569  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.314582  3608 net.cpp:125] fc8 needs backward computation.
I0909 13:48:51.314590  3608 net.cpp:66] Creating Layer relu8
I0909 13:48:51.314597  3608 net.cpp:329] relu8 <- fc8
I0909 13:48:51.314604  3608 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:48:51.314615  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.314621  3608 net.cpp:125] relu8 needs backward computation.
I0909 13:48:51.314628  3608 net.cpp:66] Creating Layer drop8
I0909 13:48:51.314635  3608 net.cpp:329] drop8 <- fc8
I0909 13:48:51.314641  3608 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:48:51.314649  3608 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:48:51.314656  3608 net.cpp:125] drop8 needs backward computation.
I0909 13:48:51.314664  3608 net.cpp:66] Creating Layer fc9
I0909 13:48:51.314671  3608 net.cpp:329] fc9 <- fc8
I0909 13:48:51.314681  3608 net.cpp:290] fc9 -> fc9
I0909 13:48:51.315053  3608 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:48:51.315067  3608 net.cpp:125] fc9 needs backward computation.
I0909 13:48:51.315076  3608 net.cpp:66] Creating Layer fc10
I0909 13:48:51.315083  3608 net.cpp:329] fc10 <- fc9
I0909 13:48:51.315090  3608 net.cpp:290] fc10 -> fc10
I0909 13:48:51.315105  3608 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:48:51.315114  3608 net.cpp:125] fc10 needs backward computation.
I0909 13:48:51.315121  3608 net.cpp:66] Creating Layer prob
I0909 13:48:51.315129  3608 net.cpp:329] prob <- fc10
I0909 13:48:51.315135  3608 net.cpp:290] prob -> prob
I0909 13:48:51.315145  3608 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:48:51.315155  3608 net.cpp:125] prob needs backward computation.
I0909 13:48:51.315160  3608 net.cpp:156] This network produces output prob
I0909 13:48:51.315172  3608 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:48:51.315181  3608 net.cpp:167] Network initialization done.
I0909 13:48:51.315186  3608 net.cpp:168] Memory required for data: 6183480
Classifying 94 inputs.
Done in 59.03 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:49:54.562548  3657 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:49:54.562710  3657 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:49:54.562719  3657 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:49:54.562878  3657 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:49:54.562932  3657 net.cpp:292] Input 0 -> data
I0909 13:49:54.562958  3657 net.cpp:66] Creating Layer conv1
I0909 13:49:54.562966  3657 net.cpp:329] conv1 <- data
I0909 13:49:54.562975  3657 net.cpp:290] conv1 -> conv1
I0909 13:49:54.564322  3657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:49:54.564342  3657 net.cpp:125] conv1 needs backward computation.
I0909 13:49:54.564350  3657 net.cpp:66] Creating Layer relu1
I0909 13:49:54.564357  3657 net.cpp:329] relu1 <- conv1
I0909 13:49:54.564364  3657 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:49:54.564374  3657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:49:54.564380  3657 net.cpp:125] relu1 needs backward computation.
I0909 13:49:54.564388  3657 net.cpp:66] Creating Layer pool1
I0909 13:49:54.564394  3657 net.cpp:329] pool1 <- conv1
I0909 13:49:54.564402  3657 net.cpp:290] pool1 -> pool1
I0909 13:49:54.564414  3657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:49:54.564424  3657 net.cpp:125] pool1 needs backward computation.
I0909 13:49:54.564432  3657 net.cpp:66] Creating Layer norm1
I0909 13:49:54.564438  3657 net.cpp:329] norm1 <- pool1
I0909 13:49:54.564446  3657 net.cpp:290] norm1 -> norm1
I0909 13:49:54.564457  3657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:49:54.564463  3657 net.cpp:125] norm1 needs backward computation.
I0909 13:49:54.564471  3657 net.cpp:66] Creating Layer conv2
I0909 13:49:54.564477  3657 net.cpp:329] conv2 <- norm1
I0909 13:49:54.564486  3657 net.cpp:290] conv2 -> conv2
I0909 13:49:54.573395  3657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:49:54.573413  3657 net.cpp:125] conv2 needs backward computation.
I0909 13:49:54.573428  3657 net.cpp:66] Creating Layer relu2
I0909 13:49:54.573436  3657 net.cpp:329] relu2 <- conv2
I0909 13:49:54.573442  3657 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:49:54.573451  3657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:49:54.573457  3657 net.cpp:125] relu2 needs backward computation.
I0909 13:49:54.573472  3657 net.cpp:66] Creating Layer pool2
I0909 13:49:54.573477  3657 net.cpp:329] pool2 <- conv2
I0909 13:49:54.573484  3657 net.cpp:290] pool2 -> pool2
I0909 13:49:54.573500  3657 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:49:54.573506  3657 net.cpp:125] pool2 needs backward computation.
I0909 13:49:54.573518  3657 net.cpp:66] Creating Layer fc7
I0909 13:49:54.573525  3657 net.cpp:329] fc7 <- pool2
I0909 13:49:54.573534  3657 net.cpp:290] fc7 -> fc7
I0909 13:49:55.214061  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.214118  3657 net.cpp:125] fc7 needs backward computation.
I0909 13:49:55.214133  3657 net.cpp:66] Creating Layer relu7
I0909 13:49:55.214140  3657 net.cpp:329] relu7 <- fc7
I0909 13:49:55.214149  3657 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:49:55.214160  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.214167  3657 net.cpp:125] relu7 needs backward computation.
I0909 13:49:55.214182  3657 net.cpp:66] Creating Layer drop7
I0909 13:49:55.214190  3657 net.cpp:329] drop7 <- fc7
I0909 13:49:55.214197  3657 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:49:55.214210  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.214215  3657 net.cpp:125] drop7 needs backward computation.
I0909 13:49:55.214226  3657 net.cpp:66] Creating Layer fc8
I0909 13:49:55.214231  3657 net.cpp:329] fc8 <- fc7
I0909 13:49:55.214241  3657 net.cpp:290] fc8 -> fc8
I0909 13:49:55.222045  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.222059  3657 net.cpp:125] fc8 needs backward computation.
I0909 13:49:55.222067  3657 net.cpp:66] Creating Layer relu8
I0909 13:49:55.222074  3657 net.cpp:329] relu8 <- fc8
I0909 13:49:55.222081  3657 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:49:55.222090  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.222096  3657 net.cpp:125] relu8 needs backward computation.
I0909 13:49:55.222105  3657 net.cpp:66] Creating Layer drop8
I0909 13:49:55.222110  3657 net.cpp:329] drop8 <- fc8
I0909 13:49:55.222117  3657 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:49:55.222126  3657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:49:55.222131  3657 net.cpp:125] drop8 needs backward computation.
I0909 13:49:55.222139  3657 net.cpp:66] Creating Layer fc9
I0909 13:49:55.222146  3657 net.cpp:329] fc9 <- fc8
I0909 13:49:55.222154  3657 net.cpp:290] fc9 -> fc9
I0909 13:49:55.222529  3657 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:49:55.222543  3657 net.cpp:125] fc9 needs backward computation.
I0909 13:49:55.222551  3657 net.cpp:66] Creating Layer fc10
I0909 13:49:55.222558  3657 net.cpp:329] fc10 <- fc9
I0909 13:49:55.222566  3657 net.cpp:290] fc10 -> fc10
I0909 13:49:55.222579  3657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:49:55.222589  3657 net.cpp:125] fc10 needs backward computation.
I0909 13:49:55.222596  3657 net.cpp:66] Creating Layer prob
I0909 13:49:55.222602  3657 net.cpp:329] prob <- fc10
I0909 13:49:55.222609  3657 net.cpp:290] prob -> prob
I0909 13:49:55.222620  3657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:49:55.222640  3657 net.cpp:125] prob needs backward computation.
I0909 13:49:55.222646  3657 net.cpp:156] This network produces output prob
I0909 13:49:55.222658  3657 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:49:55.222667  3657 net.cpp:167] Network initialization done.
I0909 13:49:55.222673  3657 net.cpp:168] Memory required for data: 6183480
Classifying 214 inputs.
Done in 138.08 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:52:18.163609  3723 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:52:18.163756  3723 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:52:18.163766  3723 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:52:18.163914  3723 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:52:18.163979  3723 net.cpp:292] Input 0 -> data
I0909 13:52:18.164005  3723 net.cpp:66] Creating Layer conv1
I0909 13:52:18.164013  3723 net.cpp:329] conv1 <- data
I0909 13:52:18.164021  3723 net.cpp:290] conv1 -> conv1
I0909 13:52:18.165387  3723 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:52:18.165406  3723 net.cpp:125] conv1 needs backward computation.
I0909 13:52:18.165416  3723 net.cpp:66] Creating Layer relu1
I0909 13:52:18.165423  3723 net.cpp:329] relu1 <- conv1
I0909 13:52:18.165431  3723 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:52:18.165441  3723 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:52:18.165446  3723 net.cpp:125] relu1 needs backward computation.
I0909 13:52:18.165454  3723 net.cpp:66] Creating Layer pool1
I0909 13:52:18.165460  3723 net.cpp:329] pool1 <- conv1
I0909 13:52:18.165467  3723 net.cpp:290] pool1 -> pool1
I0909 13:52:18.165479  3723 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:52:18.165487  3723 net.cpp:125] pool1 needs backward computation.
I0909 13:52:18.165494  3723 net.cpp:66] Creating Layer norm1
I0909 13:52:18.165500  3723 net.cpp:329] norm1 <- pool1
I0909 13:52:18.165508  3723 net.cpp:290] norm1 -> norm1
I0909 13:52:18.165534  3723 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:52:18.165541  3723 net.cpp:125] norm1 needs backward computation.
I0909 13:52:18.165549  3723 net.cpp:66] Creating Layer conv2
I0909 13:52:18.165556  3723 net.cpp:329] conv2 <- norm1
I0909 13:52:18.165565  3723 net.cpp:290] conv2 -> conv2
I0909 13:52:18.174705  3723 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:52:18.174722  3723 net.cpp:125] conv2 needs backward computation.
I0909 13:52:18.174738  3723 net.cpp:66] Creating Layer relu2
I0909 13:52:18.174744  3723 net.cpp:329] relu2 <- conv2
I0909 13:52:18.174752  3723 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:52:18.174759  3723 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:52:18.174767  3723 net.cpp:125] relu2 needs backward computation.
I0909 13:52:18.174780  3723 net.cpp:66] Creating Layer pool2
I0909 13:52:18.174787  3723 net.cpp:329] pool2 <- conv2
I0909 13:52:18.174793  3723 net.cpp:290] pool2 -> pool2
I0909 13:52:18.174801  3723 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:52:18.174808  3723 net.cpp:125] pool2 needs backward computation.
I0909 13:52:18.174815  3723 net.cpp:66] Creating Layer fc7
I0909 13:52:18.174821  3723 net.cpp:329] fc7 <- pool2
I0909 13:52:18.174829  3723 net.cpp:290] fc7 -> fc7
I0909 13:52:18.815542  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.815593  3723 net.cpp:125] fc7 needs backward computation.
I0909 13:52:18.815608  3723 net.cpp:66] Creating Layer relu7
I0909 13:52:18.815616  3723 net.cpp:329] relu7 <- fc7
I0909 13:52:18.815625  3723 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:52:18.815635  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.815642  3723 net.cpp:125] relu7 needs backward computation.
I0909 13:52:18.815650  3723 net.cpp:66] Creating Layer drop7
I0909 13:52:18.815656  3723 net.cpp:329] drop7 <- fc7
I0909 13:52:18.815665  3723 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:52:18.815677  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.815685  3723 net.cpp:125] drop7 needs backward computation.
I0909 13:52:18.815693  3723 net.cpp:66] Creating Layer fc8
I0909 13:52:18.815700  3723 net.cpp:329] fc8 <- fc7
I0909 13:52:18.815711  3723 net.cpp:290] fc8 -> fc8
I0909 13:52:18.823750  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.823765  3723 net.cpp:125] fc8 needs backward computation.
I0909 13:52:18.823773  3723 net.cpp:66] Creating Layer relu8
I0909 13:52:18.823779  3723 net.cpp:329] relu8 <- fc8
I0909 13:52:18.823788  3723 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:52:18.823796  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.823803  3723 net.cpp:125] relu8 needs backward computation.
I0909 13:52:18.823812  3723 net.cpp:66] Creating Layer drop8
I0909 13:52:18.823827  3723 net.cpp:329] drop8 <- fc8
I0909 13:52:18.823835  3723 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:52:18.823843  3723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:52:18.823850  3723 net.cpp:125] drop8 needs backward computation.
I0909 13:52:18.823858  3723 net.cpp:66] Creating Layer fc9
I0909 13:52:18.823864  3723 net.cpp:329] fc9 <- fc8
I0909 13:52:18.823874  3723 net.cpp:290] fc9 -> fc9
I0909 13:52:18.824260  3723 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:52:18.824273  3723 net.cpp:125] fc9 needs backward computation.
I0909 13:52:18.824282  3723 net.cpp:66] Creating Layer fc10
I0909 13:52:18.824290  3723 net.cpp:329] fc10 <- fc9
I0909 13:52:18.824297  3723 net.cpp:290] fc10 -> fc10
I0909 13:52:18.824311  3723 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:52:18.824321  3723 net.cpp:125] fc10 needs backward computation.
I0909 13:52:18.824328  3723 net.cpp:66] Creating Layer prob
I0909 13:52:18.824336  3723 net.cpp:329] prob <- fc10
I0909 13:52:18.824344  3723 net.cpp:290] prob -> prob
I0909 13:52:18.824354  3723 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:52:18.824362  3723 net.cpp:125] prob needs backward computation.
I0909 13:52:18.824368  3723 net.cpp:156] This network produces output prob
I0909 13:52:18.824379  3723 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:52:18.824388  3723 net.cpp:167] Network initialization done.
I0909 13:52:18.824394  3723 net.cpp:168] Memory required for data: 6183480
Classifying 149 inputs.
Done in 96.27 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:54:00.460922  3736 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:54:00.461082  3736 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:54:00.461092  3736 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:54:00.461236  3736 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:54:00.461297  3736 net.cpp:292] Input 0 -> data
I0909 13:54:00.461324  3736 net.cpp:66] Creating Layer conv1
I0909 13:54:00.461333  3736 net.cpp:329] conv1 <- data
I0909 13:54:00.461340  3736 net.cpp:290] conv1 -> conv1
I0909 13:54:00.462703  3736 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:54:00.462723  3736 net.cpp:125] conv1 needs backward computation.
I0909 13:54:00.462740  3736 net.cpp:66] Creating Layer relu1
I0909 13:54:00.462748  3736 net.cpp:329] relu1 <- conv1
I0909 13:54:00.462755  3736 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:54:00.462764  3736 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:54:00.462771  3736 net.cpp:125] relu1 needs backward computation.
I0909 13:54:00.462779  3736 net.cpp:66] Creating Layer pool1
I0909 13:54:00.462785  3736 net.cpp:329] pool1 <- conv1
I0909 13:54:00.462793  3736 net.cpp:290] pool1 -> pool1
I0909 13:54:00.462805  3736 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:54:00.462811  3736 net.cpp:125] pool1 needs backward computation.
I0909 13:54:00.462820  3736 net.cpp:66] Creating Layer norm1
I0909 13:54:00.462826  3736 net.cpp:329] norm1 <- pool1
I0909 13:54:00.462833  3736 net.cpp:290] norm1 -> norm1
I0909 13:54:00.462843  3736 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:54:00.462851  3736 net.cpp:125] norm1 needs backward computation.
I0909 13:54:00.462858  3736 net.cpp:66] Creating Layer conv2
I0909 13:54:00.462865  3736 net.cpp:329] conv2 <- norm1
I0909 13:54:00.462873  3736 net.cpp:290] conv2 -> conv2
I0909 13:54:00.471873  3736 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:54:00.471890  3736 net.cpp:125] conv2 needs backward computation.
I0909 13:54:00.471906  3736 net.cpp:66] Creating Layer relu2
I0909 13:54:00.471912  3736 net.cpp:329] relu2 <- conv2
I0909 13:54:00.471920  3736 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:54:00.471928  3736 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:54:00.471935  3736 net.cpp:125] relu2 needs backward computation.
I0909 13:54:00.471948  3736 net.cpp:66] Creating Layer pool2
I0909 13:54:00.471954  3736 net.cpp:329] pool2 <- conv2
I0909 13:54:00.471961  3736 net.cpp:290] pool2 -> pool2
I0909 13:54:00.471969  3736 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:54:00.471976  3736 net.cpp:125] pool2 needs backward computation.
I0909 13:54:00.471983  3736 net.cpp:66] Creating Layer fc7
I0909 13:54:00.471989  3736 net.cpp:329] fc7 <- pool2
I0909 13:54:00.471997  3736 net.cpp:290] fc7 -> fc7
I0909 13:54:01.121228  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.121275  3736 net.cpp:125] fc7 needs backward computation.
I0909 13:54:01.121290  3736 net.cpp:66] Creating Layer relu7
I0909 13:54:01.121299  3736 net.cpp:329] relu7 <- fc7
I0909 13:54:01.121315  3736 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:54:01.121325  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.121330  3736 net.cpp:125] relu7 needs backward computation.
I0909 13:54:01.121346  3736 net.cpp:66] Creating Layer drop7
I0909 13:54:01.121352  3736 net.cpp:329] drop7 <- fc7
I0909 13:54:01.121361  3736 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:54:01.121373  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.121379  3736 net.cpp:125] drop7 needs backward computation.
I0909 13:54:01.121388  3736 net.cpp:66] Creating Layer fc8
I0909 13:54:01.121394  3736 net.cpp:329] fc8 <- fc7
I0909 13:54:01.121403  3736 net.cpp:290] fc8 -> fc8
I0909 13:54:01.129329  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.129345  3736 net.cpp:125] fc8 needs backward computation.
I0909 13:54:01.129354  3736 net.cpp:66] Creating Layer relu8
I0909 13:54:01.129360  3736 net.cpp:329] relu8 <- fc8
I0909 13:54:01.129369  3736 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:54:01.129376  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.129384  3736 net.cpp:125] relu8 needs backward computation.
I0909 13:54:01.129390  3736 net.cpp:66] Creating Layer drop8
I0909 13:54:01.129396  3736 net.cpp:329] drop8 <- fc8
I0909 13:54:01.129403  3736 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:54:01.129411  3736 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:01.129417  3736 net.cpp:125] drop8 needs backward computation.
I0909 13:54:01.129426  3736 net.cpp:66] Creating Layer fc9
I0909 13:54:01.129431  3736 net.cpp:329] fc9 <- fc8
I0909 13:54:01.129441  3736 net.cpp:290] fc9 -> fc9
I0909 13:54:01.129822  3736 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:54:01.129837  3736 net.cpp:125] fc9 needs backward computation.
I0909 13:54:01.129847  3736 net.cpp:66] Creating Layer fc10
I0909 13:54:01.129853  3736 net.cpp:329] fc10 <- fc9
I0909 13:54:01.129860  3736 net.cpp:290] fc10 -> fc10
I0909 13:54:01.129874  3736 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:54:01.129884  3736 net.cpp:125] fc10 needs backward computation.
I0909 13:54:01.129891  3736 net.cpp:66] Creating Layer prob
I0909 13:54:01.129897  3736 net.cpp:329] prob <- fc10
I0909 13:54:01.129905  3736 net.cpp:290] prob -> prob
I0909 13:54:01.129916  3736 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:54:01.129922  3736 net.cpp:125] prob needs backward computation.
I0909 13:54:01.129928  3736 net.cpp:156] This network produces output prob
I0909 13:54:01.129940  3736 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:54:01.129948  3736 net.cpp:167] Network initialization done.
I0909 13:54:01.129955  3736 net.cpp:168] Memory required for data: 6183480
Classifying 61 inputs.
Done in 37.72 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:54:42.514166  3745 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:54:42.514319  3745 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:54:42.514330  3745 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:54:42.514485  3745 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:54:42.514551  3745 net.cpp:292] Input 0 -> data
I0909 13:54:42.514580  3745 net.cpp:66] Creating Layer conv1
I0909 13:54:42.514587  3745 net.cpp:329] conv1 <- data
I0909 13:54:42.514596  3745 net.cpp:290] conv1 -> conv1
I0909 13:54:42.516048  3745 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:54:42.516074  3745 net.cpp:125] conv1 needs backward computation.
I0909 13:54:42.516085  3745 net.cpp:66] Creating Layer relu1
I0909 13:54:42.516093  3745 net.cpp:329] relu1 <- conv1
I0909 13:54:42.516100  3745 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:54:42.516110  3745 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:54:42.516118  3745 net.cpp:125] relu1 needs backward computation.
I0909 13:54:42.516124  3745 net.cpp:66] Creating Layer pool1
I0909 13:54:42.516131  3745 net.cpp:329] pool1 <- conv1
I0909 13:54:42.516139  3745 net.cpp:290] pool1 -> pool1
I0909 13:54:42.516151  3745 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:54:42.516157  3745 net.cpp:125] pool1 needs backward computation.
I0909 13:54:42.516165  3745 net.cpp:66] Creating Layer norm1
I0909 13:54:42.516172  3745 net.cpp:329] norm1 <- pool1
I0909 13:54:42.516180  3745 net.cpp:290] norm1 -> norm1
I0909 13:54:42.516191  3745 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:54:42.516196  3745 net.cpp:125] norm1 needs backward computation.
I0909 13:54:42.516206  3745 net.cpp:66] Creating Layer conv2
I0909 13:54:42.516211  3745 net.cpp:329] conv2 <- norm1
I0909 13:54:42.516219  3745 net.cpp:290] conv2 -> conv2
I0909 13:54:42.525660  3745 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:54:42.525714  3745 net.cpp:125] conv2 needs backward computation.
I0909 13:54:42.525727  3745 net.cpp:66] Creating Layer relu2
I0909 13:54:42.525734  3745 net.cpp:329] relu2 <- conv2
I0909 13:54:42.525743  3745 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:54:42.525754  3745 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:54:42.525779  3745 net.cpp:125] relu2 needs backward computation.
I0909 13:54:42.525789  3745 net.cpp:66] Creating Layer pool2
I0909 13:54:42.525794  3745 net.cpp:329] pool2 <- conv2
I0909 13:54:42.525810  3745 net.cpp:290] pool2 -> pool2
I0909 13:54:42.525820  3745 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:54:42.525827  3745 net.cpp:125] pool2 needs backward computation.
I0909 13:54:42.525843  3745 net.cpp:66] Creating Layer fc7
I0909 13:54:42.525849  3745 net.cpp:329] fc7 <- pool2
I0909 13:54:42.525857  3745 net.cpp:290] fc7 -> fc7
I0909 13:54:43.173434  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.173482  3745 net.cpp:125] fc7 needs backward computation.
I0909 13:54:43.173496  3745 net.cpp:66] Creating Layer relu7
I0909 13:54:43.173503  3745 net.cpp:329] relu7 <- fc7
I0909 13:54:43.173516  3745 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:54:43.173527  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.173534  3745 net.cpp:125] relu7 needs backward computation.
I0909 13:54:43.173542  3745 net.cpp:66] Creating Layer drop7
I0909 13:54:43.173555  3745 net.cpp:329] drop7 <- fc7
I0909 13:54:43.173563  3745 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:54:43.173574  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.173581  3745 net.cpp:125] drop7 needs backward computation.
I0909 13:54:43.173589  3745 net.cpp:66] Creating Layer fc8
I0909 13:54:43.173595  3745 net.cpp:329] fc8 <- fc7
I0909 13:54:43.173605  3745 net.cpp:290] fc8 -> fc8
I0909 13:54:43.181354  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.181368  3745 net.cpp:125] fc8 needs backward computation.
I0909 13:54:43.181376  3745 net.cpp:66] Creating Layer relu8
I0909 13:54:43.181383  3745 net.cpp:329] relu8 <- fc8
I0909 13:54:43.181391  3745 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:54:43.181399  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.181406  3745 net.cpp:125] relu8 needs backward computation.
I0909 13:54:43.181413  3745 net.cpp:66] Creating Layer drop8
I0909 13:54:43.181419  3745 net.cpp:329] drop8 <- fc8
I0909 13:54:43.181427  3745 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:54:43.181433  3745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:54:43.181439  3745 net.cpp:125] drop8 needs backward computation.
I0909 13:54:43.181447  3745 net.cpp:66] Creating Layer fc9
I0909 13:54:43.181453  3745 net.cpp:329] fc9 <- fc8
I0909 13:54:43.181463  3745 net.cpp:290] fc9 -> fc9
I0909 13:54:43.181849  3745 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:54:43.181864  3745 net.cpp:125] fc9 needs backward computation.
I0909 13:54:43.181872  3745 net.cpp:66] Creating Layer fc10
I0909 13:54:43.181879  3745 net.cpp:329] fc10 <- fc9
I0909 13:54:43.181885  3745 net.cpp:290] fc10 -> fc10
I0909 13:54:43.181898  3745 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:54:43.181907  3745 net.cpp:125] fc10 needs backward computation.
I0909 13:54:43.181915  3745 net.cpp:66] Creating Layer prob
I0909 13:54:43.181921  3745 net.cpp:329] prob <- fc10
I0909 13:54:43.181927  3745 net.cpp:290] prob -> prob
I0909 13:54:43.181937  3745 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:54:43.181946  3745 net.cpp:125] prob needs backward computation.
I0909 13:54:43.181951  3745 net.cpp:156] This network produces output prob
I0909 13:54:43.181962  3745 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:54:43.181970  3745 net.cpp:167] Network initialization done.
I0909 13:54:43.181975  3745 net.cpp:168] Memory required for data: 6183480
Classifying 131 inputs.
Done in 83.23 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:56:10.249021  3764 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:56:10.249184  3764 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:56:10.249194  3764 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:56:10.249377  3764 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:56:10.249430  3764 net.cpp:292] Input 0 -> data
I0909 13:56:10.249459  3764 net.cpp:66] Creating Layer conv1
I0909 13:56:10.249467  3764 net.cpp:329] conv1 <- data
I0909 13:56:10.249476  3764 net.cpp:290] conv1 -> conv1
I0909 13:56:10.250999  3764 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:56:10.251020  3764 net.cpp:125] conv1 needs backward computation.
I0909 13:56:10.251030  3764 net.cpp:66] Creating Layer relu1
I0909 13:56:10.251047  3764 net.cpp:329] relu1 <- conv1
I0909 13:56:10.251055  3764 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:56:10.251065  3764 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:56:10.251072  3764 net.cpp:125] relu1 needs backward computation.
I0909 13:56:10.251080  3764 net.cpp:66] Creating Layer pool1
I0909 13:56:10.251087  3764 net.cpp:329] pool1 <- conv1
I0909 13:56:10.251099  3764 net.cpp:290] pool1 -> pool1
I0909 13:56:10.251113  3764 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:56:10.251121  3764 net.cpp:125] pool1 needs backward computation.
I0909 13:56:10.251128  3764 net.cpp:66] Creating Layer norm1
I0909 13:56:10.251135  3764 net.cpp:329] norm1 <- pool1
I0909 13:56:10.251143  3764 net.cpp:290] norm1 -> norm1
I0909 13:56:10.251154  3764 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:56:10.251162  3764 net.cpp:125] norm1 needs backward computation.
I0909 13:56:10.251170  3764 net.cpp:66] Creating Layer conv2
I0909 13:56:10.251178  3764 net.cpp:329] conv2 <- norm1
I0909 13:56:10.251194  3764 net.cpp:290] conv2 -> conv2
I0909 13:56:10.260432  3764 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:56:10.260457  3764 net.cpp:125] conv2 needs backward computation.
I0909 13:56:10.260474  3764 net.cpp:66] Creating Layer relu2
I0909 13:56:10.260481  3764 net.cpp:329] relu2 <- conv2
I0909 13:56:10.260489  3764 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:56:10.260499  3764 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:56:10.260504  3764 net.cpp:125] relu2 needs backward computation.
I0909 13:56:10.260519  3764 net.cpp:66] Creating Layer pool2
I0909 13:56:10.260525  3764 net.cpp:329] pool2 <- conv2
I0909 13:56:10.260534  3764 net.cpp:290] pool2 -> pool2
I0909 13:56:10.260542  3764 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:56:10.260550  3764 net.cpp:125] pool2 needs backward computation.
I0909 13:56:10.260557  3764 net.cpp:66] Creating Layer fc7
I0909 13:56:10.260563  3764 net.cpp:329] fc7 <- pool2
I0909 13:56:10.260571  3764 net.cpp:290] fc7 -> fc7
I0909 13:56:10.898934  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.898980  3764 net.cpp:125] fc7 needs backward computation.
I0909 13:56:10.898995  3764 net.cpp:66] Creating Layer relu7
I0909 13:56:10.899004  3764 net.cpp:329] relu7 <- fc7
I0909 13:56:10.899013  3764 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:56:10.899024  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.899031  3764 net.cpp:125] relu7 needs backward computation.
I0909 13:56:10.899039  3764 net.cpp:66] Creating Layer drop7
I0909 13:56:10.899046  3764 net.cpp:329] drop7 <- fc7
I0909 13:56:10.899055  3764 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:56:10.899067  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.899075  3764 net.cpp:125] drop7 needs backward computation.
I0909 13:56:10.899085  3764 net.cpp:66] Creating Layer fc8
I0909 13:56:10.899091  3764 net.cpp:329] fc8 <- fc7
I0909 13:56:10.899101  3764 net.cpp:290] fc8 -> fc8
I0909 13:56:10.906903  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.906918  3764 net.cpp:125] fc8 needs backward computation.
I0909 13:56:10.906925  3764 net.cpp:66] Creating Layer relu8
I0909 13:56:10.906932  3764 net.cpp:329] relu8 <- fc8
I0909 13:56:10.906939  3764 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:56:10.906949  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.906956  3764 net.cpp:125] relu8 needs backward computation.
I0909 13:56:10.906965  3764 net.cpp:66] Creating Layer drop8
I0909 13:56:10.906970  3764 net.cpp:329] drop8 <- fc8
I0909 13:56:10.906978  3764 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:56:10.906986  3764 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:56:10.906992  3764 net.cpp:125] drop8 needs backward computation.
I0909 13:56:10.907001  3764 net.cpp:66] Creating Layer fc9
I0909 13:56:10.907007  3764 net.cpp:329] fc9 <- fc8
I0909 13:56:10.907016  3764 net.cpp:290] fc9 -> fc9
I0909 13:56:10.907390  3764 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:56:10.907404  3764 net.cpp:125] fc9 needs backward computation.
I0909 13:56:10.907414  3764 net.cpp:66] Creating Layer fc10
I0909 13:56:10.907420  3764 net.cpp:329] fc10 <- fc9
I0909 13:56:10.907428  3764 net.cpp:290] fc10 -> fc10
I0909 13:56:10.907444  3764 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:56:10.907452  3764 net.cpp:125] fc10 needs backward computation.
I0909 13:56:10.907460  3764 net.cpp:66] Creating Layer prob
I0909 13:56:10.907466  3764 net.cpp:329] prob <- fc10
I0909 13:56:10.907485  3764 net.cpp:290] prob -> prob
I0909 13:56:10.907495  3764 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:56:10.907503  3764 net.cpp:125] prob needs backward computation.
I0909 13:56:10.907510  3764 net.cpp:156] This network produces output prob
I0909 13:56:10.907522  3764 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:56:10.907531  3764 net.cpp:167] Network initialization done.
I0909 13:56:10.907537  3764 net.cpp:168] Memory required for data: 6183480
Classifying 255 inputs.
Done in 156.96 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:58:57.472836  3871 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:58:57.472996  3871 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:58:57.473007  3871 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:58:57.473152  3871 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:58:57.473214  3871 net.cpp:292] Input 0 -> data
I0909 13:58:57.473242  3871 net.cpp:66] Creating Layer conv1
I0909 13:58:57.473249  3871 net.cpp:329] conv1 <- data
I0909 13:58:57.473258  3871 net.cpp:290] conv1 -> conv1
I0909 13:58:57.474622  3871 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:58:57.474642  3871 net.cpp:125] conv1 needs backward computation.
I0909 13:58:57.474653  3871 net.cpp:66] Creating Layer relu1
I0909 13:58:57.474660  3871 net.cpp:329] relu1 <- conv1
I0909 13:58:57.474668  3871 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:58:57.474678  3871 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:58:57.474684  3871 net.cpp:125] relu1 needs backward computation.
I0909 13:58:57.474692  3871 net.cpp:66] Creating Layer pool1
I0909 13:58:57.474699  3871 net.cpp:329] pool1 <- conv1
I0909 13:58:57.474706  3871 net.cpp:290] pool1 -> pool1
I0909 13:58:57.474719  3871 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:58:57.474725  3871 net.cpp:125] pool1 needs backward computation.
I0909 13:58:57.474733  3871 net.cpp:66] Creating Layer norm1
I0909 13:58:57.474740  3871 net.cpp:329] norm1 <- pool1
I0909 13:58:57.474748  3871 net.cpp:290] norm1 -> norm1
I0909 13:58:57.474758  3871 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:58:57.474766  3871 net.cpp:125] norm1 needs backward computation.
I0909 13:58:57.474773  3871 net.cpp:66] Creating Layer conv2
I0909 13:58:57.474781  3871 net.cpp:329] conv2 <- norm1
I0909 13:58:57.474788  3871 net.cpp:290] conv2 -> conv2
I0909 13:58:57.483680  3871 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:58:57.483697  3871 net.cpp:125] conv2 needs backward computation.
I0909 13:58:57.483713  3871 net.cpp:66] Creating Layer relu2
I0909 13:58:57.483721  3871 net.cpp:329] relu2 <- conv2
I0909 13:58:57.483728  3871 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:58:57.483737  3871 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:58:57.483743  3871 net.cpp:125] relu2 needs backward computation.
I0909 13:58:57.483758  3871 net.cpp:66] Creating Layer pool2
I0909 13:58:57.483764  3871 net.cpp:329] pool2 <- conv2
I0909 13:58:57.483772  3871 net.cpp:290] pool2 -> pool2
I0909 13:58:57.483780  3871 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:58:57.483788  3871 net.cpp:125] pool2 needs backward computation.
I0909 13:58:57.483795  3871 net.cpp:66] Creating Layer fc7
I0909 13:58:57.483801  3871 net.cpp:329] fc7 <- pool2
I0909 13:58:57.483809  3871 net.cpp:290] fc7 -> fc7
I0909 13:58:58.124951  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.125000  3871 net.cpp:125] fc7 needs backward computation.
I0909 13:58:58.125015  3871 net.cpp:66] Creating Layer relu7
I0909 13:58:58.125025  3871 net.cpp:329] relu7 <- fc7
I0909 13:58:58.125033  3871 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:58:58.125044  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.125051  3871 net.cpp:125] relu7 needs backward computation.
I0909 13:58:58.125059  3871 net.cpp:66] Creating Layer drop7
I0909 13:58:58.125066  3871 net.cpp:329] drop7 <- fc7
I0909 13:58:58.125075  3871 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:58:58.125087  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.125094  3871 net.cpp:125] drop7 needs backward computation.
I0909 13:58:58.125103  3871 net.cpp:66] Creating Layer fc8
I0909 13:58:58.125110  3871 net.cpp:329] fc8 <- fc7
I0909 13:58:58.125120  3871 net.cpp:290] fc8 -> fc8
I0909 13:58:58.132941  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.132956  3871 net.cpp:125] fc8 needs backward computation.
I0909 13:58:58.132964  3871 net.cpp:66] Creating Layer relu8
I0909 13:58:58.132972  3871 net.cpp:329] relu8 <- fc8
I0909 13:58:58.132980  3871 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:58:58.132988  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.132995  3871 net.cpp:125] relu8 needs backward computation.
I0909 13:58:58.133013  3871 net.cpp:66] Creating Layer drop8
I0909 13:58:58.133020  3871 net.cpp:329] drop8 <- fc8
I0909 13:58:58.133028  3871 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:58:58.133035  3871 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:58:58.133043  3871 net.cpp:125] drop8 needs backward computation.
I0909 13:58:58.133050  3871 net.cpp:66] Creating Layer fc9
I0909 13:58:58.133057  3871 net.cpp:329] fc9 <- fc8
I0909 13:58:58.133066  3871 net.cpp:290] fc9 -> fc9
I0909 13:58:58.133441  3871 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:58:58.133455  3871 net.cpp:125] fc9 needs backward computation.
I0909 13:58:58.133466  3871 net.cpp:66] Creating Layer fc10
I0909 13:58:58.133472  3871 net.cpp:329] fc10 <- fc9
I0909 13:58:58.133481  3871 net.cpp:290] fc10 -> fc10
I0909 13:58:58.133494  3871 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:58:58.133503  3871 net.cpp:125] fc10 needs backward computation.
I0909 13:58:58.133519  3871 net.cpp:66] Creating Layer prob
I0909 13:58:58.133527  3871 net.cpp:329] prob <- fc10
I0909 13:58:58.133538  3871 net.cpp:290] prob -> prob
I0909 13:58:58.133548  3871 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:58:58.133555  3871 net.cpp:125] prob needs backward computation.
I0909 13:58:58.133561  3871 net.cpp:156] This network produces output prob
I0909 13:58:58.133574  3871 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:58:58.133582  3871 net.cpp:167] Network initialization done.
I0909 13:58:58.133589  3871 net.cpp:168] Memory required for data: 6183480
Classifying 2 inputs.
Done in 1.29 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:59:00.140593  3874 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:59:00.140743  3874 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:59:00.140753  3874 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:59:00.140900  3874 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:59:00.140965  3874 net.cpp:292] Input 0 -> data
I0909 13:59:00.140993  3874 net.cpp:66] Creating Layer conv1
I0909 13:59:00.141000  3874 net.cpp:329] conv1 <- data
I0909 13:59:00.141010  3874 net.cpp:290] conv1 -> conv1
I0909 13:59:00.142408  3874 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:59:00.142429  3874 net.cpp:125] conv1 needs backward computation.
I0909 13:59:00.142438  3874 net.cpp:66] Creating Layer relu1
I0909 13:59:00.142446  3874 net.cpp:329] relu1 <- conv1
I0909 13:59:00.142453  3874 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:59:00.142463  3874 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:59:00.142469  3874 net.cpp:125] relu1 needs backward computation.
I0909 13:59:00.142477  3874 net.cpp:66] Creating Layer pool1
I0909 13:59:00.142483  3874 net.cpp:329] pool1 <- conv1
I0909 13:59:00.142491  3874 net.cpp:290] pool1 -> pool1
I0909 13:59:00.142503  3874 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:59:00.142509  3874 net.cpp:125] pool1 needs backward computation.
I0909 13:59:00.142518  3874 net.cpp:66] Creating Layer norm1
I0909 13:59:00.142524  3874 net.cpp:329] norm1 <- pool1
I0909 13:59:00.142531  3874 net.cpp:290] norm1 -> norm1
I0909 13:59:00.142541  3874 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:59:00.142547  3874 net.cpp:125] norm1 needs backward computation.
I0909 13:59:00.142555  3874 net.cpp:66] Creating Layer conv2
I0909 13:59:00.142562  3874 net.cpp:329] conv2 <- norm1
I0909 13:59:00.142570  3874 net.cpp:290] conv2 -> conv2
I0909 13:59:00.151679  3874 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:59:00.151695  3874 net.cpp:125] conv2 needs backward computation.
I0909 13:59:00.151703  3874 net.cpp:66] Creating Layer relu2
I0909 13:59:00.151710  3874 net.cpp:329] relu2 <- conv2
I0909 13:59:00.151717  3874 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:59:00.151726  3874 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:59:00.151732  3874 net.cpp:125] relu2 needs backward computation.
I0909 13:59:00.151739  3874 net.cpp:66] Creating Layer pool2
I0909 13:59:00.151746  3874 net.cpp:329] pool2 <- conv2
I0909 13:59:00.151752  3874 net.cpp:290] pool2 -> pool2
I0909 13:59:00.151762  3874 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:59:00.151767  3874 net.cpp:125] pool2 needs backward computation.
I0909 13:59:00.151775  3874 net.cpp:66] Creating Layer fc7
I0909 13:59:00.151782  3874 net.cpp:329] fc7 <- pool2
I0909 13:59:00.151789  3874 net.cpp:290] fc7 -> fc7
I0909 13:59:00.792928  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.792987  3874 net.cpp:125] fc7 needs backward computation.
I0909 13:59:00.793004  3874 net.cpp:66] Creating Layer relu7
I0909 13:59:00.793011  3874 net.cpp:329] relu7 <- fc7
I0909 13:59:00.793020  3874 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:59:00.793030  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.793054  3874 net.cpp:125] relu7 needs backward computation.
I0909 13:59:00.793063  3874 net.cpp:66] Creating Layer drop7
I0909 13:59:00.793069  3874 net.cpp:329] drop7 <- fc7
I0909 13:59:00.793077  3874 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:59:00.793089  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.793095  3874 net.cpp:125] drop7 needs backward computation.
I0909 13:59:00.793104  3874 net.cpp:66] Creating Layer fc8
I0909 13:59:00.793110  3874 net.cpp:329] fc8 <- fc7
I0909 13:59:00.793119  3874 net.cpp:290] fc8 -> fc8
I0909 13:59:00.800670  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.800683  3874 net.cpp:125] fc8 needs backward computation.
I0909 13:59:00.800691  3874 net.cpp:66] Creating Layer relu8
I0909 13:59:00.800698  3874 net.cpp:329] relu8 <- fc8
I0909 13:59:00.800704  3874 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:59:00.800714  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.800719  3874 net.cpp:125] relu8 needs backward computation.
I0909 13:59:00.800726  3874 net.cpp:66] Creating Layer drop8
I0909 13:59:00.800732  3874 net.cpp:329] drop8 <- fc8
I0909 13:59:00.800739  3874 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:59:00.800746  3874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:00.800752  3874 net.cpp:125] drop8 needs backward computation.
I0909 13:59:00.800760  3874 net.cpp:66] Creating Layer fc9
I0909 13:59:00.800765  3874 net.cpp:329] fc9 <- fc8
I0909 13:59:00.800776  3874 net.cpp:290] fc9 -> fc9
I0909 13:59:00.801138  3874 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:59:00.801151  3874 net.cpp:125] fc9 needs backward computation.
I0909 13:59:00.801161  3874 net.cpp:66] Creating Layer fc10
I0909 13:59:00.801167  3874 net.cpp:329] fc10 <- fc9
I0909 13:59:00.801174  3874 net.cpp:290] fc10 -> fc10
I0909 13:59:00.801187  3874 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:59:00.801197  3874 net.cpp:125] fc10 needs backward computation.
I0909 13:59:00.801203  3874 net.cpp:66] Creating Layer prob
I0909 13:59:00.801209  3874 net.cpp:329] prob <- fc10
I0909 13:59:00.801216  3874 net.cpp:290] prob -> prob
I0909 13:59:00.801225  3874 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:59:00.801234  3874 net.cpp:125] prob needs backward computation.
I0909 13:59:00.801239  3874 net.cpp:156] This network produces output prob
I0909 13:59:00.801250  3874 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:59:00.801259  3874 net.cpp:167] Network initialization done.
I0909 13:59:00.801265  3874 net.cpp:168] Memory required for data: 6183480
Classifying 69 inputs.
Done in 42.68 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 13:59:45.779654  3939 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 13:59:45.779800  3939 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 13:59:45.779810  3939 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 13:59:45.779959  3939 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 13:59:45.780047  3939 net.cpp:292] Input 0 -> data
I0909 13:59:45.780074  3939 net.cpp:66] Creating Layer conv1
I0909 13:59:45.780082  3939 net.cpp:329] conv1 <- data
I0909 13:59:45.780092  3939 net.cpp:290] conv1 -> conv1
I0909 13:59:45.781453  3939 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:59:45.781473  3939 net.cpp:125] conv1 needs backward computation.
I0909 13:59:45.781482  3939 net.cpp:66] Creating Layer relu1
I0909 13:59:45.781489  3939 net.cpp:329] relu1 <- conv1
I0909 13:59:45.781497  3939 net.cpp:280] relu1 -> conv1 (in-place)
I0909 13:59:45.781507  3939 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 13:59:45.781529  3939 net.cpp:125] relu1 needs backward computation.
I0909 13:59:45.781546  3939 net.cpp:66] Creating Layer pool1
I0909 13:59:45.781553  3939 net.cpp:329] pool1 <- conv1
I0909 13:59:45.781560  3939 net.cpp:290] pool1 -> pool1
I0909 13:59:45.781572  3939 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:59:45.781587  3939 net.cpp:125] pool1 needs backward computation.
I0909 13:59:45.781594  3939 net.cpp:66] Creating Layer norm1
I0909 13:59:45.781600  3939 net.cpp:329] norm1 <- pool1
I0909 13:59:45.781608  3939 net.cpp:290] norm1 -> norm1
I0909 13:59:45.781618  3939 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 13:59:45.781625  3939 net.cpp:125] norm1 needs backward computation.
I0909 13:59:45.781633  3939 net.cpp:66] Creating Layer conv2
I0909 13:59:45.781640  3939 net.cpp:329] conv2 <- norm1
I0909 13:59:45.781647  3939 net.cpp:290] conv2 -> conv2
I0909 13:59:45.790793  3939 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:59:45.790814  3939 net.cpp:125] conv2 needs backward computation.
I0909 13:59:45.790832  3939 net.cpp:66] Creating Layer relu2
I0909 13:59:45.790838  3939 net.cpp:329] relu2 <- conv2
I0909 13:59:45.790851  3939 net.cpp:280] relu2 -> conv2 (in-place)
I0909 13:59:45.790860  3939 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 13:59:45.790866  3939 net.cpp:125] relu2 needs backward computation.
I0909 13:59:45.790874  3939 net.cpp:66] Creating Layer pool2
I0909 13:59:45.790879  3939 net.cpp:329] pool2 <- conv2
I0909 13:59:45.790887  3939 net.cpp:290] pool2 -> pool2
I0909 13:59:45.790895  3939 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 13:59:45.790902  3939 net.cpp:125] pool2 needs backward computation.
I0909 13:59:45.790910  3939 net.cpp:66] Creating Layer fc7
I0909 13:59:45.790916  3939 net.cpp:329] fc7 <- pool2
I0909 13:59:45.790925  3939 net.cpp:290] fc7 -> fc7
I0909 13:59:46.431886  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.431937  3939 net.cpp:125] fc7 needs backward computation.
I0909 13:59:46.431951  3939 net.cpp:66] Creating Layer relu7
I0909 13:59:46.431959  3939 net.cpp:329] relu7 <- fc7
I0909 13:59:46.431968  3939 net.cpp:280] relu7 -> fc7 (in-place)
I0909 13:59:46.431979  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.431987  3939 net.cpp:125] relu7 needs backward computation.
I0909 13:59:46.431994  3939 net.cpp:66] Creating Layer drop7
I0909 13:59:46.432001  3939 net.cpp:329] drop7 <- fc7
I0909 13:59:46.432009  3939 net.cpp:280] drop7 -> fc7 (in-place)
I0909 13:59:46.432023  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.432029  3939 net.cpp:125] drop7 needs backward computation.
I0909 13:59:46.432039  3939 net.cpp:66] Creating Layer fc8
I0909 13:59:46.432044  3939 net.cpp:329] fc8 <- fc7
I0909 13:59:46.432054  3939 net.cpp:290] fc8 -> fc8
I0909 13:59:46.439904  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.439928  3939 net.cpp:125] fc8 needs backward computation.
I0909 13:59:46.439936  3939 net.cpp:66] Creating Layer relu8
I0909 13:59:46.439944  3939 net.cpp:329] relu8 <- fc8
I0909 13:59:46.439951  3939 net.cpp:280] relu8 -> fc8 (in-place)
I0909 13:59:46.439962  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.439968  3939 net.cpp:125] relu8 needs backward computation.
I0909 13:59:46.439976  3939 net.cpp:66] Creating Layer drop8
I0909 13:59:46.439982  3939 net.cpp:329] drop8 <- fc8
I0909 13:59:46.439990  3939 net.cpp:280] drop8 -> fc8 (in-place)
I0909 13:59:46.439998  3939 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 13:59:46.440004  3939 net.cpp:125] drop8 needs backward computation.
I0909 13:59:46.440013  3939 net.cpp:66] Creating Layer fc9
I0909 13:59:46.440019  3939 net.cpp:329] fc9 <- fc8
I0909 13:59:46.440029  3939 net.cpp:290] fc9 -> fc9
I0909 13:59:46.440405  3939 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 13:59:46.440419  3939 net.cpp:125] fc9 needs backward computation.
I0909 13:59:46.440428  3939 net.cpp:66] Creating Layer fc10
I0909 13:59:46.440435  3939 net.cpp:329] fc10 <- fc9
I0909 13:59:46.440443  3939 net.cpp:290] fc10 -> fc10
I0909 13:59:46.440457  3939 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:59:46.440466  3939 net.cpp:125] fc10 needs backward computation.
I0909 13:59:46.440474  3939 net.cpp:66] Creating Layer prob
I0909 13:59:46.440480  3939 net.cpp:329] prob <- fc10
I0909 13:59:46.440487  3939 net.cpp:290] prob -> prob
I0909 13:59:46.440498  3939 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 13:59:46.440506  3939 net.cpp:125] prob needs backward computation.
I0909 13:59:46.440512  3939 net.cpp:156] This network produces output prob
I0909 13:59:46.440525  3939 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 13:59:46.440534  3939 net.cpp:167] Network initialization done.
I0909 13:59:46.440541  3939 net.cpp:168] Memory required for data: 6183480
Classifying 102 inputs.
Done in 64.94 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:00:54.886170  3969 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:00:54.886317  3969 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:00:54.886327  3969 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:00:54.886488  3969 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:00:54.886541  3969 net.cpp:292] Input 0 -> data
I0909 14:00:54.886569  3969 net.cpp:66] Creating Layer conv1
I0909 14:00:54.886575  3969 net.cpp:329] conv1 <- data
I0909 14:00:54.886584  3969 net.cpp:290] conv1 -> conv1
I0909 14:00:54.887959  3969 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:00:54.887979  3969 net.cpp:125] conv1 needs backward computation.
I0909 14:00:54.887989  3969 net.cpp:66] Creating Layer relu1
I0909 14:00:54.887996  3969 net.cpp:329] relu1 <- conv1
I0909 14:00:54.888003  3969 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:00:54.888013  3969 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:00:54.888020  3969 net.cpp:125] relu1 needs backward computation.
I0909 14:00:54.888027  3969 net.cpp:66] Creating Layer pool1
I0909 14:00:54.888038  3969 net.cpp:329] pool1 <- conv1
I0909 14:00:54.888047  3969 net.cpp:290] pool1 -> pool1
I0909 14:00:54.888059  3969 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:00:54.888067  3969 net.cpp:125] pool1 needs backward computation.
I0909 14:00:54.888073  3969 net.cpp:66] Creating Layer norm1
I0909 14:00:54.888079  3969 net.cpp:329] norm1 <- pool1
I0909 14:00:54.888087  3969 net.cpp:290] norm1 -> norm1
I0909 14:00:54.888098  3969 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:00:54.888103  3969 net.cpp:125] norm1 needs backward computation.
I0909 14:00:54.888111  3969 net.cpp:66] Creating Layer conv2
I0909 14:00:54.888118  3969 net.cpp:329] conv2 <- norm1
I0909 14:00:54.888126  3969 net.cpp:290] conv2 -> conv2
I0909 14:00:54.897284  3969 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:00:54.897302  3969 net.cpp:125] conv2 needs backward computation.
I0909 14:00:54.897317  3969 net.cpp:66] Creating Layer relu2
I0909 14:00:54.897325  3969 net.cpp:329] relu2 <- conv2
I0909 14:00:54.897332  3969 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:00:54.897339  3969 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:00:54.897346  3969 net.cpp:125] relu2 needs backward computation.
I0909 14:00:54.897361  3969 net.cpp:66] Creating Layer pool2
I0909 14:00:54.897367  3969 net.cpp:329] pool2 <- conv2
I0909 14:00:54.897373  3969 net.cpp:290] pool2 -> pool2
I0909 14:00:54.897382  3969 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:00:54.897388  3969 net.cpp:125] pool2 needs backward computation.
I0909 14:00:54.897395  3969 net.cpp:66] Creating Layer fc7
I0909 14:00:54.897402  3969 net.cpp:329] fc7 <- pool2
I0909 14:00:54.897409  3969 net.cpp:290] fc7 -> fc7
I0909 14:00:55.537955  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.538009  3969 net.cpp:125] fc7 needs backward computation.
I0909 14:00:55.538024  3969 net.cpp:66] Creating Layer relu7
I0909 14:00:55.538033  3969 net.cpp:329] relu7 <- fc7
I0909 14:00:55.538043  3969 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:00:55.538053  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.538058  3969 net.cpp:125] relu7 needs backward computation.
I0909 14:00:55.538067  3969 net.cpp:66] Creating Layer drop7
I0909 14:00:55.538074  3969 net.cpp:329] drop7 <- fc7
I0909 14:00:55.538081  3969 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:00:55.538094  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.538100  3969 net.cpp:125] drop7 needs backward computation.
I0909 14:00:55.538110  3969 net.cpp:66] Creating Layer fc8
I0909 14:00:55.538123  3969 net.cpp:329] fc8 <- fc7
I0909 14:00:55.538135  3969 net.cpp:290] fc8 -> fc8
I0909 14:00:55.546018  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.546037  3969 net.cpp:125] fc8 needs backward computation.
I0909 14:00:55.546046  3969 net.cpp:66] Creating Layer relu8
I0909 14:00:55.546052  3969 net.cpp:329] relu8 <- fc8
I0909 14:00:55.546061  3969 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:00:55.546071  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.546077  3969 net.cpp:125] relu8 needs backward computation.
I0909 14:00:55.546083  3969 net.cpp:66] Creating Layer drop8
I0909 14:00:55.546089  3969 net.cpp:329] drop8 <- fc8
I0909 14:00:55.546097  3969 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:00:55.546104  3969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:00:55.546110  3969 net.cpp:125] drop8 needs backward computation.
I0909 14:00:55.546118  3969 net.cpp:66] Creating Layer fc9
I0909 14:00:55.546125  3969 net.cpp:329] fc9 <- fc8
I0909 14:00:55.546134  3969 net.cpp:290] fc9 -> fc9
I0909 14:00:55.546507  3969 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:00:55.546521  3969 net.cpp:125] fc9 needs backward computation.
I0909 14:00:55.546530  3969 net.cpp:66] Creating Layer fc10
I0909 14:00:55.546537  3969 net.cpp:329] fc10 <- fc9
I0909 14:00:55.546545  3969 net.cpp:290] fc10 -> fc10
I0909 14:00:55.546561  3969 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:00:55.546569  3969 net.cpp:125] fc10 needs backward computation.
I0909 14:00:55.546586  3969 net.cpp:66] Creating Layer prob
I0909 14:00:55.546593  3969 net.cpp:329] prob <- fc10
I0909 14:00:55.546600  3969 net.cpp:290] prob -> prob
I0909 14:00:55.546610  3969 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:00:55.546619  3969 net.cpp:125] prob needs backward computation.
I0909 14:00:55.546624  3969 net.cpp:156] This network produces output prob
I0909 14:00:55.546636  3969 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:00:55.546645  3969 net.cpp:167] Network initialization done.
I0909 14:00:55.546651  3969 net.cpp:168] Memory required for data: 6183480
Classifying 5 inputs.
Done in 3.18 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:00:59.473225  3984 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:00:59.473387  3984 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:00:59.473397  3984 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:00:59.473566  3984 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:00:59.473640  3984 net.cpp:292] Input 0 -> data
I0909 14:00:59.473669  3984 net.cpp:66] Creating Layer conv1
I0909 14:00:59.473675  3984 net.cpp:329] conv1 <- data
I0909 14:00:59.473685  3984 net.cpp:290] conv1 -> conv1
I0909 14:00:59.475009  3984 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:00:59.475028  3984 net.cpp:125] conv1 needs backward computation.
I0909 14:00:59.475038  3984 net.cpp:66] Creating Layer relu1
I0909 14:00:59.475044  3984 net.cpp:329] relu1 <- conv1
I0909 14:00:59.475051  3984 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:00:59.475061  3984 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:00:59.475067  3984 net.cpp:125] relu1 needs backward computation.
I0909 14:00:59.475075  3984 net.cpp:66] Creating Layer pool1
I0909 14:00:59.475080  3984 net.cpp:329] pool1 <- conv1
I0909 14:00:59.475088  3984 net.cpp:290] pool1 -> pool1
I0909 14:00:59.475100  3984 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:00:59.475106  3984 net.cpp:125] pool1 needs backward computation.
I0909 14:00:59.475113  3984 net.cpp:66] Creating Layer norm1
I0909 14:00:59.475119  3984 net.cpp:329] norm1 <- pool1
I0909 14:00:59.475127  3984 net.cpp:290] norm1 -> norm1
I0909 14:00:59.475137  3984 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:00:59.475143  3984 net.cpp:125] norm1 needs backward computation.
I0909 14:00:59.475152  3984 net.cpp:66] Creating Layer conv2
I0909 14:00:59.475157  3984 net.cpp:329] conv2 <- norm1
I0909 14:00:59.475164  3984 net.cpp:290] conv2 -> conv2
I0909 14:00:59.484205  3984 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:00:59.484222  3984 net.cpp:125] conv2 needs backward computation.
I0909 14:00:59.484231  3984 net.cpp:66] Creating Layer relu2
I0909 14:00:59.484237  3984 net.cpp:329] relu2 <- conv2
I0909 14:00:59.484246  3984 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:00:59.484252  3984 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:00:59.484259  3984 net.cpp:125] relu2 needs backward computation.
I0909 14:00:59.484273  3984 net.cpp:66] Creating Layer pool2
I0909 14:00:59.484279  3984 net.cpp:329] pool2 <- conv2
I0909 14:00:59.484294  3984 net.cpp:290] pool2 -> pool2
I0909 14:00:59.484303  3984 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:00:59.484309  3984 net.cpp:125] pool2 needs backward computation.
I0909 14:00:59.484316  3984 net.cpp:66] Creating Layer fc7
I0909 14:00:59.484323  3984 net.cpp:329] fc7 <- pool2
I0909 14:00:59.484330  3984 net.cpp:290] fc7 -> fc7
I0909 14:01:00.132086  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.132135  3984 net.cpp:125] fc7 needs backward computation.
I0909 14:01:00.132150  3984 net.cpp:66] Creating Layer relu7
I0909 14:01:00.132158  3984 net.cpp:329] relu7 <- fc7
I0909 14:01:00.132168  3984 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:01:00.132177  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.132184  3984 net.cpp:125] relu7 needs backward computation.
I0909 14:01:00.132192  3984 net.cpp:66] Creating Layer drop7
I0909 14:01:00.132199  3984 net.cpp:329] drop7 <- fc7
I0909 14:01:00.132206  3984 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:01:00.132220  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.132225  3984 net.cpp:125] drop7 needs backward computation.
I0909 14:01:00.132235  3984 net.cpp:66] Creating Layer fc8
I0909 14:01:00.132241  3984 net.cpp:329] fc8 <- fc7
I0909 14:01:00.132251  3984 net.cpp:290] fc8 -> fc8
I0909 14:01:00.140120  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.140135  3984 net.cpp:125] fc8 needs backward computation.
I0909 14:01:00.140142  3984 net.cpp:66] Creating Layer relu8
I0909 14:01:00.140149  3984 net.cpp:329] relu8 <- fc8
I0909 14:01:00.140156  3984 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:01:00.140166  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.140183  3984 net.cpp:125] relu8 needs backward computation.
I0909 14:01:00.140190  3984 net.cpp:66] Creating Layer drop8
I0909 14:01:00.140197  3984 net.cpp:329] drop8 <- fc8
I0909 14:01:00.140204  3984 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:01:00.140211  3984 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:00.140218  3984 net.cpp:125] drop8 needs backward computation.
I0909 14:01:00.140226  3984 net.cpp:66] Creating Layer fc9
I0909 14:01:00.140233  3984 net.cpp:329] fc9 <- fc8
I0909 14:01:00.140241  3984 net.cpp:290] fc9 -> fc9
I0909 14:01:00.140619  3984 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:01:00.140631  3984 net.cpp:125] fc9 needs backward computation.
I0909 14:01:00.140641  3984 net.cpp:66] Creating Layer fc10
I0909 14:01:00.140647  3984 net.cpp:329] fc10 <- fc9
I0909 14:01:00.140655  3984 net.cpp:290] fc10 -> fc10
I0909 14:01:00.140668  3984 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:00.140677  3984 net.cpp:125] fc10 needs backward computation.
I0909 14:01:00.140686  3984 net.cpp:66] Creating Layer prob
I0909 14:01:00.140691  3984 net.cpp:329] prob <- fc10
I0909 14:01:00.140698  3984 net.cpp:290] prob -> prob
I0909 14:01:00.140708  3984 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:00.140717  3984 net.cpp:125] prob needs backward computation.
I0909 14:01:00.140722  3984 net.cpp:156] This network produces output prob
I0909 14:01:00.140733  3984 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:01:00.140743  3984 net.cpp:167] Network initialization done.
I0909 14:01:00.140748  3984 net.cpp:168] Memory required for data: 6183480
Classifying 49 inputs.
Done in 30.17 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:01:32.170747  4000 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:01:32.170893  4000 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:01:32.170904  4000 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:01:32.171051  4000 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:01:32.171115  4000 net.cpp:292] Input 0 -> data
I0909 14:01:32.171144  4000 net.cpp:66] Creating Layer conv1
I0909 14:01:32.171151  4000 net.cpp:329] conv1 <- data
I0909 14:01:32.171160  4000 net.cpp:290] conv1 -> conv1
I0909 14:01:32.172540  4000 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:01:32.172559  4000 net.cpp:125] conv1 needs backward computation.
I0909 14:01:32.172569  4000 net.cpp:66] Creating Layer relu1
I0909 14:01:32.172575  4000 net.cpp:329] relu1 <- conv1
I0909 14:01:32.172583  4000 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:01:32.172592  4000 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:01:32.172600  4000 net.cpp:125] relu1 needs backward computation.
I0909 14:01:32.172607  4000 net.cpp:66] Creating Layer pool1
I0909 14:01:32.172613  4000 net.cpp:329] pool1 <- conv1
I0909 14:01:32.172621  4000 net.cpp:290] pool1 -> pool1
I0909 14:01:32.172632  4000 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:01:32.172639  4000 net.cpp:125] pool1 needs backward computation.
I0909 14:01:32.172647  4000 net.cpp:66] Creating Layer norm1
I0909 14:01:32.172653  4000 net.cpp:329] norm1 <- pool1
I0909 14:01:32.172662  4000 net.cpp:290] norm1 -> norm1
I0909 14:01:32.172672  4000 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:01:32.172678  4000 net.cpp:125] norm1 needs backward computation.
I0909 14:01:32.172688  4000 net.cpp:66] Creating Layer conv2
I0909 14:01:32.172693  4000 net.cpp:329] conv2 <- norm1
I0909 14:01:32.172701  4000 net.cpp:290] conv2 -> conv2
I0909 14:01:32.181843  4000 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:01:32.181860  4000 net.cpp:125] conv2 needs backward computation.
I0909 14:01:32.181869  4000 net.cpp:66] Creating Layer relu2
I0909 14:01:32.181875  4000 net.cpp:329] relu2 <- conv2
I0909 14:01:32.181882  4000 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:01:32.181890  4000 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:01:32.181897  4000 net.cpp:125] relu2 needs backward computation.
I0909 14:01:32.181905  4000 net.cpp:66] Creating Layer pool2
I0909 14:01:32.181910  4000 net.cpp:329] pool2 <- conv2
I0909 14:01:32.181918  4000 net.cpp:290] pool2 -> pool2
I0909 14:01:32.181927  4000 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:01:32.181933  4000 net.cpp:125] pool2 needs backward computation.
I0909 14:01:32.181941  4000 net.cpp:66] Creating Layer fc7
I0909 14:01:32.181947  4000 net.cpp:329] fc7 <- pool2
I0909 14:01:32.181956  4000 net.cpp:290] fc7 -> fc7
I0909 14:01:32.840028  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.840080  4000 net.cpp:125] fc7 needs backward computation.
I0909 14:01:32.840095  4000 net.cpp:66] Creating Layer relu7
I0909 14:01:32.840102  4000 net.cpp:329] relu7 <- fc7
I0909 14:01:32.840122  4000 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:01:32.840134  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.840140  4000 net.cpp:125] relu7 needs backward computation.
I0909 14:01:32.840149  4000 net.cpp:66] Creating Layer drop7
I0909 14:01:32.840155  4000 net.cpp:329] drop7 <- fc7
I0909 14:01:32.840164  4000 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:01:32.840176  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.840183  4000 net.cpp:125] drop7 needs backward computation.
I0909 14:01:32.840191  4000 net.cpp:66] Creating Layer fc8
I0909 14:01:32.840198  4000 net.cpp:329] fc8 <- fc7
I0909 14:01:32.840208  4000 net.cpp:290] fc8 -> fc8
I0909 14:01:32.848156  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.848170  4000 net.cpp:125] fc8 needs backward computation.
I0909 14:01:32.848177  4000 net.cpp:66] Creating Layer relu8
I0909 14:01:32.848183  4000 net.cpp:329] relu8 <- fc8
I0909 14:01:32.848191  4000 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:01:32.848201  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.848207  4000 net.cpp:125] relu8 needs backward computation.
I0909 14:01:32.848214  4000 net.cpp:66] Creating Layer drop8
I0909 14:01:32.848220  4000 net.cpp:329] drop8 <- fc8
I0909 14:01:32.848227  4000 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:01:32.848235  4000 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:32.848242  4000 net.cpp:125] drop8 needs backward computation.
I0909 14:01:32.848249  4000 net.cpp:66] Creating Layer fc9
I0909 14:01:32.848255  4000 net.cpp:329] fc9 <- fc8
I0909 14:01:32.848264  4000 net.cpp:290] fc9 -> fc9
I0909 14:01:32.848649  4000 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:01:32.848662  4000 net.cpp:125] fc9 needs backward computation.
I0909 14:01:32.848671  4000 net.cpp:66] Creating Layer fc10
I0909 14:01:32.848677  4000 net.cpp:329] fc10 <- fc9
I0909 14:01:32.848685  4000 net.cpp:290] fc10 -> fc10
I0909 14:01:32.848700  4000 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:32.848708  4000 net.cpp:125] fc10 needs backward computation.
I0909 14:01:32.848716  4000 net.cpp:66] Creating Layer prob
I0909 14:01:32.848722  4000 net.cpp:329] prob <- fc10
I0909 14:01:32.848729  4000 net.cpp:290] prob -> prob
I0909 14:01:32.848739  4000 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:32.848748  4000 net.cpp:125] prob needs backward computation.
I0909 14:01:32.848753  4000 net.cpp:156] This network produces output prob
I0909 14:01:32.848765  4000 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:01:32.848774  4000 net.cpp:167] Network initialization done.
I0909 14:01:32.848779  4000 net.cpp:168] Memory required for data: 6183480
Classifying 24 inputs.
Done in 15.48 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:01:49.475564  4010 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:01:49.475709  4010 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:01:49.475719  4010 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:01:49.475868  4010 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:01:49.475931  4010 net.cpp:292] Input 0 -> data
I0909 14:01:49.475958  4010 net.cpp:66] Creating Layer conv1
I0909 14:01:49.475965  4010 net.cpp:329] conv1 <- data
I0909 14:01:49.475975  4010 net.cpp:290] conv1 -> conv1
I0909 14:01:49.477341  4010 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:01:49.477361  4010 net.cpp:125] conv1 needs backward computation.
I0909 14:01:49.477370  4010 net.cpp:66] Creating Layer relu1
I0909 14:01:49.477376  4010 net.cpp:329] relu1 <- conv1
I0909 14:01:49.477385  4010 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:01:49.477393  4010 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:01:49.477401  4010 net.cpp:125] relu1 needs backward computation.
I0909 14:01:49.477407  4010 net.cpp:66] Creating Layer pool1
I0909 14:01:49.477413  4010 net.cpp:329] pool1 <- conv1
I0909 14:01:49.477421  4010 net.cpp:290] pool1 -> pool1
I0909 14:01:49.477432  4010 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:01:49.477439  4010 net.cpp:125] pool1 needs backward computation.
I0909 14:01:49.477447  4010 net.cpp:66] Creating Layer norm1
I0909 14:01:49.477452  4010 net.cpp:329] norm1 <- pool1
I0909 14:01:49.477459  4010 net.cpp:290] norm1 -> norm1
I0909 14:01:49.477470  4010 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:01:49.477478  4010 net.cpp:125] norm1 needs backward computation.
I0909 14:01:49.477484  4010 net.cpp:66] Creating Layer conv2
I0909 14:01:49.477491  4010 net.cpp:329] conv2 <- norm1
I0909 14:01:49.477499  4010 net.cpp:290] conv2 -> conv2
I0909 14:01:49.486503  4010 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:01:49.486521  4010 net.cpp:125] conv2 needs backward computation.
I0909 14:01:49.486531  4010 net.cpp:66] Creating Layer relu2
I0909 14:01:49.486541  4010 net.cpp:329] relu2 <- conv2
I0909 14:01:49.486549  4010 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:01:49.486557  4010 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:01:49.486563  4010 net.cpp:125] relu2 needs backward computation.
I0909 14:01:49.486570  4010 net.cpp:66] Creating Layer pool2
I0909 14:01:49.486577  4010 net.cpp:329] pool2 <- conv2
I0909 14:01:49.486583  4010 net.cpp:290] pool2 -> pool2
I0909 14:01:49.486593  4010 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:01:49.486598  4010 net.cpp:125] pool2 needs backward computation.
I0909 14:01:49.486605  4010 net.cpp:66] Creating Layer fc7
I0909 14:01:49.486613  4010 net.cpp:329] fc7 <- pool2
I0909 14:01:49.486619  4010 net.cpp:290] fc7 -> fc7
I0909 14:01:50.125197  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.125248  4010 net.cpp:125] fc7 needs backward computation.
I0909 14:01:50.125262  4010 net.cpp:66] Creating Layer relu7
I0909 14:01:50.125270  4010 net.cpp:329] relu7 <- fc7
I0909 14:01:50.125279  4010 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:01:50.125290  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.125296  4010 net.cpp:125] relu7 needs backward computation.
I0909 14:01:50.125304  4010 net.cpp:66] Creating Layer drop7
I0909 14:01:50.125310  4010 net.cpp:329] drop7 <- fc7
I0909 14:01:50.125319  4010 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:01:50.125330  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.125337  4010 net.cpp:125] drop7 needs backward computation.
I0909 14:01:50.125346  4010 net.cpp:66] Creating Layer fc8
I0909 14:01:50.125352  4010 net.cpp:329] fc8 <- fc7
I0909 14:01:50.125361  4010 net.cpp:290] fc8 -> fc8
I0909 14:01:50.133049  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.133062  4010 net.cpp:125] fc8 needs backward computation.
I0909 14:01:50.133070  4010 net.cpp:66] Creating Layer relu8
I0909 14:01:50.133076  4010 net.cpp:329] relu8 <- fc8
I0909 14:01:50.133083  4010 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:01:50.133092  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.133098  4010 net.cpp:125] relu8 needs backward computation.
I0909 14:01:50.133105  4010 net.cpp:66] Creating Layer drop8
I0909 14:01:50.133111  4010 net.cpp:329] drop8 <- fc8
I0909 14:01:50.133118  4010 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:01:50.133126  4010 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:01:50.133132  4010 net.cpp:125] drop8 needs backward computation.
I0909 14:01:50.133139  4010 net.cpp:66] Creating Layer fc9
I0909 14:01:50.133146  4010 net.cpp:329] fc9 <- fc8
I0909 14:01:50.133154  4010 net.cpp:290] fc9 -> fc9
I0909 14:01:50.133533  4010 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:01:50.133548  4010 net.cpp:125] fc9 needs backward computation.
I0909 14:01:50.133558  4010 net.cpp:66] Creating Layer fc10
I0909 14:01:50.133563  4010 net.cpp:329] fc10 <- fc9
I0909 14:01:50.133571  4010 net.cpp:290] fc10 -> fc10
I0909 14:01:50.133584  4010 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:50.133594  4010 net.cpp:125] fc10 needs backward computation.
I0909 14:01:50.133600  4010 net.cpp:66] Creating Layer prob
I0909 14:01:50.133606  4010 net.cpp:329] prob <- fc10
I0909 14:01:50.133613  4010 net.cpp:290] prob -> prob
I0909 14:01:50.133623  4010 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:01:50.133631  4010 net.cpp:125] prob needs backward computation.
I0909 14:01:50.133636  4010 net.cpp:156] This network produces output prob
I0909 14:01:50.133648  4010 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:01:50.133656  4010 net.cpp:167] Network initialization done.
I0909 14:01:50.133662  4010 net.cpp:168] Memory required for data: 6183480
Classifying 116 inputs.
Done in 70.75 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:03:03.970741  4034 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:03:03.970899  4034 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:03:03.970921  4034 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:03:03.971066  4034 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:03:03.971118  4034 net.cpp:292] Input 0 -> data
I0909 14:03:03.971145  4034 net.cpp:66] Creating Layer conv1
I0909 14:03:03.971153  4034 net.cpp:329] conv1 <- data
I0909 14:03:03.971161  4034 net.cpp:290] conv1 -> conv1
I0909 14:03:03.972487  4034 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:03:03.972506  4034 net.cpp:125] conv1 needs backward computation.
I0909 14:03:03.972515  4034 net.cpp:66] Creating Layer relu1
I0909 14:03:03.972522  4034 net.cpp:329] relu1 <- conv1
I0909 14:03:03.972529  4034 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:03:03.972537  4034 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:03:03.972544  4034 net.cpp:125] relu1 needs backward computation.
I0909 14:03:03.972556  4034 net.cpp:66] Creating Layer pool1
I0909 14:03:03.972563  4034 net.cpp:329] pool1 <- conv1
I0909 14:03:03.972570  4034 net.cpp:290] pool1 -> pool1
I0909 14:03:03.972581  4034 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:03:03.972589  4034 net.cpp:125] pool1 needs backward computation.
I0909 14:03:03.972595  4034 net.cpp:66] Creating Layer norm1
I0909 14:03:03.972601  4034 net.cpp:329] norm1 <- pool1
I0909 14:03:03.972609  4034 net.cpp:290] norm1 -> norm1
I0909 14:03:03.972618  4034 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:03:03.972625  4034 net.cpp:125] norm1 needs backward computation.
I0909 14:03:03.972633  4034 net.cpp:66] Creating Layer conv2
I0909 14:03:03.972640  4034 net.cpp:329] conv2 <- norm1
I0909 14:03:03.972646  4034 net.cpp:290] conv2 -> conv2
I0909 14:03:03.981784  4034 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:03:03.981811  4034 net.cpp:125] conv2 needs backward computation.
I0909 14:03:03.981828  4034 net.cpp:66] Creating Layer relu2
I0909 14:03:03.981834  4034 net.cpp:329] relu2 <- conv2
I0909 14:03:03.981843  4034 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:03:03.981853  4034 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:03:03.981858  4034 net.cpp:125] relu2 needs backward computation.
I0909 14:03:03.981873  4034 net.cpp:66] Creating Layer pool2
I0909 14:03:03.981878  4034 net.cpp:329] pool2 <- conv2
I0909 14:03:03.981886  4034 net.cpp:290] pool2 -> pool2
I0909 14:03:03.981894  4034 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:03:03.981901  4034 net.cpp:125] pool2 needs backward computation.
I0909 14:03:03.981909  4034 net.cpp:66] Creating Layer fc7
I0909 14:03:03.981915  4034 net.cpp:329] fc7 <- pool2
I0909 14:03:03.981923  4034 net.cpp:290] fc7 -> fc7
I0909 14:03:04.626416  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.626461  4034 net.cpp:125] fc7 needs backward computation.
I0909 14:03:04.626474  4034 net.cpp:66] Creating Layer relu7
I0909 14:03:04.626482  4034 net.cpp:329] relu7 <- fc7
I0909 14:03:04.626490  4034 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:03:04.626500  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.626507  4034 net.cpp:125] relu7 needs backward computation.
I0909 14:03:04.626515  4034 net.cpp:66] Creating Layer drop7
I0909 14:03:04.626521  4034 net.cpp:329] drop7 <- fc7
I0909 14:03:04.626530  4034 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:03:04.626543  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.626549  4034 net.cpp:125] drop7 needs backward computation.
I0909 14:03:04.626559  4034 net.cpp:66] Creating Layer fc8
I0909 14:03:04.626564  4034 net.cpp:329] fc8 <- fc7
I0909 14:03:04.626574  4034 net.cpp:290] fc8 -> fc8
I0909 14:03:04.634423  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.634436  4034 net.cpp:125] fc8 needs backward computation.
I0909 14:03:04.634444  4034 net.cpp:66] Creating Layer relu8
I0909 14:03:04.634451  4034 net.cpp:329] relu8 <- fc8
I0909 14:03:04.634459  4034 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:03:04.634469  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.634474  4034 net.cpp:125] relu8 needs backward computation.
I0909 14:03:04.634482  4034 net.cpp:66] Creating Layer drop8
I0909 14:03:04.634488  4034 net.cpp:329] drop8 <- fc8
I0909 14:03:04.634495  4034 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:03:04.634502  4034 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:03:04.634510  4034 net.cpp:125] drop8 needs backward computation.
I0909 14:03:04.634517  4034 net.cpp:66] Creating Layer fc9
I0909 14:03:04.634523  4034 net.cpp:329] fc9 <- fc8
I0909 14:03:04.634532  4034 net.cpp:290] fc9 -> fc9
I0909 14:03:04.634910  4034 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:03:04.634924  4034 net.cpp:125] fc9 needs backward computation.
I0909 14:03:04.634933  4034 net.cpp:66] Creating Layer fc10
I0909 14:03:04.634940  4034 net.cpp:329] fc10 <- fc9
I0909 14:03:04.634948  4034 net.cpp:290] fc10 -> fc10
I0909 14:03:04.634961  4034 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:03:04.634980  4034 net.cpp:125] fc10 needs backward computation.
I0909 14:03:04.634989  4034 net.cpp:66] Creating Layer prob
I0909 14:03:04.634994  4034 net.cpp:329] prob <- fc10
I0909 14:03:04.635001  4034 net.cpp:290] prob -> prob
I0909 14:03:04.635012  4034 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:03:04.635020  4034 net.cpp:125] prob needs backward computation.
I0909 14:03:04.635026  4034 net.cpp:156] This network produces output prob
I0909 14:03:04.635038  4034 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:03:04.635047  4034 net.cpp:167] Network initialization done.
I0909 14:03:04.635053  4034 net.cpp:168] Memory required for data: 6183480
Classifying 161 inputs.
Done in 1569.14 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:34:31.590278  4338 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:34:31.590422  4338 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:34:31.590431  4338 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:34:31.590642  4338 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:34:31.590703  4338 net.cpp:292] Input 0 -> data
I0909 14:34:31.590728  4338 net.cpp:66] Creating Layer conv1
I0909 14:34:31.590735  4338 net.cpp:329] conv1 <- data
I0909 14:34:31.590744  4338 net.cpp:290] conv1 -> conv1
I0909 14:34:31.625985  4338 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:34:31.626034  4338 net.cpp:125] conv1 needs backward computation.
I0909 14:34:31.626055  4338 net.cpp:66] Creating Layer relu1
I0909 14:34:31.626068  4338 net.cpp:329] relu1 <- conv1
I0909 14:34:31.626082  4338 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:34:31.626101  4338 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:34:31.626113  4338 net.cpp:125] relu1 needs backward computation.
I0909 14:34:31.626127  4338 net.cpp:66] Creating Layer pool1
I0909 14:34:31.626139  4338 net.cpp:329] pool1 <- conv1
I0909 14:34:31.626153  4338 net.cpp:290] pool1 -> pool1
I0909 14:34:31.626175  4338 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:34:31.626188  4338 net.cpp:125] pool1 needs backward computation.
I0909 14:34:31.626201  4338 net.cpp:66] Creating Layer norm1
I0909 14:34:31.626214  4338 net.cpp:329] norm1 <- pool1
I0909 14:34:31.626229  4338 net.cpp:290] norm1 -> norm1
I0909 14:34:31.626247  4338 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:34:31.626281  4338 net.cpp:125] norm1 needs backward computation.
I0909 14:34:31.626298  4338 net.cpp:66] Creating Layer conv2
I0909 14:34:31.626310  4338 net.cpp:329] conv2 <- norm1
I0909 14:34:31.626325  4338 net.cpp:290] conv2 -> conv2
I0909 14:34:31.641136  4338 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:34:31.641151  4338 net.cpp:125] conv2 needs backward computation.
I0909 14:34:31.641158  4338 net.cpp:66] Creating Layer relu2
I0909 14:34:31.641163  4338 net.cpp:329] relu2 <- conv2
I0909 14:34:31.641170  4338 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:34:31.641177  4338 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:34:31.641182  4338 net.cpp:125] relu2 needs backward computation.
I0909 14:34:31.641188  4338 net.cpp:66] Creating Layer pool2
I0909 14:34:31.641194  4338 net.cpp:329] pool2 <- conv2
I0909 14:34:31.641201  4338 net.cpp:290] pool2 -> pool2
I0909 14:34:31.641208  4338 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:34:31.641213  4338 net.cpp:125] pool2 needs backward computation.
I0909 14:34:31.641221  4338 net.cpp:66] Creating Layer fc7
I0909 14:34:31.641227  4338 net.cpp:329] fc7 <- pool2
I0909 14:34:31.641233  4338 net.cpp:290] fc7 -> fc7
I0909 14:34:32.265874  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.265923  4338 net.cpp:125] fc7 needs backward computation.
I0909 14:34:32.265935  4338 net.cpp:66] Creating Layer relu7
I0909 14:34:32.265943  4338 net.cpp:329] relu7 <- fc7
I0909 14:34:32.265950  4338 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:34:32.265960  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.265965  4338 net.cpp:125] relu7 needs backward computation.
I0909 14:34:32.265972  4338 net.cpp:66] Creating Layer drop7
I0909 14:34:32.265977  4338 net.cpp:329] drop7 <- fc7
I0909 14:34:32.265985  4338 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:34:32.265996  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.266002  4338 net.cpp:125] drop7 needs backward computation.
I0909 14:34:32.266010  4338 net.cpp:66] Creating Layer fc8
I0909 14:34:32.266016  4338 net.cpp:329] fc8 <- fc7
I0909 14:34:32.266026  4338 net.cpp:290] fc8 -> fc8
I0909 14:34:32.273579  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.273598  4338 net.cpp:125] fc8 needs backward computation.
I0909 14:34:32.273605  4338 net.cpp:66] Creating Layer relu8
I0909 14:34:32.273612  4338 net.cpp:329] relu8 <- fc8
I0909 14:34:32.273629  4338 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:34:32.273638  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.273644  4338 net.cpp:125] relu8 needs backward computation.
I0909 14:34:32.273651  4338 net.cpp:66] Creating Layer drop8
I0909 14:34:32.273656  4338 net.cpp:329] drop8 <- fc8
I0909 14:34:32.273663  4338 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:34:32.273669  4338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:32.273674  4338 net.cpp:125] drop8 needs backward computation.
I0909 14:34:32.273682  4338 net.cpp:66] Creating Layer fc9
I0909 14:34:32.273687  4338 net.cpp:329] fc9 <- fc8
I0909 14:34:32.273695  4338 net.cpp:290] fc9 -> fc9
I0909 14:34:32.274057  4338 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:34:32.274070  4338 net.cpp:125] fc9 needs backward computation.
I0909 14:34:32.274078  4338 net.cpp:66] Creating Layer fc10
I0909 14:34:32.274083  4338 net.cpp:329] fc10 <- fc9
I0909 14:34:32.274091  4338 net.cpp:290] fc10 -> fc10
I0909 14:34:32.274106  4338 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:34:32.274113  4338 net.cpp:125] fc10 needs backward computation.
I0909 14:34:32.274121  4338 net.cpp:66] Creating Layer prob
I0909 14:34:32.274126  4338 net.cpp:329] prob <- fc10
I0909 14:34:32.274132  4338 net.cpp:290] prob -> prob
I0909 14:34:32.274142  4338 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:34:32.274148  4338 net.cpp:125] prob needs backward computation.
I0909 14:34:32.274154  4338 net.cpp:156] This network produces output prob
I0909 14:34:32.274164  4338 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:34:32.274173  4338 net.cpp:167] Network initialization done.
I0909 14:34:32.274178  4338 net.cpp:168] Memory required for data: 6183480
Classifying 240 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:34:40.053370  4341 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:34:40.053534  4341 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:34:40.053551  4341 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:34:40.053704  4341 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:34:40.053767  4341 net.cpp:292] Input 0 -> data
I0909 14:34:40.053792  4341 net.cpp:66] Creating Layer conv1
I0909 14:34:40.053799  4341 net.cpp:329] conv1 <- data
I0909 14:34:40.053807  4341 net.cpp:290] conv1 -> conv1
I0909 14:34:40.055131  4341 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:34:40.055150  4341 net.cpp:125] conv1 needs backward computation.
I0909 14:34:40.055158  4341 net.cpp:66] Creating Layer relu1
I0909 14:34:40.055165  4341 net.cpp:329] relu1 <- conv1
I0909 14:34:40.055171  4341 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:34:40.055179  4341 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:34:40.055186  4341 net.cpp:125] relu1 needs backward computation.
I0909 14:34:40.055191  4341 net.cpp:66] Creating Layer pool1
I0909 14:34:40.055197  4341 net.cpp:329] pool1 <- conv1
I0909 14:34:40.055203  4341 net.cpp:290] pool1 -> pool1
I0909 14:34:40.055214  4341 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:34:40.055220  4341 net.cpp:125] pool1 needs backward computation.
I0909 14:34:40.055227  4341 net.cpp:66] Creating Layer norm1
I0909 14:34:40.055233  4341 net.cpp:329] norm1 <- pool1
I0909 14:34:40.055238  4341 net.cpp:290] norm1 -> norm1
I0909 14:34:40.055248  4341 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:34:40.055253  4341 net.cpp:125] norm1 needs backward computation.
I0909 14:34:40.055260  4341 net.cpp:66] Creating Layer conv2
I0909 14:34:40.055265  4341 net.cpp:329] conv2 <- norm1
I0909 14:34:40.055272  4341 net.cpp:290] conv2 -> conv2
I0909 14:34:40.064144  4341 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:34:40.064158  4341 net.cpp:125] conv2 needs backward computation.
I0909 14:34:40.064165  4341 net.cpp:66] Creating Layer relu2
I0909 14:34:40.064170  4341 net.cpp:329] relu2 <- conv2
I0909 14:34:40.064177  4341 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:34:40.064184  4341 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:34:40.064189  4341 net.cpp:125] relu2 needs backward computation.
I0909 14:34:40.064195  4341 net.cpp:66] Creating Layer pool2
I0909 14:34:40.064200  4341 net.cpp:329] pool2 <- conv2
I0909 14:34:40.064208  4341 net.cpp:290] pool2 -> pool2
I0909 14:34:40.064214  4341 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:34:40.064224  4341 net.cpp:125] pool2 needs backward computation.
I0909 14:34:40.064232  4341 net.cpp:66] Creating Layer fc7
I0909 14:34:40.064237  4341 net.cpp:329] fc7 <- pool2
I0909 14:34:40.064244  4341 net.cpp:290] fc7 -> fc7
I0909 14:34:40.688874  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.688921  4341 net.cpp:125] fc7 needs backward computation.
I0909 14:34:40.688933  4341 net.cpp:66] Creating Layer relu7
I0909 14:34:40.688941  4341 net.cpp:329] relu7 <- fc7
I0909 14:34:40.688947  4341 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:34:40.688957  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.688963  4341 net.cpp:125] relu7 needs backward computation.
I0909 14:34:40.688971  4341 net.cpp:66] Creating Layer drop7
I0909 14:34:40.688976  4341 net.cpp:329] drop7 <- fc7
I0909 14:34:40.688983  4341 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:34:40.688994  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.688999  4341 net.cpp:125] drop7 needs backward computation.
I0909 14:34:40.689008  4341 net.cpp:66] Creating Layer fc8
I0909 14:34:40.689013  4341 net.cpp:329] fc8 <- fc7
I0909 14:34:40.689023  4341 net.cpp:290] fc8 -> fc8
I0909 14:34:40.696573  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.696584  4341 net.cpp:125] fc8 needs backward computation.
I0909 14:34:40.696591  4341 net.cpp:66] Creating Layer relu8
I0909 14:34:40.696596  4341 net.cpp:329] relu8 <- fc8
I0909 14:34:40.696604  4341 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:34:40.696611  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.696616  4341 net.cpp:125] relu8 needs backward computation.
I0909 14:34:40.696624  4341 net.cpp:66] Creating Layer drop8
I0909 14:34:40.696629  4341 net.cpp:329] drop8 <- fc8
I0909 14:34:40.696635  4341 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:34:40.696640  4341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:34:40.696646  4341 net.cpp:125] drop8 needs backward computation.
I0909 14:34:40.696653  4341 net.cpp:66] Creating Layer fc9
I0909 14:34:40.696658  4341 net.cpp:329] fc9 <- fc8
I0909 14:34:40.696666  4341 net.cpp:290] fc9 -> fc9
I0909 14:34:40.697027  4341 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:34:40.697039  4341 net.cpp:125] fc9 needs backward computation.
I0909 14:34:40.697047  4341 net.cpp:66] Creating Layer fc10
I0909 14:34:40.697052  4341 net.cpp:329] fc10 <- fc9
I0909 14:34:40.697059  4341 net.cpp:290] fc10 -> fc10
I0909 14:34:40.697072  4341 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:34:40.697079  4341 net.cpp:125] fc10 needs backward computation.
I0909 14:34:40.697085  4341 net.cpp:66] Creating Layer prob
I0909 14:34:40.697091  4341 net.cpp:329] prob <- fc10
I0909 14:34:40.697098  4341 net.cpp:290] prob -> prob
I0909 14:34:40.697106  4341 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:34:40.697113  4341 net.cpp:125] prob needs backward computation.
I0909 14:34:40.697118  4341 net.cpp:156] This network produces output prob
I0909 14:34:40.697129  4341 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:34:40.697136  4341 net.cpp:167] Network initialization done.
I0909 14:34:40.697141  4341 net.cpp:168] Memory required for data: 6183480
Classifying 229 inputs.
Done in 131.88 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:36:58.734907  4348 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:36:58.735044  4348 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:36:58.735052  4348 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:36:58.735196  4348 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:36:58.735257  4348 net.cpp:292] Input 0 -> data
I0909 14:36:58.735283  4348 net.cpp:66] Creating Layer conv1
I0909 14:36:58.735290  4348 net.cpp:329] conv1 <- data
I0909 14:36:58.735297  4348 net.cpp:290] conv1 -> conv1
I0909 14:36:58.736621  4348 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:36:58.736639  4348 net.cpp:125] conv1 needs backward computation.
I0909 14:36:58.736647  4348 net.cpp:66] Creating Layer relu1
I0909 14:36:58.736654  4348 net.cpp:329] relu1 <- conv1
I0909 14:36:58.736660  4348 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:36:58.736668  4348 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:36:58.736673  4348 net.cpp:125] relu1 needs backward computation.
I0909 14:36:58.736680  4348 net.cpp:66] Creating Layer pool1
I0909 14:36:58.736685  4348 net.cpp:329] pool1 <- conv1
I0909 14:36:58.736692  4348 net.cpp:290] pool1 -> pool1
I0909 14:36:58.736702  4348 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:36:58.736708  4348 net.cpp:125] pool1 needs backward computation.
I0909 14:36:58.736716  4348 net.cpp:66] Creating Layer norm1
I0909 14:36:58.736721  4348 net.cpp:329] norm1 <- pool1
I0909 14:36:58.736726  4348 net.cpp:290] norm1 -> norm1
I0909 14:36:58.736740  4348 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:36:58.736747  4348 net.cpp:125] norm1 needs backward computation.
I0909 14:36:58.736754  4348 net.cpp:66] Creating Layer conv2
I0909 14:36:58.736759  4348 net.cpp:329] conv2 <- norm1
I0909 14:36:58.736767  4348 net.cpp:290] conv2 -> conv2
I0909 14:36:58.745647  4348 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:36:58.745662  4348 net.cpp:125] conv2 needs backward computation.
I0909 14:36:58.745669  4348 net.cpp:66] Creating Layer relu2
I0909 14:36:58.745674  4348 net.cpp:329] relu2 <- conv2
I0909 14:36:58.745681  4348 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:36:58.745687  4348 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:36:58.745693  4348 net.cpp:125] relu2 needs backward computation.
I0909 14:36:58.745698  4348 net.cpp:66] Creating Layer pool2
I0909 14:36:58.745704  4348 net.cpp:329] pool2 <- conv2
I0909 14:36:58.745710  4348 net.cpp:290] pool2 -> pool2
I0909 14:36:58.745718  4348 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:36:58.745723  4348 net.cpp:125] pool2 needs backward computation.
I0909 14:36:58.745730  4348 net.cpp:66] Creating Layer fc7
I0909 14:36:58.745736  4348 net.cpp:329] fc7 <- pool2
I0909 14:36:58.745743  4348 net.cpp:290] fc7 -> fc7
I0909 14:36:59.370429  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.370477  4348 net.cpp:125] fc7 needs backward computation.
I0909 14:36:59.370491  4348 net.cpp:66] Creating Layer relu7
I0909 14:36:59.370497  4348 net.cpp:329] relu7 <- fc7
I0909 14:36:59.370506  4348 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:36:59.370514  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.370520  4348 net.cpp:125] relu7 needs backward computation.
I0909 14:36:59.370527  4348 net.cpp:66] Creating Layer drop7
I0909 14:36:59.370532  4348 net.cpp:329] drop7 <- fc7
I0909 14:36:59.370540  4348 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:36:59.370551  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.370558  4348 net.cpp:125] drop7 needs backward computation.
I0909 14:36:59.370565  4348 net.cpp:66] Creating Layer fc8
I0909 14:36:59.370571  4348 net.cpp:329] fc8 <- fc7
I0909 14:36:59.370579  4348 net.cpp:290] fc8 -> fc8
I0909 14:36:59.378128  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.378139  4348 net.cpp:125] fc8 needs backward computation.
I0909 14:36:59.378146  4348 net.cpp:66] Creating Layer relu8
I0909 14:36:59.378151  4348 net.cpp:329] relu8 <- fc8
I0909 14:36:59.378160  4348 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:36:59.378168  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.378173  4348 net.cpp:125] relu8 needs backward computation.
I0909 14:36:59.378180  4348 net.cpp:66] Creating Layer drop8
I0909 14:36:59.378185  4348 net.cpp:329] drop8 <- fc8
I0909 14:36:59.378191  4348 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:36:59.378197  4348 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:36:59.378203  4348 net.cpp:125] drop8 needs backward computation.
I0909 14:36:59.378211  4348 net.cpp:66] Creating Layer fc9
I0909 14:36:59.378216  4348 net.cpp:329] fc9 <- fc8
I0909 14:36:59.378223  4348 net.cpp:290] fc9 -> fc9
I0909 14:36:59.378584  4348 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:36:59.378597  4348 net.cpp:125] fc9 needs backward computation.
I0909 14:36:59.378604  4348 net.cpp:66] Creating Layer fc10
I0909 14:36:59.378610  4348 net.cpp:329] fc10 <- fc9
I0909 14:36:59.378618  4348 net.cpp:290] fc10 -> fc10
I0909 14:36:59.378636  4348 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:36:59.378644  4348 net.cpp:125] fc10 needs backward computation.
I0909 14:36:59.378651  4348 net.cpp:66] Creating Layer prob
I0909 14:36:59.378664  4348 net.cpp:329] prob <- fc10
I0909 14:36:59.378671  4348 net.cpp:290] prob -> prob
I0909 14:36:59.378680  4348 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:36:59.378686  4348 net.cpp:125] prob needs backward computation.
I0909 14:36:59.378690  4348 net.cpp:156] This network produces output prob
I0909 14:36:59.378701  4348 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:36:59.378720  4348 net.cpp:167] Network initialization done.
I0909 14:36:59.378726  4348 net.cpp:168] Memory required for data: 6183480
Classifying 49 inputs.
Done in 30.48 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:37:32.922338  4367 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:37:32.922472  4367 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:37:32.922480  4367 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:37:32.922624  4367 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:37:32.922674  4367 net.cpp:292] Input 0 -> data
I0909 14:37:32.922698  4367 net.cpp:66] Creating Layer conv1
I0909 14:37:32.922716  4367 net.cpp:329] conv1 <- data
I0909 14:37:32.922725  4367 net.cpp:290] conv1 -> conv1
I0909 14:37:32.924047  4367 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:37:32.924065  4367 net.cpp:125] conv1 needs backward computation.
I0909 14:37:32.924073  4367 net.cpp:66] Creating Layer relu1
I0909 14:37:32.924079  4367 net.cpp:329] relu1 <- conv1
I0909 14:37:32.924087  4367 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:37:32.924094  4367 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:37:32.924100  4367 net.cpp:125] relu1 needs backward computation.
I0909 14:37:32.924108  4367 net.cpp:66] Creating Layer pool1
I0909 14:37:32.924113  4367 net.cpp:329] pool1 <- conv1
I0909 14:37:32.924118  4367 net.cpp:290] pool1 -> pool1
I0909 14:37:32.924129  4367 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:37:32.924135  4367 net.cpp:125] pool1 needs backward computation.
I0909 14:37:32.924141  4367 net.cpp:66] Creating Layer norm1
I0909 14:37:32.924147  4367 net.cpp:329] norm1 <- pool1
I0909 14:37:32.924154  4367 net.cpp:290] norm1 -> norm1
I0909 14:37:32.924162  4367 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:37:32.924168  4367 net.cpp:125] norm1 needs backward computation.
I0909 14:37:32.924175  4367 net.cpp:66] Creating Layer conv2
I0909 14:37:32.924180  4367 net.cpp:329] conv2 <- norm1
I0909 14:37:32.924187  4367 net.cpp:290] conv2 -> conv2
I0909 14:37:32.933060  4367 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:37:32.933074  4367 net.cpp:125] conv2 needs backward computation.
I0909 14:37:32.933081  4367 net.cpp:66] Creating Layer relu2
I0909 14:37:32.933087  4367 net.cpp:329] relu2 <- conv2
I0909 14:37:32.933094  4367 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:37:32.933100  4367 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:37:32.933105  4367 net.cpp:125] relu2 needs backward computation.
I0909 14:37:32.933111  4367 net.cpp:66] Creating Layer pool2
I0909 14:37:32.933116  4367 net.cpp:329] pool2 <- conv2
I0909 14:37:32.933123  4367 net.cpp:290] pool2 -> pool2
I0909 14:37:32.933130  4367 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:37:32.933136  4367 net.cpp:125] pool2 needs backward computation.
I0909 14:37:32.933142  4367 net.cpp:66] Creating Layer fc7
I0909 14:37:32.933148  4367 net.cpp:329] fc7 <- pool2
I0909 14:37:32.933154  4367 net.cpp:290] fc7 -> fc7
I0909 14:37:33.557771  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.557818  4367 net.cpp:125] fc7 needs backward computation.
I0909 14:37:33.557832  4367 net.cpp:66] Creating Layer relu7
I0909 14:37:33.557838  4367 net.cpp:329] relu7 <- fc7
I0909 14:37:33.557847  4367 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:37:33.557857  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.557862  4367 net.cpp:125] relu7 needs backward computation.
I0909 14:37:33.557869  4367 net.cpp:66] Creating Layer drop7
I0909 14:37:33.557875  4367 net.cpp:329] drop7 <- fc7
I0909 14:37:33.557883  4367 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:37:33.557894  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.557900  4367 net.cpp:125] drop7 needs backward computation.
I0909 14:37:33.557909  4367 net.cpp:66] Creating Layer fc8
I0909 14:37:33.557914  4367 net.cpp:329] fc8 <- fc7
I0909 14:37:33.557924  4367 net.cpp:290] fc8 -> fc8
I0909 14:37:33.565475  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.565489  4367 net.cpp:125] fc8 needs backward computation.
I0909 14:37:33.565495  4367 net.cpp:66] Creating Layer relu8
I0909 14:37:33.565501  4367 net.cpp:329] relu8 <- fc8
I0909 14:37:33.565507  4367 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:37:33.565521  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.565526  4367 net.cpp:125] relu8 needs backward computation.
I0909 14:37:33.565532  4367 net.cpp:66] Creating Layer drop8
I0909 14:37:33.565538  4367 net.cpp:329] drop8 <- fc8
I0909 14:37:33.565544  4367 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:37:33.565551  4367 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:37:33.565557  4367 net.cpp:125] drop8 needs backward computation.
I0909 14:37:33.565575  4367 net.cpp:66] Creating Layer fc9
I0909 14:37:33.565582  4367 net.cpp:329] fc9 <- fc8
I0909 14:37:33.565589  4367 net.cpp:290] fc9 -> fc9
I0909 14:37:33.565951  4367 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:37:33.565963  4367 net.cpp:125] fc9 needs backward computation.
I0909 14:37:33.565973  4367 net.cpp:66] Creating Layer fc10
I0909 14:37:33.565978  4367 net.cpp:329] fc10 <- fc9
I0909 14:37:33.565984  4367 net.cpp:290] fc10 -> fc10
I0909 14:37:33.565996  4367 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:37:33.566004  4367 net.cpp:125] fc10 needs backward computation.
I0909 14:37:33.566011  4367 net.cpp:66] Creating Layer prob
I0909 14:37:33.566016  4367 net.cpp:329] prob <- fc10
I0909 14:37:33.566022  4367 net.cpp:290] prob -> prob
I0909 14:37:33.566031  4367 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:37:33.566040  4367 net.cpp:125] prob needs backward computation.
I0909 14:37:33.566043  4367 net.cpp:156] This network produces output prob
I0909 14:37:33.566054  4367 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:37:33.566062  4367 net.cpp:167] Network initialization done.
I0909 14:37:33.566067  4367 net.cpp:168] Memory required for data: 6183480
Classifying 305 inputs.
Done in 195.97 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:40:56.102108  4386 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:40:56.102244  4386 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:40:56.102253  4386 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:40:56.102396  4386 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:40:56.102458  4386 net.cpp:292] Input 0 -> data
I0909 14:40:56.102483  4386 net.cpp:66] Creating Layer conv1
I0909 14:40:56.102490  4386 net.cpp:329] conv1 <- data
I0909 14:40:56.102498  4386 net.cpp:290] conv1 -> conv1
I0909 14:40:56.103837  4386 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:40:56.103854  4386 net.cpp:125] conv1 needs backward computation.
I0909 14:40:56.103863  4386 net.cpp:66] Creating Layer relu1
I0909 14:40:56.103869  4386 net.cpp:329] relu1 <- conv1
I0909 14:40:56.103876  4386 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:40:56.103884  4386 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:40:56.103890  4386 net.cpp:125] relu1 needs backward computation.
I0909 14:40:56.103896  4386 net.cpp:66] Creating Layer pool1
I0909 14:40:56.103903  4386 net.cpp:329] pool1 <- conv1
I0909 14:40:56.103909  4386 net.cpp:290] pool1 -> pool1
I0909 14:40:56.103919  4386 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:40:56.103925  4386 net.cpp:125] pool1 needs backward computation.
I0909 14:40:56.103932  4386 net.cpp:66] Creating Layer norm1
I0909 14:40:56.103937  4386 net.cpp:329] norm1 <- pool1
I0909 14:40:56.103943  4386 net.cpp:290] norm1 -> norm1
I0909 14:40:56.103953  4386 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:40:56.103958  4386 net.cpp:125] norm1 needs backward computation.
I0909 14:40:56.103965  4386 net.cpp:66] Creating Layer conv2
I0909 14:40:56.103971  4386 net.cpp:329] conv2 <- norm1
I0909 14:40:56.103978  4386 net.cpp:290] conv2 -> conv2
I0909 14:40:56.112897  4386 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:40:56.112911  4386 net.cpp:125] conv2 needs backward computation.
I0909 14:40:56.112918  4386 net.cpp:66] Creating Layer relu2
I0909 14:40:56.112925  4386 net.cpp:329] relu2 <- conv2
I0909 14:40:56.112931  4386 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:40:56.112937  4386 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:40:56.112942  4386 net.cpp:125] relu2 needs backward computation.
I0909 14:40:56.112948  4386 net.cpp:66] Creating Layer pool2
I0909 14:40:56.112953  4386 net.cpp:329] pool2 <- conv2
I0909 14:40:56.112961  4386 net.cpp:290] pool2 -> pool2
I0909 14:40:56.112967  4386 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:40:56.112973  4386 net.cpp:125] pool2 needs backward computation.
I0909 14:40:56.112980  4386 net.cpp:66] Creating Layer fc7
I0909 14:40:56.112985  4386 net.cpp:329] fc7 <- pool2
I0909 14:40:56.112993  4386 net.cpp:290] fc7 -> fc7
I0909 14:40:56.737648  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.737695  4386 net.cpp:125] fc7 needs backward computation.
I0909 14:40:56.737707  4386 net.cpp:66] Creating Layer relu7
I0909 14:40:56.737715  4386 net.cpp:329] relu7 <- fc7
I0909 14:40:56.737722  4386 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:40:56.737731  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.737737  4386 net.cpp:125] relu7 needs backward computation.
I0909 14:40:56.737745  4386 net.cpp:66] Creating Layer drop7
I0909 14:40:56.737751  4386 net.cpp:329] drop7 <- fc7
I0909 14:40:56.737757  4386 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:40:56.737768  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.737786  4386 net.cpp:125] drop7 needs backward computation.
I0909 14:40:56.737794  4386 net.cpp:66] Creating Layer fc8
I0909 14:40:56.737799  4386 net.cpp:329] fc8 <- fc7
I0909 14:40:56.737808  4386 net.cpp:290] fc8 -> fc8
I0909 14:40:56.745357  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.745368  4386 net.cpp:125] fc8 needs backward computation.
I0909 14:40:56.745375  4386 net.cpp:66] Creating Layer relu8
I0909 14:40:56.745381  4386 net.cpp:329] relu8 <- fc8
I0909 14:40:56.745389  4386 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:40:56.745396  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.745401  4386 net.cpp:125] relu8 needs backward computation.
I0909 14:40:56.745407  4386 net.cpp:66] Creating Layer drop8
I0909 14:40:56.745414  4386 net.cpp:329] drop8 <- fc8
I0909 14:40:56.745419  4386 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:40:56.745425  4386 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:40:56.745431  4386 net.cpp:125] drop8 needs backward computation.
I0909 14:40:56.745439  4386 net.cpp:66] Creating Layer fc9
I0909 14:40:56.745443  4386 net.cpp:329] fc9 <- fc8
I0909 14:40:56.745451  4386 net.cpp:290] fc9 -> fc9
I0909 14:40:56.745817  4386 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:40:56.745831  4386 net.cpp:125] fc9 needs backward computation.
I0909 14:40:56.745839  4386 net.cpp:66] Creating Layer fc10
I0909 14:40:56.745844  4386 net.cpp:329] fc10 <- fc9
I0909 14:40:56.745851  4386 net.cpp:290] fc10 -> fc10
I0909 14:40:56.745864  4386 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:40:56.745872  4386 net.cpp:125] fc10 needs backward computation.
I0909 14:40:56.745878  4386 net.cpp:66] Creating Layer prob
I0909 14:40:56.745883  4386 net.cpp:329] prob <- fc10
I0909 14:40:56.745892  4386 net.cpp:290] prob -> prob
I0909 14:40:56.745900  4386 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:40:56.745906  4386 net.cpp:125] prob needs backward computation.
I0909 14:40:56.745911  4386 net.cpp:156] This network produces output prob
I0909 14:40:56.745921  4386 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:40:56.745929  4386 net.cpp:167] Network initialization done.
I0909 14:40:56.745934  4386 net.cpp:168] Memory required for data: 6183480
Classifying 75 inputs.
Done in 45.02 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:41:44.163445  4390 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:41:44.163583  4390 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:41:44.163591  4390 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:41:44.163735  4390 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:41:44.163796  4390 net.cpp:292] Input 0 -> data
I0909 14:41:44.163822  4390 net.cpp:66] Creating Layer conv1
I0909 14:41:44.163830  4390 net.cpp:329] conv1 <- data
I0909 14:41:44.163837  4390 net.cpp:290] conv1 -> conv1
I0909 14:41:44.165161  4390 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:41:44.165179  4390 net.cpp:125] conv1 needs backward computation.
I0909 14:41:44.165189  4390 net.cpp:66] Creating Layer relu1
I0909 14:41:44.165194  4390 net.cpp:329] relu1 <- conv1
I0909 14:41:44.165200  4390 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:41:44.165209  4390 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:41:44.165215  4390 net.cpp:125] relu1 needs backward computation.
I0909 14:41:44.165220  4390 net.cpp:66] Creating Layer pool1
I0909 14:41:44.165226  4390 net.cpp:329] pool1 <- conv1
I0909 14:41:44.165232  4390 net.cpp:290] pool1 -> pool1
I0909 14:41:44.165243  4390 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:41:44.165248  4390 net.cpp:125] pool1 needs backward computation.
I0909 14:41:44.165256  4390 net.cpp:66] Creating Layer norm1
I0909 14:41:44.165261  4390 net.cpp:329] norm1 <- pool1
I0909 14:41:44.165267  4390 net.cpp:290] norm1 -> norm1
I0909 14:41:44.165277  4390 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:41:44.165282  4390 net.cpp:125] norm1 needs backward computation.
I0909 14:41:44.165288  4390 net.cpp:66] Creating Layer conv2
I0909 14:41:44.165294  4390 net.cpp:329] conv2 <- norm1
I0909 14:41:44.165302  4390 net.cpp:290] conv2 -> conv2
I0909 14:41:44.174202  4390 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:41:44.174217  4390 net.cpp:125] conv2 needs backward computation.
I0909 14:41:44.174224  4390 net.cpp:66] Creating Layer relu2
I0909 14:41:44.174229  4390 net.cpp:329] relu2 <- conv2
I0909 14:41:44.174237  4390 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:41:44.174243  4390 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:41:44.174248  4390 net.cpp:125] relu2 needs backward computation.
I0909 14:41:44.174254  4390 net.cpp:66] Creating Layer pool2
I0909 14:41:44.174259  4390 net.cpp:329] pool2 <- conv2
I0909 14:41:44.174270  4390 net.cpp:290] pool2 -> pool2
I0909 14:41:44.174278  4390 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:41:44.174284  4390 net.cpp:125] pool2 needs backward computation.
I0909 14:41:44.174291  4390 net.cpp:66] Creating Layer fc7
I0909 14:41:44.174296  4390 net.cpp:329] fc7 <- pool2
I0909 14:41:44.174304  4390 net.cpp:290] fc7 -> fc7
I0909 14:41:44.798620  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.798668  4390 net.cpp:125] fc7 needs backward computation.
I0909 14:41:44.798681  4390 net.cpp:66] Creating Layer relu7
I0909 14:41:44.798688  4390 net.cpp:329] relu7 <- fc7
I0909 14:41:44.798696  4390 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:41:44.798706  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.798710  4390 net.cpp:125] relu7 needs backward computation.
I0909 14:41:44.798717  4390 net.cpp:66] Creating Layer drop7
I0909 14:41:44.798723  4390 net.cpp:329] drop7 <- fc7
I0909 14:41:44.798730  4390 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:41:44.798741  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.798748  4390 net.cpp:125] drop7 needs backward computation.
I0909 14:41:44.798755  4390 net.cpp:66] Creating Layer fc8
I0909 14:41:44.798760  4390 net.cpp:329] fc8 <- fc7
I0909 14:41:44.798769  4390 net.cpp:290] fc8 -> fc8
I0909 14:41:44.806340  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.806352  4390 net.cpp:125] fc8 needs backward computation.
I0909 14:41:44.806360  4390 net.cpp:66] Creating Layer relu8
I0909 14:41:44.806365  4390 net.cpp:329] relu8 <- fc8
I0909 14:41:44.806373  4390 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:41:44.806380  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.806385  4390 net.cpp:125] relu8 needs backward computation.
I0909 14:41:44.806392  4390 net.cpp:66] Creating Layer drop8
I0909 14:41:44.806397  4390 net.cpp:329] drop8 <- fc8
I0909 14:41:44.806403  4390 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:41:44.806409  4390 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:44.806416  4390 net.cpp:125] drop8 needs backward computation.
I0909 14:41:44.806422  4390 net.cpp:66] Creating Layer fc9
I0909 14:41:44.806427  4390 net.cpp:329] fc9 <- fc8
I0909 14:41:44.806435  4390 net.cpp:290] fc9 -> fc9
I0909 14:41:44.806797  4390 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:41:44.806809  4390 net.cpp:125] fc9 needs backward computation.
I0909 14:41:44.806818  4390 net.cpp:66] Creating Layer fc10
I0909 14:41:44.806823  4390 net.cpp:329] fc10 <- fc9
I0909 14:41:44.806829  4390 net.cpp:290] fc10 -> fc10
I0909 14:41:44.806843  4390 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:41:44.806850  4390 net.cpp:125] fc10 needs backward computation.
I0909 14:41:44.806856  4390 net.cpp:66] Creating Layer prob
I0909 14:41:44.806862  4390 net.cpp:329] prob <- fc10
I0909 14:41:44.806869  4390 net.cpp:290] prob -> prob
I0909 14:41:44.806879  4390 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:41:44.806885  4390 net.cpp:125] prob needs backward computation.
I0909 14:41:44.806890  4390 net.cpp:156] This network produces output prob
I0909 14:41:44.806900  4390 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:41:44.806908  4390 net.cpp:167] Network initialization done.
I0909 14:41:44.806913  4390 net.cpp:168] Memory required for data: 6183480
Classifying 12 inputs.
Done in 7.48 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:41:53.798022  4393 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:41:53.798158  4393 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:41:53.798167  4393 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:41:53.798310  4393 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:41:53.798372  4393 net.cpp:292] Input 0 -> data
I0909 14:41:53.798396  4393 net.cpp:66] Creating Layer conv1
I0909 14:41:53.798403  4393 net.cpp:329] conv1 <- data
I0909 14:41:53.798410  4393 net.cpp:290] conv1 -> conv1
I0909 14:41:53.799736  4393 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:41:53.799753  4393 net.cpp:125] conv1 needs backward computation.
I0909 14:41:53.799762  4393 net.cpp:66] Creating Layer relu1
I0909 14:41:53.799768  4393 net.cpp:329] relu1 <- conv1
I0909 14:41:53.799775  4393 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:41:53.799783  4393 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:41:53.799789  4393 net.cpp:125] relu1 needs backward computation.
I0909 14:41:53.799795  4393 net.cpp:66] Creating Layer pool1
I0909 14:41:53.799801  4393 net.cpp:329] pool1 <- conv1
I0909 14:41:53.799808  4393 net.cpp:290] pool1 -> pool1
I0909 14:41:53.799818  4393 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:41:53.799823  4393 net.cpp:125] pool1 needs backward computation.
I0909 14:41:53.799830  4393 net.cpp:66] Creating Layer norm1
I0909 14:41:53.799839  4393 net.cpp:329] norm1 <- pool1
I0909 14:41:53.799846  4393 net.cpp:290] norm1 -> norm1
I0909 14:41:53.799856  4393 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:41:53.799862  4393 net.cpp:125] norm1 needs backward computation.
I0909 14:41:53.799870  4393 net.cpp:66] Creating Layer conv2
I0909 14:41:53.799875  4393 net.cpp:329] conv2 <- norm1
I0909 14:41:53.799881  4393 net.cpp:290] conv2 -> conv2
I0909 14:41:53.808766  4393 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:41:53.808780  4393 net.cpp:125] conv2 needs backward computation.
I0909 14:41:53.808787  4393 net.cpp:66] Creating Layer relu2
I0909 14:41:53.808794  4393 net.cpp:329] relu2 <- conv2
I0909 14:41:53.808799  4393 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:41:53.808806  4393 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:41:53.808812  4393 net.cpp:125] relu2 needs backward computation.
I0909 14:41:53.808818  4393 net.cpp:66] Creating Layer pool2
I0909 14:41:53.808823  4393 net.cpp:329] pool2 <- conv2
I0909 14:41:53.808830  4393 net.cpp:290] pool2 -> pool2
I0909 14:41:53.808837  4393 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:41:53.808842  4393 net.cpp:125] pool2 needs backward computation.
I0909 14:41:53.808850  4393 net.cpp:66] Creating Layer fc7
I0909 14:41:53.808854  4393 net.cpp:329] fc7 <- pool2
I0909 14:41:53.808861  4393 net.cpp:290] fc7 -> fc7
I0909 14:41:54.437407  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.437450  4393 net.cpp:125] fc7 needs backward computation.
I0909 14:41:54.437463  4393 net.cpp:66] Creating Layer relu7
I0909 14:41:54.437469  4393 net.cpp:329] relu7 <- fc7
I0909 14:41:54.437475  4393 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:41:54.437485  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.437490  4393 net.cpp:125] relu7 needs backward computation.
I0909 14:41:54.437497  4393 net.cpp:66] Creating Layer drop7
I0909 14:41:54.437502  4393 net.cpp:329] drop7 <- fc7
I0909 14:41:54.437517  4393 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:41:54.437530  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.437536  4393 net.cpp:125] drop7 needs backward computation.
I0909 14:41:54.437544  4393 net.cpp:66] Creating Layer fc8
I0909 14:41:54.437549  4393 net.cpp:329] fc8 <- fc7
I0909 14:41:54.437559  4393 net.cpp:290] fc8 -> fc8
I0909 14:41:54.445154  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.445166  4393 net.cpp:125] fc8 needs backward computation.
I0909 14:41:54.445173  4393 net.cpp:66] Creating Layer relu8
I0909 14:41:54.445178  4393 net.cpp:329] relu8 <- fc8
I0909 14:41:54.445186  4393 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:41:54.445194  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.445199  4393 net.cpp:125] relu8 needs backward computation.
I0909 14:41:54.445205  4393 net.cpp:66] Creating Layer drop8
I0909 14:41:54.445210  4393 net.cpp:329] drop8 <- fc8
I0909 14:41:54.445216  4393 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:41:54.445222  4393 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:41:54.445229  4393 net.cpp:125] drop8 needs backward computation.
I0909 14:41:54.445235  4393 net.cpp:66] Creating Layer fc9
I0909 14:41:54.445240  4393 net.cpp:329] fc9 <- fc8
I0909 14:41:54.445248  4393 net.cpp:290] fc9 -> fc9
I0909 14:41:54.445616  4393 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:41:54.445629  4393 net.cpp:125] fc9 needs backward computation.
I0909 14:41:54.445637  4393 net.cpp:66] Creating Layer fc10
I0909 14:41:54.445643  4393 net.cpp:329] fc10 <- fc9
I0909 14:41:54.445651  4393 net.cpp:290] fc10 -> fc10
I0909 14:41:54.445662  4393 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:41:54.445670  4393 net.cpp:125] fc10 needs backward computation.
I0909 14:41:54.445677  4393 net.cpp:66] Creating Layer prob
I0909 14:41:54.445683  4393 net.cpp:329] prob <- fc10
I0909 14:41:54.445689  4393 net.cpp:290] prob -> prob
I0909 14:41:54.445699  4393 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:41:54.445704  4393 net.cpp:125] prob needs backward computation.
I0909 14:41:54.445709  4393 net.cpp:156] This network produces output prob
I0909 14:41:54.445729  4393 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:41:54.445737  4393 net.cpp:167] Network initialization done.
I0909 14:41:54.445742  4393 net.cpp:168] Memory required for data: 6183480
Classifying 82 inputs.
Done in 51.22 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:42:49.134321  4397 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:42:49.134457  4397 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:42:49.134466  4397 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:42:49.134609  4397 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:42:49.134660  4397 net.cpp:292] Input 0 -> data
I0909 14:42:49.134697  4397 net.cpp:66] Creating Layer conv1
I0909 14:42:49.134704  4397 net.cpp:329] conv1 <- data
I0909 14:42:49.134712  4397 net.cpp:290] conv1 -> conv1
I0909 14:42:49.136037  4397 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:42:49.136055  4397 net.cpp:125] conv1 needs backward computation.
I0909 14:42:49.136065  4397 net.cpp:66] Creating Layer relu1
I0909 14:42:49.136070  4397 net.cpp:329] relu1 <- conv1
I0909 14:42:49.136076  4397 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:42:49.136085  4397 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:42:49.136091  4397 net.cpp:125] relu1 needs backward computation.
I0909 14:42:49.136098  4397 net.cpp:66] Creating Layer pool1
I0909 14:42:49.136103  4397 net.cpp:329] pool1 <- conv1
I0909 14:42:49.136111  4397 net.cpp:290] pool1 -> pool1
I0909 14:42:49.136121  4397 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:42:49.136127  4397 net.cpp:125] pool1 needs backward computation.
I0909 14:42:49.136133  4397 net.cpp:66] Creating Layer norm1
I0909 14:42:49.136139  4397 net.cpp:329] norm1 <- pool1
I0909 14:42:49.136145  4397 net.cpp:290] norm1 -> norm1
I0909 14:42:49.136155  4397 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:42:49.136160  4397 net.cpp:125] norm1 needs backward computation.
I0909 14:42:49.136168  4397 net.cpp:66] Creating Layer conv2
I0909 14:42:49.136173  4397 net.cpp:329] conv2 <- norm1
I0909 14:42:49.136180  4397 net.cpp:290] conv2 -> conv2
I0909 14:42:49.145057  4397 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:42:49.145071  4397 net.cpp:125] conv2 needs backward computation.
I0909 14:42:49.145078  4397 net.cpp:66] Creating Layer relu2
I0909 14:42:49.145083  4397 net.cpp:329] relu2 <- conv2
I0909 14:42:49.145090  4397 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:42:49.145097  4397 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:42:49.145102  4397 net.cpp:125] relu2 needs backward computation.
I0909 14:42:49.145108  4397 net.cpp:66] Creating Layer pool2
I0909 14:42:49.145113  4397 net.cpp:329] pool2 <- conv2
I0909 14:42:49.145119  4397 net.cpp:290] pool2 -> pool2
I0909 14:42:49.145128  4397 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:42:49.145133  4397 net.cpp:125] pool2 needs backward computation.
I0909 14:42:49.145139  4397 net.cpp:66] Creating Layer fc7
I0909 14:42:49.145145  4397 net.cpp:329] fc7 <- pool2
I0909 14:42:49.145153  4397 net.cpp:290] fc7 -> fc7
I0909 14:42:49.769683  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.769732  4397 net.cpp:125] fc7 needs backward computation.
I0909 14:42:49.769744  4397 net.cpp:66] Creating Layer relu7
I0909 14:42:49.769752  4397 net.cpp:329] relu7 <- fc7
I0909 14:42:49.769759  4397 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:42:49.769768  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.769774  4397 net.cpp:125] relu7 needs backward computation.
I0909 14:42:49.769781  4397 net.cpp:66] Creating Layer drop7
I0909 14:42:49.769786  4397 net.cpp:329] drop7 <- fc7
I0909 14:42:49.769794  4397 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:42:49.769805  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.769811  4397 net.cpp:125] drop7 needs backward computation.
I0909 14:42:49.769819  4397 net.cpp:66] Creating Layer fc8
I0909 14:42:49.769824  4397 net.cpp:329] fc8 <- fc7
I0909 14:42:49.769834  4397 net.cpp:290] fc8 -> fc8
I0909 14:42:49.777385  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.777397  4397 net.cpp:125] fc8 needs backward computation.
I0909 14:42:49.777405  4397 net.cpp:66] Creating Layer relu8
I0909 14:42:49.777410  4397 net.cpp:329] relu8 <- fc8
I0909 14:42:49.777416  4397 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:42:49.777425  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.777429  4397 net.cpp:125] relu8 needs backward computation.
I0909 14:42:49.777436  4397 net.cpp:66] Creating Layer drop8
I0909 14:42:49.777441  4397 net.cpp:329] drop8 <- fc8
I0909 14:42:49.777447  4397 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:42:49.777454  4397 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:42:49.777469  4397 net.cpp:125] drop8 needs backward computation.
I0909 14:42:49.777477  4397 net.cpp:66] Creating Layer fc9
I0909 14:42:49.777482  4397 net.cpp:329] fc9 <- fc8
I0909 14:42:49.777493  4397 net.cpp:290] fc9 -> fc9
I0909 14:42:49.777858  4397 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:42:49.777869  4397 net.cpp:125] fc9 needs backward computation.
I0909 14:42:49.777878  4397 net.cpp:66] Creating Layer fc10
I0909 14:42:49.777884  4397 net.cpp:329] fc10 <- fc9
I0909 14:42:49.777890  4397 net.cpp:290] fc10 -> fc10
I0909 14:42:49.777902  4397 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:42:49.777910  4397 net.cpp:125] fc10 needs backward computation.
I0909 14:42:49.777917  4397 net.cpp:66] Creating Layer prob
I0909 14:42:49.777922  4397 net.cpp:329] prob <- fc10
I0909 14:42:49.777928  4397 net.cpp:290] prob -> prob
I0909 14:42:49.777937  4397 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:42:49.777945  4397 net.cpp:125] prob needs backward computation.
I0909 14:42:49.777951  4397 net.cpp:156] This network produces output prob
I0909 14:42:49.777961  4397 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:42:49.777968  4397 net.cpp:167] Network initialization done.
I0909 14:42:49.777973  4397 net.cpp:168] Memory required for data: 6183480
Classifying 62 inputs.
Done in 37.37 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:43:29.833565  4401 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:43:29.833715  4401 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:43:29.833724  4401 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:43:29.833868  4401 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:43:29.833930  4401 net.cpp:292] Input 0 -> data
I0909 14:43:29.833956  4401 net.cpp:66] Creating Layer conv1
I0909 14:43:29.833962  4401 net.cpp:329] conv1 <- data
I0909 14:43:29.833971  4401 net.cpp:290] conv1 -> conv1
I0909 14:43:29.835294  4401 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:43:29.835314  4401 net.cpp:125] conv1 needs backward computation.
I0909 14:43:29.835321  4401 net.cpp:66] Creating Layer relu1
I0909 14:43:29.835327  4401 net.cpp:329] relu1 <- conv1
I0909 14:43:29.835335  4401 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:43:29.835342  4401 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:43:29.835348  4401 net.cpp:125] relu1 needs backward computation.
I0909 14:43:29.835355  4401 net.cpp:66] Creating Layer pool1
I0909 14:43:29.835361  4401 net.cpp:329] pool1 <- conv1
I0909 14:43:29.835367  4401 net.cpp:290] pool1 -> pool1
I0909 14:43:29.835378  4401 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:43:29.835383  4401 net.cpp:125] pool1 needs backward computation.
I0909 14:43:29.835391  4401 net.cpp:66] Creating Layer norm1
I0909 14:43:29.835396  4401 net.cpp:329] norm1 <- pool1
I0909 14:43:29.835402  4401 net.cpp:290] norm1 -> norm1
I0909 14:43:29.835412  4401 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:43:29.835417  4401 net.cpp:125] norm1 needs backward computation.
I0909 14:43:29.835425  4401 net.cpp:66] Creating Layer conv2
I0909 14:43:29.835430  4401 net.cpp:329] conv2 <- norm1
I0909 14:43:29.835438  4401 net.cpp:290] conv2 -> conv2
I0909 14:43:29.844315  4401 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:43:29.844329  4401 net.cpp:125] conv2 needs backward computation.
I0909 14:43:29.844336  4401 net.cpp:66] Creating Layer relu2
I0909 14:43:29.844342  4401 net.cpp:329] relu2 <- conv2
I0909 14:43:29.844348  4401 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:43:29.844355  4401 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:43:29.844362  4401 net.cpp:125] relu2 needs backward computation.
I0909 14:43:29.844367  4401 net.cpp:66] Creating Layer pool2
I0909 14:43:29.844373  4401 net.cpp:329] pool2 <- conv2
I0909 14:43:29.844379  4401 net.cpp:290] pool2 -> pool2
I0909 14:43:29.844388  4401 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:43:29.844393  4401 net.cpp:125] pool2 needs backward computation.
I0909 14:43:29.844400  4401 net.cpp:66] Creating Layer fc7
I0909 14:43:29.844405  4401 net.cpp:329] fc7 <- pool2
I0909 14:43:29.844413  4401 net.cpp:290] fc7 -> fc7
I0909 14:43:30.468829  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.468876  4401 net.cpp:125] fc7 needs backward computation.
I0909 14:43:30.468889  4401 net.cpp:66] Creating Layer relu7
I0909 14:43:30.468896  4401 net.cpp:329] relu7 <- fc7
I0909 14:43:30.468904  4401 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:43:30.468914  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.468919  4401 net.cpp:125] relu7 needs backward computation.
I0909 14:43:30.468927  4401 net.cpp:66] Creating Layer drop7
I0909 14:43:30.468932  4401 net.cpp:329] drop7 <- fc7
I0909 14:43:30.468951  4401 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:43:30.468962  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.468968  4401 net.cpp:125] drop7 needs backward computation.
I0909 14:43:30.468977  4401 net.cpp:66] Creating Layer fc8
I0909 14:43:30.468982  4401 net.cpp:329] fc8 <- fc7
I0909 14:43:30.468991  4401 net.cpp:290] fc8 -> fc8
I0909 14:43:30.476541  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.476552  4401 net.cpp:125] fc8 needs backward computation.
I0909 14:43:30.476559  4401 net.cpp:66] Creating Layer relu8
I0909 14:43:30.476565  4401 net.cpp:329] relu8 <- fc8
I0909 14:43:30.476573  4401 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:43:30.476580  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.476586  4401 net.cpp:125] relu8 needs backward computation.
I0909 14:43:30.476593  4401 net.cpp:66] Creating Layer drop8
I0909 14:43:30.476598  4401 net.cpp:329] drop8 <- fc8
I0909 14:43:30.476604  4401 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:43:30.476611  4401 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:30.476616  4401 net.cpp:125] drop8 needs backward computation.
I0909 14:43:30.476624  4401 net.cpp:66] Creating Layer fc9
I0909 14:43:30.476629  4401 net.cpp:329] fc9 <- fc8
I0909 14:43:30.476637  4401 net.cpp:290] fc9 -> fc9
I0909 14:43:30.476999  4401 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:43:30.477012  4401 net.cpp:125] fc9 needs backward computation.
I0909 14:43:30.477020  4401 net.cpp:66] Creating Layer fc10
I0909 14:43:30.477025  4401 net.cpp:329] fc10 <- fc9
I0909 14:43:30.477032  4401 net.cpp:290] fc10 -> fc10
I0909 14:43:30.477046  4401 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:43:30.477053  4401 net.cpp:125] fc10 needs backward computation.
I0909 14:43:30.477061  4401 net.cpp:66] Creating Layer prob
I0909 14:43:30.477066  4401 net.cpp:329] prob <- fc10
I0909 14:43:30.477073  4401 net.cpp:290] prob -> prob
I0909 14:43:30.477082  4401 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:43:30.477088  4401 net.cpp:125] prob needs backward computation.
I0909 14:43:30.477093  4401 net.cpp:156] This network produces output prob
I0909 14:43:30.477104  4401 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:43:30.477113  4401 net.cpp:167] Network initialization done.
I0909 14:43:30.477118  4401 net.cpp:168] Memory required for data: 6183480
Classifying 110 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:43:34.334210  4404 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:43:34.334347  4404 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:43:34.334357  4404 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:43:34.334499  4404 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:43:34.334560  4404 net.cpp:292] Input 0 -> data
I0909 14:43:34.334588  4404 net.cpp:66] Creating Layer conv1
I0909 14:43:34.334594  4404 net.cpp:329] conv1 <- data
I0909 14:43:34.334601  4404 net.cpp:290] conv1 -> conv1
I0909 14:43:34.335925  4404 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:43:34.335943  4404 net.cpp:125] conv1 needs backward computation.
I0909 14:43:34.335952  4404 net.cpp:66] Creating Layer relu1
I0909 14:43:34.335958  4404 net.cpp:329] relu1 <- conv1
I0909 14:43:34.335964  4404 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:43:34.335973  4404 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:43:34.335979  4404 net.cpp:125] relu1 needs backward computation.
I0909 14:43:34.335985  4404 net.cpp:66] Creating Layer pool1
I0909 14:43:34.335991  4404 net.cpp:329] pool1 <- conv1
I0909 14:43:34.335997  4404 net.cpp:290] pool1 -> pool1
I0909 14:43:34.336009  4404 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:43:34.336014  4404 net.cpp:125] pool1 needs backward computation.
I0909 14:43:34.336021  4404 net.cpp:66] Creating Layer norm1
I0909 14:43:34.336026  4404 net.cpp:329] norm1 <- pool1
I0909 14:43:34.336033  4404 net.cpp:290] norm1 -> norm1
I0909 14:43:34.336042  4404 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:43:34.336048  4404 net.cpp:125] norm1 needs backward computation.
I0909 14:43:34.336055  4404 net.cpp:66] Creating Layer conv2
I0909 14:43:34.336061  4404 net.cpp:329] conv2 <- norm1
I0909 14:43:34.336068  4404 net.cpp:290] conv2 -> conv2
I0909 14:43:34.344974  4404 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:43:34.344993  4404 net.cpp:125] conv2 needs backward computation.
I0909 14:43:34.345000  4404 net.cpp:66] Creating Layer relu2
I0909 14:43:34.345006  4404 net.cpp:329] relu2 <- conv2
I0909 14:43:34.345012  4404 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:43:34.345021  4404 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:43:34.345026  4404 net.cpp:125] relu2 needs backward computation.
I0909 14:43:34.345031  4404 net.cpp:66] Creating Layer pool2
I0909 14:43:34.345036  4404 net.cpp:329] pool2 <- conv2
I0909 14:43:34.345043  4404 net.cpp:290] pool2 -> pool2
I0909 14:43:34.345052  4404 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:43:34.345057  4404 net.cpp:125] pool2 needs backward computation.
I0909 14:43:34.345064  4404 net.cpp:66] Creating Layer fc7
I0909 14:43:34.345069  4404 net.cpp:329] fc7 <- pool2
I0909 14:43:34.345077  4404 net.cpp:290] fc7 -> fc7
I0909 14:43:34.968765  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.968814  4404 net.cpp:125] fc7 needs backward computation.
I0909 14:43:34.968825  4404 net.cpp:66] Creating Layer relu7
I0909 14:43:34.968832  4404 net.cpp:329] relu7 <- fc7
I0909 14:43:34.968839  4404 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:43:34.968849  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.968855  4404 net.cpp:125] relu7 needs backward computation.
I0909 14:43:34.968863  4404 net.cpp:66] Creating Layer drop7
I0909 14:43:34.968868  4404 net.cpp:329] drop7 <- fc7
I0909 14:43:34.968875  4404 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:43:34.968886  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.968891  4404 net.cpp:125] drop7 needs backward computation.
I0909 14:43:34.968900  4404 net.cpp:66] Creating Layer fc8
I0909 14:43:34.968905  4404 net.cpp:329] fc8 <- fc7
I0909 14:43:34.968914  4404 net.cpp:290] fc8 -> fc8
I0909 14:43:34.976466  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.976479  4404 net.cpp:125] fc8 needs backward computation.
I0909 14:43:34.976485  4404 net.cpp:66] Creating Layer relu8
I0909 14:43:34.976491  4404 net.cpp:329] relu8 <- fc8
I0909 14:43:34.976497  4404 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:43:34.976505  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.976511  4404 net.cpp:125] relu8 needs backward computation.
I0909 14:43:34.976517  4404 net.cpp:66] Creating Layer drop8
I0909 14:43:34.976523  4404 net.cpp:329] drop8 <- fc8
I0909 14:43:34.976529  4404 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:43:34.976536  4404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:43:34.976541  4404 net.cpp:125] drop8 needs backward computation.
I0909 14:43:34.976548  4404 net.cpp:66] Creating Layer fc9
I0909 14:43:34.976553  4404 net.cpp:329] fc9 <- fc8
I0909 14:43:34.976562  4404 net.cpp:290] fc9 -> fc9
I0909 14:43:34.976925  4404 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:43:34.976938  4404 net.cpp:125] fc9 needs backward computation.
I0909 14:43:34.976946  4404 net.cpp:66] Creating Layer fc10
I0909 14:43:34.976951  4404 net.cpp:329] fc10 <- fc9
I0909 14:43:34.976958  4404 net.cpp:290] fc10 -> fc10
I0909 14:43:34.976970  4404 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:43:34.976979  4404 net.cpp:125] fc10 needs backward computation.
I0909 14:43:34.976985  4404 net.cpp:66] Creating Layer prob
I0909 14:43:34.976990  4404 net.cpp:329] prob <- fc10
I0909 14:43:34.976996  4404 net.cpp:290] prob -> prob
I0909 14:43:34.977006  4404 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:43:34.977013  4404 net.cpp:125] prob needs backward computation.
I0909 14:43:34.977018  4404 net.cpp:156] This network produces output prob
I0909 14:43:34.977030  4404 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:43:34.977037  4404 net.cpp:167] Network initialization done.
I0909 14:43:34.977043  4404 net.cpp:168] Memory required for data: 6183480
Classifying 271 inputs.
Done in 162.74 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:46:25.755475  4411 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:46:25.755623  4411 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:46:25.755632  4411 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:46:25.755775  4411 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:46:25.755825  4411 net.cpp:292] Input 0 -> data
I0909 14:46:25.755851  4411 net.cpp:66] Creating Layer conv1
I0909 14:46:25.755857  4411 net.cpp:329] conv1 <- data
I0909 14:46:25.755866  4411 net.cpp:290] conv1 -> conv1
I0909 14:46:25.757189  4411 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:46:25.757207  4411 net.cpp:125] conv1 needs backward computation.
I0909 14:46:25.757216  4411 net.cpp:66] Creating Layer relu1
I0909 14:46:25.757222  4411 net.cpp:329] relu1 <- conv1
I0909 14:46:25.757228  4411 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:46:25.757241  4411 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:46:25.757247  4411 net.cpp:125] relu1 needs backward computation.
I0909 14:46:25.757254  4411 net.cpp:66] Creating Layer pool1
I0909 14:46:25.757261  4411 net.cpp:329] pool1 <- conv1
I0909 14:46:25.757266  4411 net.cpp:290] pool1 -> pool1
I0909 14:46:25.757277  4411 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:46:25.757282  4411 net.cpp:125] pool1 needs backward computation.
I0909 14:46:25.757289  4411 net.cpp:66] Creating Layer norm1
I0909 14:46:25.757294  4411 net.cpp:329] norm1 <- pool1
I0909 14:46:25.757300  4411 net.cpp:290] norm1 -> norm1
I0909 14:46:25.757309  4411 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:46:25.757315  4411 net.cpp:125] norm1 needs backward computation.
I0909 14:46:25.757323  4411 net.cpp:66] Creating Layer conv2
I0909 14:46:25.757328  4411 net.cpp:329] conv2 <- norm1
I0909 14:46:25.757334  4411 net.cpp:290] conv2 -> conv2
I0909 14:46:25.766234  4411 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:46:25.766248  4411 net.cpp:125] conv2 needs backward computation.
I0909 14:46:25.766255  4411 net.cpp:66] Creating Layer relu2
I0909 14:46:25.766262  4411 net.cpp:329] relu2 <- conv2
I0909 14:46:25.766268  4411 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:46:25.766274  4411 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:46:25.766279  4411 net.cpp:125] relu2 needs backward computation.
I0909 14:46:25.766286  4411 net.cpp:66] Creating Layer pool2
I0909 14:46:25.766291  4411 net.cpp:329] pool2 <- conv2
I0909 14:46:25.766297  4411 net.cpp:290] pool2 -> pool2
I0909 14:46:25.766304  4411 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:46:25.766310  4411 net.cpp:125] pool2 needs backward computation.
I0909 14:46:25.766316  4411 net.cpp:66] Creating Layer fc7
I0909 14:46:25.766322  4411 net.cpp:329] fc7 <- pool2
I0909 14:46:25.766329  4411 net.cpp:290] fc7 -> fc7
I0909 14:46:26.392843  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.392889  4411 net.cpp:125] fc7 needs backward computation.
I0909 14:46:26.392900  4411 net.cpp:66] Creating Layer relu7
I0909 14:46:26.392907  4411 net.cpp:329] relu7 <- fc7
I0909 14:46:26.392915  4411 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:46:26.392925  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.392930  4411 net.cpp:125] relu7 needs backward computation.
I0909 14:46:26.392937  4411 net.cpp:66] Creating Layer drop7
I0909 14:46:26.392942  4411 net.cpp:329] drop7 <- fc7
I0909 14:46:26.392951  4411 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:46:26.392961  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.392966  4411 net.cpp:125] drop7 needs backward computation.
I0909 14:46:26.392974  4411 net.cpp:66] Creating Layer fc8
I0909 14:46:26.392979  4411 net.cpp:329] fc8 <- fc7
I0909 14:46:26.392988  4411 net.cpp:290] fc8 -> fc8
I0909 14:46:26.400557  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.400569  4411 net.cpp:125] fc8 needs backward computation.
I0909 14:46:26.400576  4411 net.cpp:66] Creating Layer relu8
I0909 14:46:26.400581  4411 net.cpp:329] relu8 <- fc8
I0909 14:46:26.400589  4411 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:46:26.400595  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.400601  4411 net.cpp:125] relu8 needs backward computation.
I0909 14:46:26.400607  4411 net.cpp:66] Creating Layer drop8
I0909 14:46:26.400612  4411 net.cpp:329] drop8 <- fc8
I0909 14:46:26.400619  4411 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:46:26.400625  4411 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:46:26.400630  4411 net.cpp:125] drop8 needs backward computation.
I0909 14:46:26.400638  4411 net.cpp:66] Creating Layer fc9
I0909 14:46:26.400643  4411 net.cpp:329] fc9 <- fc8
I0909 14:46:26.400651  4411 net.cpp:290] fc9 -> fc9
I0909 14:46:26.401013  4411 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:46:26.401026  4411 net.cpp:125] fc9 needs backward computation.
I0909 14:46:26.401033  4411 net.cpp:66] Creating Layer fc10
I0909 14:46:26.401038  4411 net.cpp:329] fc10 <- fc9
I0909 14:46:26.401056  4411 net.cpp:290] fc10 -> fc10
I0909 14:46:26.401068  4411 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:46:26.401077  4411 net.cpp:125] fc10 needs backward computation.
I0909 14:46:26.401083  4411 net.cpp:66] Creating Layer prob
I0909 14:46:26.401088  4411 net.cpp:329] prob <- fc10
I0909 14:46:26.401095  4411 net.cpp:290] prob -> prob
I0909 14:46:26.401105  4411 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:46:26.401110  4411 net.cpp:125] prob needs backward computation.
I0909 14:46:26.401115  4411 net.cpp:156] This network produces output prob
I0909 14:46:26.401126  4411 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:46:26.401134  4411 net.cpp:167] Network initialization done.
I0909 14:46:26.401139  4411 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:50:42.188380  4474 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:50:42.188720  4474 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:50:42.188740  4474 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:50:42.189056  4474 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:50:42.189174  4474 net.cpp:292] Input 0 -> data
I0909 14:50:42.189257  4474 net.cpp:66] Creating Layer conv1
I0909 14:50:42.189275  4474 net.cpp:329] conv1 <- data
I0909 14:50:42.189291  4474 net.cpp:290] conv1 -> conv1
I0909 14:50:42.221467  4474 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:50:42.221544  4474 net.cpp:125] conv1 needs backward computation.
I0909 14:50:42.221568  4474 net.cpp:66] Creating Layer relu1
I0909 14:50:42.221581  4474 net.cpp:329] relu1 <- conv1
I0909 14:50:42.221596  4474 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:50:42.221616  4474 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:50:42.221633  4474 net.cpp:125] relu1 needs backward computation.
I0909 14:50:42.221640  4474 net.cpp:66] Creating Layer pool1
I0909 14:50:42.221645  4474 net.cpp:329] pool1 <- conv1
I0909 14:50:42.221652  4474 net.cpp:290] pool1 -> pool1
I0909 14:50:42.221662  4474 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:50:42.221668  4474 net.cpp:125] pool1 needs backward computation.
I0909 14:50:42.221675  4474 net.cpp:66] Creating Layer norm1
I0909 14:50:42.221680  4474 net.cpp:329] norm1 <- pool1
I0909 14:50:42.221688  4474 net.cpp:290] norm1 -> norm1
I0909 14:50:42.221696  4474 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:50:42.221703  4474 net.cpp:125] norm1 needs backward computation.
I0909 14:50:42.221710  4474 net.cpp:66] Creating Layer conv2
I0909 14:50:42.221715  4474 net.cpp:329] conv2 <- norm1
I0909 14:50:42.221722  4474 net.cpp:290] conv2 -> conv2
I0909 14:50:42.230566  4474 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:50:42.230579  4474 net.cpp:125] conv2 needs backward computation.
I0909 14:50:42.230587  4474 net.cpp:66] Creating Layer relu2
I0909 14:50:42.230592  4474 net.cpp:329] relu2 <- conv2
I0909 14:50:42.230598  4474 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:50:42.230605  4474 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:50:42.230612  4474 net.cpp:125] relu2 needs backward computation.
I0909 14:50:42.230618  4474 net.cpp:66] Creating Layer pool2
I0909 14:50:42.230623  4474 net.cpp:329] pool2 <- conv2
I0909 14:50:42.230628  4474 net.cpp:290] pool2 -> pool2
I0909 14:50:42.230636  4474 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:50:42.230641  4474 net.cpp:125] pool2 needs backward computation.
I0909 14:50:42.230648  4474 net.cpp:66] Creating Layer fc7
I0909 14:50:42.230654  4474 net.cpp:329] fc7 <- pool2
I0909 14:50:42.230661  4474 net.cpp:290] fc7 -> fc7
I0909 14:50:42.855135  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.855182  4474 net.cpp:125] fc7 needs backward computation.
I0909 14:50:42.855195  4474 net.cpp:66] Creating Layer relu7
I0909 14:50:42.855202  4474 net.cpp:329] relu7 <- fc7
I0909 14:50:42.855209  4474 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:50:42.855219  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.855224  4474 net.cpp:125] relu7 needs backward computation.
I0909 14:50:42.855232  4474 net.cpp:66] Creating Layer drop7
I0909 14:50:42.855237  4474 net.cpp:329] drop7 <- fc7
I0909 14:50:42.855245  4474 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:50:42.855255  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.855262  4474 net.cpp:125] drop7 needs backward computation.
I0909 14:50:42.855269  4474 net.cpp:66] Creating Layer fc8
I0909 14:50:42.855274  4474 net.cpp:329] fc8 <- fc7
I0909 14:50:42.855283  4474 net.cpp:290] fc8 -> fc8
I0909 14:50:42.862833  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.862845  4474 net.cpp:125] fc8 needs backward computation.
I0909 14:50:42.862853  4474 net.cpp:66] Creating Layer relu8
I0909 14:50:42.862869  4474 net.cpp:329] relu8 <- fc8
I0909 14:50:42.862876  4474 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:50:42.862884  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.862890  4474 net.cpp:125] relu8 needs backward computation.
I0909 14:50:42.862896  4474 net.cpp:66] Creating Layer drop8
I0909 14:50:42.862902  4474 net.cpp:329] drop8 <- fc8
I0909 14:50:42.862908  4474 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:50:42.862915  4474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:50:42.862920  4474 net.cpp:125] drop8 needs backward computation.
I0909 14:50:42.862927  4474 net.cpp:66] Creating Layer fc9
I0909 14:50:42.862932  4474 net.cpp:329] fc9 <- fc8
I0909 14:50:42.862941  4474 net.cpp:290] fc9 -> fc9
I0909 14:50:42.863301  4474 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:50:42.863314  4474 net.cpp:125] fc9 needs backward computation.
I0909 14:50:42.863322  4474 net.cpp:66] Creating Layer fc10
I0909 14:50:42.863328  4474 net.cpp:329] fc10 <- fc9
I0909 14:50:42.863335  4474 net.cpp:290] fc10 -> fc10
I0909 14:50:42.863348  4474 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:50:42.863355  4474 net.cpp:125] fc10 needs backward computation.
I0909 14:50:42.863361  4474 net.cpp:66] Creating Layer prob
I0909 14:50:42.863368  4474 net.cpp:329] prob <- fc10
I0909 14:50:42.863373  4474 net.cpp:290] prob -> prob
I0909 14:50:42.863383  4474 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:50:42.863391  4474 net.cpp:125] prob needs backward computation.
I0909 14:50:42.863396  4474 net.cpp:156] This network produces output prob
I0909 14:50:42.863407  4474 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:50:42.863415  4474 net.cpp:167] Network initialization done.
I0909 14:50:42.863420  4474 net.cpp:168] Memory required for data: 6183480
Classifying 87 inputs.
Done in 53.82 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:51:42.630970  4478 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:51:42.631106  4478 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:51:42.631115  4478 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:51:42.631258  4478 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:51:42.631319  4478 net.cpp:292] Input 0 -> data
I0909 14:51:42.631345  4478 net.cpp:66] Creating Layer conv1
I0909 14:51:42.631351  4478 net.cpp:329] conv1 <- data
I0909 14:51:42.631360  4478 net.cpp:290] conv1 -> conv1
I0909 14:51:42.632679  4478 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:51:42.632696  4478 net.cpp:125] conv1 needs backward computation.
I0909 14:51:42.632705  4478 net.cpp:66] Creating Layer relu1
I0909 14:51:42.632711  4478 net.cpp:329] relu1 <- conv1
I0909 14:51:42.632719  4478 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:51:42.632727  4478 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:51:42.632733  4478 net.cpp:125] relu1 needs backward computation.
I0909 14:51:42.632740  4478 net.cpp:66] Creating Layer pool1
I0909 14:51:42.632745  4478 net.cpp:329] pool1 <- conv1
I0909 14:51:42.632752  4478 net.cpp:290] pool1 -> pool1
I0909 14:51:42.632763  4478 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:51:42.632769  4478 net.cpp:125] pool1 needs backward computation.
I0909 14:51:42.632776  4478 net.cpp:66] Creating Layer norm1
I0909 14:51:42.632781  4478 net.cpp:329] norm1 <- pool1
I0909 14:51:42.632787  4478 net.cpp:290] norm1 -> norm1
I0909 14:51:42.632797  4478 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:51:42.632802  4478 net.cpp:125] norm1 needs backward computation.
I0909 14:51:42.632810  4478 net.cpp:66] Creating Layer conv2
I0909 14:51:42.632815  4478 net.cpp:329] conv2 <- norm1
I0909 14:51:42.632822  4478 net.cpp:290] conv2 -> conv2
I0909 14:51:42.641690  4478 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:51:42.641705  4478 net.cpp:125] conv2 needs backward computation.
I0909 14:51:42.641711  4478 net.cpp:66] Creating Layer relu2
I0909 14:51:42.641716  4478 net.cpp:329] relu2 <- conv2
I0909 14:51:42.641723  4478 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:51:42.641731  4478 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:51:42.641736  4478 net.cpp:125] relu2 needs backward computation.
I0909 14:51:42.641742  4478 net.cpp:66] Creating Layer pool2
I0909 14:51:42.641747  4478 net.cpp:329] pool2 <- conv2
I0909 14:51:42.641754  4478 net.cpp:290] pool2 -> pool2
I0909 14:51:42.641762  4478 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:51:42.641767  4478 net.cpp:125] pool2 needs backward computation.
I0909 14:51:42.641773  4478 net.cpp:66] Creating Layer fc7
I0909 14:51:42.641779  4478 net.cpp:329] fc7 <- pool2
I0909 14:51:42.641787  4478 net.cpp:290] fc7 -> fc7
I0909 14:51:43.266348  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.266391  4478 net.cpp:125] fc7 needs backward computation.
I0909 14:51:43.266413  4478 net.cpp:66] Creating Layer relu7
I0909 14:51:43.266422  4478 net.cpp:329] relu7 <- fc7
I0909 14:51:43.266429  4478 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:51:43.266438  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.266444  4478 net.cpp:125] relu7 needs backward computation.
I0909 14:51:43.266453  4478 net.cpp:66] Creating Layer drop7
I0909 14:51:43.266458  4478 net.cpp:329] drop7 <- fc7
I0909 14:51:43.266466  4478 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:51:43.266477  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.266484  4478 net.cpp:125] drop7 needs backward computation.
I0909 14:51:43.266491  4478 net.cpp:66] Creating Layer fc8
I0909 14:51:43.266496  4478 net.cpp:329] fc8 <- fc7
I0909 14:51:43.266505  4478 net.cpp:290] fc8 -> fc8
I0909 14:51:43.274055  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.274067  4478 net.cpp:125] fc8 needs backward computation.
I0909 14:51:43.274075  4478 net.cpp:66] Creating Layer relu8
I0909 14:51:43.274080  4478 net.cpp:329] relu8 <- fc8
I0909 14:51:43.274086  4478 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:51:43.274096  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.274101  4478 net.cpp:125] relu8 needs backward computation.
I0909 14:51:43.274107  4478 net.cpp:66] Creating Layer drop8
I0909 14:51:43.274112  4478 net.cpp:329] drop8 <- fc8
I0909 14:51:43.274118  4478 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:51:43.274126  4478 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:51:43.274130  4478 net.cpp:125] drop8 needs backward computation.
I0909 14:51:43.274137  4478 net.cpp:66] Creating Layer fc9
I0909 14:51:43.274142  4478 net.cpp:329] fc9 <- fc8
I0909 14:51:43.274152  4478 net.cpp:290] fc9 -> fc9
I0909 14:51:43.274513  4478 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:51:43.274524  4478 net.cpp:125] fc9 needs backward computation.
I0909 14:51:43.274533  4478 net.cpp:66] Creating Layer fc10
I0909 14:51:43.274538  4478 net.cpp:329] fc10 <- fc9
I0909 14:51:43.274545  4478 net.cpp:290] fc10 -> fc10
I0909 14:51:43.274559  4478 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:51:43.274566  4478 net.cpp:125] fc10 needs backward computation.
I0909 14:51:43.274572  4478 net.cpp:66] Creating Layer prob
I0909 14:51:43.274579  4478 net.cpp:329] prob <- fc10
I0909 14:51:43.274585  4478 net.cpp:290] prob -> prob
I0909 14:51:43.274593  4478 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:51:43.274600  4478 net.cpp:125] prob needs backward computation.
I0909 14:51:43.274605  4478 net.cpp:156] This network produces output prob
I0909 14:51:43.274616  4478 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:51:43.274624  4478 net.cpp:167] Network initialization done.
I0909 14:51:43.274629  4478 net.cpp:168] Memory required for data: 6183480
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 119, in main
    inputs.append(caffe.io.load_image(im_f))
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py", line 23, in load_image
    img = skimage.img_as_float(skimage.io.imread(filename)).astype(np.float32)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 287, in img_as_float
    return convert(image, np.float64, force_copy)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 237, in convert
    return image.astype(dtype)
MemoryError
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 14:57:36.556305  4487 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 14:57:36.584761  4487 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 14:57:36.584772  4487 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 14:57:36.584924  4487 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 14:57:36.584972  4487 net.cpp:292] Input 0 -> data
I0909 14:57:36.584997  4487 net.cpp:66] Creating Layer conv1
I0909 14:57:36.585003  4487 net.cpp:329] conv1 <- data
I0909 14:57:36.585011  4487 net.cpp:290] conv1 -> conv1
I0909 14:57:36.607758  4487 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:57:36.607780  4487 net.cpp:125] conv1 needs backward computation.
I0909 14:57:36.607790  4487 net.cpp:66] Creating Layer relu1
I0909 14:57:36.607796  4487 net.cpp:329] relu1 <- conv1
I0909 14:57:36.607802  4487 net.cpp:280] relu1 -> conv1 (in-place)
I0909 14:57:36.607811  4487 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 14:57:36.607817  4487 net.cpp:125] relu1 needs backward computation.
I0909 14:57:36.607823  4487 net.cpp:66] Creating Layer pool1
I0909 14:57:36.607830  4487 net.cpp:329] pool1 <- conv1
I0909 14:57:36.607836  4487 net.cpp:290] pool1 -> pool1
I0909 14:57:36.607851  4487 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:57:36.607857  4487 net.cpp:125] pool1 needs backward computation.
I0909 14:57:36.607864  4487 net.cpp:66] Creating Layer norm1
I0909 14:57:36.607869  4487 net.cpp:329] norm1 <- pool1
I0909 14:57:36.607877  4487 net.cpp:290] norm1 -> norm1
I0909 14:57:36.607885  4487 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 14:57:36.607892  4487 net.cpp:125] norm1 needs backward computation.
I0909 14:57:36.607898  4487 net.cpp:66] Creating Layer conv2
I0909 14:57:36.607903  4487 net.cpp:329] conv2 <- norm1
I0909 14:57:36.607910  4487 net.cpp:290] conv2 -> conv2
I0909 14:57:36.616734  4487 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:57:36.616750  4487 net.cpp:125] conv2 needs backward computation.
I0909 14:57:36.616755  4487 net.cpp:66] Creating Layer relu2
I0909 14:57:36.616761  4487 net.cpp:329] relu2 <- conv2
I0909 14:57:36.616767  4487 net.cpp:280] relu2 -> conv2 (in-place)
I0909 14:57:36.616775  4487 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 14:57:36.616780  4487 net.cpp:125] relu2 needs backward computation.
I0909 14:57:36.616786  4487 net.cpp:66] Creating Layer pool2
I0909 14:57:36.616791  4487 net.cpp:329] pool2 <- conv2
I0909 14:57:36.616797  4487 net.cpp:290] pool2 -> pool2
I0909 14:57:36.616806  4487 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 14:57:36.616811  4487 net.cpp:125] pool2 needs backward computation.
I0909 14:57:36.616817  4487 net.cpp:66] Creating Layer fc7
I0909 14:57:36.616822  4487 net.cpp:329] fc7 <- pool2
I0909 14:57:36.616829  4487 net.cpp:290] fc7 -> fc7
I0909 14:57:37.244199  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.244246  4487 net.cpp:125] fc7 needs backward computation.
I0909 14:57:37.244259  4487 net.cpp:66] Creating Layer relu7
I0909 14:57:37.244266  4487 net.cpp:329] relu7 <- fc7
I0909 14:57:37.244273  4487 net.cpp:280] relu7 -> fc7 (in-place)
I0909 14:57:37.244283  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.244289  4487 net.cpp:125] relu7 needs backward computation.
I0909 14:57:37.244297  4487 net.cpp:66] Creating Layer drop7
I0909 14:57:37.244302  4487 net.cpp:329] drop7 <- fc7
I0909 14:57:37.244309  4487 net.cpp:280] drop7 -> fc7 (in-place)
I0909 14:57:37.244320  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.244325  4487 net.cpp:125] drop7 needs backward computation.
I0909 14:57:37.244334  4487 net.cpp:66] Creating Layer fc8
I0909 14:57:37.244339  4487 net.cpp:329] fc8 <- fc7
I0909 14:57:37.244349  4487 net.cpp:290] fc8 -> fc8
I0909 14:57:37.251905  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.251916  4487 net.cpp:125] fc8 needs backward computation.
I0909 14:57:37.251924  4487 net.cpp:66] Creating Layer relu8
I0909 14:57:37.251929  4487 net.cpp:329] relu8 <- fc8
I0909 14:57:37.251935  4487 net.cpp:280] relu8 -> fc8 (in-place)
I0909 14:57:37.251943  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.251950  4487 net.cpp:125] relu8 needs backward computation.
I0909 14:57:37.251955  4487 net.cpp:66] Creating Layer drop8
I0909 14:57:37.251960  4487 net.cpp:329] drop8 <- fc8
I0909 14:57:37.251966  4487 net.cpp:280] drop8 -> fc8 (in-place)
I0909 14:57:37.251973  4487 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 14:57:37.251978  4487 net.cpp:125] drop8 needs backward computation.
I0909 14:57:37.251986  4487 net.cpp:66] Creating Layer fc9
I0909 14:57:37.251991  4487 net.cpp:329] fc9 <- fc8
I0909 14:57:37.251998  4487 net.cpp:290] fc9 -> fc9
I0909 14:57:37.252362  4487 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 14:57:37.252374  4487 net.cpp:125] fc9 needs backward computation.
I0909 14:57:37.252382  4487 net.cpp:66] Creating Layer fc10
I0909 14:57:37.252388  4487 net.cpp:329] fc10 <- fc9
I0909 14:57:37.252395  4487 net.cpp:290] fc10 -> fc10
I0909 14:57:37.252408  4487 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:57:37.252415  4487 net.cpp:125] fc10 needs backward computation.
I0909 14:57:37.252421  4487 net.cpp:66] Creating Layer prob
I0909 14:57:37.252426  4487 net.cpp:329] prob <- fc10
I0909 14:57:37.252434  4487 net.cpp:290] prob -> prob
I0909 14:57:37.267799  4487 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 14:57:37.267845  4487 net.cpp:125] prob needs backward computation.
I0909 14:57:37.267851  4487 net.cpp:156] This network produces output prob
I0909 14:57:37.267863  4487 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 14:57:37.267871  4487 net.cpp:167] Network initialization done.
I0909 14:57:37.267876  4487 net.cpp:168] Memory required for data: 6183480
Classifying 239 inputs.
Done in 146.02 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:00:13.191509  4519 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:00:13.191735  4519 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:00:13.191745  4519 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:00:13.191889  4519 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:00:13.192013  4519 net.cpp:292] Input 0 -> data
I0909 15:00:13.192039  4519 net.cpp:66] Creating Layer conv1
I0909 15:00:13.192046  4519 net.cpp:329] conv1 <- data
I0909 15:00:13.192054  4519 net.cpp:290] conv1 -> conv1
I0909 15:00:13.193533  4519 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:00:13.193560  4519 net.cpp:125] conv1 needs backward computation.
I0909 15:00:13.193569  4519 net.cpp:66] Creating Layer relu1
I0909 15:00:13.193578  4519 net.cpp:329] relu1 <- conv1
I0909 15:00:13.193585  4519 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:00:13.193601  4519 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:00:13.193608  4519 net.cpp:125] relu1 needs backward computation.
I0909 15:00:13.193614  4519 net.cpp:66] Creating Layer pool1
I0909 15:00:13.193619  4519 net.cpp:329] pool1 <- conv1
I0909 15:00:13.193625  4519 net.cpp:290] pool1 -> pool1
I0909 15:00:13.193636  4519 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:00:13.193641  4519 net.cpp:125] pool1 needs backward computation.
I0909 15:00:13.193649  4519 net.cpp:66] Creating Layer norm1
I0909 15:00:13.193653  4519 net.cpp:329] norm1 <- pool1
I0909 15:00:13.193660  4519 net.cpp:290] norm1 -> norm1
I0909 15:00:13.193670  4519 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:00:13.193675  4519 net.cpp:125] norm1 needs backward computation.
I0909 15:00:13.193681  4519 net.cpp:66] Creating Layer conv2
I0909 15:00:13.193687  4519 net.cpp:329] conv2 <- norm1
I0909 15:00:13.193693  4519 net.cpp:290] conv2 -> conv2
I0909 15:00:13.202618  4519 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:00:13.202633  4519 net.cpp:125] conv2 needs backward computation.
I0909 15:00:13.202641  4519 net.cpp:66] Creating Layer relu2
I0909 15:00:13.202646  4519 net.cpp:329] relu2 <- conv2
I0909 15:00:13.202652  4519 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:00:13.202658  4519 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:00:13.202664  4519 net.cpp:125] relu2 needs backward computation.
I0909 15:00:13.202671  4519 net.cpp:66] Creating Layer pool2
I0909 15:00:13.202675  4519 net.cpp:329] pool2 <- conv2
I0909 15:00:13.202682  4519 net.cpp:290] pool2 -> pool2
I0909 15:00:13.202689  4519 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:00:13.202694  4519 net.cpp:125] pool2 needs backward computation.
I0909 15:00:13.202702  4519 net.cpp:66] Creating Layer fc7
I0909 15:00:13.202707  4519 net.cpp:329] fc7 <- pool2
I0909 15:00:13.202713  4519 net.cpp:290] fc7 -> fc7
I0909 15:00:13.835095  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.835141  4519 net.cpp:125] fc7 needs backward computation.
I0909 15:00:13.835153  4519 net.cpp:66] Creating Layer relu7
I0909 15:00:13.835160  4519 net.cpp:329] relu7 <- fc7
I0909 15:00:13.835167  4519 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:00:13.835176  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.835182  4519 net.cpp:125] relu7 needs backward computation.
I0909 15:00:13.835189  4519 net.cpp:66] Creating Layer drop7
I0909 15:00:13.835194  4519 net.cpp:329] drop7 <- fc7
I0909 15:00:13.835202  4519 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:00:13.835212  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.835218  4519 net.cpp:125] drop7 needs backward computation.
I0909 15:00:13.835227  4519 net.cpp:66] Creating Layer fc8
I0909 15:00:13.835232  4519 net.cpp:329] fc8 <- fc7
I0909 15:00:13.835240  4519 net.cpp:290] fc8 -> fc8
I0909 15:00:13.842965  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.842979  4519 net.cpp:125] fc8 needs backward computation.
I0909 15:00:13.842986  4519 net.cpp:66] Creating Layer relu8
I0909 15:00:13.842993  4519 net.cpp:329] relu8 <- fc8
I0909 15:00:13.842998  4519 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:00:13.843006  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.843013  4519 net.cpp:125] relu8 needs backward computation.
I0909 15:00:13.843019  4519 net.cpp:66] Creating Layer drop8
I0909 15:00:13.843032  4519 net.cpp:329] drop8 <- fc8
I0909 15:00:13.843039  4519 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:00:13.843045  4519 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:13.843051  4519 net.cpp:125] drop8 needs backward computation.
I0909 15:00:13.843058  4519 net.cpp:66] Creating Layer fc9
I0909 15:00:13.843063  4519 net.cpp:329] fc9 <- fc8
I0909 15:00:13.843071  4519 net.cpp:290] fc9 -> fc9
I0909 15:00:13.843433  4519 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:00:13.843446  4519 net.cpp:125] fc9 needs backward computation.
I0909 15:00:13.843453  4519 net.cpp:66] Creating Layer fc10
I0909 15:00:13.843459  4519 net.cpp:329] fc10 <- fc9
I0909 15:00:13.843473  4519 net.cpp:290] fc10 -> fc10
I0909 15:00:13.843487  4519 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:00:13.843494  4519 net.cpp:125] fc10 needs backward computation.
I0909 15:00:13.843508  4519 net.cpp:66] Creating Layer prob
I0909 15:00:13.843513  4519 net.cpp:329] prob <- fc10
I0909 15:00:13.843519  4519 net.cpp:290] prob -> prob
I0909 15:00:13.843528  4519 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:00:13.843536  4519 net.cpp:125] prob needs backward computation.
I0909 15:00:13.843541  4519 net.cpp:156] This network produces output prob
I0909 15:00:13.843552  4519 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:00:13.843560  4519 net.cpp:167] Network initialization done.
I0909 15:00:13.843565  4519 net.cpp:168] Memory required for data: 6183480
Classifying 13 inputs.
Done in 7.87 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:00:23.780967  4551 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:00:23.781131  4551 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:00:23.781141  4551 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:00:23.781286  4551 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:00:23.781350  4551 net.cpp:292] Input 0 -> data
I0909 15:00:23.781376  4551 net.cpp:66] Creating Layer conv1
I0909 15:00:23.781384  4551 net.cpp:329] conv1 <- data
I0909 15:00:23.781393  4551 net.cpp:290] conv1 -> conv1
I0909 15:00:23.782747  4551 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:00:23.782768  4551 net.cpp:125] conv1 needs backward computation.
I0909 15:00:23.782778  4551 net.cpp:66] Creating Layer relu1
I0909 15:00:23.782783  4551 net.cpp:329] relu1 <- conv1
I0909 15:00:23.782791  4551 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:00:23.782800  4551 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:00:23.782806  4551 net.cpp:125] relu1 needs backward computation.
I0909 15:00:23.782814  4551 net.cpp:66] Creating Layer pool1
I0909 15:00:23.782820  4551 net.cpp:329] pool1 <- conv1
I0909 15:00:23.782827  4551 net.cpp:290] pool1 -> pool1
I0909 15:00:23.782838  4551 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:00:23.782845  4551 net.cpp:125] pool1 needs backward computation.
I0909 15:00:23.782852  4551 net.cpp:66] Creating Layer norm1
I0909 15:00:23.782858  4551 net.cpp:329] norm1 <- pool1
I0909 15:00:23.782866  4551 net.cpp:290] norm1 -> norm1
I0909 15:00:23.782876  4551 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:00:23.782882  4551 net.cpp:125] norm1 needs backward computation.
I0909 15:00:23.782891  4551 net.cpp:66] Creating Layer conv2
I0909 15:00:23.782896  4551 net.cpp:329] conv2 <- norm1
I0909 15:00:23.782904  4551 net.cpp:290] conv2 -> conv2
I0909 15:00:23.791962  4551 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:00:23.791982  4551 net.cpp:125] conv2 needs backward computation.
I0909 15:00:23.792000  4551 net.cpp:66] Creating Layer relu2
I0909 15:00:23.792006  4551 net.cpp:329] relu2 <- conv2
I0909 15:00:23.792013  4551 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:00:23.792021  4551 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:00:23.792028  4551 net.cpp:125] relu2 needs backward computation.
I0909 15:00:23.792042  4551 net.cpp:66] Creating Layer pool2
I0909 15:00:23.792048  4551 net.cpp:329] pool2 <- conv2
I0909 15:00:23.792055  4551 net.cpp:290] pool2 -> pool2
I0909 15:00:23.792063  4551 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:00:23.792070  4551 net.cpp:125] pool2 needs backward computation.
I0909 15:00:23.792078  4551 net.cpp:66] Creating Layer fc7
I0909 15:00:23.792083  4551 net.cpp:329] fc7 <- pool2
I0909 15:00:23.792091  4551 net.cpp:290] fc7 -> fc7
I0909 15:00:24.422454  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.422516  4551 net.cpp:125] fc7 needs backward computation.
I0909 15:00:24.422533  4551 net.cpp:66] Creating Layer relu7
I0909 15:00:24.422540  4551 net.cpp:329] relu7 <- fc7
I0909 15:00:24.422549  4551 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:00:24.422560  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.422585  4551 net.cpp:125] relu7 needs backward computation.
I0909 15:00:24.422593  4551 net.cpp:66] Creating Layer drop7
I0909 15:00:24.422600  4551 net.cpp:329] drop7 <- fc7
I0909 15:00:24.422607  4551 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:00:24.422619  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.422626  4551 net.cpp:125] drop7 needs backward computation.
I0909 15:00:24.422634  4551 net.cpp:66] Creating Layer fc8
I0909 15:00:24.422641  4551 net.cpp:329] fc8 <- fc7
I0909 15:00:24.422649  4551 net.cpp:290] fc8 -> fc8
I0909 15:00:24.430297  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.430312  4551 net.cpp:125] fc8 needs backward computation.
I0909 15:00:24.430320  4551 net.cpp:66] Creating Layer relu8
I0909 15:00:24.430326  4551 net.cpp:329] relu8 <- fc8
I0909 15:00:24.430333  4551 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:00:24.430343  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.430349  4551 net.cpp:125] relu8 needs backward computation.
I0909 15:00:24.430357  4551 net.cpp:66] Creating Layer drop8
I0909 15:00:24.430363  4551 net.cpp:329] drop8 <- fc8
I0909 15:00:24.430371  4551 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:00:24.430377  4551 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:00:24.430384  4551 net.cpp:125] drop8 needs backward computation.
I0909 15:00:24.430392  4551 net.cpp:66] Creating Layer fc9
I0909 15:00:24.430398  4551 net.cpp:329] fc9 <- fc8
I0909 15:00:24.430407  4551 net.cpp:290] fc9 -> fc9
I0909 15:00:24.430779  4551 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:00:24.430794  4551 net.cpp:125] fc9 needs backward computation.
I0909 15:00:24.430802  4551 net.cpp:66] Creating Layer fc10
I0909 15:00:24.430809  4551 net.cpp:329] fc10 <- fc9
I0909 15:00:24.430816  4551 net.cpp:290] fc10 -> fc10
I0909 15:00:24.430830  4551 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:00:24.430840  4551 net.cpp:125] fc10 needs backward computation.
I0909 15:00:24.430846  4551 net.cpp:66] Creating Layer prob
I0909 15:00:24.430852  4551 net.cpp:329] prob <- fc10
I0909 15:00:24.430860  4551 net.cpp:290] prob -> prob
I0909 15:00:24.430869  4551 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:00:24.430878  4551 net.cpp:125] prob needs backward computation.
I0909 15:00:24.430883  4551 net.cpp:156] This network produces output prob
I0909 15:00:24.430896  4551 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:00:24.430904  4551 net.cpp:167] Network initialization done.
I0909 15:00:24.430910  4551 net.cpp:168] Memory required for data: 6183480
Classifying 53 inputs.
Done in 35.51 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:01:02.159040  4556 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:01:02.159176  4556 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:01:02.159185  4556 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:01:02.159329  4556 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:01:02.159390  4556 net.cpp:292] Input 0 -> data
I0909 15:01:02.159415  4556 net.cpp:66] Creating Layer conv1
I0909 15:01:02.159422  4556 net.cpp:329] conv1 <- data
I0909 15:01:02.159430  4556 net.cpp:290] conv1 -> conv1
I0909 15:01:02.160756  4556 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:01:02.160774  4556 net.cpp:125] conv1 needs backward computation.
I0909 15:01:02.160784  4556 net.cpp:66] Creating Layer relu1
I0909 15:01:02.160789  4556 net.cpp:329] relu1 <- conv1
I0909 15:01:02.160796  4556 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:01:02.160804  4556 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:01:02.160810  4556 net.cpp:125] relu1 needs backward computation.
I0909 15:01:02.160816  4556 net.cpp:66] Creating Layer pool1
I0909 15:01:02.160821  4556 net.cpp:329] pool1 <- conv1
I0909 15:01:02.160828  4556 net.cpp:290] pool1 -> pool1
I0909 15:01:02.160838  4556 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:01:02.160845  4556 net.cpp:125] pool1 needs backward computation.
I0909 15:01:02.160851  4556 net.cpp:66] Creating Layer norm1
I0909 15:01:02.160856  4556 net.cpp:329] norm1 <- pool1
I0909 15:01:02.160862  4556 net.cpp:290] norm1 -> norm1
I0909 15:01:02.160872  4556 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:01:02.160877  4556 net.cpp:125] norm1 needs backward computation.
I0909 15:01:02.160884  4556 net.cpp:66] Creating Layer conv2
I0909 15:01:02.160890  4556 net.cpp:329] conv2 <- norm1
I0909 15:01:02.160897  4556 net.cpp:290] conv2 -> conv2
I0909 15:01:02.169838  4556 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:01:02.169852  4556 net.cpp:125] conv2 needs backward computation.
I0909 15:01:02.169859  4556 net.cpp:66] Creating Layer relu2
I0909 15:01:02.169865  4556 net.cpp:329] relu2 <- conv2
I0909 15:01:02.169872  4556 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:01:02.169883  4556 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:01:02.169889  4556 net.cpp:125] relu2 needs backward computation.
I0909 15:01:02.169895  4556 net.cpp:66] Creating Layer pool2
I0909 15:01:02.169900  4556 net.cpp:329] pool2 <- conv2
I0909 15:01:02.169908  4556 net.cpp:290] pool2 -> pool2
I0909 15:01:02.169914  4556 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:01:02.169920  4556 net.cpp:125] pool2 needs backward computation.
I0909 15:01:02.169926  4556 net.cpp:66] Creating Layer fc7
I0909 15:01:02.169932  4556 net.cpp:329] fc7 <- pool2
I0909 15:01:02.169939  4556 net.cpp:290] fc7 -> fc7
I0909 15:01:02.797632  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.797673  4556 net.cpp:125] fc7 needs backward computation.
I0909 15:01:02.797685  4556 net.cpp:66] Creating Layer relu7
I0909 15:01:02.797693  4556 net.cpp:329] relu7 <- fc7
I0909 15:01:02.797700  4556 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:01:02.797709  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.797715  4556 net.cpp:125] relu7 needs backward computation.
I0909 15:01:02.797723  4556 net.cpp:66] Creating Layer drop7
I0909 15:01:02.797727  4556 net.cpp:329] drop7 <- fc7
I0909 15:01:02.797735  4556 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:01:02.797746  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.797752  4556 net.cpp:125] drop7 needs backward computation.
I0909 15:01:02.797760  4556 net.cpp:66] Creating Layer fc8
I0909 15:01:02.797765  4556 net.cpp:329] fc8 <- fc7
I0909 15:01:02.797773  4556 net.cpp:290] fc8 -> fc8
I0909 15:01:02.805304  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.805315  4556 net.cpp:125] fc8 needs backward computation.
I0909 15:01:02.805322  4556 net.cpp:66] Creating Layer relu8
I0909 15:01:02.805327  4556 net.cpp:329] relu8 <- fc8
I0909 15:01:02.805336  4556 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:01:02.805344  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.805349  4556 net.cpp:125] relu8 needs backward computation.
I0909 15:01:02.805356  4556 net.cpp:66] Creating Layer drop8
I0909 15:01:02.805361  4556 net.cpp:329] drop8 <- fc8
I0909 15:01:02.805367  4556 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:01:02.805373  4556 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:01:02.805379  4556 net.cpp:125] drop8 needs backward computation.
I0909 15:01:02.805385  4556 net.cpp:66] Creating Layer fc9
I0909 15:01:02.805390  4556 net.cpp:329] fc9 <- fc8
I0909 15:01:02.805398  4556 net.cpp:290] fc9 -> fc9
I0909 15:01:02.805763  4556 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:01:02.805776  4556 net.cpp:125] fc9 needs backward computation.
I0909 15:01:02.805784  4556 net.cpp:66] Creating Layer fc10
I0909 15:01:02.805789  4556 net.cpp:329] fc10 <- fc9
I0909 15:01:02.805796  4556 net.cpp:290] fc10 -> fc10
I0909 15:01:02.805809  4556 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:01:02.805817  4556 net.cpp:125] fc10 needs backward computation.
I0909 15:01:02.805824  4556 net.cpp:66] Creating Layer prob
I0909 15:01:02.805830  4556 net.cpp:329] prob <- fc10
I0909 15:01:02.805836  4556 net.cpp:290] prob -> prob
I0909 15:01:02.805845  4556 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:01:02.805851  4556 net.cpp:125] prob needs backward computation.
I0909 15:01:02.805856  4556 net.cpp:156] This network produces output prob
I0909 15:01:02.805867  4556 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:01:02.805874  4556 net.cpp:167] Network initialization done.
I0909 15:01:02.805879  4556 net.cpp:168] Memory required for data: 6183480
Classifying 445 inputs.
Done in 787.98 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:16:12.640555  5582 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:16:12.640702  5582 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:16:12.640712  5582 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:16:12.640866  5582 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:16:12.640916  5582 net.cpp:292] Input 0 -> data
I0909 15:16:12.640943  5582 net.cpp:66] Creating Layer conv1
I0909 15:16:12.640949  5582 net.cpp:329] conv1 <- data
I0909 15:16:12.640956  5582 net.cpp:290] conv1 -> conv1
I0909 15:16:12.649386  5582 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:16:12.649410  5582 net.cpp:125] conv1 needs backward computation.
I0909 15:16:12.649420  5582 net.cpp:66] Creating Layer relu1
I0909 15:16:12.649426  5582 net.cpp:329] relu1 <- conv1
I0909 15:16:12.649432  5582 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:16:12.649441  5582 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:16:12.649447  5582 net.cpp:125] relu1 needs backward computation.
I0909 15:16:12.649453  5582 net.cpp:66] Creating Layer pool1
I0909 15:16:12.649459  5582 net.cpp:329] pool1 <- conv1
I0909 15:16:12.649469  5582 net.cpp:290] pool1 -> pool1
I0909 15:16:12.649482  5582 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:16:12.649487  5582 net.cpp:125] pool1 needs backward computation.
I0909 15:16:12.649500  5582 net.cpp:66] Creating Layer norm1
I0909 15:16:12.649507  5582 net.cpp:329] norm1 <- pool1
I0909 15:16:12.649528  5582 net.cpp:290] norm1 -> norm1
I0909 15:16:12.649548  5582 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:16:12.649554  5582 net.cpp:125] norm1 needs backward computation.
I0909 15:16:12.649560  5582 net.cpp:66] Creating Layer conv2
I0909 15:16:12.649566  5582 net.cpp:329] conv2 <- norm1
I0909 15:16:12.649572  5582 net.cpp:290] conv2 -> conv2
I0909 15:16:12.658656  5582 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:16:12.658673  5582 net.cpp:125] conv2 needs backward computation.
I0909 15:16:12.658679  5582 net.cpp:66] Creating Layer relu2
I0909 15:16:12.658685  5582 net.cpp:329] relu2 <- conv2
I0909 15:16:12.658692  5582 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:16:12.658699  5582 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:16:12.658704  5582 net.cpp:125] relu2 needs backward computation.
I0909 15:16:12.658710  5582 net.cpp:66] Creating Layer pool2
I0909 15:16:12.658716  5582 net.cpp:329] pool2 <- conv2
I0909 15:16:12.658722  5582 net.cpp:290] pool2 -> pool2
I0909 15:16:12.658730  5582 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:16:12.658735  5582 net.cpp:125] pool2 needs backward computation.
I0909 15:16:12.658742  5582 net.cpp:66] Creating Layer fc7
I0909 15:16:12.658748  5582 net.cpp:329] fc7 <- pool2
I0909 15:16:12.658756  5582 net.cpp:290] fc7 -> fc7
I0909 15:16:13.289270  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.289314  5582 net.cpp:125] fc7 needs backward computation.
I0909 15:16:13.289325  5582 net.cpp:66] Creating Layer relu7
I0909 15:16:13.289332  5582 net.cpp:329] relu7 <- fc7
I0909 15:16:13.289340  5582 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:16:13.289350  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.289355  5582 net.cpp:125] relu7 needs backward computation.
I0909 15:16:13.289361  5582 net.cpp:66] Creating Layer drop7
I0909 15:16:13.289366  5582 net.cpp:329] drop7 <- fc7
I0909 15:16:13.289374  5582 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:16:13.289384  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.289389  5582 net.cpp:125] drop7 needs backward computation.
I0909 15:16:13.289398  5582 net.cpp:66] Creating Layer fc8
I0909 15:16:13.289403  5582 net.cpp:329] fc8 <- fc7
I0909 15:16:13.289412  5582 net.cpp:290] fc8 -> fc8
I0909 15:16:13.297165  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.297181  5582 net.cpp:125] fc8 needs backward computation.
I0909 15:16:13.297188  5582 net.cpp:66] Creating Layer relu8
I0909 15:16:13.297194  5582 net.cpp:329] relu8 <- fc8
I0909 15:16:13.297201  5582 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:16:13.297209  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.297215  5582 net.cpp:125] relu8 needs backward computation.
I0909 15:16:13.297222  5582 net.cpp:66] Creating Layer drop8
I0909 15:16:13.297227  5582 net.cpp:329] drop8 <- fc8
I0909 15:16:13.297235  5582 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:16:13.297240  5582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:16:13.297246  5582 net.cpp:125] drop8 needs backward computation.
I0909 15:16:13.297255  5582 net.cpp:66] Creating Layer fc9
I0909 15:16:13.297260  5582 net.cpp:329] fc9 <- fc8
I0909 15:16:13.297268  5582 net.cpp:290] fc9 -> fc9
I0909 15:16:13.297683  5582 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:16:13.297698  5582 net.cpp:125] fc9 needs backward computation.
I0909 15:16:13.297706  5582 net.cpp:66] Creating Layer fc10
I0909 15:16:13.297711  5582 net.cpp:329] fc10 <- fc9
I0909 15:16:13.297719  5582 net.cpp:290] fc10 -> fc10
I0909 15:16:13.297732  5582 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:16:13.297740  5582 net.cpp:125] fc10 needs backward computation.
I0909 15:16:13.297746  5582 net.cpp:66] Creating Layer prob
I0909 15:16:13.297760  5582 net.cpp:329] prob <- fc10
I0909 15:16:13.297767  5582 net.cpp:290] prob -> prob
I0909 15:16:13.297777  5582 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:16:13.297785  5582 net.cpp:125] prob needs backward computation.
I0909 15:16:13.297790  5582 net.cpp:156] This network produces output prob
I0909 15:16:13.297801  5582 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:16:13.297809  5582 net.cpp:167] Network initialization done.
I0909 15:16:13.297814  5582 net.cpp:168] Memory required for data: 6183480
Classifying 92 inputs.
Done in 100.68 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:18:21.637238  6259 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:18:21.637387  6259 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:18:21.637397  6259 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:18:21.637572  6259 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:18:21.637632  6259 net.cpp:292] Input 0 -> data
I0909 15:18:21.637658  6259 net.cpp:66] Creating Layer conv1
I0909 15:18:21.637665  6259 net.cpp:329] conv1 <- data
I0909 15:18:21.637673  6259 net.cpp:290] conv1 -> conv1
I0909 15:18:21.645129  6259 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:18:21.645153  6259 net.cpp:125] conv1 needs backward computation.
I0909 15:18:21.645164  6259 net.cpp:66] Creating Layer relu1
I0909 15:18:21.645169  6259 net.cpp:329] relu1 <- conv1
I0909 15:18:21.645176  6259 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:18:21.645185  6259 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:18:21.645191  6259 net.cpp:125] relu1 needs backward computation.
I0909 15:18:21.645197  6259 net.cpp:66] Creating Layer pool1
I0909 15:18:21.645203  6259 net.cpp:329] pool1 <- conv1
I0909 15:18:21.645210  6259 net.cpp:290] pool1 -> pool1
I0909 15:18:21.645285  6259 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:18:21.645293  6259 net.cpp:125] pool1 needs backward computation.
I0909 15:18:21.645300  6259 net.cpp:66] Creating Layer norm1
I0909 15:18:21.645305  6259 net.cpp:329] norm1 <- pool1
I0909 15:18:21.645313  6259 net.cpp:290] norm1 -> norm1
I0909 15:18:21.645323  6259 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:18:21.645328  6259 net.cpp:125] norm1 needs backward computation.
I0909 15:18:21.645335  6259 net.cpp:66] Creating Layer conv2
I0909 15:18:21.645340  6259 net.cpp:329] conv2 <- norm1
I0909 15:18:21.645349  6259 net.cpp:290] conv2 -> conv2
I0909 15:18:21.654227  6259 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:18:21.654240  6259 net.cpp:125] conv2 needs backward computation.
I0909 15:18:21.654247  6259 net.cpp:66] Creating Layer relu2
I0909 15:18:21.654253  6259 net.cpp:329] relu2 <- conv2
I0909 15:18:21.654259  6259 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:18:21.654266  6259 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:18:21.654271  6259 net.cpp:125] relu2 needs backward computation.
I0909 15:18:21.654278  6259 net.cpp:66] Creating Layer pool2
I0909 15:18:21.654284  6259 net.cpp:329] pool2 <- conv2
I0909 15:18:21.654289  6259 net.cpp:290] pool2 -> pool2
I0909 15:18:21.654297  6259 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:18:21.654304  6259 net.cpp:125] pool2 needs backward computation.
I0909 15:18:21.654309  6259 net.cpp:66] Creating Layer fc7
I0909 15:18:21.654314  6259 net.cpp:329] fc7 <- pool2
I0909 15:18:21.654322  6259 net.cpp:290] fc7 -> fc7
I0909 15:18:22.278915  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.278965  6259 net.cpp:125] fc7 needs backward computation.
I0909 15:18:22.278978  6259 net.cpp:66] Creating Layer relu7
I0909 15:18:22.278985  6259 net.cpp:329] relu7 <- fc7
I0909 15:18:22.278993  6259 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:18:22.279002  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.279008  6259 net.cpp:125] relu7 needs backward computation.
I0909 15:18:22.279016  6259 net.cpp:66] Creating Layer drop7
I0909 15:18:22.279021  6259 net.cpp:329] drop7 <- fc7
I0909 15:18:22.279027  6259 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:18:22.279037  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.279043  6259 net.cpp:125] drop7 needs backward computation.
I0909 15:18:22.279052  6259 net.cpp:66] Creating Layer fc8
I0909 15:18:22.279057  6259 net.cpp:329] fc8 <- fc7
I0909 15:18:22.279065  6259 net.cpp:290] fc8 -> fc8
I0909 15:18:22.286650  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.286662  6259 net.cpp:125] fc8 needs backward computation.
I0909 15:18:22.286669  6259 net.cpp:66] Creating Layer relu8
I0909 15:18:22.286674  6259 net.cpp:329] relu8 <- fc8
I0909 15:18:22.286682  6259 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:18:22.286689  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.286706  6259 net.cpp:125] relu8 needs backward computation.
I0909 15:18:22.286713  6259 net.cpp:66] Creating Layer drop8
I0909 15:18:22.286718  6259 net.cpp:329] drop8 <- fc8
I0909 15:18:22.286725  6259 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:18:22.286732  6259 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:22.286737  6259 net.cpp:125] drop8 needs backward computation.
I0909 15:18:22.286746  6259 net.cpp:66] Creating Layer fc9
I0909 15:18:22.286751  6259 net.cpp:329] fc9 <- fc8
I0909 15:18:22.286758  6259 net.cpp:290] fc9 -> fc9
I0909 15:18:22.287120  6259 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:18:22.287132  6259 net.cpp:125] fc9 needs backward computation.
I0909 15:18:22.287139  6259 net.cpp:66] Creating Layer fc10
I0909 15:18:22.287145  6259 net.cpp:329] fc10 <- fc9
I0909 15:18:22.287153  6259 net.cpp:290] fc10 -> fc10
I0909 15:18:22.287164  6259 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:18:22.287173  6259 net.cpp:125] fc10 needs backward computation.
I0909 15:18:22.287179  6259 net.cpp:66] Creating Layer prob
I0909 15:18:22.287184  6259 net.cpp:329] prob <- fc10
I0909 15:18:22.287192  6259 net.cpp:290] prob -> prob
I0909 15:18:22.287201  6259 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:18:22.287207  6259 net.cpp:125] prob needs backward computation.
I0909 15:18:22.287211  6259 net.cpp:156] This network produces output prob
I0909 15:18:22.287225  6259 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:18:22.287235  6259 net.cpp:167] Network initialization done.
I0909 15:18:22.287240  6259 net.cpp:168] Memory required for data: 6183480
Classifying 26 inputs.
Done in 16.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:18:40.949126  6263 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:18:40.949265  6263 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:18:40.949273  6263 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:18:40.949417  6263 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:18:40.949481  6263 net.cpp:292] Input 0 -> data
I0909 15:18:40.949507  6263 net.cpp:66] Creating Layer conv1
I0909 15:18:40.949538  6263 net.cpp:329] conv1 <- data
I0909 15:18:40.949555  6263 net.cpp:290] conv1 -> conv1
I0909 15:18:40.950889  6263 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:18:40.950907  6263 net.cpp:125] conv1 needs backward computation.
I0909 15:18:40.950917  6263 net.cpp:66] Creating Layer relu1
I0909 15:18:40.950922  6263 net.cpp:329] relu1 <- conv1
I0909 15:18:40.950929  6263 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:18:40.950938  6263 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:18:40.950943  6263 net.cpp:125] relu1 needs backward computation.
I0909 15:18:40.950950  6263 net.cpp:66] Creating Layer pool1
I0909 15:18:40.950956  6263 net.cpp:329] pool1 <- conv1
I0909 15:18:40.950963  6263 net.cpp:290] pool1 -> pool1
I0909 15:18:40.950973  6263 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:18:40.950979  6263 net.cpp:125] pool1 needs backward computation.
I0909 15:18:40.950986  6263 net.cpp:66] Creating Layer norm1
I0909 15:18:40.950992  6263 net.cpp:329] norm1 <- pool1
I0909 15:18:40.950999  6263 net.cpp:290] norm1 -> norm1
I0909 15:18:40.951009  6263 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:18:40.951014  6263 net.cpp:125] norm1 needs backward computation.
I0909 15:18:40.951021  6263 net.cpp:66] Creating Layer conv2
I0909 15:18:40.951027  6263 net.cpp:329] conv2 <- norm1
I0909 15:18:40.951035  6263 net.cpp:290] conv2 -> conv2
I0909 15:18:40.959908  6263 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:18:40.959923  6263 net.cpp:125] conv2 needs backward computation.
I0909 15:18:40.959929  6263 net.cpp:66] Creating Layer relu2
I0909 15:18:40.959934  6263 net.cpp:329] relu2 <- conv2
I0909 15:18:40.959941  6263 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:18:40.959949  6263 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:18:40.959954  6263 net.cpp:125] relu2 needs backward computation.
I0909 15:18:40.959960  6263 net.cpp:66] Creating Layer pool2
I0909 15:18:40.959966  6263 net.cpp:329] pool2 <- conv2
I0909 15:18:40.959972  6263 net.cpp:290] pool2 -> pool2
I0909 15:18:40.959980  6263 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:18:40.959986  6263 net.cpp:125] pool2 needs backward computation.
I0909 15:18:40.959995  6263 net.cpp:66] Creating Layer fc7
I0909 15:18:40.960000  6263 net.cpp:329] fc7 <- pool2
I0909 15:18:40.960008  6263 net.cpp:290] fc7 -> fc7
I0909 15:18:41.584090  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.584138  6263 net.cpp:125] fc7 needs backward computation.
I0909 15:18:41.584151  6263 net.cpp:66] Creating Layer relu7
I0909 15:18:41.584158  6263 net.cpp:329] relu7 <- fc7
I0909 15:18:41.584167  6263 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:18:41.584187  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.584193  6263 net.cpp:125] relu7 needs backward computation.
I0909 15:18:41.584202  6263 net.cpp:66] Creating Layer drop7
I0909 15:18:41.584208  6263 net.cpp:329] drop7 <- fc7
I0909 15:18:41.584213  6263 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:18:41.584225  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.584230  6263 net.cpp:125] drop7 needs backward computation.
I0909 15:18:41.584241  6263 net.cpp:66] Creating Layer fc8
I0909 15:18:41.584246  6263 net.cpp:329] fc8 <- fc7
I0909 15:18:41.584254  6263 net.cpp:290] fc8 -> fc8
I0909 15:18:41.591791  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.591804  6263 net.cpp:125] fc8 needs backward computation.
I0909 15:18:41.591814  6263 net.cpp:66] Creating Layer relu8
I0909 15:18:41.591820  6263 net.cpp:329] relu8 <- fc8
I0909 15:18:41.591825  6263 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:18:41.591833  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.591838  6263 net.cpp:125] relu8 needs backward computation.
I0909 15:18:41.591845  6263 net.cpp:66] Creating Layer drop8
I0909 15:18:41.591850  6263 net.cpp:329] drop8 <- fc8
I0909 15:18:41.591858  6263 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:18:41.591866  6263 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:18:41.591871  6263 net.cpp:125] drop8 needs backward computation.
I0909 15:18:41.591878  6263 net.cpp:66] Creating Layer fc9
I0909 15:18:41.591884  6263 net.cpp:329] fc9 <- fc8
I0909 15:18:41.591892  6263 net.cpp:290] fc9 -> fc9
I0909 15:18:41.592255  6263 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:18:41.592267  6263 net.cpp:125] fc9 needs backward computation.
I0909 15:18:41.592274  6263 net.cpp:66] Creating Layer fc10
I0909 15:18:41.592279  6263 net.cpp:329] fc10 <- fc9
I0909 15:18:41.592288  6263 net.cpp:290] fc10 -> fc10
I0909 15:18:41.592300  6263 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:18:41.592308  6263 net.cpp:125] fc10 needs backward computation.
I0909 15:18:41.592314  6263 net.cpp:66] Creating Layer prob
I0909 15:18:41.592320  6263 net.cpp:329] prob <- fc10
I0909 15:18:41.592329  6263 net.cpp:290] prob -> prob
I0909 15:18:41.592337  6263 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:18:41.592344  6263 net.cpp:125] prob needs backward computation.
I0909 15:18:41.592349  6263 net.cpp:156] This network produces output prob
I0909 15:18:41.592362  6263 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:18:41.592370  6263 net.cpp:167] Network initialization done.
I0909 15:18:41.592376  6263 net.cpp:168] Memory required for data: 6183480
Classifying 397 inputs.
Done in 231.84 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:22:42.226979  6330 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:22:42.227121  6330 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:22:42.227131  6330 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:22:42.227283  6330 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:22:42.227349  6330 net.cpp:292] Input 0 -> data
I0909 15:22:42.227375  6330 net.cpp:66] Creating Layer conv1
I0909 15:22:42.227383  6330 net.cpp:329] conv1 <- data
I0909 15:22:42.227391  6330 net.cpp:290] conv1 -> conv1
I0909 15:22:42.228801  6330 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:22:42.228819  6330 net.cpp:125] conv1 needs backward computation.
I0909 15:22:42.228828  6330 net.cpp:66] Creating Layer relu1
I0909 15:22:42.228834  6330 net.cpp:329] relu1 <- conv1
I0909 15:22:42.228842  6330 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:22:42.228850  6330 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:22:42.228857  6330 net.cpp:125] relu1 needs backward computation.
I0909 15:22:42.228863  6330 net.cpp:66] Creating Layer pool1
I0909 15:22:42.228868  6330 net.cpp:329] pool1 <- conv1
I0909 15:22:42.228875  6330 net.cpp:290] pool1 -> pool1
I0909 15:22:42.228888  6330 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:22:42.228893  6330 net.cpp:125] pool1 needs backward computation.
I0909 15:22:42.228900  6330 net.cpp:66] Creating Layer norm1
I0909 15:22:42.228905  6330 net.cpp:329] norm1 <- pool1
I0909 15:22:42.228912  6330 net.cpp:290] norm1 -> norm1
I0909 15:22:42.228922  6330 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:22:42.228929  6330 net.cpp:125] norm1 needs backward computation.
I0909 15:22:42.228935  6330 net.cpp:66] Creating Layer conv2
I0909 15:22:42.228941  6330 net.cpp:329] conv2 <- norm1
I0909 15:22:42.228948  6330 net.cpp:290] conv2 -> conv2
I0909 15:22:42.238039  6330 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:22:42.238060  6330 net.cpp:125] conv2 needs backward computation.
I0909 15:22:42.238068  6330 net.cpp:66] Creating Layer relu2
I0909 15:22:42.238080  6330 net.cpp:329] relu2 <- conv2
I0909 15:22:42.238088  6330 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:22:42.238095  6330 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:22:42.238101  6330 net.cpp:125] relu2 needs backward computation.
I0909 15:22:42.238107  6330 net.cpp:66] Creating Layer pool2
I0909 15:22:42.238113  6330 net.cpp:329] pool2 <- conv2
I0909 15:22:42.238119  6330 net.cpp:290] pool2 -> pool2
I0909 15:22:42.238127  6330 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:22:42.238133  6330 net.cpp:125] pool2 needs backward computation.
I0909 15:22:42.238140  6330 net.cpp:66] Creating Layer fc7
I0909 15:22:42.238147  6330 net.cpp:329] fc7 <- pool2
I0909 15:22:42.238153  6330 net.cpp:290] fc7 -> fc7
I0909 15:22:42.877368  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.877411  6330 net.cpp:125] fc7 needs backward computation.
I0909 15:22:42.877424  6330 net.cpp:66] Creating Layer relu7
I0909 15:22:42.877430  6330 net.cpp:329] relu7 <- fc7
I0909 15:22:42.877439  6330 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:22:42.877449  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.877454  6330 net.cpp:125] relu7 needs backward computation.
I0909 15:22:42.877461  6330 net.cpp:66] Creating Layer drop7
I0909 15:22:42.877466  6330 net.cpp:329] drop7 <- fc7
I0909 15:22:42.877473  6330 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:22:42.877485  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.877490  6330 net.cpp:125] drop7 needs backward computation.
I0909 15:22:42.877498  6330 net.cpp:66] Creating Layer fc8
I0909 15:22:42.877503  6330 net.cpp:329] fc8 <- fc7
I0909 15:22:42.877519  6330 net.cpp:290] fc8 -> fc8
I0909 15:22:42.885260  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.885273  6330 net.cpp:125] fc8 needs backward computation.
I0909 15:22:42.885279  6330 net.cpp:66] Creating Layer relu8
I0909 15:22:42.885285  6330 net.cpp:329] relu8 <- fc8
I0909 15:22:42.885293  6330 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:22:42.885300  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.885306  6330 net.cpp:125] relu8 needs backward computation.
I0909 15:22:42.885313  6330 net.cpp:66] Creating Layer drop8
I0909 15:22:42.885318  6330 net.cpp:329] drop8 <- fc8
I0909 15:22:42.885324  6330 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:22:42.885330  6330 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:22:42.885336  6330 net.cpp:125] drop8 needs backward computation.
I0909 15:22:42.885345  6330 net.cpp:66] Creating Layer fc9
I0909 15:22:42.885350  6330 net.cpp:329] fc9 <- fc8
I0909 15:22:42.885357  6330 net.cpp:290] fc9 -> fc9
I0909 15:22:42.885742  6330 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:22:42.885756  6330 net.cpp:125] fc9 needs backward computation.
I0909 15:22:42.885763  6330 net.cpp:66] Creating Layer fc10
I0909 15:22:42.885769  6330 net.cpp:329] fc10 <- fc9
I0909 15:22:42.885777  6330 net.cpp:290] fc10 -> fc10
I0909 15:22:42.885789  6330 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:22:42.885797  6330 net.cpp:125] fc10 needs backward computation.
I0909 15:22:42.885803  6330 net.cpp:66] Creating Layer prob
I0909 15:22:42.885809  6330 net.cpp:329] prob <- fc10
I0909 15:22:42.885817  6330 net.cpp:290] prob -> prob
I0909 15:22:42.885828  6330 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:22:42.885833  6330 net.cpp:125] prob needs backward computation.
I0909 15:22:42.885838  6330 net.cpp:156] This network produces output prob
I0909 15:22:42.885850  6330 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:22:42.885859  6330 net.cpp:167] Network initialization done.
I0909 15:22:42.885864  6330 net.cpp:168] Memory required for data: 6183480
Classifying 27 inputs.
Done in 17.13 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:23:01.291441  6334 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:23:01.291587  6334 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:23:01.291607  6334 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:23:01.291756  6334 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:23:01.291810  6334 net.cpp:292] Input 0 -> data
I0909 15:23:01.291836  6334 net.cpp:66] Creating Layer conv1
I0909 15:23:01.291843  6334 net.cpp:329] conv1 <- data
I0909 15:23:01.291851  6334 net.cpp:290] conv1 -> conv1
I0909 15:23:01.293280  6334 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:23:01.293303  6334 net.cpp:125] conv1 needs backward computation.
I0909 15:23:01.293313  6334 net.cpp:66] Creating Layer relu1
I0909 15:23:01.293319  6334 net.cpp:329] relu1 <- conv1
I0909 15:23:01.293326  6334 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:23:01.293336  6334 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:23:01.293342  6334 net.cpp:125] relu1 needs backward computation.
I0909 15:23:01.293355  6334 net.cpp:66] Creating Layer pool1
I0909 15:23:01.293361  6334 net.cpp:329] pool1 <- conv1
I0909 15:23:01.293369  6334 net.cpp:290] pool1 -> pool1
I0909 15:23:01.293380  6334 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:23:01.293386  6334 net.cpp:125] pool1 needs backward computation.
I0909 15:23:01.293393  6334 net.cpp:66] Creating Layer norm1
I0909 15:23:01.293400  6334 net.cpp:329] norm1 <- pool1
I0909 15:23:01.293406  6334 net.cpp:290] norm1 -> norm1
I0909 15:23:01.293416  6334 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:23:01.293421  6334 net.cpp:125] norm1 needs backward computation.
I0909 15:23:01.293429  6334 net.cpp:66] Creating Layer conv2
I0909 15:23:01.293436  6334 net.cpp:329] conv2 <- norm1
I0909 15:23:01.293442  6334 net.cpp:290] conv2 -> conv2
I0909 15:23:01.302652  6334 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:23:01.302678  6334 net.cpp:125] conv2 needs backward computation.
I0909 15:23:01.302687  6334 net.cpp:66] Creating Layer relu2
I0909 15:23:01.302693  6334 net.cpp:329] relu2 <- conv2
I0909 15:23:01.302700  6334 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:23:01.302708  6334 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:23:01.302714  6334 net.cpp:125] relu2 needs backward computation.
I0909 15:23:01.302721  6334 net.cpp:66] Creating Layer pool2
I0909 15:23:01.302726  6334 net.cpp:329] pool2 <- conv2
I0909 15:23:01.302732  6334 net.cpp:290] pool2 -> pool2
I0909 15:23:01.302742  6334 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:23:01.302747  6334 net.cpp:125] pool2 needs backward computation.
I0909 15:23:01.302757  6334 net.cpp:66] Creating Layer fc7
I0909 15:23:01.302762  6334 net.cpp:329] fc7 <- pool2
I0909 15:23:01.302769  6334 net.cpp:290] fc7 -> fc7
I0909 15:23:01.941826  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.941869  6334 net.cpp:125] fc7 needs backward computation.
I0909 15:23:01.941880  6334 net.cpp:66] Creating Layer relu7
I0909 15:23:01.941887  6334 net.cpp:329] relu7 <- fc7
I0909 15:23:01.941895  6334 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:23:01.941905  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.941910  6334 net.cpp:125] relu7 needs backward computation.
I0909 15:23:01.941917  6334 net.cpp:66] Creating Layer drop7
I0909 15:23:01.941923  6334 net.cpp:329] drop7 <- fc7
I0909 15:23:01.941929  6334 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:23:01.941941  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.941946  6334 net.cpp:125] drop7 needs backward computation.
I0909 15:23:01.941956  6334 net.cpp:66] Creating Layer fc8
I0909 15:23:01.941968  6334 net.cpp:329] fc8 <- fc7
I0909 15:23:01.941977  6334 net.cpp:290] fc8 -> fc8
I0909 15:23:01.949774  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.949787  6334 net.cpp:125] fc8 needs backward computation.
I0909 15:23:01.949796  6334 net.cpp:66] Creating Layer relu8
I0909 15:23:01.949801  6334 net.cpp:329] relu8 <- fc8
I0909 15:23:01.949808  6334 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:23:01.949815  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.949820  6334 net.cpp:125] relu8 needs backward computation.
I0909 15:23:01.949826  6334 net.cpp:66] Creating Layer drop8
I0909 15:23:01.949831  6334 net.cpp:329] drop8 <- fc8
I0909 15:23:01.949839  6334 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:23:01.949846  6334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:23:01.949852  6334 net.cpp:125] drop8 needs backward computation.
I0909 15:23:01.949859  6334 net.cpp:66] Creating Layer fc9
I0909 15:23:01.949864  6334 net.cpp:329] fc9 <- fc8
I0909 15:23:01.949872  6334 net.cpp:290] fc9 -> fc9
I0909 15:23:01.950235  6334 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:23:01.950245  6334 net.cpp:125] fc9 needs backward computation.
I0909 15:23:01.950253  6334 net.cpp:66] Creating Layer fc10
I0909 15:23:01.950258  6334 net.cpp:329] fc10 <- fc9
I0909 15:23:01.950268  6334 net.cpp:290] fc10 -> fc10
I0909 15:23:01.950278  6334 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:23:01.950285  6334 net.cpp:125] fc10 needs backward computation.
I0909 15:23:01.950302  6334 net.cpp:66] Creating Layer prob
I0909 15:23:01.950307  6334 net.cpp:329] prob <- fc10
I0909 15:23:01.950315  6334 net.cpp:290] prob -> prob
I0909 15:23:01.950325  6334 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:23:01.950330  6334 net.cpp:125] prob needs backward computation.
I0909 15:23:01.950335  6334 net.cpp:156] This network produces output prob
I0909 15:23:01.950347  6334 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:23:01.950356  6334 net.cpp:167] Network initialization done.
I0909 15:23:01.950361  6334 net.cpp:168] Memory required for data: 6183480
Classifying 237 inputs.
Done in 370.74 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:30:34.610677  6364 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:30:34.610833  6364 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:30:34.610841  6364 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:30:34.611068  6364 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:30:34.611129  6364 net.cpp:292] Input 0 -> data
I0909 15:30:34.611155  6364 net.cpp:66] Creating Layer conv1
I0909 15:30:34.611161  6364 net.cpp:329] conv1 <- data
I0909 15:30:34.611170  6364 net.cpp:290] conv1 -> conv1
I0909 15:30:34.651850  6364 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:30:34.651890  6364 net.cpp:125] conv1 needs backward computation.
I0909 15:30:34.651903  6364 net.cpp:66] Creating Layer relu1
I0909 15:30:34.651911  6364 net.cpp:329] relu1 <- conv1
I0909 15:30:34.651918  6364 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:30:34.651929  6364 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:30:34.651934  6364 net.cpp:125] relu1 needs backward computation.
I0909 15:30:34.651943  6364 net.cpp:66] Creating Layer pool1
I0909 15:30:34.651948  6364 net.cpp:329] pool1 <- conv1
I0909 15:30:34.651955  6364 net.cpp:290] pool1 -> pool1
I0909 15:30:34.651968  6364 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:30:34.651973  6364 net.cpp:125] pool1 needs backward computation.
I0909 15:30:34.651980  6364 net.cpp:66] Creating Layer norm1
I0909 15:30:34.651986  6364 net.cpp:329] norm1 <- pool1
I0909 15:30:34.651993  6364 net.cpp:290] norm1 -> norm1
I0909 15:30:34.652003  6364 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:30:34.652009  6364 net.cpp:125] norm1 needs backward computation.
I0909 15:30:34.652017  6364 net.cpp:66] Creating Layer conv2
I0909 15:30:34.652024  6364 net.cpp:329] conv2 <- norm1
I0909 15:30:34.652030  6364 net.cpp:290] conv2 -> conv2
I0909 15:30:34.661207  6364 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:30:34.661238  6364 net.cpp:125] conv2 needs backward computation.
I0909 15:30:34.661247  6364 net.cpp:66] Creating Layer relu2
I0909 15:30:34.661253  6364 net.cpp:329] relu2 <- conv2
I0909 15:30:34.661260  6364 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:30:34.661269  6364 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:30:34.661275  6364 net.cpp:125] relu2 needs backward computation.
I0909 15:30:34.661283  6364 net.cpp:66] Creating Layer pool2
I0909 15:30:34.661288  6364 net.cpp:329] pool2 <- conv2
I0909 15:30:34.661294  6364 net.cpp:290] pool2 -> pool2
I0909 15:30:34.661303  6364 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:30:34.661309  6364 net.cpp:125] pool2 needs backward computation.
I0909 15:30:34.661319  6364 net.cpp:66] Creating Layer fc7
I0909 15:30:34.661324  6364 net.cpp:329] fc7 <- pool2
I0909 15:30:34.661332  6364 net.cpp:290] fc7 -> fc7
I0909 15:30:35.305351  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.305393  6364 net.cpp:125] fc7 needs backward computation.
I0909 15:30:35.305407  6364 net.cpp:66] Creating Layer relu7
I0909 15:30:35.305413  6364 net.cpp:329] relu7 <- fc7
I0909 15:30:35.305421  6364 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:30:35.305430  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.305436  6364 net.cpp:125] relu7 needs backward computation.
I0909 15:30:35.305444  6364 net.cpp:66] Creating Layer drop7
I0909 15:30:35.305449  6364 net.cpp:329] drop7 <- fc7
I0909 15:30:35.305455  6364 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:30:35.305465  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.305471  6364 net.cpp:125] drop7 needs backward computation.
I0909 15:30:35.305481  6364 net.cpp:66] Creating Layer fc8
I0909 15:30:35.305487  6364 net.cpp:329] fc8 <- fc7
I0909 15:30:35.305501  6364 net.cpp:290] fc8 -> fc8
I0909 15:30:35.313112  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.313127  6364 net.cpp:125] fc8 needs backward computation.
I0909 15:30:35.313135  6364 net.cpp:66] Creating Layer relu8
I0909 15:30:35.313141  6364 net.cpp:329] relu8 <- fc8
I0909 15:30:35.313148  6364 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:30:35.313163  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.313169  6364 net.cpp:125] relu8 needs backward computation.
I0909 15:30:35.313176  6364 net.cpp:66] Creating Layer drop8
I0909 15:30:35.313181  6364 net.cpp:329] drop8 <- fc8
I0909 15:30:35.313189  6364 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:30:35.313195  6364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:30:35.313201  6364 net.cpp:125] drop8 needs backward computation.
I0909 15:30:35.313208  6364 net.cpp:66] Creating Layer fc9
I0909 15:30:35.313213  6364 net.cpp:329] fc9 <- fc8
I0909 15:30:35.313220  6364 net.cpp:290] fc9 -> fc9
I0909 15:30:35.313601  6364 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:30:35.313613  6364 net.cpp:125] fc9 needs backward computation.
I0909 15:30:35.313621  6364 net.cpp:66] Creating Layer fc10
I0909 15:30:35.313627  6364 net.cpp:329] fc10 <- fc9
I0909 15:30:35.313637  6364 net.cpp:290] fc10 -> fc10
I0909 15:30:35.313647  6364 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:30:35.313655  6364 net.cpp:125] fc10 needs backward computation.
I0909 15:30:35.313662  6364 net.cpp:66] Creating Layer prob
I0909 15:30:35.313668  6364 net.cpp:329] prob <- fc10
I0909 15:30:35.313674  6364 net.cpp:290] prob -> prob
I0909 15:30:35.313683  6364 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:30:35.313689  6364 net.cpp:125] prob needs backward computation.
I0909 15:30:35.313694  6364 net.cpp:156] This network produces output prob
I0909 15:30:35.313706  6364 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:30:35.313714  6364 net.cpp:167] Network initialization done.
I0909 15:30:35.313719  6364 net.cpp:168] Memory required for data: 6183480
Classifying 28 inputs.
Done in 17.44 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:31:01.004667  6370 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:31:01.004802  6370 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:31:01.004812  6370 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:31:01.004956  6370 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:31:01.005018  6370 net.cpp:292] Input 0 -> data
I0909 15:31:01.005043  6370 net.cpp:66] Creating Layer conv1
I0909 15:31:01.005050  6370 net.cpp:329] conv1 <- data
I0909 15:31:01.005059  6370 net.cpp:290] conv1 -> conv1
I0909 15:31:01.006430  6370 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:01.006449  6370 net.cpp:125] conv1 needs backward computation.
I0909 15:31:01.006458  6370 net.cpp:66] Creating Layer relu1
I0909 15:31:01.006464  6370 net.cpp:329] relu1 <- conv1
I0909 15:31:01.006470  6370 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:31:01.006479  6370 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:01.006484  6370 net.cpp:125] relu1 needs backward computation.
I0909 15:31:01.006491  6370 net.cpp:66] Creating Layer pool1
I0909 15:31:01.006496  6370 net.cpp:329] pool1 <- conv1
I0909 15:31:01.006502  6370 net.cpp:290] pool1 -> pool1
I0909 15:31:01.006513  6370 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:01.006520  6370 net.cpp:125] pool1 needs backward computation.
I0909 15:31:01.006526  6370 net.cpp:66] Creating Layer norm1
I0909 15:31:01.006531  6370 net.cpp:329] norm1 <- pool1
I0909 15:31:01.006537  6370 net.cpp:290] norm1 -> norm1
I0909 15:31:01.006547  6370 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:01.006552  6370 net.cpp:125] norm1 needs backward computation.
I0909 15:31:01.006559  6370 net.cpp:66] Creating Layer conv2
I0909 15:31:01.006566  6370 net.cpp:329] conv2 <- norm1
I0909 15:31:01.006572  6370 net.cpp:290] conv2 -> conv2
I0909 15:31:01.015876  6370 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:01.015908  6370 net.cpp:125] conv2 needs backward computation.
I0909 15:31:01.015918  6370 net.cpp:66] Creating Layer relu2
I0909 15:31:01.015924  6370 net.cpp:329] relu2 <- conv2
I0909 15:31:01.015931  6370 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:31:01.015940  6370 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:01.015945  6370 net.cpp:125] relu2 needs backward computation.
I0909 15:31:01.015952  6370 net.cpp:66] Creating Layer pool2
I0909 15:31:01.015959  6370 net.cpp:329] pool2 <- conv2
I0909 15:31:01.015964  6370 net.cpp:290] pool2 -> pool2
I0909 15:31:01.015974  6370 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:31:01.015980  6370 net.cpp:125] pool2 needs backward computation.
I0909 15:31:01.015986  6370 net.cpp:66] Creating Layer fc7
I0909 15:31:01.015992  6370 net.cpp:329] fc7 <- pool2
I0909 15:31:01.016000  6370 net.cpp:290] fc7 -> fc7
I0909 15:31:01.659675  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.659720  6370 net.cpp:125] fc7 needs backward computation.
I0909 15:31:01.659734  6370 net.cpp:66] Creating Layer relu7
I0909 15:31:01.659751  6370 net.cpp:329] relu7 <- fc7
I0909 15:31:01.659760  6370 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:31:01.659770  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.659776  6370 net.cpp:125] relu7 needs backward computation.
I0909 15:31:01.659783  6370 net.cpp:66] Creating Layer drop7
I0909 15:31:01.659788  6370 net.cpp:329] drop7 <- fc7
I0909 15:31:01.659795  6370 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:31:01.659806  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.659811  6370 net.cpp:125] drop7 needs backward computation.
I0909 15:31:01.659819  6370 net.cpp:66] Creating Layer fc8
I0909 15:31:01.659826  6370 net.cpp:329] fc8 <- fc7
I0909 15:31:01.659834  6370 net.cpp:290] fc8 -> fc8
I0909 15:31:01.667511  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.667526  6370 net.cpp:125] fc8 needs backward computation.
I0909 15:31:01.667532  6370 net.cpp:66] Creating Layer relu8
I0909 15:31:01.667538  6370 net.cpp:329] relu8 <- fc8
I0909 15:31:01.667546  6370 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:31:01.667554  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.667559  6370 net.cpp:125] relu8 needs backward computation.
I0909 15:31:01.667567  6370 net.cpp:66] Creating Layer drop8
I0909 15:31:01.667572  6370 net.cpp:329] drop8 <- fc8
I0909 15:31:01.667577  6370 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:31:01.667584  6370 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:01.667589  6370 net.cpp:125] drop8 needs backward computation.
I0909 15:31:01.667598  6370 net.cpp:66] Creating Layer fc9
I0909 15:31:01.667603  6370 net.cpp:329] fc9 <- fc8
I0909 15:31:01.667618  6370 net.cpp:290] fc9 -> fc9
I0909 15:31:01.667987  6370 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:31:01.667999  6370 net.cpp:125] fc9 needs backward computation.
I0909 15:31:01.668006  6370 net.cpp:66] Creating Layer fc10
I0909 15:31:01.668012  6370 net.cpp:329] fc10 <- fc9
I0909 15:31:01.668021  6370 net.cpp:290] fc10 -> fc10
I0909 15:31:01.668033  6370 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:01.668040  6370 net.cpp:125] fc10 needs backward computation.
I0909 15:31:01.668047  6370 net.cpp:66] Creating Layer prob
I0909 15:31:01.668052  6370 net.cpp:329] prob <- fc10
I0909 15:31:01.668059  6370 net.cpp:290] prob -> prob
I0909 15:31:01.668068  6370 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:01.668074  6370 net.cpp:125] prob needs backward computation.
I0909 15:31:01.668079  6370 net.cpp:156] This network produces output prob
I0909 15:31:01.668092  6370 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:31:01.668099  6370 net.cpp:167] Network initialization done.
I0909 15:31:01.668104  6370 net.cpp:168] Memory required for data: 6183480
Classifying 200 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:31:11.434918  6374 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:31:11.435055  6374 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:31:11.435071  6374 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:31:11.435214  6374 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:31:11.435273  6374 net.cpp:292] Input 0 -> data
I0909 15:31:11.435298  6374 net.cpp:66] Creating Layer conv1
I0909 15:31:11.435304  6374 net.cpp:329] conv1 <- data
I0909 15:31:11.435312  6374 net.cpp:290] conv1 -> conv1
I0909 15:31:11.436632  6374 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:11.436650  6374 net.cpp:125] conv1 needs backward computation.
I0909 15:31:11.436658  6374 net.cpp:66] Creating Layer relu1
I0909 15:31:11.436663  6374 net.cpp:329] relu1 <- conv1
I0909 15:31:11.436671  6374 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:31:11.436678  6374 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:11.436684  6374 net.cpp:125] relu1 needs backward computation.
I0909 15:31:11.436691  6374 net.cpp:66] Creating Layer pool1
I0909 15:31:11.436697  6374 net.cpp:329] pool1 <- conv1
I0909 15:31:11.436702  6374 net.cpp:290] pool1 -> pool1
I0909 15:31:11.436713  6374 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:11.436719  6374 net.cpp:125] pool1 needs backward computation.
I0909 15:31:11.436725  6374 net.cpp:66] Creating Layer norm1
I0909 15:31:11.436730  6374 net.cpp:329] norm1 <- pool1
I0909 15:31:11.436738  6374 net.cpp:290] norm1 -> norm1
I0909 15:31:11.436751  6374 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:11.436758  6374 net.cpp:125] norm1 needs backward computation.
I0909 15:31:11.436764  6374 net.cpp:66] Creating Layer conv2
I0909 15:31:11.436770  6374 net.cpp:329] conv2 <- norm1
I0909 15:31:11.436777  6374 net.cpp:290] conv2 -> conv2
I0909 15:31:11.445940  6374 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:11.445971  6374 net.cpp:125] conv2 needs backward computation.
I0909 15:31:11.445979  6374 net.cpp:66] Creating Layer relu2
I0909 15:31:11.445986  6374 net.cpp:329] relu2 <- conv2
I0909 15:31:11.445993  6374 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:31:11.446002  6374 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:11.446007  6374 net.cpp:125] relu2 needs backward computation.
I0909 15:31:11.446013  6374 net.cpp:66] Creating Layer pool2
I0909 15:31:11.446019  6374 net.cpp:329] pool2 <- conv2
I0909 15:31:11.446027  6374 net.cpp:290] pool2 -> pool2
I0909 15:31:11.446034  6374 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:31:11.446040  6374 net.cpp:125] pool2 needs backward computation.
I0909 15:31:11.446048  6374 net.cpp:66] Creating Layer fc7
I0909 15:31:11.446053  6374 net.cpp:329] fc7 <- pool2
I0909 15:31:11.446065  6374 net.cpp:290] fc7 -> fc7
I0909 15:31:12.088357  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.088400  6374 net.cpp:125] fc7 needs backward computation.
I0909 15:31:12.088413  6374 net.cpp:66] Creating Layer relu7
I0909 15:31:12.088421  6374 net.cpp:329] relu7 <- fc7
I0909 15:31:12.088429  6374 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:31:12.088438  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.088444  6374 net.cpp:125] relu7 needs backward computation.
I0909 15:31:12.088453  6374 net.cpp:66] Creating Layer drop7
I0909 15:31:12.088457  6374 net.cpp:329] drop7 <- fc7
I0909 15:31:12.088464  6374 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:31:12.088474  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.088480  6374 net.cpp:125] drop7 needs backward computation.
I0909 15:31:12.088488  6374 net.cpp:66] Creating Layer fc8
I0909 15:31:12.088493  6374 net.cpp:329] fc8 <- fc7
I0909 15:31:12.088502  6374 net.cpp:290] fc8 -> fc8
I0909 15:31:12.096161  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.096174  6374 net.cpp:125] fc8 needs backward computation.
I0909 15:31:12.096181  6374 net.cpp:66] Creating Layer relu8
I0909 15:31:12.096186  6374 net.cpp:329] relu8 <- fc8
I0909 15:31:12.096195  6374 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:31:12.096204  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.096209  6374 net.cpp:125] relu8 needs backward computation.
I0909 15:31:12.096215  6374 net.cpp:66] Creating Layer drop8
I0909 15:31:12.096220  6374 net.cpp:329] drop8 <- fc8
I0909 15:31:12.096226  6374 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:31:12.096232  6374 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:12.096238  6374 net.cpp:125] drop8 needs backward computation.
I0909 15:31:12.096253  6374 net.cpp:66] Creating Layer fc9
I0909 15:31:12.096259  6374 net.cpp:329] fc9 <- fc8
I0909 15:31:12.096266  6374 net.cpp:290] fc9 -> fc9
I0909 15:31:12.096637  6374 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:31:12.096647  6374 net.cpp:125] fc9 needs backward computation.
I0909 15:31:12.096662  6374 net.cpp:66] Creating Layer fc10
I0909 15:31:12.096668  6374 net.cpp:329] fc10 <- fc9
I0909 15:31:12.096676  6374 net.cpp:290] fc10 -> fc10
I0909 15:31:12.096688  6374 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:12.096696  6374 net.cpp:125] fc10 needs backward computation.
I0909 15:31:12.096703  6374 net.cpp:66] Creating Layer prob
I0909 15:31:12.096709  6374 net.cpp:329] prob <- fc10
I0909 15:31:12.096715  6374 net.cpp:290] prob -> prob
I0909 15:31:12.096725  6374 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:12.096731  6374 net.cpp:125] prob needs backward computation.
I0909 15:31:12.096736  6374 net.cpp:156] This network produces output prob
I0909 15:31:12.096748  6374 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:31:12.096775  6374 net.cpp:167] Network initialization done.
I0909 15:31:12.096779  6374 net.cpp:168] Memory required for data: 6183480
Classifying 33 inputs.
Done in 22.52 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:31:37.233417  6381 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:31:37.233575  6381 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:31:37.233587  6381 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:31:37.233736  6381 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:31:37.233789  6381 net.cpp:292] Input 0 -> data
I0909 15:31:37.233824  6381 net.cpp:66] Creating Layer conv1
I0909 15:31:37.233830  6381 net.cpp:329] conv1 <- data
I0909 15:31:37.233851  6381 net.cpp:290] conv1 -> conv1
I0909 15:31:37.235302  6381 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:37.235321  6381 net.cpp:125] conv1 needs backward computation.
I0909 15:31:37.235330  6381 net.cpp:66] Creating Layer relu1
I0909 15:31:37.235337  6381 net.cpp:329] relu1 <- conv1
I0909 15:31:37.235343  6381 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:31:37.235352  6381 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:31:37.235357  6381 net.cpp:125] relu1 needs backward computation.
I0909 15:31:37.235364  6381 net.cpp:66] Creating Layer pool1
I0909 15:31:37.235370  6381 net.cpp:329] pool1 <- conv1
I0909 15:31:37.235376  6381 net.cpp:290] pool1 -> pool1
I0909 15:31:37.235388  6381 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:37.235393  6381 net.cpp:125] pool1 needs backward computation.
I0909 15:31:37.235399  6381 net.cpp:66] Creating Layer norm1
I0909 15:31:37.235405  6381 net.cpp:329] norm1 <- pool1
I0909 15:31:37.235411  6381 net.cpp:290] norm1 -> norm1
I0909 15:31:37.235421  6381 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:31:37.235426  6381 net.cpp:125] norm1 needs backward computation.
I0909 15:31:37.235435  6381 net.cpp:66] Creating Layer conv2
I0909 15:31:37.235440  6381 net.cpp:329] conv2 <- norm1
I0909 15:31:37.235447  6381 net.cpp:290] conv2 -> conv2
I0909 15:31:37.244750  6381 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:37.244796  6381 net.cpp:125] conv2 needs backward computation.
I0909 15:31:37.244808  6381 net.cpp:66] Creating Layer relu2
I0909 15:31:37.244815  6381 net.cpp:329] relu2 <- conv2
I0909 15:31:37.244823  6381 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:31:37.244832  6381 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:31:37.244838  6381 net.cpp:125] relu2 needs backward computation.
I0909 15:31:37.244846  6381 net.cpp:66] Creating Layer pool2
I0909 15:31:37.244853  6381 net.cpp:329] pool2 <- conv2
I0909 15:31:37.244859  6381 net.cpp:290] pool2 -> pool2
I0909 15:31:37.244868  6381 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:31:37.244874  6381 net.cpp:125] pool2 needs backward computation.
I0909 15:31:37.244884  6381 net.cpp:66] Creating Layer fc7
I0909 15:31:37.244890  6381 net.cpp:329] fc7 <- pool2
I0909 15:31:37.244899  6381 net.cpp:290] fc7 -> fc7
I0909 15:31:37.895010  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.895052  6381 net.cpp:125] fc7 needs backward computation.
I0909 15:31:37.895066  6381 net.cpp:66] Creating Layer relu7
I0909 15:31:37.895072  6381 net.cpp:329] relu7 <- fc7
I0909 15:31:37.895081  6381 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:31:37.895090  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.895095  6381 net.cpp:125] relu7 needs backward computation.
I0909 15:31:37.895103  6381 net.cpp:66] Creating Layer drop7
I0909 15:31:37.895108  6381 net.cpp:329] drop7 <- fc7
I0909 15:31:37.895115  6381 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:31:37.895126  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.895133  6381 net.cpp:125] drop7 needs backward computation.
I0909 15:31:37.895143  6381 net.cpp:66] Creating Layer fc8
I0909 15:31:37.895148  6381 net.cpp:329] fc8 <- fc7
I0909 15:31:37.895156  6381 net.cpp:290] fc8 -> fc8
I0909 15:31:37.902972  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.902992  6381 net.cpp:125] fc8 needs backward computation.
I0909 15:31:37.903000  6381 net.cpp:66] Creating Layer relu8
I0909 15:31:37.903007  6381 net.cpp:329] relu8 <- fc8
I0909 15:31:37.903013  6381 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:31:37.903022  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.903028  6381 net.cpp:125] relu8 needs backward computation.
I0909 15:31:37.903033  6381 net.cpp:66] Creating Layer drop8
I0909 15:31:37.903039  6381 net.cpp:329] drop8 <- fc8
I0909 15:31:37.903046  6381 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:31:37.903054  6381 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:31:37.903059  6381 net.cpp:125] drop8 needs backward computation.
I0909 15:31:37.903067  6381 net.cpp:66] Creating Layer fc9
I0909 15:31:37.903084  6381 net.cpp:329] fc9 <- fc8
I0909 15:31:37.903091  6381 net.cpp:290] fc9 -> fc9
I0909 15:31:37.903466  6381 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:31:37.903477  6381 net.cpp:125] fc9 needs backward computation.
I0909 15:31:37.903486  6381 net.cpp:66] Creating Layer fc10
I0909 15:31:37.903491  6381 net.cpp:329] fc10 <- fc9
I0909 15:31:37.903499  6381 net.cpp:290] fc10 -> fc10
I0909 15:31:37.903512  6381 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:37.903518  6381 net.cpp:125] fc10 needs backward computation.
I0909 15:31:37.903525  6381 net.cpp:66] Creating Layer prob
I0909 15:31:37.903532  6381 net.cpp:329] prob <- fc10
I0909 15:31:37.903538  6381 net.cpp:290] prob -> prob
I0909 15:31:37.903548  6381 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:31:37.903554  6381 net.cpp:125] prob needs backward computation.
I0909 15:31:37.903559  6381 net.cpp:156] This network produces output prob
I0909 15:31:37.903573  6381 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:31:37.903580  6381 net.cpp:167] Network initialization done.
I0909 15:31:37.903585  6381 net.cpp:168] Memory required for data: 6183480
Classifying 139 inputs.
Done in 89.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:33:10.707944  6385 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:33:10.708083  6385 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:33:10.708091  6385 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:33:10.708235  6385 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:33:10.708297  6385 net.cpp:292] Input 0 -> data
I0909 15:33:10.708322  6385 net.cpp:66] Creating Layer conv1
I0909 15:33:10.708328  6385 net.cpp:329] conv1 <- data
I0909 15:33:10.708336  6385 net.cpp:290] conv1 -> conv1
I0909 15:33:10.709692  6385 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:10.709712  6385 net.cpp:125] conv1 needs backward computation.
I0909 15:33:10.709720  6385 net.cpp:66] Creating Layer relu1
I0909 15:33:10.709725  6385 net.cpp:329] relu1 <- conv1
I0909 15:33:10.709733  6385 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:33:10.709741  6385 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:10.709746  6385 net.cpp:125] relu1 needs backward computation.
I0909 15:33:10.709753  6385 net.cpp:66] Creating Layer pool1
I0909 15:33:10.709758  6385 net.cpp:329] pool1 <- conv1
I0909 15:33:10.709765  6385 net.cpp:290] pool1 -> pool1
I0909 15:33:10.709776  6385 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:10.709781  6385 net.cpp:125] pool1 needs backward computation.
I0909 15:33:10.709789  6385 net.cpp:66] Creating Layer norm1
I0909 15:33:10.709794  6385 net.cpp:329] norm1 <- pool1
I0909 15:33:10.709800  6385 net.cpp:290] norm1 -> norm1
I0909 15:33:10.709810  6385 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:10.709815  6385 net.cpp:125] norm1 needs backward computation.
I0909 15:33:10.709822  6385 net.cpp:66] Creating Layer conv2
I0909 15:33:10.709827  6385 net.cpp:329] conv2 <- norm1
I0909 15:33:10.709835  6385 net.cpp:290] conv2 -> conv2
I0909 15:33:10.718833  6385 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:10.718850  6385 net.cpp:125] conv2 needs backward computation.
I0909 15:33:10.718858  6385 net.cpp:66] Creating Layer relu2
I0909 15:33:10.718863  6385 net.cpp:329] relu2 <- conv2
I0909 15:33:10.718870  6385 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:33:10.718878  6385 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:10.718883  6385 net.cpp:125] relu2 needs backward computation.
I0909 15:33:10.718889  6385 net.cpp:66] Creating Layer pool2
I0909 15:33:10.718895  6385 net.cpp:329] pool2 <- conv2
I0909 15:33:10.718901  6385 net.cpp:290] pool2 -> pool2
I0909 15:33:10.718909  6385 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:33:10.718915  6385 net.cpp:125] pool2 needs backward computation.
I0909 15:33:10.718924  6385 net.cpp:66] Creating Layer fc7
I0909 15:33:10.718930  6385 net.cpp:329] fc7 <- pool2
I0909 15:33:10.718937  6385 net.cpp:290] fc7 -> fc7
I0909 15:33:11.359482  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.359524  6385 net.cpp:125] fc7 needs backward computation.
I0909 15:33:11.359535  6385 net.cpp:66] Creating Layer relu7
I0909 15:33:11.359544  6385 net.cpp:329] relu7 <- fc7
I0909 15:33:11.359550  6385 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:33:11.359560  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.359565  6385 net.cpp:125] relu7 needs backward computation.
I0909 15:33:11.359573  6385 net.cpp:66] Creating Layer drop7
I0909 15:33:11.359578  6385 net.cpp:329] drop7 <- fc7
I0909 15:33:11.359585  6385 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:33:11.359596  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.359611  6385 net.cpp:125] drop7 needs backward computation.
I0909 15:33:11.359622  6385 net.cpp:66] Creating Layer fc8
I0909 15:33:11.359627  6385 net.cpp:329] fc8 <- fc7
I0909 15:33:11.359635  6385 net.cpp:290] fc8 -> fc8
I0909 15:33:11.367552  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.367579  6385 net.cpp:125] fc8 needs backward computation.
I0909 15:33:11.367590  6385 net.cpp:66] Creating Layer relu8
I0909 15:33:11.367597  6385 net.cpp:329] relu8 <- fc8
I0909 15:33:11.367604  6385 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:33:11.367614  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.367619  6385 net.cpp:125] relu8 needs backward computation.
I0909 15:33:11.367625  6385 net.cpp:66] Creating Layer drop8
I0909 15:33:11.367630  6385 net.cpp:329] drop8 <- fc8
I0909 15:33:11.367638  6385 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:33:11.367646  6385 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:11.367651  6385 net.cpp:125] drop8 needs backward computation.
I0909 15:33:11.367660  6385 net.cpp:66] Creating Layer fc9
I0909 15:33:11.367666  6385 net.cpp:329] fc9 <- fc8
I0909 15:33:11.367672  6385 net.cpp:290] fc9 -> fc9
I0909 15:33:11.368051  6385 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:33:11.368062  6385 net.cpp:125] fc9 needs backward computation.
I0909 15:33:11.368069  6385 net.cpp:66] Creating Layer fc10
I0909 15:33:11.368075  6385 net.cpp:329] fc10 <- fc9
I0909 15:33:11.368084  6385 net.cpp:290] fc10 -> fc10
I0909 15:33:11.368095  6385 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:11.368103  6385 net.cpp:125] fc10 needs backward computation.
I0909 15:33:11.368110  6385 net.cpp:66] Creating Layer prob
I0909 15:33:11.368115  6385 net.cpp:329] prob <- fc10
I0909 15:33:11.368124  6385 net.cpp:290] prob -> prob
I0909 15:33:11.368134  6385 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:11.368139  6385 net.cpp:125] prob needs backward computation.
I0909 15:33:11.368144  6385 net.cpp:156] This network produces output prob
I0909 15:33:11.368157  6385 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:33:11.368165  6385 net.cpp:167] Network initialization done.
I0909 15:33:11.368170  6385 net.cpp:168] Memory required for data: 6183480
Classifying 15 inputs.
Done in 9.16 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:33:22.180158  6389 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:33:22.180295  6389 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:33:22.180305  6389 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:33:22.180445  6389 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:33:22.180506  6389 net.cpp:292] Input 0 -> data
I0909 15:33:22.180531  6389 net.cpp:66] Creating Layer conv1
I0909 15:33:22.180537  6389 net.cpp:329] conv1 <- data
I0909 15:33:22.180546  6389 net.cpp:290] conv1 -> conv1
I0909 15:33:22.181911  6389 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:22.181937  6389 net.cpp:125] conv1 needs backward computation.
I0909 15:33:22.181946  6389 net.cpp:66] Creating Layer relu1
I0909 15:33:22.181952  6389 net.cpp:329] relu1 <- conv1
I0909 15:33:22.181958  6389 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:33:22.181967  6389 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:22.181973  6389 net.cpp:125] relu1 needs backward computation.
I0909 15:33:22.181980  6389 net.cpp:66] Creating Layer pool1
I0909 15:33:22.181985  6389 net.cpp:329] pool1 <- conv1
I0909 15:33:22.181993  6389 net.cpp:290] pool1 -> pool1
I0909 15:33:22.182003  6389 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:22.182009  6389 net.cpp:125] pool1 needs backward computation.
I0909 15:33:22.182016  6389 net.cpp:66] Creating Layer norm1
I0909 15:33:22.182021  6389 net.cpp:329] norm1 <- pool1
I0909 15:33:22.182029  6389 net.cpp:290] norm1 -> norm1
I0909 15:33:22.182037  6389 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:22.182044  6389 net.cpp:125] norm1 needs backward computation.
I0909 15:33:22.182050  6389 net.cpp:66] Creating Layer conv2
I0909 15:33:22.182056  6389 net.cpp:329] conv2 <- norm1
I0909 15:33:22.182063  6389 net.cpp:290] conv2 -> conv2
I0909 15:33:22.191037  6389 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:22.191051  6389 net.cpp:125] conv2 needs backward computation.
I0909 15:33:22.191058  6389 net.cpp:66] Creating Layer relu2
I0909 15:33:22.191063  6389 net.cpp:329] relu2 <- conv2
I0909 15:33:22.191071  6389 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:33:22.191076  6389 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:22.191082  6389 net.cpp:125] relu2 needs backward computation.
I0909 15:33:22.191088  6389 net.cpp:66] Creating Layer pool2
I0909 15:33:22.191093  6389 net.cpp:329] pool2 <- conv2
I0909 15:33:22.191099  6389 net.cpp:290] pool2 -> pool2
I0909 15:33:22.191112  6389 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:33:22.191118  6389 net.cpp:125] pool2 needs backward computation.
I0909 15:33:22.191125  6389 net.cpp:66] Creating Layer fc7
I0909 15:33:22.191130  6389 net.cpp:329] fc7 <- pool2
I0909 15:33:22.191139  6389 net.cpp:290] fc7 -> fc7
I0909 15:33:22.831909  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.831951  6389 net.cpp:125] fc7 needs backward computation.
I0909 15:33:22.831964  6389 net.cpp:66] Creating Layer relu7
I0909 15:33:22.831972  6389 net.cpp:329] relu7 <- fc7
I0909 15:33:22.831980  6389 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:33:22.831990  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.831995  6389 net.cpp:125] relu7 needs backward computation.
I0909 15:33:22.832002  6389 net.cpp:66] Creating Layer drop7
I0909 15:33:22.832008  6389 net.cpp:329] drop7 <- fc7
I0909 15:33:22.832015  6389 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:33:22.832026  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.832031  6389 net.cpp:125] drop7 needs backward computation.
I0909 15:33:22.832039  6389 net.cpp:66] Creating Layer fc8
I0909 15:33:22.832044  6389 net.cpp:329] fc8 <- fc7
I0909 15:33:22.832053  6389 net.cpp:290] fc8 -> fc8
I0909 15:33:22.839815  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.839831  6389 net.cpp:125] fc8 needs backward computation.
I0909 15:33:22.839838  6389 net.cpp:66] Creating Layer relu8
I0909 15:33:22.839844  6389 net.cpp:329] relu8 <- fc8
I0909 15:33:22.839853  6389 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:33:22.839859  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.839864  6389 net.cpp:125] relu8 needs backward computation.
I0909 15:33:22.839871  6389 net.cpp:66] Creating Layer drop8
I0909 15:33:22.839876  6389 net.cpp:329] drop8 <- fc8
I0909 15:33:22.839882  6389 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:33:22.839890  6389 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:22.839895  6389 net.cpp:125] drop8 needs backward computation.
I0909 15:33:22.839903  6389 net.cpp:66] Creating Layer fc9
I0909 15:33:22.839908  6389 net.cpp:329] fc9 <- fc8
I0909 15:33:22.839915  6389 net.cpp:290] fc9 -> fc9
I0909 15:33:22.840279  6389 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:33:22.840291  6389 net.cpp:125] fc9 needs backward computation.
I0909 15:33:22.840299  6389 net.cpp:66] Creating Layer fc10
I0909 15:33:22.840304  6389 net.cpp:329] fc10 <- fc9
I0909 15:33:22.840312  6389 net.cpp:290] fc10 -> fc10
I0909 15:33:22.840324  6389 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:22.840332  6389 net.cpp:125] fc10 needs backward computation.
I0909 15:33:22.840338  6389 net.cpp:66] Creating Layer prob
I0909 15:33:22.840343  6389 net.cpp:329] prob <- fc10
I0909 15:33:22.840351  6389 net.cpp:290] prob -> prob
I0909 15:33:22.840360  6389 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:22.840366  6389 net.cpp:125] prob needs backward computation.
I0909 15:33:22.840371  6389 net.cpp:156] This network produces output prob
I0909 15:33:22.840384  6389 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:33:22.840392  6389 net.cpp:167] Network initialization done.
I0909 15:33:22.840397  6389 net.cpp:168] Memory required for data: 6183480
Classifying 34 inputs.
Done in 21.24 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:33:45.719794  6392 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:33:45.719938  6392 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:33:45.719945  6392 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:33:45.720096  6392 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:33:45.720160  6392 net.cpp:292] Input 0 -> data
I0909 15:33:45.720186  6392 net.cpp:66] Creating Layer conv1
I0909 15:33:45.720193  6392 net.cpp:329] conv1 <- data
I0909 15:33:45.720201  6392 net.cpp:290] conv1 -> conv1
I0909 15:33:45.721611  6392 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:45.721629  6392 net.cpp:125] conv1 needs backward computation.
I0909 15:33:45.721638  6392 net.cpp:66] Creating Layer relu1
I0909 15:33:45.721644  6392 net.cpp:329] relu1 <- conv1
I0909 15:33:45.721652  6392 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:33:45.721659  6392 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:45.721665  6392 net.cpp:125] relu1 needs backward computation.
I0909 15:33:45.721673  6392 net.cpp:66] Creating Layer pool1
I0909 15:33:45.721678  6392 net.cpp:329] pool1 <- conv1
I0909 15:33:45.721684  6392 net.cpp:290] pool1 -> pool1
I0909 15:33:45.721694  6392 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:45.721700  6392 net.cpp:125] pool1 needs backward computation.
I0909 15:33:45.721707  6392 net.cpp:66] Creating Layer norm1
I0909 15:33:45.721714  6392 net.cpp:329] norm1 <- pool1
I0909 15:33:45.721724  6392 net.cpp:290] norm1 -> norm1
I0909 15:33:45.721735  6392 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:45.721740  6392 net.cpp:125] norm1 needs backward computation.
I0909 15:33:45.721747  6392 net.cpp:66] Creating Layer conv2
I0909 15:33:45.721753  6392 net.cpp:329] conv2 <- norm1
I0909 15:33:45.721760  6392 net.cpp:290] conv2 -> conv2
I0909 15:33:45.730808  6392 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:45.730823  6392 net.cpp:125] conv2 needs backward computation.
I0909 15:33:45.730831  6392 net.cpp:66] Creating Layer relu2
I0909 15:33:45.730836  6392 net.cpp:329] relu2 <- conv2
I0909 15:33:45.730844  6392 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:33:45.730850  6392 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:45.730855  6392 net.cpp:125] relu2 needs backward computation.
I0909 15:33:45.730861  6392 net.cpp:66] Creating Layer pool2
I0909 15:33:45.730867  6392 net.cpp:329] pool2 <- conv2
I0909 15:33:45.730873  6392 net.cpp:290] pool2 -> pool2
I0909 15:33:45.730881  6392 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:33:45.730887  6392 net.cpp:125] pool2 needs backward computation.
I0909 15:33:45.730895  6392 net.cpp:66] Creating Layer fc7
I0909 15:33:45.730901  6392 net.cpp:329] fc7 <- pool2
I0909 15:33:45.730908  6392 net.cpp:290] fc7 -> fc7
I0909 15:33:46.372504  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.372546  6392 net.cpp:125] fc7 needs backward computation.
I0909 15:33:46.372558  6392 net.cpp:66] Creating Layer relu7
I0909 15:33:46.372565  6392 net.cpp:329] relu7 <- fc7
I0909 15:33:46.372573  6392 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:33:46.372582  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.372588  6392 net.cpp:125] relu7 needs backward computation.
I0909 15:33:46.372596  6392 net.cpp:66] Creating Layer drop7
I0909 15:33:46.372601  6392 net.cpp:329] drop7 <- fc7
I0909 15:33:46.372607  6392 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:33:46.372618  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.372624  6392 net.cpp:125] drop7 needs backward computation.
I0909 15:33:46.372633  6392 net.cpp:66] Creating Layer fc8
I0909 15:33:46.372639  6392 net.cpp:329] fc8 <- fc7
I0909 15:33:46.372647  6392 net.cpp:290] fc8 -> fc8
I0909 15:33:46.380362  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.380375  6392 net.cpp:125] fc8 needs backward computation.
I0909 15:33:46.380384  6392 net.cpp:66] Creating Layer relu8
I0909 15:33:46.380390  6392 net.cpp:329] relu8 <- fc8
I0909 15:33:46.380398  6392 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:33:46.380404  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.380409  6392 net.cpp:125] relu8 needs backward computation.
I0909 15:33:46.380416  6392 net.cpp:66] Creating Layer drop8
I0909 15:33:46.380421  6392 net.cpp:329] drop8 <- fc8
I0909 15:33:46.380429  6392 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:33:46.380436  6392 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:46.380442  6392 net.cpp:125] drop8 needs backward computation.
I0909 15:33:46.380450  6392 net.cpp:66] Creating Layer fc9
I0909 15:33:46.380455  6392 net.cpp:329] fc9 <- fc8
I0909 15:33:46.380462  6392 net.cpp:290] fc9 -> fc9
I0909 15:33:46.380837  6392 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:33:46.380848  6392 net.cpp:125] fc9 needs backward computation.
I0909 15:33:46.380856  6392 net.cpp:66] Creating Layer fc10
I0909 15:33:46.380861  6392 net.cpp:329] fc10 <- fc9
I0909 15:33:46.380870  6392 net.cpp:290] fc10 -> fc10
I0909 15:33:46.380882  6392 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:46.380890  6392 net.cpp:125] fc10 needs backward computation.
I0909 15:33:46.380897  6392 net.cpp:66] Creating Layer prob
I0909 15:33:46.380903  6392 net.cpp:329] prob <- fc10
I0909 15:33:46.380911  6392 net.cpp:290] prob -> prob
I0909 15:33:46.380921  6392 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:46.380928  6392 net.cpp:125] prob needs backward computation.
I0909 15:33:46.380933  6392 net.cpp:156] This network produces output prob
I0909 15:33:46.380954  6392 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:33:46.380964  6392 net.cpp:167] Network initialization done.
I0909 15:33:46.380969  6392 net.cpp:168] Memory required for data: 6183480
Classifying 5 inputs.
Done in 3.28 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:33:50.765064  6395 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:33:50.765208  6395 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:33:50.765218  6395 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:33:50.765364  6395 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:33:50.765415  6395 net.cpp:292] Input 0 -> data
I0909 15:33:50.765440  6395 net.cpp:66] Creating Layer conv1
I0909 15:33:50.765457  6395 net.cpp:329] conv1 <- data
I0909 15:33:50.765466  6395 net.cpp:290] conv1 -> conv1
I0909 15:33:50.766880  6395 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:50.766901  6395 net.cpp:125] conv1 needs backward computation.
I0909 15:33:50.766909  6395 net.cpp:66] Creating Layer relu1
I0909 15:33:50.766914  6395 net.cpp:329] relu1 <- conv1
I0909 15:33:50.766921  6395 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:33:50.766929  6395 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:50.766935  6395 net.cpp:125] relu1 needs backward computation.
I0909 15:33:50.766942  6395 net.cpp:66] Creating Layer pool1
I0909 15:33:50.766947  6395 net.cpp:329] pool1 <- conv1
I0909 15:33:50.766954  6395 net.cpp:290] pool1 -> pool1
I0909 15:33:50.766965  6395 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:50.766971  6395 net.cpp:125] pool1 needs backward computation.
I0909 15:33:50.766978  6395 net.cpp:66] Creating Layer norm1
I0909 15:33:50.766983  6395 net.cpp:329] norm1 <- pool1
I0909 15:33:50.766989  6395 net.cpp:290] norm1 -> norm1
I0909 15:33:50.766999  6395 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:50.767004  6395 net.cpp:125] norm1 needs backward computation.
I0909 15:33:50.767012  6395 net.cpp:66] Creating Layer conv2
I0909 15:33:50.767017  6395 net.cpp:329] conv2 <- norm1
I0909 15:33:50.767024  6395 net.cpp:290] conv2 -> conv2
I0909 15:33:50.775938  6395 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:50.775953  6395 net.cpp:125] conv2 needs backward computation.
I0909 15:33:50.775959  6395 net.cpp:66] Creating Layer relu2
I0909 15:33:50.775965  6395 net.cpp:329] relu2 <- conv2
I0909 15:33:50.775971  6395 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:33:50.775979  6395 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:50.775984  6395 net.cpp:125] relu2 needs backward computation.
I0909 15:33:50.775990  6395 net.cpp:66] Creating Layer pool2
I0909 15:33:50.775995  6395 net.cpp:329] pool2 <- conv2
I0909 15:33:50.776001  6395 net.cpp:290] pool2 -> pool2
I0909 15:33:50.776008  6395 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:33:50.776015  6395 net.cpp:125] pool2 needs backward computation.
I0909 15:33:50.776021  6395 net.cpp:66] Creating Layer fc7
I0909 15:33:50.776026  6395 net.cpp:329] fc7 <- pool2
I0909 15:33:50.776036  6395 net.cpp:290] fc7 -> fc7
I0909 15:33:51.417588  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.417629  6395 net.cpp:125] fc7 needs backward computation.
I0909 15:33:51.417642  6395 net.cpp:66] Creating Layer relu7
I0909 15:33:51.417650  6395 net.cpp:329] relu7 <- fc7
I0909 15:33:51.417659  6395 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:33:51.417668  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.417673  6395 net.cpp:125] relu7 needs backward computation.
I0909 15:33:51.417681  6395 net.cpp:66] Creating Layer drop7
I0909 15:33:51.417686  6395 net.cpp:329] drop7 <- fc7
I0909 15:33:51.417692  6395 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:33:51.417703  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.417709  6395 net.cpp:125] drop7 needs backward computation.
I0909 15:33:51.417717  6395 net.cpp:66] Creating Layer fc8
I0909 15:33:51.417722  6395 net.cpp:329] fc8 <- fc7
I0909 15:33:51.417732  6395 net.cpp:290] fc8 -> fc8
I0909 15:33:51.425499  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.425520  6395 net.cpp:125] fc8 needs backward computation.
I0909 15:33:51.425529  6395 net.cpp:66] Creating Layer relu8
I0909 15:33:51.425535  6395 net.cpp:329] relu8 <- fc8
I0909 15:33:51.425550  6395 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:33:51.425557  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.425562  6395 net.cpp:125] relu8 needs backward computation.
I0909 15:33:51.425568  6395 net.cpp:66] Creating Layer drop8
I0909 15:33:51.425573  6395 net.cpp:329] drop8 <- fc8
I0909 15:33:51.425580  6395 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:33:51.425586  6395 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:51.425601  6395 net.cpp:125] drop8 needs backward computation.
I0909 15:33:51.425611  6395 net.cpp:66] Creating Layer fc9
I0909 15:33:51.425616  6395 net.cpp:329] fc9 <- fc8
I0909 15:33:51.425622  6395 net.cpp:290] fc9 -> fc9
I0909 15:33:51.425987  6395 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:33:51.425998  6395 net.cpp:125] fc9 needs backward computation.
I0909 15:33:51.426007  6395 net.cpp:66] Creating Layer fc10
I0909 15:33:51.426012  6395 net.cpp:329] fc10 <- fc9
I0909 15:33:51.426019  6395 net.cpp:290] fc10 -> fc10
I0909 15:33:51.426030  6395 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:51.426038  6395 net.cpp:125] fc10 needs backward computation.
I0909 15:33:51.426044  6395 net.cpp:66] Creating Layer prob
I0909 15:33:51.426050  6395 net.cpp:329] prob <- fc10
I0909 15:33:51.426058  6395 net.cpp:290] prob -> prob
I0909 15:33:51.426066  6395 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:51.426072  6395 net.cpp:125] prob needs backward computation.
I0909 15:33:51.426077  6395 net.cpp:156] This network produces output prob
I0909 15:33:51.426090  6395 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:33:51.426097  6395 net.cpp:167] Network initialization done.
I0909 15:33:51.426102  6395 net.cpp:168] Memory required for data: 6183480
Classifying 40 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:33:52.649371  6398 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:33:52.649531  6398 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:33:52.649544  6398 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:33:52.649695  6398 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:33:52.649759  6398 net.cpp:292] Input 0 -> data
I0909 15:33:52.649786  6398 net.cpp:66] Creating Layer conv1
I0909 15:33:52.649793  6398 net.cpp:329] conv1 <- data
I0909 15:33:52.649801  6398 net.cpp:290] conv1 -> conv1
I0909 15:33:52.651206  6398 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:52.651226  6398 net.cpp:125] conv1 needs backward computation.
I0909 15:33:52.651235  6398 net.cpp:66] Creating Layer relu1
I0909 15:33:52.651242  6398 net.cpp:329] relu1 <- conv1
I0909 15:33:52.651248  6398 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:33:52.651257  6398 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:33:52.651263  6398 net.cpp:125] relu1 needs backward computation.
I0909 15:33:52.651270  6398 net.cpp:66] Creating Layer pool1
I0909 15:33:52.651276  6398 net.cpp:329] pool1 <- conv1
I0909 15:33:52.651283  6398 net.cpp:290] pool1 -> pool1
I0909 15:33:52.651294  6398 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:52.651300  6398 net.cpp:125] pool1 needs backward computation.
I0909 15:33:52.651307  6398 net.cpp:66] Creating Layer norm1
I0909 15:33:52.651314  6398 net.cpp:329] norm1 <- pool1
I0909 15:33:52.651319  6398 net.cpp:290] norm1 -> norm1
I0909 15:33:52.651329  6398 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:33:52.651335  6398 net.cpp:125] norm1 needs backward computation.
I0909 15:33:52.651343  6398 net.cpp:66] Creating Layer conv2
I0909 15:33:52.651350  6398 net.cpp:329] conv2 <- norm1
I0909 15:33:52.651356  6398 net.cpp:290] conv2 -> conv2
I0909 15:33:52.660395  6398 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:52.660413  6398 net.cpp:125] conv2 needs backward computation.
I0909 15:33:52.660420  6398 net.cpp:66] Creating Layer relu2
I0909 15:33:52.660425  6398 net.cpp:329] relu2 <- conv2
I0909 15:33:52.660433  6398 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:33:52.660439  6398 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:33:52.660444  6398 net.cpp:125] relu2 needs backward computation.
I0909 15:33:52.660450  6398 net.cpp:66] Creating Layer pool2
I0909 15:33:52.660456  6398 net.cpp:329] pool2 <- conv2
I0909 15:33:52.660462  6398 net.cpp:290] pool2 -> pool2
I0909 15:33:52.660470  6398 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:33:52.660475  6398 net.cpp:125] pool2 needs backward computation.
I0909 15:33:52.660482  6398 net.cpp:66] Creating Layer fc7
I0909 15:33:52.660488  6398 net.cpp:329] fc7 <- pool2
I0909 15:33:52.660497  6398 net.cpp:290] fc7 -> fc7
I0909 15:33:53.373975  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.374017  6398 net.cpp:125] fc7 needs backward computation.
I0909 15:33:53.374029  6398 net.cpp:66] Creating Layer relu7
I0909 15:33:53.374047  6398 net.cpp:329] relu7 <- fc7
I0909 15:33:53.374055  6398 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:33:53.374064  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.374070  6398 net.cpp:125] relu7 needs backward computation.
I0909 15:33:53.374078  6398 net.cpp:66] Creating Layer drop7
I0909 15:33:53.374083  6398 net.cpp:329] drop7 <- fc7
I0909 15:33:53.374089  6398 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:33:53.374099  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.374104  6398 net.cpp:125] drop7 needs backward computation.
I0909 15:33:53.374112  6398 net.cpp:66] Creating Layer fc8
I0909 15:33:53.374117  6398 net.cpp:329] fc8 <- fc7
I0909 15:33:53.374126  6398 net.cpp:290] fc8 -> fc8
I0909 15:33:53.382783  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.382809  6398 net.cpp:125] fc8 needs backward computation.
I0909 15:33:53.382818  6398 net.cpp:66] Creating Layer relu8
I0909 15:33:53.382824  6398 net.cpp:329] relu8 <- fc8
I0909 15:33:53.382833  6398 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:33:53.382841  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.382848  6398 net.cpp:125] relu8 needs backward computation.
I0909 15:33:53.382854  6398 net.cpp:66] Creating Layer drop8
I0909 15:33:53.382859  6398 net.cpp:329] drop8 <- fc8
I0909 15:33:53.382865  6398 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:33:53.382872  6398 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:33:53.382879  6398 net.cpp:125] drop8 needs backward computation.
I0909 15:33:53.382887  6398 net.cpp:66] Creating Layer fc9
I0909 15:33:53.382894  6398 net.cpp:329] fc9 <- fc8
I0909 15:33:53.382900  6398 net.cpp:290] fc9 -> fc9
I0909 15:33:53.383316  6398 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:33:53.383328  6398 net.cpp:125] fc9 needs backward computation.
I0909 15:33:53.383337  6398 net.cpp:66] Creating Layer fc10
I0909 15:33:53.383342  6398 net.cpp:329] fc10 <- fc9
I0909 15:33:53.383352  6398 net.cpp:290] fc10 -> fc10
I0909 15:33:53.383363  6398 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:53.383370  6398 net.cpp:125] fc10 needs backward computation.
I0909 15:33:53.383378  6398 net.cpp:66] Creating Layer prob
I0909 15:33:53.383383  6398 net.cpp:329] prob <- fc10
I0909 15:33:53.383390  6398 net.cpp:290] prob -> prob
I0909 15:33:53.383401  6398 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:33:53.383407  6398 net.cpp:125] prob needs backward computation.
I0909 15:33:53.383412  6398 net.cpp:156] This network produces output prob
I0909 15:33:53.383425  6398 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:33:53.383433  6398 net.cpp:167] Network initialization done.
I0909 15:33:53.383438  6398 net.cpp:168] Memory required for data: 6183480
Classifying 322 inputs.
Done in 193.73 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:37:14.114039  6579 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:37:14.114176  6579 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:37:14.114186  6579 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:37:14.114330  6579 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:37:14.114392  6579 net.cpp:292] Input 0 -> data
I0909 15:37:14.114418  6579 net.cpp:66] Creating Layer conv1
I0909 15:37:14.114424  6579 net.cpp:329] conv1 <- data
I0909 15:37:14.114433  6579 net.cpp:290] conv1 -> conv1
I0909 15:37:14.115754  6579 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:37:14.115772  6579 net.cpp:125] conv1 needs backward computation.
I0909 15:37:14.115780  6579 net.cpp:66] Creating Layer relu1
I0909 15:37:14.115787  6579 net.cpp:329] relu1 <- conv1
I0909 15:37:14.115793  6579 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:37:14.115802  6579 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:37:14.115808  6579 net.cpp:125] relu1 needs backward computation.
I0909 15:37:14.115814  6579 net.cpp:66] Creating Layer pool1
I0909 15:37:14.115819  6579 net.cpp:329] pool1 <- conv1
I0909 15:37:14.115826  6579 net.cpp:290] pool1 -> pool1
I0909 15:37:14.115838  6579 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:37:14.115842  6579 net.cpp:125] pool1 needs backward computation.
I0909 15:37:14.115849  6579 net.cpp:66] Creating Layer norm1
I0909 15:37:14.115854  6579 net.cpp:329] norm1 <- pool1
I0909 15:37:14.115861  6579 net.cpp:290] norm1 -> norm1
I0909 15:37:14.115870  6579 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:37:14.115876  6579 net.cpp:125] norm1 needs backward computation.
I0909 15:37:14.115883  6579 net.cpp:66] Creating Layer conv2
I0909 15:37:14.115888  6579 net.cpp:329] conv2 <- norm1
I0909 15:37:14.115895  6579 net.cpp:290] conv2 -> conv2
I0909 15:37:14.124765  6579 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:37:14.124780  6579 net.cpp:125] conv2 needs backward computation.
I0909 15:37:14.124793  6579 net.cpp:66] Creating Layer relu2
I0909 15:37:14.124799  6579 net.cpp:329] relu2 <- conv2
I0909 15:37:14.124804  6579 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:37:14.124811  6579 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:37:14.124817  6579 net.cpp:125] relu2 needs backward computation.
I0909 15:37:14.124824  6579 net.cpp:66] Creating Layer pool2
I0909 15:37:14.124829  6579 net.cpp:329] pool2 <- conv2
I0909 15:37:14.124835  6579 net.cpp:290] pool2 -> pool2
I0909 15:37:14.124843  6579 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:37:14.124848  6579 net.cpp:125] pool2 needs backward computation.
I0909 15:37:14.124855  6579 net.cpp:66] Creating Layer fc7
I0909 15:37:14.124861  6579 net.cpp:329] fc7 <- pool2
I0909 15:37:14.124867  6579 net.cpp:290] fc7 -> fc7
I0909 15:37:14.766194  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.766238  6579 net.cpp:125] fc7 needs backward computation.
I0909 15:37:14.766250  6579 net.cpp:66] Creating Layer relu7
I0909 15:37:14.766258  6579 net.cpp:329] relu7 <- fc7
I0909 15:37:14.766265  6579 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:37:14.766275  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.766281  6579 net.cpp:125] relu7 needs backward computation.
I0909 15:37:14.766288  6579 net.cpp:66] Creating Layer drop7
I0909 15:37:14.766294  6579 net.cpp:329] drop7 <- fc7
I0909 15:37:14.766301  6579 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:37:14.766311  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.766317  6579 net.cpp:125] drop7 needs backward computation.
I0909 15:37:14.766325  6579 net.cpp:66] Creating Layer fc8
I0909 15:37:14.766331  6579 net.cpp:329] fc8 <- fc7
I0909 15:37:14.766340  6579 net.cpp:290] fc8 -> fc8
I0909 15:37:14.774206  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.774219  6579 net.cpp:125] fc8 needs backward computation.
I0909 15:37:14.774226  6579 net.cpp:66] Creating Layer relu8
I0909 15:37:14.774232  6579 net.cpp:329] relu8 <- fc8
I0909 15:37:14.774240  6579 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:37:14.774248  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.774253  6579 net.cpp:125] relu8 needs backward computation.
I0909 15:37:14.774260  6579 net.cpp:66] Creating Layer drop8
I0909 15:37:14.774266  6579 net.cpp:329] drop8 <- fc8
I0909 15:37:14.774272  6579 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:37:14.774279  6579 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:37:14.774284  6579 net.cpp:125] drop8 needs backward computation.
I0909 15:37:14.774293  6579 net.cpp:66] Creating Layer fc9
I0909 15:37:14.774299  6579 net.cpp:329] fc9 <- fc8
I0909 15:37:14.774307  6579 net.cpp:290] fc9 -> fc9
I0909 15:37:14.774678  6579 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:37:14.774690  6579 net.cpp:125] fc9 needs backward computation.
I0909 15:37:14.774698  6579 net.cpp:66] Creating Layer fc10
I0909 15:37:14.774703  6579 net.cpp:329] fc10 <- fc9
I0909 15:37:14.774713  6579 net.cpp:290] fc10 -> fc10
I0909 15:37:14.774724  6579 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:37:14.774731  6579 net.cpp:125] fc10 needs backward computation.
I0909 15:37:14.774739  6579 net.cpp:66] Creating Layer prob
I0909 15:37:14.774744  6579 net.cpp:329] prob <- fc10
I0909 15:37:14.774751  6579 net.cpp:290] prob -> prob
I0909 15:37:14.774761  6579 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:37:14.774766  6579 net.cpp:125] prob needs backward computation.
I0909 15:37:14.774771  6579 net.cpp:156] This network produces output prob
I0909 15:37:14.774785  6579 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:37:14.774792  6579 net.cpp:167] Network initialization done.
I0909 15:37:14.774797  6579 net.cpp:168] Memory required for data: 6183480
Classifying 226 inputs.
Done in 136.96 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:39:46.467561  6660 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:39:46.467708  6660 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:39:46.467717  6660 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:39:46.467859  6660 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:39:46.467912  6660 net.cpp:292] Input 0 -> data
I0909 15:39:46.467937  6660 net.cpp:66] Creating Layer conv1
I0909 15:39:46.467944  6660 net.cpp:329] conv1 <- data
I0909 15:39:46.467952  6660 net.cpp:290] conv1 -> conv1
I0909 15:39:46.469295  6660 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:39:46.469313  6660 net.cpp:125] conv1 needs backward computation.
I0909 15:39:46.469321  6660 net.cpp:66] Creating Layer relu1
I0909 15:39:46.469327  6660 net.cpp:329] relu1 <- conv1
I0909 15:39:46.469334  6660 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:39:46.469342  6660 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:39:46.469353  6660 net.cpp:125] relu1 needs backward computation.
I0909 15:39:46.469360  6660 net.cpp:66] Creating Layer pool1
I0909 15:39:46.469367  6660 net.cpp:329] pool1 <- conv1
I0909 15:39:46.469372  6660 net.cpp:290] pool1 -> pool1
I0909 15:39:46.469383  6660 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:39:46.469389  6660 net.cpp:125] pool1 needs backward computation.
I0909 15:39:46.469396  6660 net.cpp:66] Creating Layer norm1
I0909 15:39:46.469401  6660 net.cpp:329] norm1 <- pool1
I0909 15:39:46.469408  6660 net.cpp:290] norm1 -> norm1
I0909 15:39:46.469418  6660 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:39:46.469422  6660 net.cpp:125] norm1 needs backward computation.
I0909 15:39:46.469430  6660 net.cpp:66] Creating Layer conv2
I0909 15:39:46.469435  6660 net.cpp:329] conv2 <- norm1
I0909 15:39:46.469442  6660 net.cpp:290] conv2 -> conv2
I0909 15:39:46.478545  6660 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:39:46.478560  6660 net.cpp:125] conv2 needs backward computation.
I0909 15:39:46.478567  6660 net.cpp:66] Creating Layer relu2
I0909 15:39:46.478574  6660 net.cpp:329] relu2 <- conv2
I0909 15:39:46.478580  6660 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:39:46.478587  6660 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:39:46.478592  6660 net.cpp:125] relu2 needs backward computation.
I0909 15:39:46.478598  6660 net.cpp:66] Creating Layer pool2
I0909 15:39:46.478605  6660 net.cpp:329] pool2 <- conv2
I0909 15:39:46.478610  6660 net.cpp:290] pool2 -> pool2
I0909 15:39:46.478618  6660 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:39:46.478623  6660 net.cpp:125] pool2 needs backward computation.
I0909 15:39:46.478631  6660 net.cpp:66] Creating Layer fc7
I0909 15:39:46.478636  6660 net.cpp:329] fc7 <- pool2
I0909 15:39:46.478643  6660 net.cpp:290] fc7 -> fc7
I0909 15:39:47.103502  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.103551  6660 net.cpp:125] fc7 needs backward computation.
I0909 15:39:47.103564  6660 net.cpp:66] Creating Layer relu7
I0909 15:39:47.103571  6660 net.cpp:329] relu7 <- fc7
I0909 15:39:47.103579  6660 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:39:47.103590  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.103595  6660 net.cpp:125] relu7 needs backward computation.
I0909 15:39:47.103601  6660 net.cpp:66] Creating Layer drop7
I0909 15:39:47.103607  6660 net.cpp:329] drop7 <- fc7
I0909 15:39:47.103613  6660 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:39:47.103624  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.103631  6660 net.cpp:125] drop7 needs backward computation.
I0909 15:39:47.103638  6660 net.cpp:66] Creating Layer fc8
I0909 15:39:47.103643  6660 net.cpp:329] fc8 <- fc7
I0909 15:39:47.103652  6660 net.cpp:290] fc8 -> fc8
I0909 15:39:47.111218  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.111230  6660 net.cpp:125] fc8 needs backward computation.
I0909 15:39:47.111238  6660 net.cpp:66] Creating Layer relu8
I0909 15:39:47.111243  6660 net.cpp:329] relu8 <- fc8
I0909 15:39:47.111250  6660 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:39:47.111258  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.111263  6660 net.cpp:125] relu8 needs backward computation.
I0909 15:39:47.111270  6660 net.cpp:66] Creating Layer drop8
I0909 15:39:47.111275  6660 net.cpp:329] drop8 <- fc8
I0909 15:39:47.111281  6660 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:39:47.111289  6660 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:39:47.111294  6660 net.cpp:125] drop8 needs backward computation.
I0909 15:39:47.111302  6660 net.cpp:66] Creating Layer fc9
I0909 15:39:47.111309  6660 net.cpp:329] fc9 <- fc8
I0909 15:39:47.111315  6660 net.cpp:290] fc9 -> fc9
I0909 15:39:47.111677  6660 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:39:47.111688  6660 net.cpp:125] fc9 needs backward computation.
I0909 15:39:47.111696  6660 net.cpp:66] Creating Layer fc10
I0909 15:39:47.111702  6660 net.cpp:329] fc10 <- fc9
I0909 15:39:47.111711  6660 net.cpp:290] fc10 -> fc10
I0909 15:39:47.111732  6660 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:39:47.111740  6660 net.cpp:125] fc10 needs backward computation.
I0909 15:39:47.111747  6660 net.cpp:66] Creating Layer prob
I0909 15:39:47.111752  6660 net.cpp:329] prob <- fc10
I0909 15:39:47.111759  6660 net.cpp:290] prob -> prob
I0909 15:39:47.111768  6660 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:39:47.111774  6660 net.cpp:125] prob needs backward computation.
I0909 15:39:47.111779  6660 net.cpp:156] This network produces output prob
I0909 15:39:47.111791  6660 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:39:47.111799  6660 net.cpp:167] Network initialization done.
I0909 15:39:47.111804  6660 net.cpp:168] Memory required for data: 6183480
Classifying 63 inputs.
Done in 38.69 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:40:28.651089  6666 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:40:28.651226  6666 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:40:28.651234  6666 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:40:28.651377  6666 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:40:28.651440  6666 net.cpp:292] Input 0 -> data
I0909 15:40:28.651465  6666 net.cpp:66] Creating Layer conv1
I0909 15:40:28.651473  6666 net.cpp:329] conv1 <- data
I0909 15:40:28.651480  6666 net.cpp:290] conv1 -> conv1
I0909 15:40:28.652801  6666 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:40:28.652817  6666 net.cpp:125] conv1 needs backward computation.
I0909 15:40:28.652827  6666 net.cpp:66] Creating Layer relu1
I0909 15:40:28.652832  6666 net.cpp:329] relu1 <- conv1
I0909 15:40:28.652838  6666 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:40:28.652848  6666 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:40:28.652853  6666 net.cpp:125] relu1 needs backward computation.
I0909 15:40:28.652859  6666 net.cpp:66] Creating Layer pool1
I0909 15:40:28.652864  6666 net.cpp:329] pool1 <- conv1
I0909 15:40:28.652870  6666 net.cpp:290] pool1 -> pool1
I0909 15:40:28.652881  6666 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:40:28.652887  6666 net.cpp:125] pool1 needs backward computation.
I0909 15:40:28.652894  6666 net.cpp:66] Creating Layer norm1
I0909 15:40:28.652899  6666 net.cpp:329] norm1 <- pool1
I0909 15:40:28.652905  6666 net.cpp:290] norm1 -> norm1
I0909 15:40:28.652915  6666 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:40:28.652920  6666 net.cpp:125] norm1 needs backward computation.
I0909 15:40:28.652926  6666 net.cpp:66] Creating Layer conv2
I0909 15:40:28.652932  6666 net.cpp:329] conv2 <- norm1
I0909 15:40:28.652940  6666 net.cpp:290] conv2 -> conv2
I0909 15:40:28.661839  6666 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:40:28.661854  6666 net.cpp:125] conv2 needs backward computation.
I0909 15:40:28.661861  6666 net.cpp:66] Creating Layer relu2
I0909 15:40:28.661867  6666 net.cpp:329] relu2 <- conv2
I0909 15:40:28.661873  6666 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:40:28.661880  6666 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:40:28.661885  6666 net.cpp:125] relu2 needs backward computation.
I0909 15:40:28.661892  6666 net.cpp:66] Creating Layer pool2
I0909 15:40:28.661897  6666 net.cpp:329] pool2 <- conv2
I0909 15:40:28.661905  6666 net.cpp:290] pool2 -> pool2
I0909 15:40:28.661911  6666 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:40:28.661917  6666 net.cpp:125] pool2 needs backward computation.
I0909 15:40:28.661923  6666 net.cpp:66] Creating Layer fc7
I0909 15:40:28.661929  6666 net.cpp:329] fc7 <- pool2
I0909 15:40:28.661936  6666 net.cpp:290] fc7 -> fc7
I0909 15:40:29.307138  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.307186  6666 net.cpp:125] fc7 needs backward computation.
I0909 15:40:29.307199  6666 net.cpp:66] Creating Layer relu7
I0909 15:40:29.307206  6666 net.cpp:329] relu7 <- fc7
I0909 15:40:29.307214  6666 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:40:29.307224  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.307229  6666 net.cpp:125] relu7 needs backward computation.
I0909 15:40:29.307235  6666 net.cpp:66] Creating Layer drop7
I0909 15:40:29.307240  6666 net.cpp:329] drop7 <- fc7
I0909 15:40:29.307247  6666 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:40:29.307257  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.307263  6666 net.cpp:125] drop7 needs backward computation.
I0909 15:40:29.307271  6666 net.cpp:66] Creating Layer fc8
I0909 15:40:29.307276  6666 net.cpp:329] fc8 <- fc7
I0909 15:40:29.307284  6666 net.cpp:290] fc8 -> fc8
I0909 15:40:29.315090  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.315103  6666 net.cpp:125] fc8 needs backward computation.
I0909 15:40:29.315109  6666 net.cpp:66] Creating Layer relu8
I0909 15:40:29.315125  6666 net.cpp:329] relu8 <- fc8
I0909 15:40:29.315134  6666 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:40:29.315140  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.315146  6666 net.cpp:125] relu8 needs backward computation.
I0909 15:40:29.315152  6666 net.cpp:66] Creating Layer drop8
I0909 15:40:29.315157  6666 net.cpp:329] drop8 <- fc8
I0909 15:40:29.315163  6666 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:40:29.315170  6666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:40:29.315176  6666 net.cpp:125] drop8 needs backward computation.
I0909 15:40:29.315183  6666 net.cpp:66] Creating Layer fc9
I0909 15:40:29.315189  6666 net.cpp:329] fc9 <- fc8
I0909 15:40:29.315196  6666 net.cpp:290] fc9 -> fc9
I0909 15:40:29.315569  6666 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:40:29.315582  6666 net.cpp:125] fc9 needs backward computation.
I0909 15:40:29.315589  6666 net.cpp:66] Creating Layer fc10
I0909 15:40:29.315594  6666 net.cpp:329] fc10 <- fc9
I0909 15:40:29.315603  6666 net.cpp:290] fc10 -> fc10
I0909 15:40:29.315613  6666 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:40:29.315621  6666 net.cpp:125] fc10 needs backward computation.
I0909 15:40:29.315628  6666 net.cpp:66] Creating Layer prob
I0909 15:40:29.315634  6666 net.cpp:329] prob <- fc10
I0909 15:40:29.315640  6666 net.cpp:290] prob -> prob
I0909 15:40:29.315649  6666 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:40:29.315654  6666 net.cpp:125] prob needs backward computation.
I0909 15:40:29.315659  6666 net.cpp:156] This network produces output prob
I0909 15:40:29.315671  6666 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:40:29.315680  6666 net.cpp:167] Network initialization done.
I0909 15:40:29.315685  6666 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 407 inputs.
Done in 257.87 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:44:54.385763  6678 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:44:54.385900  6678 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:44:54.385910  6678 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:44:54.386052  6678 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:44:54.386116  6678 net.cpp:292] Input 0 -> data
I0909 15:44:54.386142  6678 net.cpp:66] Creating Layer conv1
I0909 15:44:54.386148  6678 net.cpp:329] conv1 <- data
I0909 15:44:54.386157  6678 net.cpp:290] conv1 -> conv1
I0909 15:44:54.387491  6678 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:44:54.387509  6678 net.cpp:125] conv1 needs backward computation.
I0909 15:44:54.387518  6678 net.cpp:66] Creating Layer relu1
I0909 15:44:54.387523  6678 net.cpp:329] relu1 <- conv1
I0909 15:44:54.387531  6678 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:44:54.387538  6678 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:44:54.387544  6678 net.cpp:125] relu1 needs backward computation.
I0909 15:44:54.387550  6678 net.cpp:66] Creating Layer pool1
I0909 15:44:54.387557  6678 net.cpp:329] pool1 <- conv1
I0909 15:44:54.387562  6678 net.cpp:290] pool1 -> pool1
I0909 15:44:54.387573  6678 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:44:54.387579  6678 net.cpp:125] pool1 needs backward computation.
I0909 15:44:54.387585  6678 net.cpp:66] Creating Layer norm1
I0909 15:44:54.387591  6678 net.cpp:329] norm1 <- pool1
I0909 15:44:54.387598  6678 net.cpp:290] norm1 -> norm1
I0909 15:44:54.387606  6678 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:44:54.387612  6678 net.cpp:125] norm1 needs backward computation.
I0909 15:44:54.387619  6678 net.cpp:66] Creating Layer conv2
I0909 15:44:54.387625  6678 net.cpp:329] conv2 <- norm1
I0909 15:44:54.387631  6678 net.cpp:290] conv2 -> conv2
I0909 15:44:54.396500  6678 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:44:54.396514  6678 net.cpp:125] conv2 needs backward computation.
I0909 15:44:54.396522  6678 net.cpp:66] Creating Layer relu2
I0909 15:44:54.396527  6678 net.cpp:329] relu2 <- conv2
I0909 15:44:54.396533  6678 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:44:54.396539  6678 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:44:54.396545  6678 net.cpp:125] relu2 needs backward computation.
I0909 15:44:54.396551  6678 net.cpp:66] Creating Layer pool2
I0909 15:44:54.396556  6678 net.cpp:329] pool2 <- conv2
I0909 15:44:54.396564  6678 net.cpp:290] pool2 -> pool2
I0909 15:44:54.396570  6678 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:44:54.396576  6678 net.cpp:125] pool2 needs backward computation.
I0909 15:44:54.396584  6678 net.cpp:66] Creating Layer fc7
I0909 15:44:54.396589  6678 net.cpp:329] fc7 <- pool2
I0909 15:44:54.396600  6678 net.cpp:290] fc7 -> fc7
I0909 15:44:55.022341  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.022387  6678 net.cpp:125] fc7 needs backward computation.
I0909 15:44:55.022399  6678 net.cpp:66] Creating Layer relu7
I0909 15:44:55.022407  6678 net.cpp:329] relu7 <- fc7
I0909 15:44:55.022414  6678 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:44:55.022424  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.022429  6678 net.cpp:125] relu7 needs backward computation.
I0909 15:44:55.022436  6678 net.cpp:66] Creating Layer drop7
I0909 15:44:55.022442  6678 net.cpp:329] drop7 <- fc7
I0909 15:44:55.022449  6678 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:44:55.022459  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.022465  6678 net.cpp:125] drop7 needs backward computation.
I0909 15:44:55.022474  6678 net.cpp:66] Creating Layer fc8
I0909 15:44:55.022478  6678 net.cpp:329] fc8 <- fc7
I0909 15:44:55.022487  6678 net.cpp:290] fc8 -> fc8
I0909 15:44:55.030036  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.030047  6678 net.cpp:125] fc8 needs backward computation.
I0909 15:44:55.030055  6678 net.cpp:66] Creating Layer relu8
I0909 15:44:55.030061  6678 net.cpp:329] relu8 <- fc8
I0909 15:44:55.030067  6678 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:44:55.030074  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.030081  6678 net.cpp:125] relu8 needs backward computation.
I0909 15:44:55.030086  6678 net.cpp:66] Creating Layer drop8
I0909 15:44:55.030092  6678 net.cpp:329] drop8 <- fc8
I0909 15:44:55.030098  6678 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:44:55.030104  6678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:44:55.030110  6678 net.cpp:125] drop8 needs backward computation.
I0909 15:44:55.030118  6678 net.cpp:66] Creating Layer fc9
I0909 15:44:55.030124  6678 net.cpp:329] fc9 <- fc8
I0909 15:44:55.030130  6678 net.cpp:290] fc9 -> fc9
I0909 15:44:55.030493  6678 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:44:55.030504  6678 net.cpp:125] fc9 needs backward computation.
I0909 15:44:55.030513  6678 net.cpp:66] Creating Layer fc10
I0909 15:44:55.030518  6678 net.cpp:329] fc10 <- fc9
I0909 15:44:55.030525  6678 net.cpp:290] fc10 -> fc10
I0909 15:44:55.030537  6678 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:44:55.030544  6678 net.cpp:125] fc10 needs backward computation.
I0909 15:44:55.030550  6678 net.cpp:66] Creating Layer prob
I0909 15:44:55.030556  6678 net.cpp:329] prob <- fc10
I0909 15:44:55.030563  6678 net.cpp:290] prob -> prob
I0909 15:44:55.030572  6678 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:44:55.030578  6678 net.cpp:125] prob needs backward computation.
I0909 15:44:55.030583  6678 net.cpp:156] This network produces output prob
I0909 15:44:55.030596  6678 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:44:55.030604  6678 net.cpp:167] Network initialization done.
I0909 15:44:55.030608  6678 net.cpp:168] Memory required for data: 6183480
Classifying 159 inputs.
Done in 96.25 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:46:37.872537  6685 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:46:37.872675  6685 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:46:37.872684  6685 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:46:37.872827  6685 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:46:37.872889  6685 net.cpp:292] Input 0 -> data
I0909 15:46:37.872913  6685 net.cpp:66] Creating Layer conv1
I0909 15:46:37.872920  6685 net.cpp:329] conv1 <- data
I0909 15:46:37.872928  6685 net.cpp:290] conv1 -> conv1
I0909 15:46:37.874300  6685 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:46:37.874318  6685 net.cpp:125] conv1 needs backward computation.
I0909 15:46:37.874327  6685 net.cpp:66] Creating Layer relu1
I0909 15:46:37.874333  6685 net.cpp:329] relu1 <- conv1
I0909 15:46:37.874341  6685 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:46:37.874348  6685 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:46:37.874354  6685 net.cpp:125] relu1 needs backward computation.
I0909 15:46:37.874361  6685 net.cpp:66] Creating Layer pool1
I0909 15:46:37.874366  6685 net.cpp:329] pool1 <- conv1
I0909 15:46:37.874373  6685 net.cpp:290] pool1 -> pool1
I0909 15:46:37.874384  6685 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:46:37.874390  6685 net.cpp:125] pool1 needs backward computation.
I0909 15:46:37.874397  6685 net.cpp:66] Creating Layer norm1
I0909 15:46:37.874402  6685 net.cpp:329] norm1 <- pool1
I0909 15:46:37.874408  6685 net.cpp:290] norm1 -> norm1
I0909 15:46:37.874418  6685 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:46:37.874424  6685 net.cpp:125] norm1 needs backward computation.
I0909 15:46:37.874431  6685 net.cpp:66] Creating Layer conv2
I0909 15:46:37.874441  6685 net.cpp:329] conv2 <- norm1
I0909 15:46:37.874449  6685 net.cpp:290] conv2 -> conv2
I0909 15:46:37.883292  6685 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:46:37.883306  6685 net.cpp:125] conv2 needs backward computation.
I0909 15:46:37.883313  6685 net.cpp:66] Creating Layer relu2
I0909 15:46:37.883319  6685 net.cpp:329] relu2 <- conv2
I0909 15:46:37.883326  6685 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:46:37.883332  6685 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:46:37.883338  6685 net.cpp:125] relu2 needs backward computation.
I0909 15:46:37.883344  6685 net.cpp:66] Creating Layer pool2
I0909 15:46:37.883349  6685 net.cpp:329] pool2 <- conv2
I0909 15:46:37.883357  6685 net.cpp:290] pool2 -> pool2
I0909 15:46:37.883364  6685 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:46:37.883369  6685 net.cpp:125] pool2 needs backward computation.
I0909 15:46:37.883376  6685 net.cpp:66] Creating Layer fc7
I0909 15:46:37.883383  6685 net.cpp:329] fc7 <- pool2
I0909 15:46:37.883391  6685 net.cpp:290] fc7 -> fc7
I0909 15:46:38.507807  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.507854  6685 net.cpp:125] fc7 needs backward computation.
I0909 15:46:38.507866  6685 net.cpp:66] Creating Layer relu7
I0909 15:46:38.507874  6685 net.cpp:329] relu7 <- fc7
I0909 15:46:38.507882  6685 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:46:38.507892  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.507897  6685 net.cpp:125] relu7 needs backward computation.
I0909 15:46:38.507905  6685 net.cpp:66] Creating Layer drop7
I0909 15:46:38.507910  6685 net.cpp:329] drop7 <- fc7
I0909 15:46:38.507916  6685 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:46:38.507927  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.507933  6685 net.cpp:125] drop7 needs backward computation.
I0909 15:46:38.507941  6685 net.cpp:66] Creating Layer fc8
I0909 15:46:38.507946  6685 net.cpp:329] fc8 <- fc7
I0909 15:46:38.507956  6685 net.cpp:290] fc8 -> fc8
I0909 15:46:38.515506  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.515518  6685 net.cpp:125] fc8 needs backward computation.
I0909 15:46:38.515524  6685 net.cpp:66] Creating Layer relu8
I0909 15:46:38.515530  6685 net.cpp:329] relu8 <- fc8
I0909 15:46:38.515538  6685 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:46:38.515545  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.515550  6685 net.cpp:125] relu8 needs backward computation.
I0909 15:46:38.515558  6685 net.cpp:66] Creating Layer drop8
I0909 15:46:38.515563  6685 net.cpp:329] drop8 <- fc8
I0909 15:46:38.515568  6685 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:46:38.515575  6685 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:46:38.515580  6685 net.cpp:125] drop8 needs backward computation.
I0909 15:46:38.515589  6685 net.cpp:66] Creating Layer fc9
I0909 15:46:38.515594  6685 net.cpp:329] fc9 <- fc8
I0909 15:46:38.515601  6685 net.cpp:290] fc9 -> fc9
I0909 15:46:38.515964  6685 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:46:38.515975  6685 net.cpp:125] fc9 needs backward computation.
I0909 15:46:38.515983  6685 net.cpp:66] Creating Layer fc10
I0909 15:46:38.515988  6685 net.cpp:329] fc10 <- fc9
I0909 15:46:38.515996  6685 net.cpp:290] fc10 -> fc10
I0909 15:46:38.516008  6685 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:46:38.516016  6685 net.cpp:125] fc10 needs backward computation.
I0909 15:46:38.516022  6685 net.cpp:66] Creating Layer prob
I0909 15:46:38.516028  6685 net.cpp:329] prob <- fc10
I0909 15:46:38.516036  6685 net.cpp:290] prob -> prob
I0909 15:46:38.516046  6685 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:46:38.516051  6685 net.cpp:125] prob needs backward computation.
I0909 15:46:38.516055  6685 net.cpp:156] This network produces output prob
I0909 15:46:38.516068  6685 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:46:38.516077  6685 net.cpp:167] Network initialization done.
I0909 15:46:38.516082  6685 net.cpp:168] Memory required for data: 6183480
Classifying 258 inputs.
Done in 162.51 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:49:26.716210  6888 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:49:26.716347  6888 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:49:26.716356  6888 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:49:26.716497  6888 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:49:26.716549  6888 net.cpp:292] Input 0 -> data
I0909 15:49:26.716574  6888 net.cpp:66] Creating Layer conv1
I0909 15:49:26.716581  6888 net.cpp:329] conv1 <- data
I0909 15:49:26.716590  6888 net.cpp:290] conv1 -> conv1
I0909 15:49:26.717958  6888 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:49:26.717978  6888 net.cpp:125] conv1 needs backward computation.
I0909 15:49:26.717998  6888 net.cpp:66] Creating Layer relu1
I0909 15:49:26.718003  6888 net.cpp:329] relu1 <- conv1
I0909 15:49:26.718010  6888 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:49:26.718019  6888 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:49:26.718024  6888 net.cpp:125] relu1 needs backward computation.
I0909 15:49:26.718031  6888 net.cpp:66] Creating Layer pool1
I0909 15:49:26.718037  6888 net.cpp:329] pool1 <- conv1
I0909 15:49:26.718044  6888 net.cpp:290] pool1 -> pool1
I0909 15:49:26.718055  6888 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:49:26.718060  6888 net.cpp:125] pool1 needs backward computation.
I0909 15:49:26.718067  6888 net.cpp:66] Creating Layer norm1
I0909 15:49:26.718072  6888 net.cpp:329] norm1 <- pool1
I0909 15:49:26.718078  6888 net.cpp:290] norm1 -> norm1
I0909 15:49:26.718088  6888 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:49:26.718094  6888 net.cpp:125] norm1 needs backward computation.
I0909 15:49:26.718101  6888 net.cpp:66] Creating Layer conv2
I0909 15:49:26.718107  6888 net.cpp:329] conv2 <- norm1
I0909 15:49:26.718114  6888 net.cpp:290] conv2 -> conv2
I0909 15:49:26.727108  6888 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:49:26.727124  6888 net.cpp:125] conv2 needs backward computation.
I0909 15:49:26.727133  6888 net.cpp:66] Creating Layer relu2
I0909 15:49:26.727138  6888 net.cpp:329] relu2 <- conv2
I0909 15:49:26.727144  6888 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:49:26.727151  6888 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:49:26.727157  6888 net.cpp:125] relu2 needs backward computation.
I0909 15:49:26.727164  6888 net.cpp:66] Creating Layer pool2
I0909 15:49:26.727169  6888 net.cpp:329] pool2 <- conv2
I0909 15:49:26.727175  6888 net.cpp:290] pool2 -> pool2
I0909 15:49:26.727183  6888 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:49:26.727190  6888 net.cpp:125] pool2 needs backward computation.
I0909 15:49:26.727196  6888 net.cpp:66] Creating Layer fc7
I0909 15:49:26.727210  6888 net.cpp:329] fc7 <- pool2
I0909 15:49:26.727216  6888 net.cpp:290] fc7 -> fc7
I0909 15:49:27.358325  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.358382  6888 net.cpp:125] fc7 needs backward computation.
I0909 15:49:27.358412  6888 net.cpp:66] Creating Layer relu7
I0909 15:49:27.358419  6888 net.cpp:329] relu7 <- fc7
I0909 15:49:27.358428  6888 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:49:27.358438  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.358444  6888 net.cpp:125] relu7 needs backward computation.
I0909 15:49:27.358451  6888 net.cpp:66] Creating Layer drop7
I0909 15:49:27.358458  6888 net.cpp:329] drop7 <- fc7
I0909 15:49:27.358464  6888 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:49:27.358474  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.358480  6888 net.cpp:125] drop7 needs backward computation.
I0909 15:49:27.358489  6888 net.cpp:66] Creating Layer fc8
I0909 15:49:27.358494  6888 net.cpp:329] fc8 <- fc7
I0909 15:49:27.358505  6888 net.cpp:290] fc8 -> fc8
I0909 15:49:27.366112  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.366125  6888 net.cpp:125] fc8 needs backward computation.
I0909 15:49:27.366132  6888 net.cpp:66] Creating Layer relu8
I0909 15:49:27.366137  6888 net.cpp:329] relu8 <- fc8
I0909 15:49:27.366145  6888 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:49:27.366153  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.366158  6888 net.cpp:125] relu8 needs backward computation.
I0909 15:49:27.366164  6888 net.cpp:66] Creating Layer drop8
I0909 15:49:27.366170  6888 net.cpp:329] drop8 <- fc8
I0909 15:49:27.366176  6888 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:49:27.366183  6888 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:27.366189  6888 net.cpp:125] drop8 needs backward computation.
I0909 15:49:27.366197  6888 net.cpp:66] Creating Layer fc9
I0909 15:49:27.366202  6888 net.cpp:329] fc9 <- fc8
I0909 15:49:27.366209  6888 net.cpp:290] fc9 -> fc9
I0909 15:49:27.366574  6888 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:49:27.366592  6888 net.cpp:125] fc9 needs backward computation.
I0909 15:49:27.366601  6888 net.cpp:66] Creating Layer fc10
I0909 15:49:27.366607  6888 net.cpp:329] fc10 <- fc9
I0909 15:49:27.366616  6888 net.cpp:290] fc10 -> fc10
I0909 15:49:27.366627  6888 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:49:27.366636  6888 net.cpp:125] fc10 needs backward computation.
I0909 15:49:27.366641  6888 net.cpp:66] Creating Layer prob
I0909 15:49:27.366647  6888 net.cpp:329] prob <- fc10
I0909 15:49:27.366654  6888 net.cpp:290] prob -> prob
I0909 15:49:27.366664  6888 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:49:27.366669  6888 net.cpp:125] prob needs backward computation.
I0909 15:49:27.366674  6888 net.cpp:156] This network produces output prob
I0909 15:49:27.366688  6888 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:49:27.366695  6888 net.cpp:167] Network initialization done.
I0909 15:49:27.366700  6888 net.cpp:168] Memory required for data: 6183480
Classifying 18 inputs.
Done in 10.78 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:49:39.169689  6892 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:49:39.169829  6892 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:49:39.169838  6892 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:49:39.169988  6892 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:49:39.170053  6892 net.cpp:292] Input 0 -> data
I0909 15:49:39.170078  6892 net.cpp:66] Creating Layer conv1
I0909 15:49:39.170085  6892 net.cpp:329] conv1 <- data
I0909 15:49:39.170094  6892 net.cpp:290] conv1 -> conv1
I0909 15:49:39.171502  6892 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:49:39.171520  6892 net.cpp:125] conv1 needs backward computation.
I0909 15:49:39.171530  6892 net.cpp:66] Creating Layer relu1
I0909 15:49:39.171536  6892 net.cpp:329] relu1 <- conv1
I0909 15:49:39.171543  6892 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:49:39.171551  6892 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:49:39.171558  6892 net.cpp:125] relu1 needs backward computation.
I0909 15:49:39.171566  6892 net.cpp:66] Creating Layer pool1
I0909 15:49:39.171571  6892 net.cpp:329] pool1 <- conv1
I0909 15:49:39.171577  6892 net.cpp:290] pool1 -> pool1
I0909 15:49:39.171589  6892 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:49:39.171596  6892 net.cpp:125] pool1 needs backward computation.
I0909 15:49:39.171602  6892 net.cpp:66] Creating Layer norm1
I0909 15:49:39.171608  6892 net.cpp:329] norm1 <- pool1
I0909 15:49:39.171615  6892 net.cpp:290] norm1 -> norm1
I0909 15:49:39.171624  6892 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:49:39.171630  6892 net.cpp:125] norm1 needs backward computation.
I0909 15:49:39.171638  6892 net.cpp:66] Creating Layer conv2
I0909 15:49:39.171644  6892 net.cpp:329] conv2 <- norm1
I0909 15:49:39.171651  6892 net.cpp:290] conv2 -> conv2
I0909 15:49:39.180956  6892 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:49:39.180971  6892 net.cpp:125] conv2 needs backward computation.
I0909 15:49:39.180979  6892 net.cpp:66] Creating Layer relu2
I0909 15:49:39.180984  6892 net.cpp:329] relu2 <- conv2
I0909 15:49:39.180991  6892 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:49:39.180999  6892 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:49:39.181004  6892 net.cpp:125] relu2 needs backward computation.
I0909 15:49:39.181010  6892 net.cpp:66] Creating Layer pool2
I0909 15:49:39.181015  6892 net.cpp:329] pool2 <- conv2
I0909 15:49:39.181021  6892 net.cpp:290] pool2 -> pool2
I0909 15:49:39.181030  6892 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:49:39.181035  6892 net.cpp:125] pool2 needs backward computation.
I0909 15:49:39.181042  6892 net.cpp:66] Creating Layer fc7
I0909 15:49:39.181048  6892 net.cpp:329] fc7 <- pool2
I0909 15:49:39.181057  6892 net.cpp:290] fc7 -> fc7
I0909 15:49:39.809000  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.809046  6892 net.cpp:125] fc7 needs backward computation.
I0909 15:49:39.809058  6892 net.cpp:66] Creating Layer relu7
I0909 15:49:39.809067  6892 net.cpp:329] relu7 <- fc7
I0909 15:49:39.809074  6892 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:49:39.809084  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.809089  6892 net.cpp:125] relu7 needs backward computation.
I0909 15:49:39.809097  6892 net.cpp:66] Creating Layer drop7
I0909 15:49:39.809101  6892 net.cpp:329] drop7 <- fc7
I0909 15:49:39.809108  6892 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:49:39.809118  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.809124  6892 net.cpp:125] drop7 needs backward computation.
I0909 15:49:39.809133  6892 net.cpp:66] Creating Layer fc8
I0909 15:49:39.809139  6892 net.cpp:329] fc8 <- fc7
I0909 15:49:39.809159  6892 net.cpp:290] fc8 -> fc8
I0909 15:49:39.816725  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.816738  6892 net.cpp:125] fc8 needs backward computation.
I0909 15:49:39.816745  6892 net.cpp:66] Creating Layer relu8
I0909 15:49:39.816751  6892 net.cpp:329] relu8 <- fc8
I0909 15:49:39.816759  6892 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:49:39.816766  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.816772  6892 net.cpp:125] relu8 needs backward computation.
I0909 15:49:39.816778  6892 net.cpp:66] Creating Layer drop8
I0909 15:49:39.816783  6892 net.cpp:329] drop8 <- fc8
I0909 15:49:39.816789  6892 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:49:39.816797  6892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:49:39.816802  6892 net.cpp:125] drop8 needs backward computation.
I0909 15:49:39.816810  6892 net.cpp:66] Creating Layer fc9
I0909 15:49:39.816817  6892 net.cpp:329] fc9 <- fc8
I0909 15:49:39.816823  6892 net.cpp:290] fc9 -> fc9
I0909 15:49:39.817203  6892 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:49:39.817215  6892 net.cpp:125] fc9 needs backward computation.
I0909 15:49:39.817224  6892 net.cpp:66] Creating Layer fc10
I0909 15:49:39.817229  6892 net.cpp:329] fc10 <- fc9
I0909 15:49:39.817237  6892 net.cpp:290] fc10 -> fc10
I0909 15:49:39.817250  6892 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:49:39.817256  6892 net.cpp:125] fc10 needs backward computation.
I0909 15:49:39.817263  6892 net.cpp:66] Creating Layer prob
I0909 15:49:39.817268  6892 net.cpp:329] prob <- fc10
I0909 15:49:39.817276  6892 net.cpp:290] prob -> prob
I0909 15:49:39.817286  6892 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:49:39.817291  6892 net.cpp:125] prob needs backward computation.
I0909 15:49:39.817296  6892 net.cpp:156] This network produces output prob
I0909 15:49:39.817308  6892 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:49:39.817317  6892 net.cpp:167] Network initialization done.
I0909 15:49:39.817322  6892 net.cpp:168] Memory required for data: 6183480
Classifying 173 inputs.
Done in 108.27 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:51:32.965394  6903 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:51:32.965558  6903 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:51:32.965571  6903 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:51:32.965723  6903 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:51:32.965785  6903 net.cpp:292] Input 0 -> data
I0909 15:51:32.965809  6903 net.cpp:66] Creating Layer conv1
I0909 15:51:32.965816  6903 net.cpp:329] conv1 <- data
I0909 15:51:32.965823  6903 net.cpp:290] conv1 -> conv1
I0909 15:51:32.967164  6903 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:51:32.967181  6903 net.cpp:125] conv1 needs backward computation.
I0909 15:51:32.967190  6903 net.cpp:66] Creating Layer relu1
I0909 15:51:32.967195  6903 net.cpp:329] relu1 <- conv1
I0909 15:51:32.967201  6903 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:51:32.967211  6903 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:51:32.967216  6903 net.cpp:125] relu1 needs backward computation.
I0909 15:51:32.967222  6903 net.cpp:66] Creating Layer pool1
I0909 15:51:32.967227  6903 net.cpp:329] pool1 <- conv1
I0909 15:51:32.967234  6903 net.cpp:290] pool1 -> pool1
I0909 15:51:32.967244  6903 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:51:32.967250  6903 net.cpp:125] pool1 needs backward computation.
I0909 15:51:32.967257  6903 net.cpp:66] Creating Layer norm1
I0909 15:51:32.967262  6903 net.cpp:329] norm1 <- pool1
I0909 15:51:32.967269  6903 net.cpp:290] norm1 -> norm1
I0909 15:51:32.967278  6903 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:51:32.967284  6903 net.cpp:125] norm1 needs backward computation.
I0909 15:51:32.967291  6903 net.cpp:66] Creating Layer conv2
I0909 15:51:32.967296  6903 net.cpp:329] conv2 <- norm1
I0909 15:51:32.967303  6903 net.cpp:290] conv2 -> conv2
I0909 15:51:32.976233  6903 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:51:32.976248  6903 net.cpp:125] conv2 needs backward computation.
I0909 15:51:32.976255  6903 net.cpp:66] Creating Layer relu2
I0909 15:51:32.976260  6903 net.cpp:329] relu2 <- conv2
I0909 15:51:32.976268  6903 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:51:32.976274  6903 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:51:32.976279  6903 net.cpp:125] relu2 needs backward computation.
I0909 15:51:32.976285  6903 net.cpp:66] Creating Layer pool2
I0909 15:51:32.976290  6903 net.cpp:329] pool2 <- conv2
I0909 15:51:32.976297  6903 net.cpp:290] pool2 -> pool2
I0909 15:51:32.976305  6903 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:51:32.976310  6903 net.cpp:125] pool2 needs backward computation.
I0909 15:51:32.976322  6903 net.cpp:66] Creating Layer fc7
I0909 15:51:32.976328  6903 net.cpp:329] fc7 <- pool2
I0909 15:51:32.976337  6903 net.cpp:290] fc7 -> fc7
I0909 15:51:33.605919  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.605965  6903 net.cpp:125] fc7 needs backward computation.
I0909 15:51:33.605978  6903 net.cpp:66] Creating Layer relu7
I0909 15:51:33.605986  6903 net.cpp:329] relu7 <- fc7
I0909 15:51:33.605993  6903 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:51:33.606003  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.606009  6903 net.cpp:125] relu7 needs backward computation.
I0909 15:51:33.606015  6903 net.cpp:66] Creating Layer drop7
I0909 15:51:33.606021  6903 net.cpp:329] drop7 <- fc7
I0909 15:51:33.606027  6903 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:51:33.606037  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.606044  6903 net.cpp:125] drop7 needs backward computation.
I0909 15:51:33.606051  6903 net.cpp:66] Creating Layer fc8
I0909 15:51:33.606056  6903 net.cpp:329] fc8 <- fc7
I0909 15:51:33.606065  6903 net.cpp:290] fc8 -> fc8
I0909 15:51:33.613641  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.613653  6903 net.cpp:125] fc8 needs backward computation.
I0909 15:51:33.613661  6903 net.cpp:66] Creating Layer relu8
I0909 15:51:33.613665  6903 net.cpp:329] relu8 <- fc8
I0909 15:51:33.613673  6903 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:51:33.613680  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.613685  6903 net.cpp:125] relu8 needs backward computation.
I0909 15:51:33.613692  6903 net.cpp:66] Creating Layer drop8
I0909 15:51:33.613698  6903 net.cpp:329] drop8 <- fc8
I0909 15:51:33.613703  6903 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:51:33.613709  6903 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:51:33.613715  6903 net.cpp:125] drop8 needs backward computation.
I0909 15:51:33.613723  6903 net.cpp:66] Creating Layer fc9
I0909 15:51:33.613729  6903 net.cpp:329] fc9 <- fc8
I0909 15:51:33.613735  6903 net.cpp:290] fc9 -> fc9
I0909 15:51:33.614101  6903 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:51:33.614112  6903 net.cpp:125] fc9 needs backward computation.
I0909 15:51:33.614120  6903 net.cpp:66] Creating Layer fc10
I0909 15:51:33.614126  6903 net.cpp:329] fc10 <- fc9
I0909 15:51:33.614135  6903 net.cpp:290] fc10 -> fc10
I0909 15:51:33.614145  6903 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:51:33.614153  6903 net.cpp:125] fc10 needs backward computation.
I0909 15:51:33.614159  6903 net.cpp:66] Creating Layer prob
I0909 15:51:33.614164  6903 net.cpp:329] prob <- fc10
I0909 15:51:33.614172  6903 net.cpp:290] prob -> prob
I0909 15:51:33.614181  6903 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:51:33.614187  6903 net.cpp:125] prob needs backward computation.
I0909 15:51:33.614192  6903 net.cpp:156] This network produces output prob
I0909 15:51:33.614204  6903 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:51:33.614212  6903 net.cpp:167] Network initialization done.
I0909 15:51:33.614217  6903 net.cpp:168] Memory required for data: 6183480
Classifying 55 inputs.
Done in 32.58 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:52:07.706550  6907 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:52:07.706689  6907 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:52:07.706698  6907 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:52:07.706845  6907 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:52:07.706912  6907 net.cpp:292] Input 0 -> data
I0909 15:52:07.706938  6907 net.cpp:66] Creating Layer conv1
I0909 15:52:07.706944  6907 net.cpp:329] conv1 <- data
I0909 15:52:07.706953  6907 net.cpp:290] conv1 -> conv1
I0909 15:52:07.708313  6907 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:52:07.708333  6907 net.cpp:125] conv1 needs backward computation.
I0909 15:52:07.708340  6907 net.cpp:66] Creating Layer relu1
I0909 15:52:07.708346  6907 net.cpp:329] relu1 <- conv1
I0909 15:52:07.708353  6907 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:52:07.708361  6907 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:52:07.708367  6907 net.cpp:125] relu1 needs backward computation.
I0909 15:52:07.708374  6907 net.cpp:66] Creating Layer pool1
I0909 15:52:07.708379  6907 net.cpp:329] pool1 <- conv1
I0909 15:52:07.708386  6907 net.cpp:290] pool1 -> pool1
I0909 15:52:07.708397  6907 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:52:07.708403  6907 net.cpp:125] pool1 needs backward computation.
I0909 15:52:07.708410  6907 net.cpp:66] Creating Layer norm1
I0909 15:52:07.708415  6907 net.cpp:329] norm1 <- pool1
I0909 15:52:07.708422  6907 net.cpp:290] norm1 -> norm1
I0909 15:52:07.708431  6907 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:52:07.708442  6907 net.cpp:125] norm1 needs backward computation.
I0909 15:52:07.708451  6907 net.cpp:66] Creating Layer conv2
I0909 15:52:07.708456  6907 net.cpp:329] conv2 <- norm1
I0909 15:52:07.708462  6907 net.cpp:290] conv2 -> conv2
I0909 15:52:07.717582  6907 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:52:07.717597  6907 net.cpp:125] conv2 needs backward computation.
I0909 15:52:07.717604  6907 net.cpp:66] Creating Layer relu2
I0909 15:52:07.717610  6907 net.cpp:329] relu2 <- conv2
I0909 15:52:07.717617  6907 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:52:07.717623  6907 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:52:07.717629  6907 net.cpp:125] relu2 needs backward computation.
I0909 15:52:07.717635  6907 net.cpp:66] Creating Layer pool2
I0909 15:52:07.717641  6907 net.cpp:329] pool2 <- conv2
I0909 15:52:07.717648  6907 net.cpp:290] pool2 -> pool2
I0909 15:52:07.717655  6907 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:52:07.717661  6907 net.cpp:125] pool2 needs backward computation.
I0909 15:52:07.717669  6907 net.cpp:66] Creating Layer fc7
I0909 15:52:07.717674  6907 net.cpp:329] fc7 <- pool2
I0909 15:52:07.717680  6907 net.cpp:290] fc7 -> fc7
I0909 15:52:08.346079  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.346127  6907 net.cpp:125] fc7 needs backward computation.
I0909 15:52:08.346139  6907 net.cpp:66] Creating Layer relu7
I0909 15:52:08.346148  6907 net.cpp:329] relu7 <- fc7
I0909 15:52:08.346155  6907 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:52:08.346165  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.346170  6907 net.cpp:125] relu7 needs backward computation.
I0909 15:52:08.346178  6907 net.cpp:66] Creating Layer drop7
I0909 15:52:08.346184  6907 net.cpp:329] drop7 <- fc7
I0909 15:52:08.346189  6907 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:52:08.346201  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.346207  6907 net.cpp:125] drop7 needs backward computation.
I0909 15:52:08.346215  6907 net.cpp:66] Creating Layer fc8
I0909 15:52:08.346227  6907 net.cpp:329] fc8 <- fc7
I0909 15:52:08.346236  6907 net.cpp:290] fc8 -> fc8
I0909 15:52:08.353842  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.353855  6907 net.cpp:125] fc8 needs backward computation.
I0909 15:52:08.353863  6907 net.cpp:66] Creating Layer relu8
I0909 15:52:08.353868  6907 net.cpp:329] relu8 <- fc8
I0909 15:52:08.353875  6907 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:52:08.353883  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.353888  6907 net.cpp:125] relu8 needs backward computation.
I0909 15:52:08.353894  6907 net.cpp:66] Creating Layer drop8
I0909 15:52:08.353899  6907 net.cpp:329] drop8 <- fc8
I0909 15:52:08.353905  6907 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:52:08.353912  6907 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:52:08.353917  6907 net.cpp:125] drop8 needs backward computation.
I0909 15:52:08.353926  6907 net.cpp:66] Creating Layer fc9
I0909 15:52:08.353932  6907 net.cpp:329] fc9 <- fc8
I0909 15:52:08.353938  6907 net.cpp:290] fc9 -> fc9
I0909 15:52:08.354301  6907 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:52:08.354313  6907 net.cpp:125] fc9 needs backward computation.
I0909 15:52:08.354321  6907 net.cpp:66] Creating Layer fc10
I0909 15:52:08.354326  6907 net.cpp:329] fc10 <- fc9
I0909 15:52:08.354334  6907 net.cpp:290] fc10 -> fc10
I0909 15:52:08.354346  6907 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:52:08.354353  6907 net.cpp:125] fc10 needs backward computation.
I0909 15:52:08.354359  6907 net.cpp:66] Creating Layer prob
I0909 15:52:08.354365  6907 net.cpp:329] prob <- fc10
I0909 15:52:08.354372  6907 net.cpp:290] prob -> prob
I0909 15:52:08.354382  6907 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:52:08.354387  6907 net.cpp:125] prob needs backward computation.
I0909 15:52:08.354393  6907 net.cpp:156] This network produces output prob
I0909 15:52:08.354404  6907 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:52:08.354413  6907 net.cpp:167] Network initialization done.
I0909 15:52:08.354428  6907 net.cpp:168] Memory required for data: 6183480
Classifying 98 inputs.
Done in 60.66 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:53:12.146793  6957 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:53:12.146929  6957 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:53:12.146939  6957 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:53:12.147078  6957 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:53:12.147130  6957 net.cpp:292] Input 0 -> data
I0909 15:53:12.147156  6957 net.cpp:66] Creating Layer conv1
I0909 15:53:12.147162  6957 net.cpp:329] conv1 <- data
I0909 15:53:12.147171  6957 net.cpp:290] conv1 -> conv1
I0909 15:53:12.148555  6957 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:53:12.148572  6957 net.cpp:125] conv1 needs backward computation.
I0909 15:53:12.148581  6957 net.cpp:66] Creating Layer relu1
I0909 15:53:12.148587  6957 net.cpp:329] relu1 <- conv1
I0909 15:53:12.148594  6957 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:53:12.148602  6957 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:53:12.148608  6957 net.cpp:125] relu1 needs backward computation.
I0909 15:53:12.148615  6957 net.cpp:66] Creating Layer pool1
I0909 15:53:12.148620  6957 net.cpp:329] pool1 <- conv1
I0909 15:53:12.148627  6957 net.cpp:290] pool1 -> pool1
I0909 15:53:12.148638  6957 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:53:12.148644  6957 net.cpp:125] pool1 needs backward computation.
I0909 15:53:12.148651  6957 net.cpp:66] Creating Layer norm1
I0909 15:53:12.148656  6957 net.cpp:329] norm1 <- pool1
I0909 15:53:12.148663  6957 net.cpp:290] norm1 -> norm1
I0909 15:53:12.148672  6957 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:53:12.148679  6957 net.cpp:125] norm1 needs backward computation.
I0909 15:53:12.148685  6957 net.cpp:66] Creating Layer conv2
I0909 15:53:12.148690  6957 net.cpp:329] conv2 <- norm1
I0909 15:53:12.148697  6957 net.cpp:290] conv2 -> conv2
I0909 15:53:12.157902  6957 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:53:12.157933  6957 net.cpp:125] conv2 needs backward computation.
I0909 15:53:12.157943  6957 net.cpp:66] Creating Layer relu2
I0909 15:53:12.157950  6957 net.cpp:329] relu2 <- conv2
I0909 15:53:12.157958  6957 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:53:12.157968  6957 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:53:12.157973  6957 net.cpp:125] relu2 needs backward computation.
I0909 15:53:12.157980  6957 net.cpp:66] Creating Layer pool2
I0909 15:53:12.157986  6957 net.cpp:329] pool2 <- conv2
I0909 15:53:12.157994  6957 net.cpp:290] pool2 -> pool2
I0909 15:53:12.158002  6957 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:53:12.158007  6957 net.cpp:125] pool2 needs backward computation.
I0909 15:53:12.158015  6957 net.cpp:66] Creating Layer fc7
I0909 15:53:12.158021  6957 net.cpp:329] fc7 <- pool2
I0909 15:53:12.158028  6957 net.cpp:290] fc7 -> fc7
I0909 15:53:12.798365  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.798411  6957 net.cpp:125] fc7 needs backward computation.
I0909 15:53:12.798424  6957 net.cpp:66] Creating Layer relu7
I0909 15:53:12.798431  6957 net.cpp:329] relu7 <- fc7
I0909 15:53:12.798439  6957 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:53:12.798449  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.798454  6957 net.cpp:125] relu7 needs backward computation.
I0909 15:53:12.798461  6957 net.cpp:66] Creating Layer drop7
I0909 15:53:12.798466  6957 net.cpp:329] drop7 <- fc7
I0909 15:53:12.798473  6957 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:53:12.798483  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.798490  6957 net.cpp:125] drop7 needs backward computation.
I0909 15:53:12.798497  6957 net.cpp:66] Creating Layer fc8
I0909 15:53:12.798502  6957 net.cpp:329] fc8 <- fc7
I0909 15:53:12.798511  6957 net.cpp:290] fc8 -> fc8
I0909 15:53:12.806079  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.806092  6957 net.cpp:125] fc8 needs backward computation.
I0909 15:53:12.806098  6957 net.cpp:66] Creating Layer relu8
I0909 15:53:12.806104  6957 net.cpp:329] relu8 <- fc8
I0909 15:53:12.806112  6957 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:53:12.806118  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.806124  6957 net.cpp:125] relu8 needs backward computation.
I0909 15:53:12.806130  6957 net.cpp:66] Creating Layer drop8
I0909 15:53:12.806135  6957 net.cpp:329] drop8 <- fc8
I0909 15:53:12.806141  6957 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:53:12.806149  6957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:53:12.806154  6957 net.cpp:125] drop8 needs backward computation.
I0909 15:53:12.806162  6957 net.cpp:66] Creating Layer fc9
I0909 15:53:12.806167  6957 net.cpp:329] fc9 <- fc8
I0909 15:53:12.806184  6957 net.cpp:290] fc9 -> fc9
I0909 15:53:12.806545  6957 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:53:12.806557  6957 net.cpp:125] fc9 needs backward computation.
I0909 15:53:12.806565  6957 net.cpp:66] Creating Layer fc10
I0909 15:53:12.806571  6957 net.cpp:329] fc10 <- fc9
I0909 15:53:12.806578  6957 net.cpp:290] fc10 -> fc10
I0909 15:53:12.806589  6957 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:53:12.806597  6957 net.cpp:125] fc10 needs backward computation.
I0909 15:53:12.806603  6957 net.cpp:66] Creating Layer prob
I0909 15:53:12.806609  6957 net.cpp:329] prob <- fc10
I0909 15:53:12.806617  6957 net.cpp:290] prob -> prob
I0909 15:53:12.806625  6957 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:53:12.806632  6957 net.cpp:125] prob needs backward computation.
I0909 15:53:12.806635  6957 net.cpp:156] This network produces output prob
I0909 15:53:12.806648  6957 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:53:12.806656  6957 net.cpp:167] Network initialization done.
I0909 15:53:12.806660  6957 net.cpp:168] Memory required for data: 6183480
Classifying 173 inputs.
Done in 115.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:55:12.555541  6966 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:55:12.555675  6966 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:55:12.555685  6966 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:55:12.555825  6966 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:55:12.555887  6966 net.cpp:292] Input 0 -> data
I0909 15:55:12.555912  6966 net.cpp:66] Creating Layer conv1
I0909 15:55:12.555919  6966 net.cpp:329] conv1 <- data
I0909 15:55:12.555927  6966 net.cpp:290] conv1 -> conv1
I0909 15:55:12.557248  6966 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:55:12.557266  6966 net.cpp:125] conv1 needs backward computation.
I0909 15:55:12.557274  6966 net.cpp:66] Creating Layer relu1
I0909 15:55:12.557281  6966 net.cpp:329] relu1 <- conv1
I0909 15:55:12.557286  6966 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:55:12.557296  6966 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:55:12.557301  6966 net.cpp:125] relu1 needs backward computation.
I0909 15:55:12.557307  6966 net.cpp:66] Creating Layer pool1
I0909 15:55:12.557312  6966 net.cpp:329] pool1 <- conv1
I0909 15:55:12.557318  6966 net.cpp:290] pool1 -> pool1
I0909 15:55:12.557329  6966 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:55:12.557334  6966 net.cpp:125] pool1 needs backward computation.
I0909 15:55:12.557342  6966 net.cpp:66] Creating Layer norm1
I0909 15:55:12.557346  6966 net.cpp:329] norm1 <- pool1
I0909 15:55:12.557353  6966 net.cpp:290] norm1 -> norm1
I0909 15:55:12.557361  6966 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:55:12.557368  6966 net.cpp:125] norm1 needs backward computation.
I0909 15:55:12.557374  6966 net.cpp:66] Creating Layer conv2
I0909 15:55:12.557379  6966 net.cpp:329] conv2 <- norm1
I0909 15:55:12.557386  6966 net.cpp:290] conv2 -> conv2
I0909 15:55:12.566301  6966 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:55:12.566315  6966 net.cpp:125] conv2 needs backward computation.
I0909 15:55:12.566323  6966 net.cpp:66] Creating Layer relu2
I0909 15:55:12.566328  6966 net.cpp:329] relu2 <- conv2
I0909 15:55:12.566334  6966 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:55:12.566340  6966 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:55:12.566345  6966 net.cpp:125] relu2 needs backward computation.
I0909 15:55:12.566351  6966 net.cpp:66] Creating Layer pool2
I0909 15:55:12.566357  6966 net.cpp:329] pool2 <- conv2
I0909 15:55:12.566364  6966 net.cpp:290] pool2 -> pool2
I0909 15:55:12.566371  6966 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:55:12.566376  6966 net.cpp:125] pool2 needs backward computation.
I0909 15:55:12.566383  6966 net.cpp:66] Creating Layer fc7
I0909 15:55:12.566388  6966 net.cpp:329] fc7 <- pool2
I0909 15:55:12.566397  6966 net.cpp:290] fc7 -> fc7
I0909 15:55:13.198976  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.199024  6966 net.cpp:125] fc7 needs backward computation.
I0909 15:55:13.199038  6966 net.cpp:66] Creating Layer relu7
I0909 15:55:13.199048  6966 net.cpp:329] relu7 <- fc7
I0909 15:55:13.199056  6966 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:55:13.199066  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.199071  6966 net.cpp:125] relu7 needs backward computation.
I0909 15:55:13.199079  6966 net.cpp:66] Creating Layer drop7
I0909 15:55:13.199084  6966 net.cpp:329] drop7 <- fc7
I0909 15:55:13.199090  6966 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:55:13.199100  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.199106  6966 net.cpp:125] drop7 needs backward computation.
I0909 15:55:13.199126  6966 net.cpp:66] Creating Layer fc8
I0909 15:55:13.199131  6966 net.cpp:329] fc8 <- fc7
I0909 15:55:13.199141  6966 net.cpp:290] fc8 -> fc8
I0909 15:55:13.206701  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.206712  6966 net.cpp:125] fc8 needs backward computation.
I0909 15:55:13.206719  6966 net.cpp:66] Creating Layer relu8
I0909 15:55:13.206725  6966 net.cpp:329] relu8 <- fc8
I0909 15:55:13.206732  6966 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:55:13.206739  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.206745  6966 net.cpp:125] relu8 needs backward computation.
I0909 15:55:13.206751  6966 net.cpp:66] Creating Layer drop8
I0909 15:55:13.206756  6966 net.cpp:329] drop8 <- fc8
I0909 15:55:13.206763  6966 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:55:13.206769  6966 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:55:13.206774  6966 net.cpp:125] drop8 needs backward computation.
I0909 15:55:13.206784  6966 net.cpp:66] Creating Layer fc9
I0909 15:55:13.206789  6966 net.cpp:329] fc9 <- fc8
I0909 15:55:13.206795  6966 net.cpp:290] fc9 -> fc9
I0909 15:55:13.207159  6966 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:55:13.207170  6966 net.cpp:125] fc9 needs backward computation.
I0909 15:55:13.207178  6966 net.cpp:66] Creating Layer fc10
I0909 15:55:13.207183  6966 net.cpp:329] fc10 <- fc9
I0909 15:55:13.207191  6966 net.cpp:290] fc10 -> fc10
I0909 15:55:13.207202  6966 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:55:13.207211  6966 net.cpp:125] fc10 needs backward computation.
I0909 15:55:13.207216  6966 net.cpp:66] Creating Layer prob
I0909 15:55:13.207221  6966 net.cpp:329] prob <- fc10
I0909 15:55:13.207229  6966 net.cpp:290] prob -> prob
I0909 15:55:13.207238  6966 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:55:13.207243  6966 net.cpp:125] prob needs backward computation.
I0909 15:55:13.207248  6966 net.cpp:156] This network produces output prob
I0909 15:55:13.207260  6966 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:55:13.207268  6966 net.cpp:167] Network initialization done.
I0909 15:55:13.207273  6966 net.cpp:168] Memory required for data: 6183480
Classifying 167 inputs.
Done in 109.39 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:57:06.295265  6976 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:57:06.295405  6976 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:57:06.295414  6976 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:57:06.295557  6976 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:57:06.295619  6976 net.cpp:292] Input 0 -> data
I0909 15:57:06.295644  6976 net.cpp:66] Creating Layer conv1
I0909 15:57:06.295650  6976 net.cpp:329] conv1 <- data
I0909 15:57:06.295658  6976 net.cpp:290] conv1 -> conv1
I0909 15:57:06.296984  6976 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:57:06.297001  6976 net.cpp:125] conv1 needs backward computation.
I0909 15:57:06.297010  6976 net.cpp:66] Creating Layer relu1
I0909 15:57:06.297016  6976 net.cpp:329] relu1 <- conv1
I0909 15:57:06.297022  6976 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:57:06.297030  6976 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:57:06.297036  6976 net.cpp:125] relu1 needs backward computation.
I0909 15:57:06.297044  6976 net.cpp:66] Creating Layer pool1
I0909 15:57:06.297049  6976 net.cpp:329] pool1 <- conv1
I0909 15:57:06.297055  6976 net.cpp:290] pool1 -> pool1
I0909 15:57:06.297065  6976 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:57:06.297071  6976 net.cpp:125] pool1 needs backward computation.
I0909 15:57:06.297077  6976 net.cpp:66] Creating Layer norm1
I0909 15:57:06.297083  6976 net.cpp:329] norm1 <- pool1
I0909 15:57:06.297090  6976 net.cpp:290] norm1 -> norm1
I0909 15:57:06.297098  6976 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:57:06.297104  6976 net.cpp:125] norm1 needs backward computation.
I0909 15:57:06.297111  6976 net.cpp:66] Creating Layer conv2
I0909 15:57:06.297117  6976 net.cpp:329] conv2 <- norm1
I0909 15:57:06.297124  6976 net.cpp:290] conv2 -> conv2
I0909 15:57:06.306028  6976 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:57:06.306042  6976 net.cpp:125] conv2 needs backward computation.
I0909 15:57:06.306049  6976 net.cpp:66] Creating Layer relu2
I0909 15:57:06.306056  6976 net.cpp:329] relu2 <- conv2
I0909 15:57:06.306061  6976 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:57:06.306068  6976 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:57:06.306073  6976 net.cpp:125] relu2 needs backward computation.
I0909 15:57:06.306079  6976 net.cpp:66] Creating Layer pool2
I0909 15:57:06.306085  6976 net.cpp:329] pool2 <- conv2
I0909 15:57:06.306092  6976 net.cpp:290] pool2 -> pool2
I0909 15:57:06.306099  6976 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:57:06.306109  6976 net.cpp:125] pool2 needs backward computation.
I0909 15:57:06.306118  6976 net.cpp:66] Creating Layer fc7
I0909 15:57:06.306123  6976 net.cpp:329] fc7 <- pool2
I0909 15:57:06.306130  6976 net.cpp:290] fc7 -> fc7
I0909 15:57:06.937196  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.937238  6976 net.cpp:125] fc7 needs backward computation.
I0909 15:57:06.937252  6976 net.cpp:66] Creating Layer relu7
I0909 15:57:06.937258  6976 net.cpp:329] relu7 <- fc7
I0909 15:57:06.937265  6976 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:57:06.937275  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.937280  6976 net.cpp:125] relu7 needs backward computation.
I0909 15:57:06.937288  6976 net.cpp:66] Creating Layer drop7
I0909 15:57:06.937294  6976 net.cpp:329] drop7 <- fc7
I0909 15:57:06.937299  6976 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:57:06.937310  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.937316  6976 net.cpp:125] drop7 needs backward computation.
I0909 15:57:06.937325  6976 net.cpp:66] Creating Layer fc8
I0909 15:57:06.937331  6976 net.cpp:329] fc8 <- fc7
I0909 15:57:06.937340  6976 net.cpp:290] fc8 -> fc8
I0909 15:57:06.945121  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.945133  6976 net.cpp:125] fc8 needs backward computation.
I0909 15:57:06.945142  6976 net.cpp:66] Creating Layer relu8
I0909 15:57:06.945148  6976 net.cpp:329] relu8 <- fc8
I0909 15:57:06.945154  6976 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:57:06.945161  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.945166  6976 net.cpp:125] relu8 needs backward computation.
I0909 15:57:06.945173  6976 net.cpp:66] Creating Layer drop8
I0909 15:57:06.945178  6976 net.cpp:329] drop8 <- fc8
I0909 15:57:06.945185  6976 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:57:06.945193  6976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:57:06.945199  6976 net.cpp:125] drop8 needs backward computation.
I0909 15:57:06.945205  6976 net.cpp:66] Creating Layer fc9
I0909 15:57:06.945210  6976 net.cpp:329] fc9 <- fc8
I0909 15:57:06.945217  6976 net.cpp:290] fc9 -> fc9
I0909 15:57:06.945585  6976 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:57:06.945595  6976 net.cpp:125] fc9 needs backward computation.
I0909 15:57:06.945603  6976 net.cpp:66] Creating Layer fc10
I0909 15:57:06.945610  6976 net.cpp:329] fc10 <- fc9
I0909 15:57:06.945617  6976 net.cpp:290] fc10 -> fc10
I0909 15:57:06.945629  6976 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:57:06.945636  6976 net.cpp:125] fc10 needs backward computation.
I0909 15:57:06.945643  6976 net.cpp:66] Creating Layer prob
I0909 15:57:06.945648  6976 net.cpp:329] prob <- fc10
I0909 15:57:06.945657  6976 net.cpp:290] prob -> prob
I0909 15:57:06.945667  6976 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:57:06.945672  6976 net.cpp:125] prob needs backward computation.
I0909 15:57:06.945677  6976 net.cpp:156] This network produces output prob
I0909 15:57:06.945696  6976 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:57:06.945704  6976 net.cpp:167] Network initialization done.
I0909 15:57:06.945709  6976 net.cpp:168] Memory required for data: 6183480
Classifying 243 inputs.
Done in 150.56 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 15:59:41.458073  7107 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 15:59:41.458215  7107 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 15:59:41.458225  7107 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 15:59:41.458372  7107 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 15:59:41.458436  7107 net.cpp:292] Input 0 -> data
I0909 15:59:41.458461  7107 net.cpp:66] Creating Layer conv1
I0909 15:59:41.458468  7107 net.cpp:329] conv1 <- data
I0909 15:59:41.458477  7107 net.cpp:290] conv1 -> conv1
I0909 15:59:41.459903  7107 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:59:41.459920  7107 net.cpp:125] conv1 needs backward computation.
I0909 15:59:41.459930  7107 net.cpp:66] Creating Layer relu1
I0909 15:59:41.459936  7107 net.cpp:329] relu1 <- conv1
I0909 15:59:41.459944  7107 net.cpp:280] relu1 -> conv1 (in-place)
I0909 15:59:41.459952  7107 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 15:59:41.459959  7107 net.cpp:125] relu1 needs backward computation.
I0909 15:59:41.459966  7107 net.cpp:66] Creating Layer pool1
I0909 15:59:41.459972  7107 net.cpp:329] pool1 <- conv1
I0909 15:59:41.459980  7107 net.cpp:290] pool1 -> pool1
I0909 15:59:41.459990  7107 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:59:41.459996  7107 net.cpp:125] pool1 needs backward computation.
I0909 15:59:41.460003  7107 net.cpp:66] Creating Layer norm1
I0909 15:59:41.460010  7107 net.cpp:329] norm1 <- pool1
I0909 15:59:41.460016  7107 net.cpp:290] norm1 -> norm1
I0909 15:59:41.460031  7107 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 15:59:41.460037  7107 net.cpp:125] norm1 needs backward computation.
I0909 15:59:41.460046  7107 net.cpp:66] Creating Layer conv2
I0909 15:59:41.460052  7107 net.cpp:329] conv2 <- norm1
I0909 15:59:41.460058  7107 net.cpp:290] conv2 -> conv2
I0909 15:59:41.469162  7107 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:59:41.469179  7107 net.cpp:125] conv2 needs backward computation.
I0909 15:59:41.469187  7107 net.cpp:66] Creating Layer relu2
I0909 15:59:41.469192  7107 net.cpp:329] relu2 <- conv2
I0909 15:59:41.469200  7107 net.cpp:280] relu2 -> conv2 (in-place)
I0909 15:59:41.469208  7107 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 15:59:41.469213  7107 net.cpp:125] relu2 needs backward computation.
I0909 15:59:41.469220  7107 net.cpp:66] Creating Layer pool2
I0909 15:59:41.469226  7107 net.cpp:329] pool2 <- conv2
I0909 15:59:41.469233  7107 net.cpp:290] pool2 -> pool2
I0909 15:59:41.469241  7107 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 15:59:41.469247  7107 net.cpp:125] pool2 needs backward computation.
I0909 15:59:41.469255  7107 net.cpp:66] Creating Layer fc7
I0909 15:59:41.469261  7107 net.cpp:329] fc7 <- pool2
I0909 15:59:41.469271  7107 net.cpp:290] fc7 -> fc7
I0909 15:59:42.115674  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.115716  7107 net.cpp:125] fc7 needs backward computation.
I0909 15:59:42.115728  7107 net.cpp:66] Creating Layer relu7
I0909 15:59:42.115738  7107 net.cpp:329] relu7 <- fc7
I0909 15:59:42.115746  7107 net.cpp:280] relu7 -> fc7 (in-place)
I0909 15:59:42.115756  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.115762  7107 net.cpp:125] relu7 needs backward computation.
I0909 15:59:42.115771  7107 net.cpp:66] Creating Layer drop7
I0909 15:59:42.115777  7107 net.cpp:329] drop7 <- fc7
I0909 15:59:42.115783  7107 net.cpp:280] drop7 -> fc7 (in-place)
I0909 15:59:42.115794  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.115800  7107 net.cpp:125] drop7 needs backward computation.
I0909 15:59:42.115808  7107 net.cpp:66] Creating Layer fc8
I0909 15:59:42.115814  7107 net.cpp:329] fc8 <- fc7
I0909 15:59:42.115824  7107 net.cpp:290] fc8 -> fc8
I0909 15:59:42.123615  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.123630  7107 net.cpp:125] fc8 needs backward computation.
I0909 15:59:42.123636  7107 net.cpp:66] Creating Layer relu8
I0909 15:59:42.123642  7107 net.cpp:329] relu8 <- fc8
I0909 15:59:42.123651  7107 net.cpp:280] relu8 -> fc8 (in-place)
I0909 15:59:42.123658  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.123664  7107 net.cpp:125] relu8 needs backward computation.
I0909 15:59:42.123672  7107 net.cpp:66] Creating Layer drop8
I0909 15:59:42.123677  7107 net.cpp:329] drop8 <- fc8
I0909 15:59:42.123683  7107 net.cpp:280] drop8 -> fc8 (in-place)
I0909 15:59:42.123690  7107 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 15:59:42.123697  7107 net.cpp:125] drop8 needs backward computation.
I0909 15:59:42.123705  7107 net.cpp:66] Creating Layer fc9
I0909 15:59:42.123711  7107 net.cpp:329] fc9 <- fc8
I0909 15:59:42.123719  7107 net.cpp:290] fc9 -> fc9
I0909 15:59:42.124094  7107 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 15:59:42.124107  7107 net.cpp:125] fc9 needs backward computation.
I0909 15:59:42.124115  7107 net.cpp:66] Creating Layer fc10
I0909 15:59:42.124121  7107 net.cpp:329] fc10 <- fc9
I0909 15:59:42.124130  7107 net.cpp:290] fc10 -> fc10
I0909 15:59:42.124142  7107 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:59:42.124150  7107 net.cpp:125] fc10 needs backward computation.
I0909 15:59:42.124157  7107 net.cpp:66] Creating Layer prob
I0909 15:59:42.124163  7107 net.cpp:329] prob <- fc10
I0909 15:59:42.124171  7107 net.cpp:290] prob -> prob
I0909 15:59:42.124181  7107 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 15:59:42.124187  7107 net.cpp:125] prob needs backward computation.
I0909 15:59:42.124193  7107 net.cpp:156] This network produces output prob
I0909 15:59:42.124207  7107 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 15:59:42.124224  7107 net.cpp:167] Network initialization done.
I0909 15:59:42.124229  7107 net.cpp:168] Memory required for data: 6183480
Classifying 278 inputs.
Done in 169.18 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:02:39.767297  7269 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:02:39.767436  7269 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:02:39.767444  7269 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:02:39.767591  7269 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:02:39.767644  7269 net.cpp:292] Input 0 -> data
I0909 16:02:39.767670  7269 net.cpp:66] Creating Layer conv1
I0909 16:02:39.767678  7269 net.cpp:329] conv1 <- data
I0909 16:02:39.767698  7269 net.cpp:290] conv1 -> conv1
I0909 16:02:39.769062  7269 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:02:39.769079  7269 net.cpp:125] conv1 needs backward computation.
I0909 16:02:39.769088  7269 net.cpp:66] Creating Layer relu1
I0909 16:02:39.769095  7269 net.cpp:329] relu1 <- conv1
I0909 16:02:39.769103  7269 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:02:39.769110  7269 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:02:39.769116  7269 net.cpp:125] relu1 needs backward computation.
I0909 16:02:39.769124  7269 net.cpp:66] Creating Layer pool1
I0909 16:02:39.769129  7269 net.cpp:329] pool1 <- conv1
I0909 16:02:39.769135  7269 net.cpp:290] pool1 -> pool1
I0909 16:02:39.769146  7269 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:02:39.769152  7269 net.cpp:125] pool1 needs backward computation.
I0909 16:02:39.769160  7269 net.cpp:66] Creating Layer norm1
I0909 16:02:39.769165  7269 net.cpp:329] norm1 <- pool1
I0909 16:02:39.769172  7269 net.cpp:290] norm1 -> norm1
I0909 16:02:39.769181  7269 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:02:39.769187  7269 net.cpp:125] norm1 needs backward computation.
I0909 16:02:39.769194  7269 net.cpp:66] Creating Layer conv2
I0909 16:02:39.769201  7269 net.cpp:329] conv2 <- norm1
I0909 16:02:39.769207  7269 net.cpp:290] conv2 -> conv2
I0909 16:02:39.778348  7269 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:02:39.778363  7269 net.cpp:125] conv2 needs backward computation.
I0909 16:02:39.778370  7269 net.cpp:66] Creating Layer relu2
I0909 16:02:39.778376  7269 net.cpp:329] relu2 <- conv2
I0909 16:02:39.778383  7269 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:02:39.778390  7269 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:02:39.778395  7269 net.cpp:125] relu2 needs backward computation.
I0909 16:02:39.778401  7269 net.cpp:66] Creating Layer pool2
I0909 16:02:39.778408  7269 net.cpp:329] pool2 <- conv2
I0909 16:02:39.778414  7269 net.cpp:290] pool2 -> pool2
I0909 16:02:39.778421  7269 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:02:39.778427  7269 net.cpp:125] pool2 needs backward computation.
I0909 16:02:39.778434  7269 net.cpp:66] Creating Layer fc7
I0909 16:02:39.778439  7269 net.cpp:329] fc7 <- pool2
I0909 16:02:39.778446  7269 net.cpp:290] fc7 -> fc7
I0909 16:02:40.419157  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.419208  7269 net.cpp:125] fc7 needs backward computation.
I0909 16:02:40.419221  7269 net.cpp:66] Creating Layer relu7
I0909 16:02:40.419234  7269 net.cpp:329] relu7 <- fc7
I0909 16:02:40.419241  7269 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:02:40.419251  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.419257  7269 net.cpp:125] relu7 needs backward computation.
I0909 16:02:40.419265  7269 net.cpp:66] Creating Layer drop7
I0909 16:02:40.419270  7269 net.cpp:329] drop7 <- fc7
I0909 16:02:40.419277  7269 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:02:40.419288  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.419294  7269 net.cpp:125] drop7 needs backward computation.
I0909 16:02:40.419303  7269 net.cpp:66] Creating Layer fc8
I0909 16:02:40.419309  7269 net.cpp:329] fc8 <- fc7
I0909 16:02:40.419318  7269 net.cpp:290] fc8 -> fc8
I0909 16:02:40.427139  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.427152  7269 net.cpp:125] fc8 needs backward computation.
I0909 16:02:40.427160  7269 net.cpp:66] Creating Layer relu8
I0909 16:02:40.427165  7269 net.cpp:329] relu8 <- fc8
I0909 16:02:40.427173  7269 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:02:40.427181  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.427187  7269 net.cpp:125] relu8 needs backward computation.
I0909 16:02:40.427194  7269 net.cpp:66] Creating Layer drop8
I0909 16:02:40.427199  7269 net.cpp:329] drop8 <- fc8
I0909 16:02:40.427206  7269 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:02:40.427212  7269 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:02:40.427218  7269 net.cpp:125] drop8 needs backward computation.
I0909 16:02:40.427235  7269 net.cpp:66] Creating Layer fc9
I0909 16:02:40.427242  7269 net.cpp:329] fc9 <- fc8
I0909 16:02:40.427248  7269 net.cpp:290] fc9 -> fc9
I0909 16:02:40.427624  7269 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:02:40.427636  7269 net.cpp:125] fc9 needs backward computation.
I0909 16:02:40.427644  7269 net.cpp:66] Creating Layer fc10
I0909 16:02:40.427650  7269 net.cpp:329] fc10 <- fc9
I0909 16:02:40.427659  7269 net.cpp:290] fc10 -> fc10
I0909 16:02:40.427670  7269 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:02:40.427678  7269 net.cpp:125] fc10 needs backward computation.
I0909 16:02:40.427685  7269 net.cpp:66] Creating Layer prob
I0909 16:02:40.427690  7269 net.cpp:329] prob <- fc10
I0909 16:02:40.427698  7269 net.cpp:290] prob -> prob
I0909 16:02:40.427707  7269 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:02:40.427713  7269 net.cpp:125] prob needs backward computation.
I0909 16:02:40.427718  7269 net.cpp:156] This network produces output prob
I0909 16:02:40.427731  7269 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:02:40.427739  7269 net.cpp:167] Network initialization done.
I0909 16:02:40.427745  7269 net.cpp:168] Memory required for data: 6183480
Classifying 716 inputs.
Done in 436.94 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:10:12.778859  7414 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:10:12.778997  7414 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:10:12.779006  7414 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:10:12.779150  7414 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:10:12.779211  7414 net.cpp:292] Input 0 -> data
I0909 16:10:12.779237  7414 net.cpp:66] Creating Layer conv1
I0909 16:10:12.779242  7414 net.cpp:329] conv1 <- data
I0909 16:10:12.779250  7414 net.cpp:290] conv1 -> conv1
I0909 16:10:12.780572  7414 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:10:12.780591  7414 net.cpp:125] conv1 needs backward computation.
I0909 16:10:12.780599  7414 net.cpp:66] Creating Layer relu1
I0909 16:10:12.780604  7414 net.cpp:329] relu1 <- conv1
I0909 16:10:12.780611  7414 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:10:12.780619  7414 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:10:12.780625  7414 net.cpp:125] relu1 needs backward computation.
I0909 16:10:12.780632  7414 net.cpp:66] Creating Layer pool1
I0909 16:10:12.780637  7414 net.cpp:329] pool1 <- conv1
I0909 16:10:12.780643  7414 net.cpp:290] pool1 -> pool1
I0909 16:10:12.780653  7414 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:10:12.780659  7414 net.cpp:125] pool1 needs backward computation.
I0909 16:10:12.780666  7414 net.cpp:66] Creating Layer norm1
I0909 16:10:12.780671  7414 net.cpp:329] norm1 <- pool1
I0909 16:10:12.780678  7414 net.cpp:290] norm1 -> norm1
I0909 16:10:12.780686  7414 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:10:12.780693  7414 net.cpp:125] norm1 needs backward computation.
I0909 16:10:12.780699  7414 net.cpp:66] Creating Layer conv2
I0909 16:10:12.780704  7414 net.cpp:329] conv2 <- norm1
I0909 16:10:12.780711  7414 net.cpp:290] conv2 -> conv2
I0909 16:10:12.789728  7414 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:10:12.789742  7414 net.cpp:125] conv2 needs backward computation.
I0909 16:10:12.789749  7414 net.cpp:66] Creating Layer relu2
I0909 16:10:12.789755  7414 net.cpp:329] relu2 <- conv2
I0909 16:10:12.789762  7414 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:10:12.789768  7414 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:10:12.789773  7414 net.cpp:125] relu2 needs backward computation.
I0909 16:10:12.789779  7414 net.cpp:66] Creating Layer pool2
I0909 16:10:12.789784  7414 net.cpp:329] pool2 <- conv2
I0909 16:10:12.789791  7414 net.cpp:290] pool2 -> pool2
I0909 16:10:12.789798  7414 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:10:12.789804  7414 net.cpp:125] pool2 needs backward computation.
I0909 16:10:12.789811  7414 net.cpp:66] Creating Layer fc7
I0909 16:10:12.789816  7414 net.cpp:329] fc7 <- pool2
I0909 16:10:12.789825  7414 net.cpp:290] fc7 -> fc7
I0909 16:10:13.428689  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.428735  7414 net.cpp:125] fc7 needs backward computation.
I0909 16:10:13.428748  7414 net.cpp:66] Creating Layer relu7
I0909 16:10:13.428756  7414 net.cpp:329] relu7 <- fc7
I0909 16:10:13.428764  7414 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:10:13.428774  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.428781  7414 net.cpp:125] relu7 needs backward computation.
I0909 16:10:13.428787  7414 net.cpp:66] Creating Layer drop7
I0909 16:10:13.428792  7414 net.cpp:329] drop7 <- fc7
I0909 16:10:13.428799  7414 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:10:13.428809  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.428828  7414 net.cpp:125] drop7 needs backward computation.
I0909 16:10:13.428836  7414 net.cpp:66] Creating Layer fc8
I0909 16:10:13.428841  7414 net.cpp:329] fc8 <- fc7
I0909 16:10:13.428850  7414 net.cpp:290] fc8 -> fc8
I0909 16:10:13.436635  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.436648  7414 net.cpp:125] fc8 needs backward computation.
I0909 16:10:13.436655  7414 net.cpp:66] Creating Layer relu8
I0909 16:10:13.436661  7414 net.cpp:329] relu8 <- fc8
I0909 16:10:13.436669  7414 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:10:13.436676  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.436682  7414 net.cpp:125] relu8 needs backward computation.
I0909 16:10:13.436688  7414 net.cpp:66] Creating Layer drop8
I0909 16:10:13.436693  7414 net.cpp:329] drop8 <- fc8
I0909 16:10:13.436700  7414 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:10:13.436707  7414 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:13.436713  7414 net.cpp:125] drop8 needs backward computation.
I0909 16:10:13.436722  7414 net.cpp:66] Creating Layer fc9
I0909 16:10:13.436728  7414 net.cpp:329] fc9 <- fc8
I0909 16:10:13.436734  7414 net.cpp:290] fc9 -> fc9
I0909 16:10:13.437110  7414 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:10:13.437122  7414 net.cpp:125] fc9 needs backward computation.
I0909 16:10:13.437130  7414 net.cpp:66] Creating Layer fc10
I0909 16:10:13.437136  7414 net.cpp:329] fc10 <- fc9
I0909 16:10:13.437144  7414 net.cpp:290] fc10 -> fc10
I0909 16:10:13.437156  7414 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:10:13.437165  7414 net.cpp:125] fc10 needs backward computation.
I0909 16:10:13.437170  7414 net.cpp:66] Creating Layer prob
I0909 16:10:13.437176  7414 net.cpp:329] prob <- fc10
I0909 16:10:13.437183  7414 net.cpp:290] prob -> prob
I0909 16:10:13.437193  7414 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:10:13.437199  7414 net.cpp:125] prob needs backward computation.
I0909 16:10:13.437204  7414 net.cpp:156] This network produces output prob
I0909 16:10:13.437216  7414 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:10:13.437225  7414 net.cpp:167] Network initialization done.
I0909 16:10:13.437230  7414 net.cpp:168] Memory required for data: 6183480
Classifying 35 inputs.
Done in 21.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:10:35.861503  7422 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:10:35.861675  7422 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:10:35.861685  7422 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:10:35.861830  7422 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:10:35.861893  7422 net.cpp:292] Input 0 -> data
I0909 16:10:35.861919  7422 net.cpp:66] Creating Layer conv1
I0909 16:10:35.861927  7422 net.cpp:329] conv1 <- data
I0909 16:10:35.861933  7422 net.cpp:290] conv1 -> conv1
I0909 16:10:35.863297  7422 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:10:35.863315  7422 net.cpp:125] conv1 needs backward computation.
I0909 16:10:35.863323  7422 net.cpp:66] Creating Layer relu1
I0909 16:10:35.863329  7422 net.cpp:329] relu1 <- conv1
I0909 16:10:35.863337  7422 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:10:35.863344  7422 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:10:35.863350  7422 net.cpp:125] relu1 needs backward computation.
I0909 16:10:35.863358  7422 net.cpp:66] Creating Layer pool1
I0909 16:10:35.863363  7422 net.cpp:329] pool1 <- conv1
I0909 16:10:35.863369  7422 net.cpp:290] pool1 -> pool1
I0909 16:10:35.863380  7422 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:10:35.863386  7422 net.cpp:125] pool1 needs backward computation.
I0909 16:10:35.863394  7422 net.cpp:66] Creating Layer norm1
I0909 16:10:35.863399  7422 net.cpp:329] norm1 <- pool1
I0909 16:10:35.863405  7422 net.cpp:290] norm1 -> norm1
I0909 16:10:35.863415  7422 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:10:35.863420  7422 net.cpp:125] norm1 needs backward computation.
I0909 16:10:35.863427  7422 net.cpp:66] Creating Layer conv2
I0909 16:10:35.863433  7422 net.cpp:329] conv2 <- norm1
I0909 16:10:35.863440  7422 net.cpp:290] conv2 -> conv2
I0909 16:10:35.872553  7422 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:10:35.872568  7422 net.cpp:125] conv2 needs backward computation.
I0909 16:10:35.872575  7422 net.cpp:66] Creating Layer relu2
I0909 16:10:35.872581  7422 net.cpp:329] relu2 <- conv2
I0909 16:10:35.872587  7422 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:10:35.872594  7422 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:10:35.872601  7422 net.cpp:125] relu2 needs backward computation.
I0909 16:10:35.872606  7422 net.cpp:66] Creating Layer pool2
I0909 16:10:35.872611  7422 net.cpp:329] pool2 <- conv2
I0909 16:10:35.872618  7422 net.cpp:290] pool2 -> pool2
I0909 16:10:35.872632  7422 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:10:35.872637  7422 net.cpp:125] pool2 needs backward computation.
I0909 16:10:35.872644  7422 net.cpp:66] Creating Layer fc7
I0909 16:10:35.872650  7422 net.cpp:329] fc7 <- pool2
I0909 16:10:35.872659  7422 net.cpp:290] fc7 -> fc7
I0909 16:10:36.517762  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.517807  7422 net.cpp:125] fc7 needs backward computation.
I0909 16:10:36.517820  7422 net.cpp:66] Creating Layer relu7
I0909 16:10:36.517829  7422 net.cpp:329] relu7 <- fc7
I0909 16:10:36.517838  7422 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:10:36.517848  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.517853  7422 net.cpp:125] relu7 needs backward computation.
I0909 16:10:36.517859  7422 net.cpp:66] Creating Layer drop7
I0909 16:10:36.517865  7422 net.cpp:329] drop7 <- fc7
I0909 16:10:36.517871  7422 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:10:36.517882  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.517889  7422 net.cpp:125] drop7 needs backward computation.
I0909 16:10:36.517896  7422 net.cpp:66] Creating Layer fc8
I0909 16:10:36.517901  7422 net.cpp:329] fc8 <- fc7
I0909 16:10:36.517910  7422 net.cpp:290] fc8 -> fc8
I0909 16:10:36.525759  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.525771  7422 net.cpp:125] fc8 needs backward computation.
I0909 16:10:36.525779  7422 net.cpp:66] Creating Layer relu8
I0909 16:10:36.525784  7422 net.cpp:329] relu8 <- fc8
I0909 16:10:36.525792  7422 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:10:36.525799  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.525804  7422 net.cpp:125] relu8 needs backward computation.
I0909 16:10:36.525811  7422 net.cpp:66] Creating Layer drop8
I0909 16:10:36.525816  7422 net.cpp:329] drop8 <- fc8
I0909 16:10:36.525822  7422 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:10:36.525830  7422 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:10:36.525835  7422 net.cpp:125] drop8 needs backward computation.
I0909 16:10:36.525843  7422 net.cpp:66] Creating Layer fc9
I0909 16:10:36.525849  7422 net.cpp:329] fc9 <- fc8
I0909 16:10:36.525856  7422 net.cpp:290] fc9 -> fc9
I0909 16:10:36.526234  7422 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:10:36.526247  7422 net.cpp:125] fc9 needs backward computation.
I0909 16:10:36.526254  7422 net.cpp:66] Creating Layer fc10
I0909 16:10:36.526259  7422 net.cpp:329] fc10 <- fc9
I0909 16:10:36.526268  7422 net.cpp:290] fc10 -> fc10
I0909 16:10:36.526279  7422 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:10:36.526288  7422 net.cpp:125] fc10 needs backward computation.
I0909 16:10:36.526293  7422 net.cpp:66] Creating Layer prob
I0909 16:10:36.526299  7422 net.cpp:329] prob <- fc10
I0909 16:10:36.526307  7422 net.cpp:290] prob -> prob
I0909 16:10:36.526316  7422 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:10:36.526322  7422 net.cpp:125] prob needs backward computation.
I0909 16:10:36.526327  7422 net.cpp:156] This network produces output prob
I0909 16:10:36.526340  7422 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:10:36.526348  7422 net.cpp:167] Network initialization done.
I0909 16:10:36.526353  7422 net.cpp:168] Memory required for data: 6183480
Classifying 158 inputs.
Done in 100.15 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:12:20.341614  7431 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:12:20.341755  7431 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:12:20.341765  7431 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:12:20.341912  7431 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:12:20.341974  7431 net.cpp:292] Input 0 -> data
I0909 16:12:20.342000  7431 net.cpp:66] Creating Layer conv1
I0909 16:12:20.342007  7431 net.cpp:329] conv1 <- data
I0909 16:12:20.342015  7431 net.cpp:290] conv1 -> conv1
I0909 16:12:20.343379  7431 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:12:20.343397  7431 net.cpp:125] conv1 needs backward computation.
I0909 16:12:20.343406  7431 net.cpp:66] Creating Layer relu1
I0909 16:12:20.343412  7431 net.cpp:329] relu1 <- conv1
I0909 16:12:20.343420  7431 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:12:20.343427  7431 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:12:20.343433  7431 net.cpp:125] relu1 needs backward computation.
I0909 16:12:20.343441  7431 net.cpp:66] Creating Layer pool1
I0909 16:12:20.343446  7431 net.cpp:329] pool1 <- conv1
I0909 16:12:20.343453  7431 net.cpp:290] pool1 -> pool1
I0909 16:12:20.343464  7431 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:12:20.343469  7431 net.cpp:125] pool1 needs backward computation.
I0909 16:12:20.343477  7431 net.cpp:66] Creating Layer norm1
I0909 16:12:20.343487  7431 net.cpp:329] norm1 <- pool1
I0909 16:12:20.343494  7431 net.cpp:290] norm1 -> norm1
I0909 16:12:20.343504  7431 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:12:20.343510  7431 net.cpp:125] norm1 needs backward computation.
I0909 16:12:20.343518  7431 net.cpp:66] Creating Layer conv2
I0909 16:12:20.343523  7431 net.cpp:329] conv2 <- norm1
I0909 16:12:20.343530  7431 net.cpp:290] conv2 -> conv2
I0909 16:12:20.352658  7431 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:12:20.352673  7431 net.cpp:125] conv2 needs backward computation.
I0909 16:12:20.352680  7431 net.cpp:66] Creating Layer relu2
I0909 16:12:20.352686  7431 net.cpp:329] relu2 <- conv2
I0909 16:12:20.352694  7431 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:12:20.352700  7431 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:12:20.352705  7431 net.cpp:125] relu2 needs backward computation.
I0909 16:12:20.352712  7431 net.cpp:66] Creating Layer pool2
I0909 16:12:20.352718  7431 net.cpp:329] pool2 <- conv2
I0909 16:12:20.352725  7431 net.cpp:290] pool2 -> pool2
I0909 16:12:20.352733  7431 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:12:20.352738  7431 net.cpp:125] pool2 needs backward computation.
I0909 16:12:20.352746  7431 net.cpp:66] Creating Layer fc7
I0909 16:12:20.352751  7431 net.cpp:329] fc7 <- pool2
I0909 16:12:20.352761  7431 net.cpp:290] fc7 -> fc7
I0909 16:12:20.995250  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:20.995290  7431 net.cpp:125] fc7 needs backward computation.
I0909 16:12:20.995302  7431 net.cpp:66] Creating Layer relu7
I0909 16:12:20.995311  7431 net.cpp:329] relu7 <- fc7
I0909 16:12:20.995318  7431 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:12:20.995328  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:20.995333  7431 net.cpp:125] relu7 needs backward computation.
I0909 16:12:20.995340  7431 net.cpp:66] Creating Layer drop7
I0909 16:12:20.995347  7431 net.cpp:329] drop7 <- fc7
I0909 16:12:20.995352  7431 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:12:20.995362  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:20.995368  7431 net.cpp:125] drop7 needs backward computation.
I0909 16:12:20.995376  7431 net.cpp:66] Creating Layer fc8
I0909 16:12:20.995381  7431 net.cpp:329] fc8 <- fc7
I0909 16:12:20.995390  7431 net.cpp:290] fc8 -> fc8
I0909 16:12:21.003017  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:21.003029  7431 net.cpp:125] fc8 needs backward computation.
I0909 16:12:21.003036  7431 net.cpp:66] Creating Layer relu8
I0909 16:12:21.003042  7431 net.cpp:329] relu8 <- fc8
I0909 16:12:21.003049  7431 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:12:21.003057  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:21.003062  7431 net.cpp:125] relu8 needs backward computation.
I0909 16:12:21.003069  7431 net.cpp:66] Creating Layer drop8
I0909 16:12:21.003074  7431 net.cpp:329] drop8 <- fc8
I0909 16:12:21.003080  7431 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:12:21.003087  7431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:12:21.003093  7431 net.cpp:125] drop8 needs backward computation.
I0909 16:12:21.003101  7431 net.cpp:66] Creating Layer fc9
I0909 16:12:21.003106  7431 net.cpp:329] fc9 <- fc8
I0909 16:12:21.003114  7431 net.cpp:290] fc9 -> fc9
I0909 16:12:21.003492  7431 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:12:21.003504  7431 net.cpp:125] fc9 needs backward computation.
I0909 16:12:21.003512  7431 net.cpp:66] Creating Layer fc10
I0909 16:12:21.003517  7431 net.cpp:329] fc10 <- fc9
I0909 16:12:21.003525  7431 net.cpp:290] fc10 -> fc10
I0909 16:12:21.003537  7431 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:12:21.003545  7431 net.cpp:125] fc10 needs backward computation.
I0909 16:12:21.003551  7431 net.cpp:66] Creating Layer prob
I0909 16:12:21.003556  7431 net.cpp:329] prob <- fc10
I0909 16:12:21.003564  7431 net.cpp:290] prob -> prob
I0909 16:12:21.003573  7431 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:12:21.003579  7431 net.cpp:125] prob needs backward computation.
I0909 16:12:21.003584  7431 net.cpp:156] This network produces output prob
I0909 16:12:21.003605  7431 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:12:21.003613  7431 net.cpp:167] Network initialization done.
I0909 16:12:21.003619  7431 net.cpp:168] Memory required for data: 6183480
Classifying 405 inputs.
Done in 260.92 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:16:53.138139  7564 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:16:53.138274  7564 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:16:53.138283  7564 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:16:53.138433  7564 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:16:53.138486  7564 net.cpp:292] Input 0 -> data
I0909 16:16:53.138525  7564 net.cpp:66] Creating Layer conv1
I0909 16:16:53.138532  7564 net.cpp:329] conv1 <- data
I0909 16:16:53.138540  7564 net.cpp:290] conv1 -> conv1
I0909 16:16:53.139884  7564 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:16:53.139901  7564 net.cpp:125] conv1 needs backward computation.
I0909 16:16:53.139910  7564 net.cpp:66] Creating Layer relu1
I0909 16:16:53.139916  7564 net.cpp:329] relu1 <- conv1
I0909 16:16:53.139924  7564 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:16:53.139931  7564 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:16:53.139937  7564 net.cpp:125] relu1 needs backward computation.
I0909 16:16:53.139943  7564 net.cpp:66] Creating Layer pool1
I0909 16:16:53.139950  7564 net.cpp:329] pool1 <- conv1
I0909 16:16:53.139955  7564 net.cpp:290] pool1 -> pool1
I0909 16:16:53.139966  7564 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:16:53.139972  7564 net.cpp:125] pool1 needs backward computation.
I0909 16:16:53.139978  7564 net.cpp:66] Creating Layer norm1
I0909 16:16:53.139984  7564 net.cpp:329] norm1 <- pool1
I0909 16:16:53.139991  7564 net.cpp:290] norm1 -> norm1
I0909 16:16:53.139999  7564 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:16:53.140005  7564 net.cpp:125] norm1 needs backward computation.
I0909 16:16:53.140012  7564 net.cpp:66] Creating Layer conv2
I0909 16:16:53.140018  7564 net.cpp:329] conv2 <- norm1
I0909 16:16:53.140025  7564 net.cpp:290] conv2 -> conv2
I0909 16:16:53.149240  7564 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:16:53.149272  7564 net.cpp:125] conv2 needs backward computation.
I0909 16:16:53.149282  7564 net.cpp:66] Creating Layer relu2
I0909 16:16:53.149288  7564 net.cpp:329] relu2 <- conv2
I0909 16:16:53.149296  7564 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:16:53.149304  7564 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:16:53.149310  7564 net.cpp:125] relu2 needs backward computation.
I0909 16:16:53.149317  7564 net.cpp:66] Creating Layer pool2
I0909 16:16:53.149322  7564 net.cpp:329] pool2 <- conv2
I0909 16:16:53.149330  7564 net.cpp:290] pool2 -> pool2
I0909 16:16:53.149339  7564 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:16:53.149345  7564 net.cpp:125] pool2 needs backward computation.
I0909 16:16:53.149353  7564 net.cpp:66] Creating Layer fc7
I0909 16:16:53.149358  7564 net.cpp:329] fc7 <- pool2
I0909 16:16:53.149371  7564 net.cpp:290] fc7 -> fc7
I0909 16:16:53.784703  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.784750  7564 net.cpp:125] fc7 needs backward computation.
I0909 16:16:53.784764  7564 net.cpp:66] Creating Layer relu7
I0909 16:16:53.784771  7564 net.cpp:329] relu7 <- fc7
I0909 16:16:53.784780  7564 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:16:53.784790  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.784795  7564 net.cpp:125] relu7 needs backward computation.
I0909 16:16:53.784802  7564 net.cpp:66] Creating Layer drop7
I0909 16:16:53.784807  7564 net.cpp:329] drop7 <- fc7
I0909 16:16:53.784814  7564 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:16:53.784824  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.784831  7564 net.cpp:125] drop7 needs backward computation.
I0909 16:16:53.784839  7564 net.cpp:66] Creating Layer fc8
I0909 16:16:53.784844  7564 net.cpp:329] fc8 <- fc7
I0909 16:16:53.784853  7564 net.cpp:290] fc8 -> fc8
I0909 16:16:53.792613  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.792625  7564 net.cpp:125] fc8 needs backward computation.
I0909 16:16:53.792632  7564 net.cpp:66] Creating Layer relu8
I0909 16:16:53.792637  7564 net.cpp:329] relu8 <- fc8
I0909 16:16:53.792645  7564 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:16:53.792652  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.792657  7564 net.cpp:125] relu8 needs backward computation.
I0909 16:16:53.792665  7564 net.cpp:66] Creating Layer drop8
I0909 16:16:53.792670  7564 net.cpp:329] drop8 <- fc8
I0909 16:16:53.792675  7564 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:16:53.792681  7564 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:16:53.792697  7564 net.cpp:125] drop8 needs backward computation.
I0909 16:16:53.792706  7564 net.cpp:66] Creating Layer fc9
I0909 16:16:53.792712  7564 net.cpp:329] fc9 <- fc8
I0909 16:16:53.792718  7564 net.cpp:290] fc9 -> fc9
I0909 16:16:53.793081  7564 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:16:53.793092  7564 net.cpp:125] fc9 needs backward computation.
I0909 16:16:53.793100  7564 net.cpp:66] Creating Layer fc10
I0909 16:16:53.793105  7564 net.cpp:329] fc10 <- fc9
I0909 16:16:53.793113  7564 net.cpp:290] fc10 -> fc10
I0909 16:16:53.793125  7564 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:16:53.793133  7564 net.cpp:125] fc10 needs backward computation.
I0909 16:16:53.793139  7564 net.cpp:66] Creating Layer prob
I0909 16:16:53.793144  7564 net.cpp:329] prob <- fc10
I0909 16:16:53.793151  7564 net.cpp:290] prob -> prob
I0909 16:16:53.793161  7564 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:16:53.793166  7564 net.cpp:125] prob needs backward computation.
I0909 16:16:53.793171  7564 net.cpp:156] This network produces output prob
I0909 16:16:53.793184  7564 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:16:53.793191  7564 net.cpp:167] Network initialization done.
I0909 16:16:53.793196  7564 net.cpp:168] Memory required for data: 6183480
Classifying 11 inputs.
Done in 6.83 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:17:01.859683  7568 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:17:01.859818  7568 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:17:01.859827  7568 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:17:01.859971  7568 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:17:01.860033  7568 net.cpp:292] Input 0 -> data
I0909 16:17:01.860057  7568 net.cpp:66] Creating Layer conv1
I0909 16:17:01.860064  7568 net.cpp:329] conv1 <- data
I0909 16:17:01.860071  7568 net.cpp:290] conv1 -> conv1
I0909 16:17:01.861395  7568 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:01.861413  7568 net.cpp:125] conv1 needs backward computation.
I0909 16:17:01.861421  7568 net.cpp:66] Creating Layer relu1
I0909 16:17:01.861428  7568 net.cpp:329] relu1 <- conv1
I0909 16:17:01.861434  7568 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:17:01.861443  7568 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:01.861449  7568 net.cpp:125] relu1 needs backward computation.
I0909 16:17:01.861454  7568 net.cpp:66] Creating Layer pool1
I0909 16:17:01.861459  7568 net.cpp:329] pool1 <- conv1
I0909 16:17:01.861466  7568 net.cpp:290] pool1 -> pool1
I0909 16:17:01.861476  7568 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:01.861482  7568 net.cpp:125] pool1 needs backward computation.
I0909 16:17:01.861488  7568 net.cpp:66] Creating Layer norm1
I0909 16:17:01.861495  7568 net.cpp:329] norm1 <- pool1
I0909 16:17:01.861609  7568 net.cpp:290] norm1 -> norm1
I0909 16:17:01.861623  7568 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:01.861627  7568 net.cpp:125] norm1 needs backward computation.
I0909 16:17:01.861635  7568 net.cpp:66] Creating Layer conv2
I0909 16:17:01.861641  7568 net.cpp:329] conv2 <- norm1
I0909 16:17:01.861651  7568 net.cpp:290] conv2 -> conv2
I0909 16:17:01.870575  7568 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:01.870590  7568 net.cpp:125] conv2 needs backward computation.
I0909 16:17:01.870597  7568 net.cpp:66] Creating Layer relu2
I0909 16:17:01.870602  7568 net.cpp:329] relu2 <- conv2
I0909 16:17:01.870609  7568 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:17:01.870615  7568 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:01.870621  7568 net.cpp:125] relu2 needs backward computation.
I0909 16:17:01.870627  7568 net.cpp:66] Creating Layer pool2
I0909 16:17:01.870632  7568 net.cpp:329] pool2 <- conv2
I0909 16:17:01.870640  7568 net.cpp:290] pool2 -> pool2
I0909 16:17:01.870646  7568 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:17:01.870652  7568 net.cpp:125] pool2 needs backward computation.
I0909 16:17:01.870658  7568 net.cpp:66] Creating Layer fc7
I0909 16:17:01.870664  7568 net.cpp:329] fc7 <- pool2
I0909 16:17:01.870673  7568 net.cpp:290] fc7 -> fc7
I0909 16:17:02.505875  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.505918  7568 net.cpp:125] fc7 needs backward computation.
I0909 16:17:02.505931  7568 net.cpp:66] Creating Layer relu7
I0909 16:17:02.505940  7568 net.cpp:329] relu7 <- fc7
I0909 16:17:02.505949  7568 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:17:02.505957  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.505964  7568 net.cpp:125] relu7 needs backward computation.
I0909 16:17:02.505971  7568 net.cpp:66] Creating Layer drop7
I0909 16:17:02.505976  7568 net.cpp:329] drop7 <- fc7
I0909 16:17:02.505982  7568 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:17:02.506003  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.506009  7568 net.cpp:125] drop7 needs backward computation.
I0909 16:17:02.506018  7568 net.cpp:66] Creating Layer fc8
I0909 16:17:02.506023  7568 net.cpp:329] fc8 <- fc7
I0909 16:17:02.506032  7568 net.cpp:290] fc8 -> fc8
I0909 16:17:02.513823  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.513835  7568 net.cpp:125] fc8 needs backward computation.
I0909 16:17:02.513842  7568 net.cpp:66] Creating Layer relu8
I0909 16:17:02.513849  7568 net.cpp:329] relu8 <- fc8
I0909 16:17:02.513856  7568 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:17:02.513864  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.513869  7568 net.cpp:125] relu8 needs backward computation.
I0909 16:17:02.513875  7568 net.cpp:66] Creating Layer drop8
I0909 16:17:02.513880  7568 net.cpp:329] drop8 <- fc8
I0909 16:17:02.513886  7568 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:17:02.513893  7568 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:02.513898  7568 net.cpp:125] drop8 needs backward computation.
I0909 16:17:02.513907  7568 net.cpp:66] Creating Layer fc9
I0909 16:17:02.513913  7568 net.cpp:329] fc9 <- fc8
I0909 16:17:02.513921  7568 net.cpp:290] fc9 -> fc9
I0909 16:17:02.514297  7568 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:17:02.514308  7568 net.cpp:125] fc9 needs backward computation.
I0909 16:17:02.514317  7568 net.cpp:66] Creating Layer fc10
I0909 16:17:02.514322  7568 net.cpp:329] fc10 <- fc9
I0909 16:17:02.514330  7568 net.cpp:290] fc10 -> fc10
I0909 16:17:02.514343  7568 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:02.514349  7568 net.cpp:125] fc10 needs backward computation.
I0909 16:17:02.514356  7568 net.cpp:66] Creating Layer prob
I0909 16:17:02.514361  7568 net.cpp:329] prob <- fc10
I0909 16:17:02.514376  7568 net.cpp:290] prob -> prob
I0909 16:17:02.514385  7568 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:02.514391  7568 net.cpp:125] prob needs backward computation.
I0909 16:17:02.514396  7568 net.cpp:156] This network produces output prob
I0909 16:17:02.514408  7568 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:17:02.514416  7568 net.cpp:167] Network initialization done.
I0909 16:17:02.514421  7568 net.cpp:168] Memory required for data: 6183480
Classifying 38 inputs.
Done in 24.47 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:17:29.099306  7661 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:17:29.099445  7661 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:17:29.099454  7661 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:17:29.099601  7661 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:17:29.099666  7661 net.cpp:292] Input 0 -> data
I0909 16:17:29.099692  7661 net.cpp:66] Creating Layer conv1
I0909 16:17:29.099699  7661 net.cpp:329] conv1 <- data
I0909 16:17:29.099707  7661 net.cpp:290] conv1 -> conv1
I0909 16:17:29.101070  7661 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:29.101089  7661 net.cpp:125] conv1 needs backward computation.
I0909 16:17:29.101099  7661 net.cpp:66] Creating Layer relu1
I0909 16:17:29.101104  7661 net.cpp:329] relu1 <- conv1
I0909 16:17:29.101111  7661 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:17:29.101120  7661 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:29.101126  7661 net.cpp:125] relu1 needs backward computation.
I0909 16:17:29.101133  7661 net.cpp:66] Creating Layer pool1
I0909 16:17:29.101138  7661 net.cpp:329] pool1 <- conv1
I0909 16:17:29.101145  7661 net.cpp:290] pool1 -> pool1
I0909 16:17:29.101156  7661 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:29.101162  7661 net.cpp:125] pool1 needs backward computation.
I0909 16:17:29.101169  7661 net.cpp:66] Creating Layer norm1
I0909 16:17:29.101174  7661 net.cpp:329] norm1 <- pool1
I0909 16:17:29.101181  7661 net.cpp:290] norm1 -> norm1
I0909 16:17:29.101191  7661 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:29.101197  7661 net.cpp:125] norm1 needs backward computation.
I0909 16:17:29.101204  7661 net.cpp:66] Creating Layer conv2
I0909 16:17:29.101210  7661 net.cpp:329] conv2 <- norm1
I0909 16:17:29.101217  7661 net.cpp:290] conv2 -> conv2
I0909 16:17:29.110368  7661 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:29.110381  7661 net.cpp:125] conv2 needs backward computation.
I0909 16:17:29.110388  7661 net.cpp:66] Creating Layer relu2
I0909 16:17:29.110394  7661 net.cpp:329] relu2 <- conv2
I0909 16:17:29.110401  7661 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:17:29.110409  7661 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:29.110414  7661 net.cpp:125] relu2 needs backward computation.
I0909 16:17:29.110420  7661 net.cpp:66] Creating Layer pool2
I0909 16:17:29.110430  7661 net.cpp:329] pool2 <- conv2
I0909 16:17:29.110437  7661 net.cpp:290] pool2 -> pool2
I0909 16:17:29.110446  7661 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:17:29.110451  7661 net.cpp:125] pool2 needs backward computation.
I0909 16:17:29.110458  7661 net.cpp:66] Creating Layer fc7
I0909 16:17:29.110465  7661 net.cpp:329] fc7 <- pool2
I0909 16:17:29.110472  7661 net.cpp:290] fc7 -> fc7
I0909 16:17:29.748741  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.748787  7661 net.cpp:125] fc7 needs backward computation.
I0909 16:17:29.748800  7661 net.cpp:66] Creating Layer relu7
I0909 16:17:29.748807  7661 net.cpp:329] relu7 <- fc7
I0909 16:17:29.748816  7661 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:17:29.748826  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.748831  7661 net.cpp:125] relu7 needs backward computation.
I0909 16:17:29.748837  7661 net.cpp:66] Creating Layer drop7
I0909 16:17:29.748842  7661 net.cpp:329] drop7 <- fc7
I0909 16:17:29.748849  7661 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:17:29.748860  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.748867  7661 net.cpp:125] drop7 needs backward computation.
I0909 16:17:29.748873  7661 net.cpp:66] Creating Layer fc8
I0909 16:17:29.748879  7661 net.cpp:329] fc8 <- fc7
I0909 16:17:29.748888  7661 net.cpp:290] fc8 -> fc8
I0909 16:17:29.756592  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.756603  7661 net.cpp:125] fc8 needs backward computation.
I0909 16:17:29.756610  7661 net.cpp:66] Creating Layer relu8
I0909 16:17:29.756615  7661 net.cpp:329] relu8 <- fc8
I0909 16:17:29.756623  7661 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:17:29.756630  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.756636  7661 net.cpp:125] relu8 needs backward computation.
I0909 16:17:29.756642  7661 net.cpp:66] Creating Layer drop8
I0909 16:17:29.756647  7661 net.cpp:329] drop8 <- fc8
I0909 16:17:29.756654  7661 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:17:29.756660  7661 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:29.756666  7661 net.cpp:125] drop8 needs backward computation.
I0909 16:17:29.756675  7661 net.cpp:66] Creating Layer fc9
I0909 16:17:29.756680  7661 net.cpp:329] fc9 <- fc8
I0909 16:17:29.756687  7661 net.cpp:290] fc9 -> fc9
I0909 16:17:29.757069  7661 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:17:29.757081  7661 net.cpp:125] fc9 needs backward computation.
I0909 16:17:29.757089  7661 net.cpp:66] Creating Layer fc10
I0909 16:17:29.757094  7661 net.cpp:329] fc10 <- fc9
I0909 16:17:29.757103  7661 net.cpp:290] fc10 -> fc10
I0909 16:17:29.757114  7661 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:29.757122  7661 net.cpp:125] fc10 needs backward computation.
I0909 16:17:29.757128  7661 net.cpp:66] Creating Layer prob
I0909 16:17:29.757133  7661 net.cpp:329] prob <- fc10
I0909 16:17:29.757141  7661 net.cpp:290] prob -> prob
I0909 16:17:29.757150  7661 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:29.757156  7661 net.cpp:125] prob needs backward computation.
I0909 16:17:29.757161  7661 net.cpp:156] This network produces output prob
I0909 16:17:29.757174  7661 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:17:29.757181  7661 net.cpp:167] Network initialization done.
I0909 16:17:29.757186  7661 net.cpp:168] Memory required for data: 6183480
Classifying 19 inputs.
Done in 13.47 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:17:44.305480  7666 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:17:44.305656  7666 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:17:44.305665  7666 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:17:44.305809  7666 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:17:44.305869  7666 net.cpp:292] Input 0 -> data
I0909 16:17:44.305894  7666 net.cpp:66] Creating Layer conv1
I0909 16:17:44.305902  7666 net.cpp:329] conv1 <- data
I0909 16:17:44.305909  7666 net.cpp:290] conv1 -> conv1
I0909 16:17:44.307248  7666 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:44.307266  7666 net.cpp:125] conv1 needs backward computation.
I0909 16:17:44.307276  7666 net.cpp:66] Creating Layer relu1
I0909 16:17:44.307281  7666 net.cpp:329] relu1 <- conv1
I0909 16:17:44.307287  7666 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:17:44.307296  7666 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:17:44.307302  7666 net.cpp:125] relu1 needs backward computation.
I0909 16:17:44.307308  7666 net.cpp:66] Creating Layer pool1
I0909 16:17:44.307314  7666 net.cpp:329] pool1 <- conv1
I0909 16:17:44.307322  7666 net.cpp:290] pool1 -> pool1
I0909 16:17:44.307332  7666 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:44.307337  7666 net.cpp:125] pool1 needs backward computation.
I0909 16:17:44.307349  7666 net.cpp:66] Creating Layer norm1
I0909 16:17:44.307355  7666 net.cpp:329] norm1 <- pool1
I0909 16:17:44.307361  7666 net.cpp:290] norm1 -> norm1
I0909 16:17:44.307371  7666 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:17:44.307378  7666 net.cpp:125] norm1 needs backward computation.
I0909 16:17:44.307384  7666 net.cpp:66] Creating Layer conv2
I0909 16:17:44.307390  7666 net.cpp:329] conv2 <- norm1
I0909 16:17:44.307397  7666 net.cpp:290] conv2 -> conv2
I0909 16:17:44.316308  7666 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:44.316323  7666 net.cpp:125] conv2 needs backward computation.
I0909 16:17:44.316330  7666 net.cpp:66] Creating Layer relu2
I0909 16:17:44.316335  7666 net.cpp:329] relu2 <- conv2
I0909 16:17:44.316342  7666 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:17:44.316349  7666 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:17:44.316354  7666 net.cpp:125] relu2 needs backward computation.
I0909 16:17:44.316360  7666 net.cpp:66] Creating Layer pool2
I0909 16:17:44.316365  7666 net.cpp:329] pool2 <- conv2
I0909 16:17:44.316372  7666 net.cpp:290] pool2 -> pool2
I0909 16:17:44.316380  7666 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:17:44.316385  7666 net.cpp:125] pool2 needs backward computation.
I0909 16:17:44.316395  7666 net.cpp:66] Creating Layer fc7
I0909 16:17:44.316400  7666 net.cpp:329] fc7 <- pool2
I0909 16:17:44.316406  7666 net.cpp:290] fc7 -> fc7
I0909 16:17:44.958096  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.958139  7666 net.cpp:125] fc7 needs backward computation.
I0909 16:17:44.958153  7666 net.cpp:66] Creating Layer relu7
I0909 16:17:44.958159  7666 net.cpp:329] relu7 <- fc7
I0909 16:17:44.958168  7666 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:17:44.958176  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.958183  7666 net.cpp:125] relu7 needs backward computation.
I0909 16:17:44.958189  7666 net.cpp:66] Creating Layer drop7
I0909 16:17:44.958195  7666 net.cpp:329] drop7 <- fc7
I0909 16:17:44.958201  7666 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:17:44.958212  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.958219  7666 net.cpp:125] drop7 needs backward computation.
I0909 16:17:44.958228  7666 net.cpp:66] Creating Layer fc8
I0909 16:17:44.958233  7666 net.cpp:329] fc8 <- fc7
I0909 16:17:44.958241  7666 net.cpp:290] fc8 -> fc8
I0909 16:17:44.965803  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.965816  7666 net.cpp:125] fc8 needs backward computation.
I0909 16:17:44.965823  7666 net.cpp:66] Creating Layer relu8
I0909 16:17:44.965829  7666 net.cpp:329] relu8 <- fc8
I0909 16:17:44.965836  7666 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:17:44.965842  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.965847  7666 net.cpp:125] relu8 needs backward computation.
I0909 16:17:44.965854  7666 net.cpp:66] Creating Layer drop8
I0909 16:17:44.965859  7666 net.cpp:329] drop8 <- fc8
I0909 16:17:44.965867  7666 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:17:44.965873  7666 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:17:44.965879  7666 net.cpp:125] drop8 needs backward computation.
I0909 16:17:44.965888  7666 net.cpp:66] Creating Layer fc9
I0909 16:17:44.965893  7666 net.cpp:329] fc9 <- fc8
I0909 16:17:44.965899  7666 net.cpp:290] fc9 -> fc9
I0909 16:17:44.966262  7666 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:17:44.966274  7666 net.cpp:125] fc9 needs backward computation.
I0909 16:17:44.966280  7666 net.cpp:66] Creating Layer fc10
I0909 16:17:44.966286  7666 net.cpp:329] fc10 <- fc9
I0909 16:17:44.966295  7666 net.cpp:290] fc10 -> fc10
I0909 16:17:44.966306  7666 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:44.966313  7666 net.cpp:125] fc10 needs backward computation.
I0909 16:17:44.966320  7666 net.cpp:66] Creating Layer prob
I0909 16:17:44.966325  7666 net.cpp:329] prob <- fc10
I0909 16:17:44.966332  7666 net.cpp:290] prob -> prob
I0909 16:17:44.966342  7666 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:17:44.966347  7666 net.cpp:125] prob needs backward computation.
I0909 16:17:44.966362  7666 net.cpp:156] This network produces output prob
I0909 16:17:44.966374  7666 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:17:44.966382  7666 net.cpp:167] Network initialization done.
I0909 16:17:44.966388  7666 net.cpp:168] Memory required for data: 6183480
Classifying 62 inputs.
Done in 41.32 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:18:30.439923  7717 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:18:30.440058  7717 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:18:30.440068  7717 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:18:30.440209  7717 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:18:30.440270  7717 net.cpp:292] Input 0 -> data
I0909 16:18:30.440296  7717 net.cpp:66] Creating Layer conv1
I0909 16:18:30.440302  7717 net.cpp:329] conv1 <- data
I0909 16:18:30.440310  7717 net.cpp:290] conv1 -> conv1
I0909 16:18:30.441732  7717 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:18:30.441751  7717 net.cpp:125] conv1 needs backward computation.
I0909 16:18:30.441761  7717 net.cpp:66] Creating Layer relu1
I0909 16:18:30.441766  7717 net.cpp:329] relu1 <- conv1
I0909 16:18:30.441772  7717 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:18:30.441781  7717 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:18:30.441787  7717 net.cpp:125] relu1 needs backward computation.
I0909 16:18:30.441793  7717 net.cpp:66] Creating Layer pool1
I0909 16:18:30.441798  7717 net.cpp:329] pool1 <- conv1
I0909 16:18:30.441805  7717 net.cpp:290] pool1 -> pool1
I0909 16:18:30.441815  7717 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:18:30.441822  7717 net.cpp:125] pool1 needs backward computation.
I0909 16:18:30.441828  7717 net.cpp:66] Creating Layer norm1
I0909 16:18:30.441833  7717 net.cpp:329] norm1 <- pool1
I0909 16:18:30.441839  7717 net.cpp:290] norm1 -> norm1
I0909 16:18:30.441849  7717 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:18:30.441854  7717 net.cpp:125] norm1 needs backward computation.
I0909 16:18:30.441861  7717 net.cpp:66] Creating Layer conv2
I0909 16:18:30.441867  7717 net.cpp:329] conv2 <- norm1
I0909 16:18:30.441874  7717 net.cpp:290] conv2 -> conv2
I0909 16:18:30.450901  7717 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:18:30.450916  7717 net.cpp:125] conv2 needs backward computation.
I0909 16:18:30.450922  7717 net.cpp:66] Creating Layer relu2
I0909 16:18:30.450928  7717 net.cpp:329] relu2 <- conv2
I0909 16:18:30.450934  7717 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:18:30.450942  7717 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:18:30.450947  7717 net.cpp:125] relu2 needs backward computation.
I0909 16:18:30.450953  7717 net.cpp:66] Creating Layer pool2
I0909 16:18:30.450958  7717 net.cpp:329] pool2 <- conv2
I0909 16:18:30.450964  7717 net.cpp:290] pool2 -> pool2
I0909 16:18:30.450973  7717 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:18:30.450978  7717 net.cpp:125] pool2 needs backward computation.
I0909 16:18:30.450985  7717 net.cpp:66] Creating Layer fc7
I0909 16:18:30.450990  7717 net.cpp:329] fc7 <- pool2
I0909 16:18:30.450999  7717 net.cpp:290] fc7 -> fc7
I0909 16:18:31.088901  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.088943  7717 net.cpp:125] fc7 needs backward computation.
I0909 16:18:31.088955  7717 net.cpp:66] Creating Layer relu7
I0909 16:18:31.088964  7717 net.cpp:329] relu7 <- fc7
I0909 16:18:31.088973  7717 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:18:31.088981  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.088987  7717 net.cpp:125] relu7 needs backward computation.
I0909 16:18:31.088994  7717 net.cpp:66] Creating Layer drop7
I0909 16:18:31.088999  7717 net.cpp:329] drop7 <- fc7
I0909 16:18:31.089006  7717 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:18:31.089016  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.089021  7717 net.cpp:125] drop7 needs backward computation.
I0909 16:18:31.089030  7717 net.cpp:66] Creating Layer fc8
I0909 16:18:31.089035  7717 net.cpp:329] fc8 <- fc7
I0909 16:18:31.089046  7717 net.cpp:290] fc8 -> fc8
I0909 16:18:31.096642  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.096654  7717 net.cpp:125] fc8 needs backward computation.
I0909 16:18:31.096662  7717 net.cpp:66] Creating Layer relu8
I0909 16:18:31.096667  7717 net.cpp:329] relu8 <- fc8
I0909 16:18:31.096674  7717 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:18:31.096680  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.096686  7717 net.cpp:125] relu8 needs backward computation.
I0909 16:18:31.096693  7717 net.cpp:66] Creating Layer drop8
I0909 16:18:31.096698  7717 net.cpp:329] drop8 <- fc8
I0909 16:18:31.096704  7717 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:18:31.096720  7717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:18:31.096726  7717 net.cpp:125] drop8 needs backward computation.
I0909 16:18:31.096735  7717 net.cpp:66] Creating Layer fc9
I0909 16:18:31.096740  7717 net.cpp:329] fc9 <- fc8
I0909 16:18:31.096747  7717 net.cpp:290] fc9 -> fc9
I0909 16:18:31.097110  7717 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:18:31.097121  7717 net.cpp:125] fc9 needs backward computation.
I0909 16:18:31.097128  7717 net.cpp:66] Creating Layer fc10
I0909 16:18:31.097133  7717 net.cpp:329] fc10 <- fc9
I0909 16:18:31.097141  7717 net.cpp:290] fc10 -> fc10
I0909 16:18:31.097153  7717 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:18:31.097161  7717 net.cpp:125] fc10 needs backward computation.
I0909 16:18:31.097167  7717 net.cpp:66] Creating Layer prob
I0909 16:18:31.097172  7717 net.cpp:329] prob <- fc10
I0909 16:18:31.097179  7717 net.cpp:290] prob -> prob
I0909 16:18:31.097188  7717 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:18:31.097194  7717 net.cpp:125] prob needs backward computation.
I0909 16:18:31.097199  7717 net.cpp:156] This network produces output prob
I0909 16:18:31.097211  7717 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:18:31.097219  7717 net.cpp:167] Network initialization done.
I0909 16:18:31.097224  7717 net.cpp:168] Memory required for data: 6183480
Classifying 473 inputs.
Done in 327.10 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:24:42.135778  7741 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:24:42.135920  7741 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:24:42.135929  7741 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:24:42.136080  7741 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:24:42.136145  7741 net.cpp:292] Input 0 -> data
I0909 16:24:42.136171  7741 net.cpp:66] Creating Layer conv1
I0909 16:24:42.136178  7741 net.cpp:329] conv1 <- data
I0909 16:24:42.136186  7741 net.cpp:290] conv1 -> conv1
I0909 16:24:42.144690  7741 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:24:42.144714  7741 net.cpp:125] conv1 needs backward computation.
I0909 16:24:42.144724  7741 net.cpp:66] Creating Layer relu1
I0909 16:24:42.144731  7741 net.cpp:329] relu1 <- conv1
I0909 16:24:42.144738  7741 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:24:42.144747  7741 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:24:42.144753  7741 net.cpp:125] relu1 needs backward computation.
I0909 16:24:42.144760  7741 net.cpp:66] Creating Layer pool1
I0909 16:24:42.144767  7741 net.cpp:329] pool1 <- conv1
I0909 16:24:42.144773  7741 net.cpp:290] pool1 -> pool1
I0909 16:24:42.144784  7741 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:24:42.144790  7741 net.cpp:125] pool1 needs backward computation.
I0909 16:24:42.144798  7741 net.cpp:66] Creating Layer norm1
I0909 16:24:42.144803  7741 net.cpp:329] norm1 <- pool1
I0909 16:24:42.144809  7741 net.cpp:290] norm1 -> norm1
I0909 16:24:42.144819  7741 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:24:42.144825  7741 net.cpp:125] norm1 needs backward computation.
I0909 16:24:42.144834  7741 net.cpp:66] Creating Layer conv2
I0909 16:24:42.144840  7741 net.cpp:329] conv2 <- norm1
I0909 16:24:42.144846  7741 net.cpp:290] conv2 -> conv2
I0909 16:24:42.154054  7741 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:24:42.154072  7741 net.cpp:125] conv2 needs backward computation.
I0909 16:24:42.154078  7741 net.cpp:66] Creating Layer relu2
I0909 16:24:42.154084  7741 net.cpp:329] relu2 <- conv2
I0909 16:24:42.154091  7741 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:24:42.154099  7741 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:24:42.154105  7741 net.cpp:125] relu2 needs backward computation.
I0909 16:24:42.154111  7741 net.cpp:66] Creating Layer pool2
I0909 16:24:42.154117  7741 net.cpp:329] pool2 <- conv2
I0909 16:24:42.154124  7741 net.cpp:290] pool2 -> pool2
I0909 16:24:42.154132  7741 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:24:42.154139  7741 net.cpp:125] pool2 needs backward computation.
I0909 16:24:42.154147  7741 net.cpp:66] Creating Layer fc7
I0909 16:24:42.154153  7741 net.cpp:329] fc7 <- pool2
I0909 16:24:42.154160  7741 net.cpp:290] fc7 -> fc7
I0909 16:24:42.794446  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.794492  7741 net.cpp:125] fc7 needs backward computation.
I0909 16:24:42.794505  7741 net.cpp:66] Creating Layer relu7
I0909 16:24:42.794513  7741 net.cpp:329] relu7 <- fc7
I0909 16:24:42.794522  7741 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:24:42.794531  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.794538  7741 net.cpp:125] relu7 needs backward computation.
I0909 16:24:42.794545  7741 net.cpp:66] Creating Layer drop7
I0909 16:24:42.794563  7741 net.cpp:329] drop7 <- fc7
I0909 16:24:42.794569  7741 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:24:42.794581  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.794587  7741 net.cpp:125] drop7 needs backward computation.
I0909 16:24:42.794598  7741 net.cpp:66] Creating Layer fc8
I0909 16:24:42.794605  7741 net.cpp:329] fc8 <- fc7
I0909 16:24:42.794612  7741 net.cpp:290] fc8 -> fc8
I0909 16:24:42.802404  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.802417  7741 net.cpp:125] fc8 needs backward computation.
I0909 16:24:42.802425  7741 net.cpp:66] Creating Layer relu8
I0909 16:24:42.802431  7741 net.cpp:329] relu8 <- fc8
I0909 16:24:42.802438  7741 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:24:42.802445  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.802451  7741 net.cpp:125] relu8 needs backward computation.
I0909 16:24:42.802459  7741 net.cpp:66] Creating Layer drop8
I0909 16:24:42.802464  7741 net.cpp:329] drop8 <- fc8
I0909 16:24:42.802471  7741 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:24:42.802479  7741 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:24:42.802484  7741 net.cpp:125] drop8 needs backward computation.
I0909 16:24:42.802492  7741 net.cpp:66] Creating Layer fc9
I0909 16:24:42.802497  7741 net.cpp:329] fc9 <- fc8
I0909 16:24:42.802505  7741 net.cpp:290] fc9 -> fc9
I0909 16:24:42.802880  7741 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:24:42.802891  7741 net.cpp:125] fc9 needs backward computation.
I0909 16:24:42.802898  7741 net.cpp:66] Creating Layer fc10
I0909 16:24:42.802904  7741 net.cpp:329] fc10 <- fc9
I0909 16:24:42.802913  7741 net.cpp:290] fc10 -> fc10
I0909 16:24:42.802925  7741 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:24:42.802933  7741 net.cpp:125] fc10 needs backward computation.
I0909 16:24:42.802939  7741 net.cpp:66] Creating Layer prob
I0909 16:24:42.802945  7741 net.cpp:329] prob <- fc10
I0909 16:24:42.802953  7741 net.cpp:290] prob -> prob
I0909 16:24:42.802963  7741 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:24:42.802968  7741 net.cpp:125] prob needs backward computation.
I0909 16:24:42.802973  7741 net.cpp:156] This network produces output prob
I0909 16:24:42.802986  7741 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:24:42.802995  7741 net.cpp:167] Network initialization done.
I0909 16:24:42.803000  7741 net.cpp:168] Memory required for data: 6183480
Classifying 89 inputs.
Done in 53.98 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:25:40.351647  7745 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:25:40.351786  7745 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:25:40.351795  7745 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:25:40.351943  7745 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:25:40.352007  7745 net.cpp:292] Input 0 -> data
I0909 16:25:40.352035  7745 net.cpp:66] Creating Layer conv1
I0909 16:25:40.352041  7745 net.cpp:329] conv1 <- data
I0909 16:25:40.352049  7745 net.cpp:290] conv1 -> conv1
I0909 16:25:40.353413  7745 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:25:40.353431  7745 net.cpp:125] conv1 needs backward computation.
I0909 16:25:40.353441  7745 net.cpp:66] Creating Layer relu1
I0909 16:25:40.353447  7745 net.cpp:329] relu1 <- conv1
I0909 16:25:40.353453  7745 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:25:40.353462  7745 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:25:40.353468  7745 net.cpp:125] relu1 needs backward computation.
I0909 16:25:40.353474  7745 net.cpp:66] Creating Layer pool1
I0909 16:25:40.353481  7745 net.cpp:329] pool1 <- conv1
I0909 16:25:40.353487  7745 net.cpp:290] pool1 -> pool1
I0909 16:25:40.353497  7745 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:25:40.353503  7745 net.cpp:125] pool1 needs backward computation.
I0909 16:25:40.353523  7745 net.cpp:66] Creating Layer norm1
I0909 16:25:40.353531  7745 net.cpp:329] norm1 <- pool1
I0909 16:25:40.353539  7745 net.cpp:290] norm1 -> norm1
I0909 16:25:40.353549  7745 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:25:40.353555  7745 net.cpp:125] norm1 needs backward computation.
I0909 16:25:40.353562  7745 net.cpp:66] Creating Layer conv2
I0909 16:25:40.353569  7745 net.cpp:329] conv2 <- norm1
I0909 16:25:40.353579  7745 net.cpp:290] conv2 -> conv2
I0909 16:25:40.362721  7745 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:25:40.362736  7745 net.cpp:125] conv2 needs backward computation.
I0909 16:25:40.362743  7745 net.cpp:66] Creating Layer relu2
I0909 16:25:40.362748  7745 net.cpp:329] relu2 <- conv2
I0909 16:25:40.362756  7745 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:25:40.362762  7745 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:25:40.362768  7745 net.cpp:125] relu2 needs backward computation.
I0909 16:25:40.362781  7745 net.cpp:66] Creating Layer pool2
I0909 16:25:40.362786  7745 net.cpp:329] pool2 <- conv2
I0909 16:25:40.362792  7745 net.cpp:290] pool2 -> pool2
I0909 16:25:40.362800  7745 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:25:40.362807  7745 net.cpp:125] pool2 needs backward computation.
I0909 16:25:40.362813  7745 net.cpp:66] Creating Layer fc7
I0909 16:25:40.362819  7745 net.cpp:329] fc7 <- pool2
I0909 16:25:40.362828  7745 net.cpp:290] fc7 -> fc7
I0909 16:25:41.004264  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.004312  7745 net.cpp:125] fc7 needs backward computation.
I0909 16:25:41.004324  7745 net.cpp:66] Creating Layer relu7
I0909 16:25:41.004333  7745 net.cpp:329] relu7 <- fc7
I0909 16:25:41.004341  7745 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:25:41.004351  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.004358  7745 net.cpp:125] relu7 needs backward computation.
I0909 16:25:41.004364  7745 net.cpp:66] Creating Layer drop7
I0909 16:25:41.004369  7745 net.cpp:329] drop7 <- fc7
I0909 16:25:41.004376  7745 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:25:41.004387  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.004394  7745 net.cpp:125] drop7 needs backward computation.
I0909 16:25:41.004401  7745 net.cpp:66] Creating Layer fc8
I0909 16:25:41.004407  7745 net.cpp:329] fc8 <- fc7
I0909 16:25:41.004416  7745 net.cpp:290] fc8 -> fc8
I0909 16:25:41.012217  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.012230  7745 net.cpp:125] fc8 needs backward computation.
I0909 16:25:41.012238  7745 net.cpp:66] Creating Layer relu8
I0909 16:25:41.012243  7745 net.cpp:329] relu8 <- fc8
I0909 16:25:41.012251  7745 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:25:41.012259  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.012264  7745 net.cpp:125] relu8 needs backward computation.
I0909 16:25:41.012270  7745 net.cpp:66] Creating Layer drop8
I0909 16:25:41.012275  7745 net.cpp:329] drop8 <- fc8
I0909 16:25:41.012282  7745 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:25:41.012289  7745 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:25:41.012295  7745 net.cpp:125] drop8 needs backward computation.
I0909 16:25:41.012303  7745 net.cpp:66] Creating Layer fc9
I0909 16:25:41.012310  7745 net.cpp:329] fc9 <- fc8
I0909 16:25:41.012316  7745 net.cpp:290] fc9 -> fc9
I0909 16:25:41.012689  7745 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:25:41.012704  7745 net.cpp:125] fc9 needs backward computation.
I0909 16:25:41.012712  7745 net.cpp:66] Creating Layer fc10
I0909 16:25:41.012718  7745 net.cpp:329] fc10 <- fc9
I0909 16:25:41.012727  7745 net.cpp:290] fc10 -> fc10
I0909 16:25:41.012738  7745 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:25:41.012747  7745 net.cpp:125] fc10 needs backward computation.
I0909 16:25:41.012753  7745 net.cpp:66] Creating Layer prob
I0909 16:25:41.012758  7745 net.cpp:329] prob <- fc10
I0909 16:25:41.012766  7745 net.cpp:290] prob -> prob
I0909 16:25:41.012776  7745 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:25:41.012783  7745 net.cpp:125] prob needs backward computation.
I0909 16:25:41.012787  7745 net.cpp:156] This network produces output prob
I0909 16:25:41.012800  7745 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:25:41.012809  7745 net.cpp:167] Network initialization done.
I0909 16:25:41.012814  7745 net.cpp:168] Memory required for data: 6183480
Classifying 287 inputs.
Done in 171.11 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:28:38.670958  7833 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:28:38.671104  7833 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:28:38.671614  7833 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:28:38.671789  7833 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:28:38.671844  7833 net.cpp:292] Input 0 -> data
I0909 16:28:38.671877  7833 net.cpp:66] Creating Layer conv1
I0909 16:28:38.671885  7833 net.cpp:329] conv1 <- data
I0909 16:28:38.671893  7833 net.cpp:290] conv1 -> conv1
I0909 16:28:38.673328  7833 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:28:38.673347  7833 net.cpp:125] conv1 needs backward computation.
I0909 16:28:38.673357  7833 net.cpp:66] Creating Layer relu1
I0909 16:28:38.673362  7833 net.cpp:329] relu1 <- conv1
I0909 16:28:38.673369  7833 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:28:38.673378  7833 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:28:38.673383  7833 net.cpp:125] relu1 needs backward computation.
I0909 16:28:38.673390  7833 net.cpp:66] Creating Layer pool1
I0909 16:28:38.673396  7833 net.cpp:329] pool1 <- conv1
I0909 16:28:38.673403  7833 net.cpp:290] pool1 -> pool1
I0909 16:28:38.673414  7833 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:28:38.673424  7833 net.cpp:125] pool1 needs backward computation.
I0909 16:28:38.673431  7833 net.cpp:66] Creating Layer norm1
I0909 16:28:38.673437  7833 net.cpp:329] norm1 <- pool1
I0909 16:28:38.673444  7833 net.cpp:290] norm1 -> norm1
I0909 16:28:38.673454  7833 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:28:38.673460  7833 net.cpp:125] norm1 needs backward computation.
I0909 16:28:38.673467  7833 net.cpp:66] Creating Layer conv2
I0909 16:28:38.673472  7833 net.cpp:329] conv2 <- norm1
I0909 16:28:38.673480  7833 net.cpp:290] conv2 -> conv2
I0909 16:28:38.686823  7833 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:28:38.686844  7833 net.cpp:125] conv2 needs backward computation.
I0909 16:28:38.686852  7833 net.cpp:66] Creating Layer relu2
I0909 16:28:38.686858  7833 net.cpp:329] relu2 <- conv2
I0909 16:28:38.686866  7833 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:28:38.686872  7833 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:28:38.686878  7833 net.cpp:125] relu2 needs backward computation.
I0909 16:28:38.686884  7833 net.cpp:66] Creating Layer pool2
I0909 16:28:38.686890  7833 net.cpp:329] pool2 <- conv2
I0909 16:28:38.686897  7833 net.cpp:290] pool2 -> pool2
I0909 16:28:38.686904  7833 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:28:38.686910  7833 net.cpp:125] pool2 needs backward computation.
I0909 16:28:38.686919  7833 net.cpp:66] Creating Layer fc7
I0909 16:28:38.686924  7833 net.cpp:329] fc7 <- pool2
I0909 16:28:38.686931  7833 net.cpp:290] fc7 -> fc7
I0909 16:28:39.328845  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.328894  7833 net.cpp:125] fc7 needs backward computation.
I0909 16:28:39.328907  7833 net.cpp:66] Creating Layer relu7
I0909 16:28:39.328914  7833 net.cpp:329] relu7 <- fc7
I0909 16:28:39.328922  7833 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:28:39.328933  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.328938  7833 net.cpp:125] relu7 needs backward computation.
I0909 16:28:39.328945  7833 net.cpp:66] Creating Layer drop7
I0909 16:28:39.328951  7833 net.cpp:329] drop7 <- fc7
I0909 16:28:39.328958  7833 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:28:39.328969  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.328974  7833 net.cpp:125] drop7 needs backward computation.
I0909 16:28:39.328985  7833 net.cpp:66] Creating Layer fc8
I0909 16:28:39.328990  7833 net.cpp:329] fc8 <- fc7
I0909 16:28:39.328999  7833 net.cpp:290] fc8 -> fc8
I0909 16:28:39.336833  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.336846  7833 net.cpp:125] fc8 needs backward computation.
I0909 16:28:39.336855  7833 net.cpp:66] Creating Layer relu8
I0909 16:28:39.336861  7833 net.cpp:329] relu8 <- fc8
I0909 16:28:39.336868  7833 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:28:39.336874  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.336880  7833 net.cpp:125] relu8 needs backward computation.
I0909 16:28:39.336886  7833 net.cpp:66] Creating Layer drop8
I0909 16:28:39.336892  7833 net.cpp:329] drop8 <- fc8
I0909 16:28:39.336899  7833 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:28:39.336907  7833 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:28:39.336913  7833 net.cpp:125] drop8 needs backward computation.
I0909 16:28:39.336920  7833 net.cpp:66] Creating Layer fc9
I0909 16:28:39.336925  7833 net.cpp:329] fc9 <- fc8
I0909 16:28:39.336932  7833 net.cpp:290] fc9 -> fc9
I0909 16:28:39.337307  7833 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:28:39.337318  7833 net.cpp:125] fc9 needs backward computation.
I0909 16:28:39.337327  7833 net.cpp:66] Creating Layer fc10
I0909 16:28:39.337332  7833 net.cpp:329] fc10 <- fc9
I0909 16:28:39.337340  7833 net.cpp:290] fc10 -> fc10
I0909 16:28:39.337352  7833 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:28:39.337359  7833 net.cpp:125] fc10 needs backward computation.
I0909 16:28:39.337366  7833 net.cpp:66] Creating Layer prob
I0909 16:28:39.337371  7833 net.cpp:329] prob <- fc10
I0909 16:28:39.337379  7833 net.cpp:290] prob -> prob
I0909 16:28:39.337399  7833 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:28:39.337405  7833 net.cpp:125] prob needs backward computation.
I0909 16:28:39.337410  7833 net.cpp:156] This network produces output prob
I0909 16:28:39.337426  7833 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:28:39.337435  7833 net.cpp:167] Network initialization done.
I0909 16:28:39.337440  7833 net.cpp:168] Memory required for data: 6183480
Classifying 149 inputs.
Done in 94.13 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:30:18.277915  7905 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:30:18.278055  7905 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:30:18.278064  7905 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:30:18.278211  7905 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:30:18.278275  7905 net.cpp:292] Input 0 -> data
I0909 16:30:18.278301  7905 net.cpp:66] Creating Layer conv1
I0909 16:30:18.278307  7905 net.cpp:329] conv1 <- data
I0909 16:30:18.278316  7905 net.cpp:290] conv1 -> conv1
I0909 16:30:18.279678  7905 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:30:18.279696  7905 net.cpp:125] conv1 needs backward computation.
I0909 16:30:18.279706  7905 net.cpp:66] Creating Layer relu1
I0909 16:30:18.279711  7905 net.cpp:329] relu1 <- conv1
I0909 16:30:18.279718  7905 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:30:18.279726  7905 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:30:18.279732  7905 net.cpp:125] relu1 needs backward computation.
I0909 16:30:18.279739  7905 net.cpp:66] Creating Layer pool1
I0909 16:30:18.279744  7905 net.cpp:329] pool1 <- conv1
I0909 16:30:18.279752  7905 net.cpp:290] pool1 -> pool1
I0909 16:30:18.279762  7905 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:30:18.279767  7905 net.cpp:125] pool1 needs backward computation.
I0909 16:30:18.279774  7905 net.cpp:66] Creating Layer norm1
I0909 16:30:18.279779  7905 net.cpp:329] norm1 <- pool1
I0909 16:30:18.279786  7905 net.cpp:290] norm1 -> norm1
I0909 16:30:18.279796  7905 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:30:18.279801  7905 net.cpp:125] norm1 needs backward computation.
I0909 16:30:18.279809  7905 net.cpp:66] Creating Layer conv2
I0909 16:30:18.279814  7905 net.cpp:329] conv2 <- norm1
I0909 16:30:18.279821  7905 net.cpp:290] conv2 -> conv2
I0909 16:30:18.288946  7905 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:30:18.288961  7905 net.cpp:125] conv2 needs backward computation.
I0909 16:30:18.288969  7905 net.cpp:66] Creating Layer relu2
I0909 16:30:18.288975  7905 net.cpp:329] relu2 <- conv2
I0909 16:30:18.288980  7905 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:30:18.288987  7905 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:30:18.288993  7905 net.cpp:125] relu2 needs backward computation.
I0909 16:30:18.289000  7905 net.cpp:66] Creating Layer pool2
I0909 16:30:18.289005  7905 net.cpp:329] pool2 <- conv2
I0909 16:30:18.289011  7905 net.cpp:290] pool2 -> pool2
I0909 16:30:18.289019  7905 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:30:18.289026  7905 net.cpp:125] pool2 needs backward computation.
I0909 16:30:18.289032  7905 net.cpp:66] Creating Layer fc7
I0909 16:30:18.289038  7905 net.cpp:329] fc7 <- pool2
I0909 16:30:18.289047  7905 net.cpp:290] fc7 -> fc7
I0909 16:30:18.929643  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.929690  7905 net.cpp:125] fc7 needs backward computation.
I0909 16:30:18.929703  7905 net.cpp:66] Creating Layer relu7
I0909 16:30:18.929713  7905 net.cpp:329] relu7 <- fc7
I0909 16:30:18.929720  7905 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:30:18.929730  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.929736  7905 net.cpp:125] relu7 needs backward computation.
I0909 16:30:18.929744  7905 net.cpp:66] Creating Layer drop7
I0909 16:30:18.929749  7905 net.cpp:329] drop7 <- fc7
I0909 16:30:18.929754  7905 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:30:18.929766  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.929771  7905 net.cpp:125] drop7 needs backward computation.
I0909 16:30:18.929780  7905 net.cpp:66] Creating Layer fc8
I0909 16:30:18.929785  7905 net.cpp:329] fc8 <- fc7
I0909 16:30:18.929795  7905 net.cpp:290] fc8 -> fc8
I0909 16:30:18.937584  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.937597  7905 net.cpp:125] fc8 needs backward computation.
I0909 16:30:18.937603  7905 net.cpp:66] Creating Layer relu8
I0909 16:30:18.937609  7905 net.cpp:329] relu8 <- fc8
I0909 16:30:18.937616  7905 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:30:18.937624  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.937629  7905 net.cpp:125] relu8 needs backward computation.
I0909 16:30:18.937636  7905 net.cpp:66] Creating Layer drop8
I0909 16:30:18.937652  7905 net.cpp:329] drop8 <- fc8
I0909 16:30:18.937659  7905 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:30:18.937666  7905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:30:18.937671  7905 net.cpp:125] drop8 needs backward computation.
I0909 16:30:18.937680  7905 net.cpp:66] Creating Layer fc9
I0909 16:30:18.937686  7905 net.cpp:329] fc9 <- fc8
I0909 16:30:18.937693  7905 net.cpp:290] fc9 -> fc9
I0909 16:30:18.938067  7905 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:30:18.938078  7905 net.cpp:125] fc9 needs backward computation.
I0909 16:30:18.938087  7905 net.cpp:66] Creating Layer fc10
I0909 16:30:18.938092  7905 net.cpp:329] fc10 <- fc9
I0909 16:30:18.938102  7905 net.cpp:290] fc10 -> fc10
I0909 16:30:18.938112  7905 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:30:18.938120  7905 net.cpp:125] fc10 needs backward computation.
I0909 16:30:18.938127  7905 net.cpp:66] Creating Layer prob
I0909 16:30:18.938133  7905 net.cpp:329] prob <- fc10
I0909 16:30:18.938139  7905 net.cpp:290] prob -> prob
I0909 16:30:18.938149  7905 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:30:18.938154  7905 net.cpp:125] prob needs backward computation.
I0909 16:30:18.938159  7905 net.cpp:156] This network produces output prob
I0909 16:30:18.938172  7905 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:30:18.938180  7905 net.cpp:167] Network initialization done.
I0909 16:30:18.938185  7905 net.cpp:168] Memory required for data: 6183480
Classifying 379 inputs.
Done in 235.18 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:34:22.155083  7941 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:34:22.155220  7941 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:34:22.155230  7941 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:34:22.155375  7941 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:34:22.155434  7941 net.cpp:292] Input 0 -> data
I0909 16:34:22.155460  7941 net.cpp:66] Creating Layer conv1
I0909 16:34:22.155467  7941 net.cpp:329] conv1 <- data
I0909 16:34:22.155474  7941 net.cpp:290] conv1 -> conv1
I0909 16:34:22.156868  7941 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:34:22.156885  7941 net.cpp:125] conv1 needs backward computation.
I0909 16:34:22.156894  7941 net.cpp:66] Creating Layer relu1
I0909 16:34:22.156900  7941 net.cpp:329] relu1 <- conv1
I0909 16:34:22.156908  7941 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:34:22.156915  7941 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:34:22.156921  7941 net.cpp:125] relu1 needs backward computation.
I0909 16:34:22.156927  7941 net.cpp:66] Creating Layer pool1
I0909 16:34:22.156940  7941 net.cpp:329] pool1 <- conv1
I0909 16:34:22.156970  7941 net.cpp:290] pool1 -> pool1
I0909 16:34:22.156982  7941 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:34:22.156988  7941 net.cpp:125] pool1 needs backward computation.
I0909 16:34:22.156996  7941 net.cpp:66] Creating Layer norm1
I0909 16:34:22.157001  7941 net.cpp:329] norm1 <- pool1
I0909 16:34:22.157007  7941 net.cpp:290] norm1 -> norm1
I0909 16:34:22.157017  7941 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:34:22.157023  7941 net.cpp:125] norm1 needs backward computation.
I0909 16:34:22.157030  7941 net.cpp:66] Creating Layer conv2
I0909 16:34:22.157037  7941 net.cpp:329] conv2 <- norm1
I0909 16:34:22.157043  7941 net.cpp:290] conv2 -> conv2
I0909 16:34:22.166319  7941 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:34:22.166338  7941 net.cpp:125] conv2 needs backward computation.
I0909 16:34:22.166345  7941 net.cpp:66] Creating Layer relu2
I0909 16:34:22.166352  7941 net.cpp:329] relu2 <- conv2
I0909 16:34:22.166357  7941 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:34:22.166364  7941 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:34:22.166370  7941 net.cpp:125] relu2 needs backward computation.
I0909 16:34:22.166376  7941 net.cpp:66] Creating Layer pool2
I0909 16:34:22.166381  7941 net.cpp:329] pool2 <- conv2
I0909 16:34:22.166388  7941 net.cpp:290] pool2 -> pool2
I0909 16:34:22.166395  7941 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:34:22.166401  7941 net.cpp:125] pool2 needs backward computation.
I0909 16:34:22.166409  7941 net.cpp:66] Creating Layer fc7
I0909 16:34:22.166414  7941 net.cpp:329] fc7 <- pool2
I0909 16:34:22.166424  7941 net.cpp:290] fc7 -> fc7
I0909 16:34:22.811765  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.811816  7941 net.cpp:125] fc7 needs backward computation.
I0909 16:34:22.811830  7941 net.cpp:66] Creating Layer relu7
I0909 16:34:22.811843  7941 net.cpp:329] relu7 <- fc7
I0909 16:34:22.811851  7941 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:34:22.811861  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.811866  7941 net.cpp:125] relu7 needs backward computation.
I0909 16:34:22.811882  7941 net.cpp:66] Creating Layer drop7
I0909 16:34:22.811887  7941 net.cpp:329] drop7 <- fc7
I0909 16:34:22.811894  7941 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:34:22.811907  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.811913  7941 net.cpp:125] drop7 needs backward computation.
I0909 16:34:22.811920  7941 net.cpp:66] Creating Layer fc8
I0909 16:34:22.811926  7941 net.cpp:329] fc8 <- fc7
I0909 16:34:22.811935  7941 net.cpp:290] fc8 -> fc8
I0909 16:34:22.819702  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.819716  7941 net.cpp:125] fc8 needs backward computation.
I0909 16:34:22.819722  7941 net.cpp:66] Creating Layer relu8
I0909 16:34:22.819728  7941 net.cpp:329] relu8 <- fc8
I0909 16:34:22.819736  7941 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:34:22.819743  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.819748  7941 net.cpp:125] relu8 needs backward computation.
I0909 16:34:22.819754  7941 net.cpp:66] Creating Layer drop8
I0909 16:34:22.819759  7941 net.cpp:329] drop8 <- fc8
I0909 16:34:22.819766  7941 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:34:22.819772  7941 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:34:22.819777  7941 net.cpp:125] drop8 needs backward computation.
I0909 16:34:22.819787  7941 net.cpp:66] Creating Layer fc9
I0909 16:34:22.819792  7941 net.cpp:329] fc9 <- fc8
I0909 16:34:22.819798  7941 net.cpp:290] fc9 -> fc9
I0909 16:34:22.820161  7941 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:34:22.820173  7941 net.cpp:125] fc9 needs backward computation.
I0909 16:34:22.820180  7941 net.cpp:66] Creating Layer fc10
I0909 16:34:22.820186  7941 net.cpp:329] fc10 <- fc9
I0909 16:34:22.820194  7941 net.cpp:290] fc10 -> fc10
I0909 16:34:22.820205  7941 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:34:22.820214  7941 net.cpp:125] fc10 needs backward computation.
I0909 16:34:22.820220  7941 net.cpp:66] Creating Layer prob
I0909 16:34:22.820225  7941 net.cpp:329] prob <- fc10
I0909 16:34:22.820232  7941 net.cpp:290] prob -> prob
I0909 16:34:22.820242  7941 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:34:22.820247  7941 net.cpp:125] prob needs backward computation.
I0909 16:34:22.820252  7941 net.cpp:156] This network produces output prob
I0909 16:34:22.820266  7941 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:34:22.820273  7941 net.cpp:167] Network initialization done.
I0909 16:34:22.820278  7941 net.cpp:168] Memory required for data: 6183480
Classifying 251 inputs.
Done in 166.83 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:37:15.157330  7973 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:37:15.157472  7973 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:37:15.157482  7973 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:37:15.157688  7973 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:37:15.157778  7973 net.cpp:292] Input 0 -> data
I0909 16:37:15.157805  7973 net.cpp:66] Creating Layer conv1
I0909 16:37:15.157812  7973 net.cpp:329] conv1 <- data
I0909 16:37:15.157820  7973 net.cpp:290] conv1 -> conv1
I0909 16:37:15.159189  7973 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:37:15.159209  7973 net.cpp:125] conv1 needs backward computation.
I0909 16:37:15.159217  7973 net.cpp:66] Creating Layer relu1
I0909 16:37:15.159224  7973 net.cpp:329] relu1 <- conv1
I0909 16:37:15.159230  7973 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:37:15.159240  7973 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:37:15.159245  7973 net.cpp:125] relu1 needs backward computation.
I0909 16:37:15.159251  7973 net.cpp:66] Creating Layer pool1
I0909 16:37:15.159256  7973 net.cpp:329] pool1 <- conv1
I0909 16:37:15.159263  7973 net.cpp:290] pool1 -> pool1
I0909 16:37:15.159275  7973 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:37:15.159281  7973 net.cpp:125] pool1 needs backward computation.
I0909 16:37:15.159287  7973 net.cpp:66] Creating Layer norm1
I0909 16:37:15.159292  7973 net.cpp:329] norm1 <- pool1
I0909 16:37:15.159299  7973 net.cpp:290] norm1 -> norm1
I0909 16:37:15.159309  7973 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:37:15.159314  7973 net.cpp:125] norm1 needs backward computation.
I0909 16:37:15.159322  7973 net.cpp:66] Creating Layer conv2
I0909 16:37:15.159327  7973 net.cpp:329] conv2 <- norm1
I0909 16:37:15.159334  7973 net.cpp:290] conv2 -> conv2
I0909 16:37:15.168733  7973 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:37:15.168784  7973 net.cpp:125] conv2 needs backward computation.
I0909 16:37:15.168795  7973 net.cpp:66] Creating Layer relu2
I0909 16:37:15.168802  7973 net.cpp:329] relu2 <- conv2
I0909 16:37:15.168810  7973 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:37:15.168833  7973 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:37:15.168838  7973 net.cpp:125] relu2 needs backward computation.
I0909 16:37:15.168845  7973 net.cpp:66] Creating Layer pool2
I0909 16:37:15.168850  7973 net.cpp:329] pool2 <- conv2
I0909 16:37:15.168858  7973 net.cpp:290] pool2 -> pool2
I0909 16:37:15.168867  7973 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:37:15.168872  7973 net.cpp:125] pool2 needs backward computation.
I0909 16:37:15.168880  7973 net.cpp:66] Creating Layer fc7
I0909 16:37:15.168886  7973 net.cpp:329] fc7 <- pool2
I0909 16:37:15.168897  7973 net.cpp:290] fc7 -> fc7
I0909 16:37:15.815559  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.815605  7973 net.cpp:125] fc7 needs backward computation.
I0909 16:37:15.815618  7973 net.cpp:66] Creating Layer relu7
I0909 16:37:15.815628  7973 net.cpp:329] relu7 <- fc7
I0909 16:37:15.815635  7973 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:37:15.815644  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.815650  7973 net.cpp:125] relu7 needs backward computation.
I0909 16:37:15.815657  7973 net.cpp:66] Creating Layer drop7
I0909 16:37:15.815662  7973 net.cpp:329] drop7 <- fc7
I0909 16:37:15.815670  7973 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:37:15.815680  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.815685  7973 net.cpp:125] drop7 needs backward computation.
I0909 16:37:15.815693  7973 net.cpp:66] Creating Layer fc8
I0909 16:37:15.815698  7973 net.cpp:329] fc8 <- fc7
I0909 16:37:15.815707  7973 net.cpp:290] fc8 -> fc8
I0909 16:37:15.823490  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.823503  7973 net.cpp:125] fc8 needs backward computation.
I0909 16:37:15.823509  7973 net.cpp:66] Creating Layer relu8
I0909 16:37:15.823515  7973 net.cpp:329] relu8 <- fc8
I0909 16:37:15.823524  7973 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:37:15.823531  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.823537  7973 net.cpp:125] relu8 needs backward computation.
I0909 16:37:15.823544  7973 net.cpp:66] Creating Layer drop8
I0909 16:37:15.823549  7973 net.cpp:329] drop8 <- fc8
I0909 16:37:15.823555  7973 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:37:15.823561  7973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:37:15.823567  7973 net.cpp:125] drop8 needs backward computation.
I0909 16:37:15.823576  7973 net.cpp:66] Creating Layer fc9
I0909 16:37:15.823581  7973 net.cpp:329] fc9 <- fc8
I0909 16:37:15.823588  7973 net.cpp:290] fc9 -> fc9
I0909 16:37:15.823961  7973 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:37:15.823972  7973 net.cpp:125] fc9 needs backward computation.
I0909 16:37:15.823982  7973 net.cpp:66] Creating Layer fc10
I0909 16:37:15.823987  7973 net.cpp:329] fc10 <- fc9
I0909 16:37:15.823994  7973 net.cpp:290] fc10 -> fc10
I0909 16:37:15.824007  7973 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:37:15.824014  7973 net.cpp:125] fc10 needs backward computation.
I0909 16:37:15.824020  7973 net.cpp:66] Creating Layer prob
I0909 16:37:15.824025  7973 net.cpp:329] prob <- fc10
I0909 16:37:15.824033  7973 net.cpp:290] prob -> prob
I0909 16:37:15.824043  7973 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:37:15.824048  7973 net.cpp:125] prob needs backward computation.
I0909 16:37:15.824054  7973 net.cpp:156] This network produces output prob
I0909 16:37:15.824066  7973 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:37:15.824074  7973 net.cpp:167] Network initialization done.
I0909 16:37:15.824079  7973 net.cpp:168] Memory required for data: 6183480
Classifying 352 inputs.
Done in 232.70 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:41:16.809533  8052 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:41:16.809690  8052 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:41:16.809700  8052 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:41:16.809854  8052 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:41:16.809908  8052 net.cpp:292] Input 0 -> data
I0909 16:41:16.809934  8052 net.cpp:66] Creating Layer conv1
I0909 16:41:16.809942  8052 net.cpp:329] conv1 <- data
I0909 16:41:16.809949  8052 net.cpp:290] conv1 -> conv1
I0909 16:41:16.811305  8052 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:41:16.811322  8052 net.cpp:125] conv1 needs backward computation.
I0909 16:41:16.811331  8052 net.cpp:66] Creating Layer relu1
I0909 16:41:16.811336  8052 net.cpp:329] relu1 <- conv1
I0909 16:41:16.811343  8052 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:41:16.811352  8052 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:41:16.811357  8052 net.cpp:125] relu1 needs backward computation.
I0909 16:41:16.811364  8052 net.cpp:66] Creating Layer pool1
I0909 16:41:16.811369  8052 net.cpp:329] pool1 <- conv1
I0909 16:41:16.811380  8052 net.cpp:290] pool1 -> pool1
I0909 16:41:16.811391  8052 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:41:16.811398  8052 net.cpp:125] pool1 needs backward computation.
I0909 16:41:16.811404  8052 net.cpp:66] Creating Layer norm1
I0909 16:41:16.811409  8052 net.cpp:329] norm1 <- pool1
I0909 16:41:16.811415  8052 net.cpp:290] norm1 -> norm1
I0909 16:41:16.811425  8052 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:41:16.811430  8052 net.cpp:125] norm1 needs backward computation.
I0909 16:41:16.811437  8052 net.cpp:66] Creating Layer conv2
I0909 16:41:16.811444  8052 net.cpp:329] conv2 <- norm1
I0909 16:41:16.811450  8052 net.cpp:290] conv2 -> conv2
I0909 16:41:16.820492  8052 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:41:16.820508  8052 net.cpp:125] conv2 needs backward computation.
I0909 16:41:16.820514  8052 net.cpp:66] Creating Layer relu2
I0909 16:41:16.820520  8052 net.cpp:329] relu2 <- conv2
I0909 16:41:16.820528  8052 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:41:16.820534  8052 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:41:16.820540  8052 net.cpp:125] relu2 needs backward computation.
I0909 16:41:16.820546  8052 net.cpp:66] Creating Layer pool2
I0909 16:41:16.820551  8052 net.cpp:329] pool2 <- conv2
I0909 16:41:16.820559  8052 net.cpp:290] pool2 -> pool2
I0909 16:41:16.820566  8052 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:41:16.820571  8052 net.cpp:125] pool2 needs backward computation.
I0909 16:41:16.820579  8052 net.cpp:66] Creating Layer fc7
I0909 16:41:16.820583  8052 net.cpp:329] fc7 <- pool2
I0909 16:41:16.820590  8052 net.cpp:290] fc7 -> fc7
I0909 16:41:17.461400  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.461443  8052 net.cpp:125] fc7 needs backward computation.
I0909 16:41:17.461455  8052 net.cpp:66] Creating Layer relu7
I0909 16:41:17.461463  8052 net.cpp:329] relu7 <- fc7
I0909 16:41:17.461470  8052 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:41:17.461480  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.461487  8052 net.cpp:125] relu7 needs backward computation.
I0909 16:41:17.461493  8052 net.cpp:66] Creating Layer drop7
I0909 16:41:17.461498  8052 net.cpp:329] drop7 <- fc7
I0909 16:41:17.461505  8052 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:41:17.461524  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.461532  8052 net.cpp:125] drop7 needs backward computation.
I0909 16:41:17.461540  8052 net.cpp:66] Creating Layer fc8
I0909 16:41:17.461546  8052 net.cpp:329] fc8 <- fc7
I0909 16:41:17.461555  8052 net.cpp:290] fc8 -> fc8
I0909 16:41:17.469331  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.469342  8052 net.cpp:125] fc8 needs backward computation.
I0909 16:41:17.469349  8052 net.cpp:66] Creating Layer relu8
I0909 16:41:17.469355  8052 net.cpp:329] relu8 <- fc8
I0909 16:41:17.469362  8052 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:41:17.469370  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.469375  8052 net.cpp:125] relu8 needs backward computation.
I0909 16:41:17.469383  8052 net.cpp:66] Creating Layer drop8
I0909 16:41:17.469388  8052 net.cpp:329] drop8 <- fc8
I0909 16:41:17.469393  8052 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:41:17.469400  8052 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:17.469406  8052 net.cpp:125] drop8 needs backward computation.
I0909 16:41:17.469414  8052 net.cpp:66] Creating Layer fc9
I0909 16:41:17.469420  8052 net.cpp:329] fc9 <- fc8
I0909 16:41:17.469427  8052 net.cpp:290] fc9 -> fc9
I0909 16:41:17.469804  8052 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:41:17.469816  8052 net.cpp:125] fc9 needs backward computation.
I0909 16:41:17.469825  8052 net.cpp:66] Creating Layer fc10
I0909 16:41:17.469830  8052 net.cpp:329] fc10 <- fc9
I0909 16:41:17.469838  8052 net.cpp:290] fc10 -> fc10
I0909 16:41:17.469851  8052 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:41:17.469858  8052 net.cpp:125] fc10 needs backward computation.
I0909 16:41:17.469864  8052 net.cpp:66] Creating Layer prob
I0909 16:41:17.469882  8052 net.cpp:329] prob <- fc10
I0909 16:41:17.469890  8052 net.cpp:290] prob -> prob
I0909 16:41:17.469899  8052 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:41:17.469905  8052 net.cpp:125] prob needs backward computation.
I0909 16:41:17.469910  8052 net.cpp:156] This network produces output prob
I0909 16:41:17.469923  8052 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:41:17.469931  8052 net.cpp:167] Network initialization done.
I0909 16:41:17.469936  8052 net.cpp:168] Memory required for data: 6183480
Classifying 380 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:41:28.699285  8056 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:41:28.699424  8056 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:41:28.699434  8056 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:41:28.699579  8056 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:41:28.699641  8056 net.cpp:292] Input 0 -> data
I0909 16:41:28.699668  8056 net.cpp:66] Creating Layer conv1
I0909 16:41:28.699676  8056 net.cpp:329] conv1 <- data
I0909 16:41:28.699683  8056 net.cpp:290] conv1 -> conv1
I0909 16:41:28.701007  8056 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:41:28.701025  8056 net.cpp:125] conv1 needs backward computation.
I0909 16:41:28.701033  8056 net.cpp:66] Creating Layer relu1
I0909 16:41:28.701040  8056 net.cpp:329] relu1 <- conv1
I0909 16:41:28.701046  8056 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:41:28.701055  8056 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:41:28.701061  8056 net.cpp:125] relu1 needs backward computation.
I0909 16:41:28.701067  8056 net.cpp:66] Creating Layer pool1
I0909 16:41:28.701072  8056 net.cpp:329] pool1 <- conv1
I0909 16:41:28.701078  8056 net.cpp:290] pool1 -> pool1
I0909 16:41:28.701091  8056 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:41:28.701095  8056 net.cpp:125] pool1 needs backward computation.
I0909 16:41:28.701102  8056 net.cpp:66] Creating Layer norm1
I0909 16:41:28.701107  8056 net.cpp:329] norm1 <- pool1
I0909 16:41:28.701114  8056 net.cpp:290] norm1 -> norm1
I0909 16:41:28.701124  8056 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:41:28.701129  8056 net.cpp:125] norm1 needs backward computation.
I0909 16:41:28.701136  8056 net.cpp:66] Creating Layer conv2
I0909 16:41:28.701143  8056 net.cpp:329] conv2 <- norm1
I0909 16:41:28.701149  8056 net.cpp:290] conv2 -> conv2
I0909 16:41:28.710078  8056 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:41:28.710091  8056 net.cpp:125] conv2 needs backward computation.
I0909 16:41:28.710098  8056 net.cpp:66] Creating Layer relu2
I0909 16:41:28.710104  8056 net.cpp:329] relu2 <- conv2
I0909 16:41:28.710110  8056 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:41:28.710118  8056 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:41:28.710124  8056 net.cpp:125] relu2 needs backward computation.
I0909 16:41:28.710129  8056 net.cpp:66] Creating Layer pool2
I0909 16:41:28.710134  8056 net.cpp:329] pool2 <- conv2
I0909 16:41:28.710141  8056 net.cpp:290] pool2 -> pool2
I0909 16:41:28.710149  8056 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:41:28.710155  8056 net.cpp:125] pool2 needs backward computation.
I0909 16:41:28.710162  8056 net.cpp:66] Creating Layer fc7
I0909 16:41:28.710168  8056 net.cpp:329] fc7 <- pool2
I0909 16:41:28.710175  8056 net.cpp:290] fc7 -> fc7
I0909 16:41:29.347887  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.347928  8056 net.cpp:125] fc7 needs backward computation.
I0909 16:41:29.347940  8056 net.cpp:66] Creating Layer relu7
I0909 16:41:29.347947  8056 net.cpp:329] relu7 <- fc7
I0909 16:41:29.347955  8056 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:41:29.347965  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.347971  8056 net.cpp:125] relu7 needs backward computation.
I0909 16:41:29.347978  8056 net.cpp:66] Creating Layer drop7
I0909 16:41:29.347985  8056 net.cpp:329] drop7 <- fc7
I0909 16:41:29.347991  8056 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:41:29.348002  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.348008  8056 net.cpp:125] drop7 needs backward computation.
I0909 16:41:29.348018  8056 net.cpp:66] Creating Layer fc8
I0909 16:41:29.348023  8056 net.cpp:329] fc8 <- fc7
I0909 16:41:29.348031  8056 net.cpp:290] fc8 -> fc8
I0909 16:41:29.355818  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.355830  8056 net.cpp:125] fc8 needs backward computation.
I0909 16:41:29.355839  8056 net.cpp:66] Creating Layer relu8
I0909 16:41:29.355844  8056 net.cpp:329] relu8 <- fc8
I0909 16:41:29.355851  8056 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:41:29.355859  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.355865  8056 net.cpp:125] relu8 needs backward computation.
I0909 16:41:29.355870  8056 net.cpp:66] Creating Layer drop8
I0909 16:41:29.355876  8056 net.cpp:329] drop8 <- fc8
I0909 16:41:29.355885  8056 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:41:29.355891  8056 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:41:29.355897  8056 net.cpp:125] drop8 needs backward computation.
I0909 16:41:29.355904  8056 net.cpp:66] Creating Layer fc9
I0909 16:41:29.355911  8056 net.cpp:329] fc9 <- fc8
I0909 16:41:29.355917  8056 net.cpp:290] fc9 -> fc9
I0909 16:41:29.356298  8056 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:41:29.356308  8056 net.cpp:125] fc9 needs backward computation.
I0909 16:41:29.356317  8056 net.cpp:66] Creating Layer fc10
I0909 16:41:29.356323  8056 net.cpp:329] fc10 <- fc9
I0909 16:41:29.356330  8056 net.cpp:290] fc10 -> fc10
I0909 16:41:29.356341  8056 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:41:29.356349  8056 net.cpp:125] fc10 needs backward computation.
I0909 16:41:29.356356  8056 net.cpp:66] Creating Layer prob
I0909 16:41:29.356361  8056 net.cpp:329] prob <- fc10
I0909 16:41:29.356369  8056 net.cpp:290] prob -> prob
I0909 16:41:29.356379  8056 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:41:29.356384  8056 net.cpp:125] prob needs backward computation.
I0909 16:41:29.356389  8056 net.cpp:156] This network produces output prob
I0909 16:41:29.356401  8056 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:41:29.356410  8056 net.cpp:167] Network initialization done.
I0909 16:41:29.356415  8056 net.cpp:168] Memory required for data: 6183480
Classifying 127 inputs.
Done in 82.59 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:42:54.710697  8084 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:42:54.710834  8084 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:42:54.710844  8084 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:42:54.710988  8084 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:42:54.711053  8084 net.cpp:292] Input 0 -> data
I0909 16:42:54.711079  8084 net.cpp:66] Creating Layer conv1
I0909 16:42:54.711086  8084 net.cpp:329] conv1 <- data
I0909 16:42:54.711093  8084 net.cpp:290] conv1 -> conv1
I0909 16:42:54.712415  8084 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:42:54.712432  8084 net.cpp:125] conv1 needs backward computation.
I0909 16:42:54.712441  8084 net.cpp:66] Creating Layer relu1
I0909 16:42:54.712447  8084 net.cpp:329] relu1 <- conv1
I0909 16:42:54.712453  8084 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:42:54.712462  8084 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:42:54.712467  8084 net.cpp:125] relu1 needs backward computation.
I0909 16:42:54.712474  8084 net.cpp:66] Creating Layer pool1
I0909 16:42:54.712479  8084 net.cpp:329] pool1 <- conv1
I0909 16:42:54.712486  8084 net.cpp:290] pool1 -> pool1
I0909 16:42:54.712497  8084 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:42:54.712502  8084 net.cpp:125] pool1 needs backward computation.
I0909 16:42:54.712509  8084 net.cpp:66] Creating Layer norm1
I0909 16:42:54.712515  8084 net.cpp:329] norm1 <- pool1
I0909 16:42:54.712522  8084 net.cpp:290] norm1 -> norm1
I0909 16:42:54.712532  8084 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:42:54.712537  8084 net.cpp:125] norm1 needs backward computation.
I0909 16:42:54.712543  8084 net.cpp:66] Creating Layer conv2
I0909 16:42:54.712549  8084 net.cpp:329] conv2 <- norm1
I0909 16:42:54.712556  8084 net.cpp:290] conv2 -> conv2
I0909 16:42:54.721607  8084 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:42:54.721622  8084 net.cpp:125] conv2 needs backward computation.
I0909 16:42:54.721629  8084 net.cpp:66] Creating Layer relu2
I0909 16:42:54.721635  8084 net.cpp:329] relu2 <- conv2
I0909 16:42:54.721642  8084 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:42:54.721649  8084 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:42:54.721654  8084 net.cpp:125] relu2 needs backward computation.
I0909 16:42:54.721660  8084 net.cpp:66] Creating Layer pool2
I0909 16:42:54.721667  8084 net.cpp:329] pool2 <- conv2
I0909 16:42:54.721673  8084 net.cpp:290] pool2 -> pool2
I0909 16:42:54.721680  8084 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:42:54.721686  8084 net.cpp:125] pool2 needs backward computation.
I0909 16:42:54.721693  8084 net.cpp:66] Creating Layer fc7
I0909 16:42:54.721704  8084 net.cpp:329] fc7 <- pool2
I0909 16:42:54.721711  8084 net.cpp:290] fc7 -> fc7
I0909 16:42:55.362256  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.362298  8084 net.cpp:125] fc7 needs backward computation.
I0909 16:42:55.362310  8084 net.cpp:66] Creating Layer relu7
I0909 16:42:55.362318  8084 net.cpp:329] relu7 <- fc7
I0909 16:42:55.362326  8084 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:42:55.362335  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.362340  8084 net.cpp:125] relu7 needs backward computation.
I0909 16:42:55.362349  8084 net.cpp:66] Creating Layer drop7
I0909 16:42:55.362354  8084 net.cpp:329] drop7 <- fc7
I0909 16:42:55.362360  8084 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:42:55.362370  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.362375  8084 net.cpp:125] drop7 needs backward computation.
I0909 16:42:55.362383  8084 net.cpp:66] Creating Layer fc8
I0909 16:42:55.362388  8084 net.cpp:329] fc8 <- fc7
I0909 16:42:55.362397  8084 net.cpp:290] fc8 -> fc8
I0909 16:42:55.370040  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.370055  8084 net.cpp:125] fc8 needs backward computation.
I0909 16:42:55.370062  8084 net.cpp:66] Creating Layer relu8
I0909 16:42:55.370069  8084 net.cpp:329] relu8 <- fc8
I0909 16:42:55.370076  8084 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:42:55.370084  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.370090  8084 net.cpp:125] relu8 needs backward computation.
I0909 16:42:55.370096  8084 net.cpp:66] Creating Layer drop8
I0909 16:42:55.370101  8084 net.cpp:329] drop8 <- fc8
I0909 16:42:55.370107  8084 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:42:55.370115  8084 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:42:55.370121  8084 net.cpp:125] drop8 needs backward computation.
I0909 16:42:55.370128  8084 net.cpp:66] Creating Layer fc9
I0909 16:42:55.370134  8084 net.cpp:329] fc9 <- fc8
I0909 16:42:55.370141  8084 net.cpp:290] fc9 -> fc9
I0909 16:42:55.370514  8084 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:42:55.370527  8084 net.cpp:125] fc9 needs backward computation.
I0909 16:42:55.370535  8084 net.cpp:66] Creating Layer fc10
I0909 16:42:55.370540  8084 net.cpp:329] fc10 <- fc9
I0909 16:42:55.370549  8084 net.cpp:290] fc10 -> fc10
I0909 16:42:55.370560  8084 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:42:55.370568  8084 net.cpp:125] fc10 needs backward computation.
I0909 16:42:55.370575  8084 net.cpp:66] Creating Layer prob
I0909 16:42:55.370580  8084 net.cpp:329] prob <- fc10
I0909 16:42:55.370589  8084 net.cpp:290] prob -> prob
I0909 16:42:55.370599  8084 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:42:55.370604  8084 net.cpp:125] prob needs backward computation.
I0909 16:42:55.370609  8084 net.cpp:156] This network produces output prob
I0909 16:42:55.370622  8084 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:42:55.370630  8084 net.cpp:167] Network initialization done.
I0909 16:42:55.370635  8084 net.cpp:168] Memory required for data: 6183480
Classifying 8 inputs.
Done in 5.56 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:43:01.797734  8087 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:43:01.797871  8087 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:43:01.797880  8087 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:43:01.798023  8087 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:43:01.798085  8087 net.cpp:292] Input 0 -> data
I0909 16:43:01.798111  8087 net.cpp:66] Creating Layer conv1
I0909 16:43:01.798118  8087 net.cpp:329] conv1 <- data
I0909 16:43:01.798126  8087 net.cpp:290] conv1 -> conv1
I0909 16:43:01.799451  8087 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:43:01.799469  8087 net.cpp:125] conv1 needs backward computation.
I0909 16:43:01.799479  8087 net.cpp:66] Creating Layer relu1
I0909 16:43:01.799484  8087 net.cpp:329] relu1 <- conv1
I0909 16:43:01.799491  8087 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:43:01.799500  8087 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:43:01.799506  8087 net.cpp:125] relu1 needs backward computation.
I0909 16:43:01.799514  8087 net.cpp:66] Creating Layer pool1
I0909 16:43:01.799518  8087 net.cpp:329] pool1 <- conv1
I0909 16:43:01.799525  8087 net.cpp:290] pool1 -> pool1
I0909 16:43:01.799536  8087 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:43:01.799542  8087 net.cpp:125] pool1 needs backward computation.
I0909 16:43:01.799549  8087 net.cpp:66] Creating Layer norm1
I0909 16:43:01.799554  8087 net.cpp:329] norm1 <- pool1
I0909 16:43:01.799561  8087 net.cpp:290] norm1 -> norm1
I0909 16:43:01.799571  8087 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:43:01.799577  8087 net.cpp:125] norm1 needs backward computation.
I0909 16:43:01.799589  8087 net.cpp:66] Creating Layer conv2
I0909 16:43:01.799597  8087 net.cpp:329] conv2 <- norm1
I0909 16:43:01.799603  8087 net.cpp:290] conv2 -> conv2
I0909 16:43:01.808536  8087 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:43:01.808549  8087 net.cpp:125] conv2 needs backward computation.
I0909 16:43:01.808557  8087 net.cpp:66] Creating Layer relu2
I0909 16:43:01.808562  8087 net.cpp:329] relu2 <- conv2
I0909 16:43:01.808569  8087 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:43:01.808576  8087 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:43:01.808583  8087 net.cpp:125] relu2 needs backward computation.
I0909 16:43:01.808588  8087 net.cpp:66] Creating Layer pool2
I0909 16:43:01.808594  8087 net.cpp:329] pool2 <- conv2
I0909 16:43:01.808600  8087 net.cpp:290] pool2 -> pool2
I0909 16:43:01.808609  8087 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:43:01.808614  8087 net.cpp:125] pool2 needs backward computation.
I0909 16:43:01.808622  8087 net.cpp:66] Creating Layer fc7
I0909 16:43:01.808627  8087 net.cpp:329] fc7 <- pool2
I0909 16:43:01.808637  8087 net.cpp:290] fc7 -> fc7
I0909 16:43:02.520182  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.520226  8087 net.cpp:125] fc7 needs backward computation.
I0909 16:43:02.520237  8087 net.cpp:66] Creating Layer relu7
I0909 16:43:02.520246  8087 net.cpp:329] relu7 <- fc7
I0909 16:43:02.520254  8087 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:43:02.520264  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.520272  8087 net.cpp:125] relu7 needs backward computation.
I0909 16:43:02.520278  8087 net.cpp:66] Creating Layer drop7
I0909 16:43:02.520284  8087 net.cpp:329] drop7 <- fc7
I0909 16:43:02.520290  8087 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:43:02.520303  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.520308  8087 net.cpp:125] drop7 needs backward computation.
I0909 16:43:02.520316  8087 net.cpp:66] Creating Layer fc8
I0909 16:43:02.520323  8087 net.cpp:329] fc8 <- fc7
I0909 16:43:02.520331  8087 net.cpp:290] fc8 -> fc8
I0909 16:43:02.529000  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.529016  8087 net.cpp:125] fc8 needs backward computation.
I0909 16:43:02.529023  8087 net.cpp:66] Creating Layer relu8
I0909 16:43:02.529028  8087 net.cpp:329] relu8 <- fc8
I0909 16:43:02.529037  8087 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:43:02.529044  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.529050  8087 net.cpp:125] relu8 needs backward computation.
I0909 16:43:02.529057  8087 net.cpp:66] Creating Layer drop8
I0909 16:43:02.529062  8087 net.cpp:329] drop8 <- fc8
I0909 16:43:02.529068  8087 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:43:02.529075  8087 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:43:02.529081  8087 net.cpp:125] drop8 needs backward computation.
I0909 16:43:02.529093  8087 net.cpp:66] Creating Layer fc9
I0909 16:43:02.529098  8087 net.cpp:329] fc9 <- fc8
I0909 16:43:02.529105  8087 net.cpp:290] fc9 -> fc9
I0909 16:43:02.529507  8087 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:43:02.529523  8087 net.cpp:125] fc9 needs backward computation.
I0909 16:43:02.529531  8087 net.cpp:66] Creating Layer fc10
I0909 16:43:02.529537  8087 net.cpp:329] fc10 <- fc9
I0909 16:43:02.529546  8087 net.cpp:290] fc10 -> fc10
I0909 16:43:02.529558  8087 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:43:02.529567  8087 net.cpp:125] fc10 needs backward computation.
I0909 16:43:02.529572  8087 net.cpp:66] Creating Layer prob
I0909 16:43:02.529578  8087 net.cpp:329] prob <- fc10
I0909 16:43:02.529587  8087 net.cpp:290] prob -> prob
I0909 16:43:02.529597  8087 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:43:02.529602  8087 net.cpp:125] prob needs backward computation.
I0909 16:43:02.529606  8087 net.cpp:156] This network produces output prob
I0909 16:43:02.529619  8087 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:43:02.529628  8087 net.cpp:167] Network initialization done.
I0909 16:43:02.529633  8087 net.cpp:168] Memory required for data: 6183480
Classifying 404 inputs.
Done in 254.28 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:47:29.636615  8188 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:47:29.636872  8188 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:47:29.636890  8188 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:47:29.637181  8188 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:47:29.637290  8188 net.cpp:292] Input 0 -> data
I0909 16:47:29.637339  8188 net.cpp:66] Creating Layer conv1
I0909 16:47:29.637353  8188 net.cpp:329] conv1 <- data
I0909 16:47:29.637370  8188 net.cpp:290] conv1 -> conv1
I0909 16:47:29.640081  8188 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:47:29.650713  8188 net.cpp:125] conv1 needs backward computation.
I0909 16:47:29.650745  8188 net.cpp:66] Creating Layer relu1
I0909 16:47:29.650753  8188 net.cpp:329] relu1 <- conv1
I0909 16:47:29.650760  8188 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:47:29.650770  8188 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:47:29.650776  8188 net.cpp:125] relu1 needs backward computation.
I0909 16:47:29.650784  8188 net.cpp:66] Creating Layer pool1
I0909 16:47:29.650789  8188 net.cpp:329] pool1 <- conv1
I0909 16:47:29.650795  8188 net.cpp:290] pool1 -> pool1
I0909 16:47:29.650806  8188 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:47:29.650812  8188 net.cpp:125] pool1 needs backward computation.
I0909 16:47:29.650820  8188 net.cpp:66] Creating Layer norm1
I0909 16:47:29.650825  8188 net.cpp:329] norm1 <- pool1
I0909 16:47:29.650831  8188 net.cpp:290] norm1 -> norm1
I0909 16:47:29.650841  8188 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:47:29.650847  8188 net.cpp:125] norm1 needs backward computation.
I0909 16:47:29.650854  8188 net.cpp:66] Creating Layer conv2
I0909 16:47:29.650861  8188 net.cpp:329] conv2 <- norm1
I0909 16:47:29.650867  8188 net.cpp:290] conv2 -> conv2
I0909 16:47:29.662099  8188 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:47:29.662119  8188 net.cpp:125] conv2 needs backward computation.
I0909 16:47:29.662128  8188 net.cpp:66] Creating Layer relu2
I0909 16:47:29.662134  8188 net.cpp:329] relu2 <- conv2
I0909 16:47:29.662142  8188 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:47:29.662149  8188 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:47:29.662155  8188 net.cpp:125] relu2 needs backward computation.
I0909 16:47:29.662161  8188 net.cpp:66] Creating Layer pool2
I0909 16:47:29.662168  8188 net.cpp:329] pool2 <- conv2
I0909 16:47:29.662174  8188 net.cpp:290] pool2 -> pool2
I0909 16:47:29.662183  8188 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:47:29.662189  8188 net.cpp:125] pool2 needs backward computation.
I0909 16:47:29.662196  8188 net.cpp:66] Creating Layer fc7
I0909 16:47:29.662201  8188 net.cpp:329] fc7 <- pool2
I0909 16:47:29.662209  8188 net.cpp:290] fc7 -> fc7
I0909 16:47:30.299347  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.299392  8188 net.cpp:125] fc7 needs backward computation.
I0909 16:47:30.299404  8188 net.cpp:66] Creating Layer relu7
I0909 16:47:30.299412  8188 net.cpp:329] relu7 <- fc7
I0909 16:47:30.299420  8188 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:47:30.299430  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.299437  8188 net.cpp:125] relu7 needs backward computation.
I0909 16:47:30.299443  8188 net.cpp:66] Creating Layer drop7
I0909 16:47:30.299449  8188 net.cpp:329] drop7 <- fc7
I0909 16:47:30.299455  8188 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:47:30.299466  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.299473  8188 net.cpp:125] drop7 needs backward computation.
I0909 16:47:30.299480  8188 net.cpp:66] Creating Layer fc8
I0909 16:47:30.299486  8188 net.cpp:329] fc8 <- fc7
I0909 16:47:30.299494  8188 net.cpp:290] fc8 -> fc8
I0909 16:47:30.307274  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.307287  8188 net.cpp:125] fc8 needs backward computation.
I0909 16:47:30.307294  8188 net.cpp:66] Creating Layer relu8
I0909 16:47:30.307301  8188 net.cpp:329] relu8 <- fc8
I0909 16:47:30.307308  8188 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:47:30.307317  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.307322  8188 net.cpp:125] relu8 needs backward computation.
I0909 16:47:30.307328  8188 net.cpp:66] Creating Layer drop8
I0909 16:47:30.307333  8188 net.cpp:329] drop8 <- fc8
I0909 16:47:30.307340  8188 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:47:30.307346  8188 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:47:30.307353  8188 net.cpp:125] drop8 needs backward computation.
I0909 16:47:30.307361  8188 net.cpp:66] Creating Layer fc9
I0909 16:47:30.307366  8188 net.cpp:329] fc9 <- fc8
I0909 16:47:30.307374  8188 net.cpp:290] fc9 -> fc9
I0909 16:47:30.307759  8188 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:47:30.307770  8188 net.cpp:125] fc9 needs backward computation.
I0909 16:47:30.307780  8188 net.cpp:66] Creating Layer fc10
I0909 16:47:30.307785  8188 net.cpp:329] fc10 <- fc9
I0909 16:47:30.307793  8188 net.cpp:290] fc10 -> fc10
I0909 16:47:30.307804  8188 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:47:30.307812  8188 net.cpp:125] fc10 needs backward computation.
I0909 16:47:30.307819  8188 net.cpp:66] Creating Layer prob
I0909 16:47:30.307824  8188 net.cpp:329] prob <- fc10
I0909 16:47:30.307832  8188 net.cpp:290] prob -> prob
I0909 16:47:30.307842  8188 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:47:30.307847  8188 net.cpp:125] prob needs backward computation.
I0909 16:47:30.307852  8188 net.cpp:156] This network produces output prob
I0909 16:47:30.307864  8188 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:47:30.307873  8188 net.cpp:167] Network initialization done.
I0909 16:47:30.307878  8188 net.cpp:168] Memory required for data: 6183480
Classifying 99 inputs.
Done in 64.81 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:48:38.074553  8197 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:48:38.074689  8197 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:48:38.074698  8197 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:48:38.074841  8197 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:48:38.074905  8197 net.cpp:292] Input 0 -> data
I0909 16:48:38.074931  8197 net.cpp:66] Creating Layer conv1
I0909 16:48:38.074939  8197 net.cpp:329] conv1 <- data
I0909 16:48:38.074946  8197 net.cpp:290] conv1 -> conv1
I0909 16:48:38.076268  8197 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:48:38.076287  8197 net.cpp:125] conv1 needs backward computation.
I0909 16:48:38.076295  8197 net.cpp:66] Creating Layer relu1
I0909 16:48:38.076302  8197 net.cpp:329] relu1 <- conv1
I0909 16:48:38.076308  8197 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:48:38.076316  8197 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:48:38.076321  8197 net.cpp:125] relu1 needs backward computation.
I0909 16:48:38.076328  8197 net.cpp:66] Creating Layer pool1
I0909 16:48:38.076334  8197 net.cpp:329] pool1 <- conv1
I0909 16:48:38.076340  8197 net.cpp:290] pool1 -> pool1
I0909 16:48:38.076351  8197 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:48:38.076357  8197 net.cpp:125] pool1 needs backward computation.
I0909 16:48:38.076364  8197 net.cpp:66] Creating Layer norm1
I0909 16:48:38.076370  8197 net.cpp:329] norm1 <- pool1
I0909 16:48:38.076375  8197 net.cpp:290] norm1 -> norm1
I0909 16:48:38.076385  8197 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:48:38.076391  8197 net.cpp:125] norm1 needs backward computation.
I0909 16:48:38.076398  8197 net.cpp:66] Creating Layer conv2
I0909 16:48:38.076403  8197 net.cpp:329] conv2 <- norm1
I0909 16:48:38.076411  8197 net.cpp:290] conv2 -> conv2
I0909 16:48:38.085300  8197 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:48:38.085314  8197 net.cpp:125] conv2 needs backward computation.
I0909 16:48:38.085321  8197 net.cpp:66] Creating Layer relu2
I0909 16:48:38.085327  8197 net.cpp:329] relu2 <- conv2
I0909 16:48:38.085333  8197 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:48:38.085340  8197 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:48:38.085346  8197 net.cpp:125] relu2 needs backward computation.
I0909 16:48:38.085352  8197 net.cpp:66] Creating Layer pool2
I0909 16:48:38.085357  8197 net.cpp:329] pool2 <- conv2
I0909 16:48:38.085363  8197 net.cpp:290] pool2 -> pool2
I0909 16:48:38.085371  8197 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:48:38.085376  8197 net.cpp:125] pool2 needs backward computation.
I0909 16:48:38.085383  8197 net.cpp:66] Creating Layer fc7
I0909 16:48:38.085388  8197 net.cpp:329] fc7 <- pool2
I0909 16:48:38.085396  8197 net.cpp:290] fc7 -> fc7
I0909 16:48:38.723299  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.723345  8197 net.cpp:125] fc7 needs backward computation.
I0909 16:48:38.723358  8197 net.cpp:66] Creating Layer relu7
I0909 16:48:38.723366  8197 net.cpp:329] relu7 <- fc7
I0909 16:48:38.723373  8197 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:48:38.723383  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.723389  8197 net.cpp:125] relu7 needs backward computation.
I0909 16:48:38.723397  8197 net.cpp:66] Creating Layer drop7
I0909 16:48:38.723402  8197 net.cpp:329] drop7 <- fc7
I0909 16:48:38.723408  8197 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:48:38.723419  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.723425  8197 net.cpp:125] drop7 needs backward computation.
I0909 16:48:38.723433  8197 net.cpp:66] Creating Layer fc8
I0909 16:48:38.723448  8197 net.cpp:329] fc8 <- fc7
I0909 16:48:38.723458  8197 net.cpp:290] fc8 -> fc8
I0909 16:48:38.731050  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.731062  8197 net.cpp:125] fc8 needs backward computation.
I0909 16:48:38.731070  8197 net.cpp:66] Creating Layer relu8
I0909 16:48:38.731075  8197 net.cpp:329] relu8 <- fc8
I0909 16:48:38.731082  8197 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:48:38.731089  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.731096  8197 net.cpp:125] relu8 needs backward computation.
I0909 16:48:38.731101  8197 net.cpp:66] Creating Layer drop8
I0909 16:48:38.731106  8197 net.cpp:329] drop8 <- fc8
I0909 16:48:38.731112  8197 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:48:38.731119  8197 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:38.731124  8197 net.cpp:125] drop8 needs backward computation.
I0909 16:48:38.731133  8197 net.cpp:66] Creating Layer fc9
I0909 16:48:38.731139  8197 net.cpp:329] fc9 <- fc8
I0909 16:48:38.731145  8197 net.cpp:290] fc9 -> fc9
I0909 16:48:38.731510  8197 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:48:38.731521  8197 net.cpp:125] fc9 needs backward computation.
I0909 16:48:38.731530  8197 net.cpp:66] Creating Layer fc10
I0909 16:48:38.731535  8197 net.cpp:329] fc10 <- fc9
I0909 16:48:38.731544  8197 net.cpp:290] fc10 -> fc10
I0909 16:48:38.731554  8197 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:48:38.731562  8197 net.cpp:125] fc10 needs backward computation.
I0909 16:48:38.731569  8197 net.cpp:66] Creating Layer prob
I0909 16:48:38.731575  8197 net.cpp:329] prob <- fc10
I0909 16:48:38.731581  8197 net.cpp:290] prob -> prob
I0909 16:48:38.731590  8197 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:48:38.731596  8197 net.cpp:125] prob needs backward computation.
I0909 16:48:38.731601  8197 net.cpp:156] This network produces output prob
I0909 16:48:38.731613  8197 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:48:38.731621  8197 net.cpp:167] Network initialization done.
I0909 16:48:38.731626  8197 net.cpp:168] Memory required for data: 6183480
Classifying 14 inputs.
Done in 9.04 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:48:48.806454  8202 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:48:48.806592  8202 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:48:48.806601  8202 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:48:48.806745  8202 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:48:48.806805  8202 net.cpp:292] Input 0 -> data
I0909 16:48:48.806831  8202 net.cpp:66] Creating Layer conv1
I0909 16:48:48.806838  8202 net.cpp:329] conv1 <- data
I0909 16:48:48.806846  8202 net.cpp:290] conv1 -> conv1
I0909 16:48:48.808190  8202 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:48:48.808207  8202 net.cpp:125] conv1 needs backward computation.
I0909 16:48:48.808217  8202 net.cpp:66] Creating Layer relu1
I0909 16:48:48.808223  8202 net.cpp:329] relu1 <- conv1
I0909 16:48:48.808228  8202 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:48:48.808238  8202 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:48:48.808243  8202 net.cpp:125] relu1 needs backward computation.
I0909 16:48:48.808249  8202 net.cpp:66] Creating Layer pool1
I0909 16:48:48.808254  8202 net.cpp:329] pool1 <- conv1
I0909 16:48:48.808261  8202 net.cpp:290] pool1 -> pool1
I0909 16:48:48.808271  8202 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:48:48.808277  8202 net.cpp:125] pool1 needs backward computation.
I0909 16:48:48.808284  8202 net.cpp:66] Creating Layer norm1
I0909 16:48:48.808290  8202 net.cpp:329] norm1 <- pool1
I0909 16:48:48.808295  8202 net.cpp:290] norm1 -> norm1
I0909 16:48:48.808305  8202 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:48:48.808310  8202 net.cpp:125] norm1 needs backward computation.
I0909 16:48:48.808317  8202 net.cpp:66] Creating Layer conv2
I0909 16:48:48.808323  8202 net.cpp:329] conv2 <- norm1
I0909 16:48:48.808331  8202 net.cpp:290] conv2 -> conv2
I0909 16:48:48.817294  8202 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:48:48.817309  8202 net.cpp:125] conv2 needs backward computation.
I0909 16:48:48.817317  8202 net.cpp:66] Creating Layer relu2
I0909 16:48:48.817322  8202 net.cpp:329] relu2 <- conv2
I0909 16:48:48.817328  8202 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:48:48.817335  8202 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:48:48.817342  8202 net.cpp:125] relu2 needs backward computation.
I0909 16:48:48.817348  8202 net.cpp:66] Creating Layer pool2
I0909 16:48:48.817353  8202 net.cpp:329] pool2 <- conv2
I0909 16:48:48.817359  8202 net.cpp:290] pool2 -> pool2
I0909 16:48:48.817368  8202 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:48:48.817373  8202 net.cpp:125] pool2 needs backward computation.
I0909 16:48:48.817386  8202 net.cpp:66] Creating Layer fc7
I0909 16:48:48.817392  8202 net.cpp:329] fc7 <- pool2
I0909 16:48:48.817400  8202 net.cpp:290] fc7 -> fc7
I0909 16:48:49.459492  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.459535  8202 net.cpp:125] fc7 needs backward computation.
I0909 16:48:49.459549  8202 net.cpp:66] Creating Layer relu7
I0909 16:48:49.459556  8202 net.cpp:329] relu7 <- fc7
I0909 16:48:49.459564  8202 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:48:49.459574  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.459579  8202 net.cpp:125] relu7 needs backward computation.
I0909 16:48:49.459588  8202 net.cpp:66] Creating Layer drop7
I0909 16:48:49.459592  8202 net.cpp:329] drop7 <- fc7
I0909 16:48:49.459599  8202 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:48:49.459610  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.459616  8202 net.cpp:125] drop7 needs backward computation.
I0909 16:48:49.459626  8202 net.cpp:66] Creating Layer fc8
I0909 16:48:49.459631  8202 net.cpp:329] fc8 <- fc7
I0909 16:48:49.459640  8202 net.cpp:290] fc8 -> fc8
I0909 16:48:49.467222  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.467234  8202 net.cpp:125] fc8 needs backward computation.
I0909 16:48:49.467243  8202 net.cpp:66] Creating Layer relu8
I0909 16:48:49.467249  8202 net.cpp:329] relu8 <- fc8
I0909 16:48:49.467255  8202 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:48:49.467262  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.467267  8202 net.cpp:125] relu8 needs backward computation.
I0909 16:48:49.467274  8202 net.cpp:66] Creating Layer drop8
I0909 16:48:49.467279  8202 net.cpp:329] drop8 <- fc8
I0909 16:48:49.467286  8202 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:48:49.467293  8202 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:48:49.467298  8202 net.cpp:125] drop8 needs backward computation.
I0909 16:48:49.467306  8202 net.cpp:66] Creating Layer fc9
I0909 16:48:49.467311  8202 net.cpp:329] fc9 <- fc8
I0909 16:48:49.467319  8202 net.cpp:290] fc9 -> fc9
I0909 16:48:49.467684  8202 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:48:49.467694  8202 net.cpp:125] fc9 needs backward computation.
I0909 16:48:49.467702  8202 net.cpp:66] Creating Layer fc10
I0909 16:48:49.467708  8202 net.cpp:329] fc10 <- fc9
I0909 16:48:49.467716  8202 net.cpp:290] fc10 -> fc10
I0909 16:48:49.467727  8202 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:48:49.467736  8202 net.cpp:125] fc10 needs backward computation.
I0909 16:48:49.467741  8202 net.cpp:66] Creating Layer prob
I0909 16:48:49.467747  8202 net.cpp:329] prob <- fc10
I0909 16:48:49.467754  8202 net.cpp:290] prob -> prob
I0909 16:48:49.467763  8202 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:48:49.467769  8202 net.cpp:125] prob needs backward computation.
I0909 16:48:49.467774  8202 net.cpp:156] This network produces output prob
I0909 16:48:49.467787  8202 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:48:49.467794  8202 net.cpp:167] Network initialization done.
I0909 16:48:49.467799  8202 net.cpp:168] Memory required for data: 6183480
Classifying 25 inputs.
Done in 15.27 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:49:05.815588  8215 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:49:05.815728  8215 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:49:05.815738  8215 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:49:05.815886  8215 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:49:05.815951  8215 net.cpp:292] Input 0 -> data
I0909 16:49:05.815979  8215 net.cpp:66] Creating Layer conv1
I0909 16:49:05.815986  8215 net.cpp:329] conv1 <- data
I0909 16:49:05.815994  8215 net.cpp:290] conv1 -> conv1
I0909 16:49:05.817355  8215 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:49:05.817373  8215 net.cpp:125] conv1 needs backward computation.
I0909 16:49:05.817383  8215 net.cpp:66] Creating Layer relu1
I0909 16:49:05.817389  8215 net.cpp:329] relu1 <- conv1
I0909 16:49:05.817395  8215 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:49:05.817404  8215 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:49:05.817410  8215 net.cpp:125] relu1 needs backward computation.
I0909 16:49:05.817417  8215 net.cpp:66] Creating Layer pool1
I0909 16:49:05.817423  8215 net.cpp:329] pool1 <- conv1
I0909 16:49:05.817430  8215 net.cpp:290] pool1 -> pool1
I0909 16:49:05.817441  8215 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:49:05.817447  8215 net.cpp:125] pool1 needs backward computation.
I0909 16:49:05.817455  8215 net.cpp:66] Creating Layer norm1
I0909 16:49:05.817459  8215 net.cpp:329] norm1 <- pool1
I0909 16:49:05.817466  8215 net.cpp:290] norm1 -> norm1
I0909 16:49:05.817476  8215 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:49:05.817487  8215 net.cpp:125] norm1 needs backward computation.
I0909 16:49:05.817495  8215 net.cpp:66] Creating Layer conv2
I0909 16:49:05.817502  8215 net.cpp:329] conv2 <- norm1
I0909 16:49:05.817508  8215 net.cpp:290] conv2 -> conv2
I0909 16:49:05.826550  8215 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:49:05.826566  8215 net.cpp:125] conv2 needs backward computation.
I0909 16:49:05.826575  8215 net.cpp:66] Creating Layer relu2
I0909 16:49:05.826580  8215 net.cpp:329] relu2 <- conv2
I0909 16:49:05.826586  8215 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:49:05.826593  8215 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:49:05.826599  8215 net.cpp:125] relu2 needs backward computation.
I0909 16:49:05.826606  8215 net.cpp:66] Creating Layer pool2
I0909 16:49:05.826611  8215 net.cpp:329] pool2 <- conv2
I0909 16:49:05.826617  8215 net.cpp:290] pool2 -> pool2
I0909 16:49:05.826625  8215 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:49:05.826632  8215 net.cpp:125] pool2 needs backward computation.
I0909 16:49:05.826638  8215 net.cpp:66] Creating Layer fc7
I0909 16:49:05.826643  8215 net.cpp:329] fc7 <- pool2
I0909 16:49:05.826650  8215 net.cpp:290] fc7 -> fc7
I0909 16:49:06.470401  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.470445  8215 net.cpp:125] fc7 needs backward computation.
I0909 16:49:06.470458  8215 net.cpp:66] Creating Layer relu7
I0909 16:49:06.470465  8215 net.cpp:329] relu7 <- fc7
I0909 16:49:06.470474  8215 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:49:06.470484  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.470489  8215 net.cpp:125] relu7 needs backward computation.
I0909 16:49:06.470496  8215 net.cpp:66] Creating Layer drop7
I0909 16:49:06.470501  8215 net.cpp:329] drop7 <- fc7
I0909 16:49:06.470509  8215 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:49:06.470520  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.470525  8215 net.cpp:125] drop7 needs backward computation.
I0909 16:49:06.470533  8215 net.cpp:66] Creating Layer fc8
I0909 16:49:06.470540  8215 net.cpp:329] fc8 <- fc7
I0909 16:49:06.470548  8215 net.cpp:290] fc8 -> fc8
I0909 16:49:06.478396  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.478423  8215 net.cpp:125] fc8 needs backward computation.
I0909 16:49:06.478432  8215 net.cpp:66] Creating Layer relu8
I0909 16:49:06.478440  8215 net.cpp:329] relu8 <- fc8
I0909 16:49:06.478448  8215 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:49:06.478457  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.478463  8215 net.cpp:125] relu8 needs backward computation.
I0909 16:49:06.478471  8215 net.cpp:66] Creating Layer drop8
I0909 16:49:06.478483  8215 net.cpp:329] drop8 <- fc8
I0909 16:49:06.478489  8215 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:49:06.478504  8215 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:49:06.478510  8215 net.cpp:125] drop8 needs backward computation.
I0909 16:49:06.478523  8215 net.cpp:66] Creating Layer fc9
I0909 16:49:06.478528  8215 net.cpp:329] fc9 <- fc8
I0909 16:49:06.478543  8215 net.cpp:290] fc9 -> fc9
I0909 16:49:06.478972  8215 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:49:06.479006  8215 net.cpp:125] fc9 needs backward computation.
I0909 16:49:06.479019  8215 net.cpp:66] Creating Layer fc10
I0909 16:49:06.479027  8215 net.cpp:329] fc10 <- fc9
I0909 16:49:06.479038  8215 net.cpp:290] fc10 -> fc10
I0909 16:49:06.479055  8215 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:49:06.479063  8215 net.cpp:125] fc10 needs backward computation.
I0909 16:49:06.479071  8215 net.cpp:66] Creating Layer prob
I0909 16:49:06.479078  8215 net.cpp:329] prob <- fc10
I0909 16:49:06.479085  8215 net.cpp:290] prob -> prob
I0909 16:49:06.479099  8215 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:49:06.479104  8215 net.cpp:125] prob needs backward computation.
I0909 16:49:06.479110  8215 net.cpp:156] This network produces output prob
I0909 16:49:06.479125  8215 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:49:06.479135  8215 net.cpp:167] Network initialization done.
I0909 16:49:06.479156  8215 net.cpp:168] Memory required for data: 6183480
Classifying 642 inputs.
Done in 424.02 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:56:40.394659  8297 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:56:40.394814  8297 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:56:40.394824  8297 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:56:40.394973  8297 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:56:40.395026  8297 net.cpp:292] Input 0 -> data
I0909 16:56:40.395053  8297 net.cpp:66] Creating Layer conv1
I0909 16:56:40.395061  8297 net.cpp:329] conv1 <- data
I0909 16:56:40.395068  8297 net.cpp:290] conv1 -> conv1
I0909 16:56:40.404666  8297 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:56:40.404716  8297 net.cpp:125] conv1 needs backward computation.
I0909 16:56:40.404737  8297 net.cpp:66] Creating Layer relu1
I0909 16:56:40.404750  8297 net.cpp:329] relu1 <- conv1
I0909 16:56:40.404764  8297 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:56:40.404783  8297 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:56:40.404795  8297 net.cpp:125] relu1 needs backward computation.
I0909 16:56:40.404810  8297 net.cpp:66] Creating Layer pool1
I0909 16:56:40.404821  8297 net.cpp:329] pool1 <- conv1
I0909 16:56:40.404835  8297 net.cpp:290] pool1 -> pool1
I0909 16:56:40.404857  8297 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:56:40.404870  8297 net.cpp:125] pool1 needs backward computation.
I0909 16:56:40.404883  8297 net.cpp:66] Creating Layer norm1
I0909 16:56:40.404894  8297 net.cpp:329] norm1 <- pool1
I0909 16:56:40.404908  8297 net.cpp:290] norm1 -> norm1
I0909 16:56:40.404928  8297 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:56:40.404939  8297 net.cpp:125] norm1 needs backward computation.
I0909 16:56:40.404955  8297 net.cpp:66] Creating Layer conv2
I0909 16:56:40.404968  8297 net.cpp:329] conv2 <- norm1
I0909 16:56:40.404981  8297 net.cpp:290] conv2 -> conv2
I0909 16:56:40.416445  8297 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:56:40.416466  8297 net.cpp:125] conv2 needs backward computation.
I0909 16:56:40.416474  8297 net.cpp:66] Creating Layer relu2
I0909 16:56:40.416481  8297 net.cpp:329] relu2 <- conv2
I0909 16:56:40.416488  8297 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:56:40.416496  8297 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:56:40.416501  8297 net.cpp:125] relu2 needs backward computation.
I0909 16:56:40.416508  8297 net.cpp:66] Creating Layer pool2
I0909 16:56:40.416514  8297 net.cpp:329] pool2 <- conv2
I0909 16:56:40.416522  8297 net.cpp:290] pool2 -> pool2
I0909 16:56:40.416529  8297 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:56:40.416535  8297 net.cpp:125] pool2 needs backward computation.
I0909 16:56:40.416543  8297 net.cpp:66] Creating Layer fc7
I0909 16:56:40.416548  8297 net.cpp:329] fc7 <- pool2
I0909 16:56:40.416555  8297 net.cpp:290] fc7 -> fc7
I0909 16:56:41.072108  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.072154  8297 net.cpp:125] fc7 needs backward computation.
I0909 16:56:41.072166  8297 net.cpp:66] Creating Layer relu7
I0909 16:56:41.072175  8297 net.cpp:329] relu7 <- fc7
I0909 16:56:41.072182  8297 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:56:41.072192  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.072197  8297 net.cpp:125] relu7 needs backward computation.
I0909 16:56:41.072206  8297 net.cpp:66] Creating Layer drop7
I0909 16:56:41.072211  8297 net.cpp:329] drop7 <- fc7
I0909 16:56:41.072217  8297 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:56:41.072228  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.072233  8297 net.cpp:125] drop7 needs backward computation.
I0909 16:56:41.072248  8297 net.cpp:66] Creating Layer fc8
I0909 16:56:41.072254  8297 net.cpp:329] fc8 <- fc7
I0909 16:56:41.072263  8297 net.cpp:290] fc8 -> fc8
I0909 16:56:41.080231  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.080257  8297 net.cpp:125] fc8 needs backward computation.
I0909 16:56:41.080267  8297 net.cpp:66] Creating Layer relu8
I0909 16:56:41.080273  8297 net.cpp:329] relu8 <- fc8
I0909 16:56:41.080282  8297 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:56:41.080291  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.080297  8297 net.cpp:125] relu8 needs backward computation.
I0909 16:56:41.080304  8297 net.cpp:66] Creating Layer drop8
I0909 16:56:41.080309  8297 net.cpp:329] drop8 <- fc8
I0909 16:56:41.080317  8297 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:56:41.080323  8297 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:56:41.080328  8297 net.cpp:125] drop8 needs backward computation.
I0909 16:56:41.080338  8297 net.cpp:66] Creating Layer fc9
I0909 16:56:41.080354  8297 net.cpp:329] fc9 <- fc8
I0909 16:56:41.080363  8297 net.cpp:290] fc9 -> fc9
I0909 16:56:41.080749  8297 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:56:41.080760  8297 net.cpp:125] fc9 needs backward computation.
I0909 16:56:41.080770  8297 net.cpp:66] Creating Layer fc10
I0909 16:56:41.080775  8297 net.cpp:329] fc10 <- fc9
I0909 16:56:41.080783  8297 net.cpp:290] fc10 -> fc10
I0909 16:56:41.080795  8297 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:56:41.080802  8297 net.cpp:125] fc10 needs backward computation.
I0909 16:56:41.080809  8297 net.cpp:66] Creating Layer prob
I0909 16:56:41.080814  8297 net.cpp:329] prob <- fc10
I0909 16:56:41.080822  8297 net.cpp:290] prob -> prob
I0909 16:56:41.080832  8297 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:56:41.080838  8297 net.cpp:125] prob needs backward computation.
I0909 16:56:41.080843  8297 net.cpp:156] This network produces output prob
I0909 16:56:41.080857  8297 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:56:41.080864  8297 net.cpp:167] Network initialization done.
I0909 16:56:41.080870  8297 net.cpp:168] Memory required for data: 6183480
Classifying 59 inputs.
Done in 39.54 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 16:57:24.775619  8318 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 16:57:24.775758  8318 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 16:57:24.775768  8318 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 16:57:24.775915  8318 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 16:57:24.775979  8318 net.cpp:292] Input 0 -> data
I0909 16:57:24.776005  8318 net.cpp:66] Creating Layer conv1
I0909 16:57:24.776011  8318 net.cpp:329] conv1 <- data
I0909 16:57:24.776020  8318 net.cpp:290] conv1 -> conv1
I0909 16:57:24.777382  8318 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:57:24.777401  8318 net.cpp:125] conv1 needs backward computation.
I0909 16:57:24.777410  8318 net.cpp:66] Creating Layer relu1
I0909 16:57:24.777415  8318 net.cpp:329] relu1 <- conv1
I0909 16:57:24.777422  8318 net.cpp:280] relu1 -> conv1 (in-place)
I0909 16:57:24.777431  8318 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 16:57:24.777436  8318 net.cpp:125] relu1 needs backward computation.
I0909 16:57:24.777443  8318 net.cpp:66] Creating Layer pool1
I0909 16:57:24.777449  8318 net.cpp:329] pool1 <- conv1
I0909 16:57:24.777456  8318 net.cpp:290] pool1 -> pool1
I0909 16:57:24.777467  8318 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:57:24.777472  8318 net.cpp:125] pool1 needs backward computation.
I0909 16:57:24.777479  8318 net.cpp:66] Creating Layer norm1
I0909 16:57:24.777484  8318 net.cpp:329] norm1 <- pool1
I0909 16:57:24.777492  8318 net.cpp:290] norm1 -> norm1
I0909 16:57:24.777500  8318 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 16:57:24.777506  8318 net.cpp:125] norm1 needs backward computation.
I0909 16:57:24.777530  8318 net.cpp:66] Creating Layer conv2
I0909 16:57:24.777539  8318 net.cpp:329] conv2 <- norm1
I0909 16:57:24.777546  8318 net.cpp:290] conv2 -> conv2
I0909 16:57:24.786430  8318 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:57:24.786444  8318 net.cpp:125] conv2 needs backward computation.
I0909 16:57:24.786452  8318 net.cpp:66] Creating Layer relu2
I0909 16:57:24.786456  8318 net.cpp:329] relu2 <- conv2
I0909 16:57:24.786463  8318 net.cpp:280] relu2 -> conv2 (in-place)
I0909 16:57:24.786469  8318 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 16:57:24.786474  8318 net.cpp:125] relu2 needs backward computation.
I0909 16:57:24.786480  8318 net.cpp:66] Creating Layer pool2
I0909 16:57:24.786486  8318 net.cpp:329] pool2 <- conv2
I0909 16:57:24.786492  8318 net.cpp:290] pool2 -> pool2
I0909 16:57:24.786500  8318 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 16:57:24.786505  8318 net.cpp:125] pool2 needs backward computation.
I0909 16:57:24.786512  8318 net.cpp:66] Creating Layer fc7
I0909 16:57:24.786519  8318 net.cpp:329] fc7 <- pool2
I0909 16:57:24.786527  8318 net.cpp:290] fc7 -> fc7
I0909 16:57:25.432554  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.432591  8318 net.cpp:125] fc7 needs backward computation.
I0909 16:57:25.432605  8318 net.cpp:66] Creating Layer relu7
I0909 16:57:25.432612  8318 net.cpp:329] relu7 <- fc7
I0909 16:57:25.432621  8318 net.cpp:280] relu7 -> fc7 (in-place)
I0909 16:57:25.432631  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.432636  8318 net.cpp:125] relu7 needs backward computation.
I0909 16:57:25.432644  8318 net.cpp:66] Creating Layer drop7
I0909 16:57:25.432649  8318 net.cpp:329] drop7 <- fc7
I0909 16:57:25.432657  8318 net.cpp:280] drop7 -> fc7 (in-place)
I0909 16:57:25.432667  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.432672  8318 net.cpp:125] drop7 needs backward computation.
I0909 16:57:25.432688  8318 net.cpp:66] Creating Layer fc8
I0909 16:57:25.432694  8318 net.cpp:329] fc8 <- fc7
I0909 16:57:25.432704  8318 net.cpp:290] fc8 -> fc8
I0909 16:57:25.441020  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.441053  8318 net.cpp:125] fc8 needs backward computation.
I0909 16:57:25.441063  8318 net.cpp:66] Creating Layer relu8
I0909 16:57:25.441071  8318 net.cpp:329] relu8 <- fc8
I0909 16:57:25.441079  8318 net.cpp:280] relu8 -> fc8 (in-place)
I0909 16:57:25.441088  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.441093  8318 net.cpp:125] relu8 needs backward computation.
I0909 16:57:25.441100  8318 net.cpp:66] Creating Layer drop8
I0909 16:57:25.441105  8318 net.cpp:329] drop8 <- fc8
I0909 16:57:25.441112  8318 net.cpp:280] drop8 -> fc8 (in-place)
I0909 16:57:25.441119  8318 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 16:57:25.441124  8318 net.cpp:125] drop8 needs backward computation.
I0909 16:57:25.441134  8318 net.cpp:66] Creating Layer fc9
I0909 16:57:25.441139  8318 net.cpp:329] fc9 <- fc8
I0909 16:57:25.441146  8318 net.cpp:290] fc9 -> fc9
I0909 16:57:25.441534  8318 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 16:57:25.441548  8318 net.cpp:125] fc9 needs backward computation.
I0909 16:57:25.441556  8318 net.cpp:66] Creating Layer fc10
I0909 16:57:25.441562  8318 net.cpp:329] fc10 <- fc9
I0909 16:57:25.441570  8318 net.cpp:290] fc10 -> fc10
I0909 16:57:25.441582  8318 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:57:25.441591  8318 net.cpp:125] fc10 needs backward computation.
I0909 16:57:25.441596  8318 net.cpp:66] Creating Layer prob
I0909 16:57:25.441602  8318 net.cpp:329] prob <- fc10
I0909 16:57:25.441609  8318 net.cpp:290] prob -> prob
I0909 16:57:25.441619  8318 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 16:57:25.441625  8318 net.cpp:125] prob needs backward computation.
I0909 16:57:25.441630  8318 net.cpp:156] This network produces output prob
I0909 16:57:25.441643  8318 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 16:57:25.441651  8318 net.cpp:167] Network initialization done.
I0909 16:57:25.441656  8318 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:37:13.213261  8491 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:37:13.213417  8491 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:37:13.213426  8491 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:37:13.213608  8491 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:37:13.213670  8491 net.cpp:292] Input 0 -> data
I0909 20:37:13.260156  8491 net.cpp:66] Creating Layer conv1
I0909 20:37:13.260171  8491 net.cpp:329] conv1 <- data
I0909 20:37:13.260181  8491 net.cpp:290] conv1 -> conv1
I0909 20:37:13.317383  8491 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:37:13.317406  8491 net.cpp:125] conv1 needs backward computation.
I0909 20:37:13.317416  8491 net.cpp:66] Creating Layer relu1
I0909 20:37:13.317423  8491 net.cpp:329] relu1 <- conv1
I0909 20:37:13.317430  8491 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:37:13.317438  8491 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:37:13.317445  8491 net.cpp:125] relu1 needs backward computation.
I0909 20:37:13.317451  8491 net.cpp:66] Creating Layer pool1
I0909 20:37:13.317456  8491 net.cpp:329] pool1 <- conv1
I0909 20:37:13.317463  8491 net.cpp:290] pool1 -> pool1
I0909 20:37:13.317474  8491 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:37:13.317481  8491 net.cpp:125] pool1 needs backward computation.
I0909 20:37:13.317487  8491 net.cpp:66] Creating Layer norm1
I0909 20:37:13.317493  8491 net.cpp:329] norm1 <- pool1
I0909 20:37:13.317499  8491 net.cpp:290] norm1 -> norm1
I0909 20:37:13.317509  8491 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:37:13.317533  8491 net.cpp:125] norm1 needs backward computation.
I0909 20:37:13.317541  8491 net.cpp:66] Creating Layer conv2
I0909 20:37:13.317548  8491 net.cpp:329] conv2 <- norm1
I0909 20:37:13.317555  8491 net.cpp:290] conv2 -> conv2
I0909 20:37:13.326519  8491 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:37:13.326534  8491 net.cpp:125] conv2 needs backward computation.
I0909 20:37:13.326542  8491 net.cpp:66] Creating Layer relu2
I0909 20:37:13.326547  8491 net.cpp:329] relu2 <- conv2
I0909 20:37:13.326555  8491 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:37:13.326561  8491 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:37:13.326566  8491 net.cpp:125] relu2 needs backward computation.
I0909 20:37:13.326573  8491 net.cpp:66] Creating Layer pool2
I0909 20:37:13.326578  8491 net.cpp:329] pool2 <- conv2
I0909 20:37:13.326585  8491 net.cpp:290] pool2 -> pool2
I0909 20:37:13.326592  8491 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:37:13.326604  8491 net.cpp:125] pool2 needs backward computation.
I0909 20:37:13.326613  8491 net.cpp:66] Creating Layer fc7
I0909 20:37:13.326619  8491 net.cpp:329] fc7 <- pool2
I0909 20:37:13.326627  8491 net.cpp:290] fc7 -> fc7
I0909 20:37:13.970115  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.970161  8491 net.cpp:125] fc7 needs backward computation.
I0909 20:37:13.970176  8491 net.cpp:66] Creating Layer relu7
I0909 20:37:13.970183  8491 net.cpp:329] relu7 <- fc7
I0909 20:37:13.970191  8491 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:37:13.970202  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.970207  8491 net.cpp:125] relu7 needs backward computation.
I0909 20:37:13.970216  8491 net.cpp:66] Creating Layer drop7
I0909 20:37:13.970221  8491 net.cpp:329] drop7 <- fc7
I0909 20:37:13.970228  8491 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:37:13.970238  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.970244  8491 net.cpp:125] drop7 needs backward computation.
I0909 20:37:13.970253  8491 net.cpp:66] Creating Layer fc8
I0909 20:37:13.970258  8491 net.cpp:329] fc8 <- fc7
I0909 20:37:13.970266  8491 net.cpp:290] fc8 -> fc8
I0909 20:37:13.978045  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.978059  8491 net.cpp:125] fc8 needs backward computation.
I0909 20:37:13.978066  8491 net.cpp:66] Creating Layer relu8
I0909 20:37:13.978072  8491 net.cpp:329] relu8 <- fc8
I0909 20:37:13.978080  8491 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:37:13.978086  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.978092  8491 net.cpp:125] relu8 needs backward computation.
I0909 20:37:13.978099  8491 net.cpp:66] Creating Layer drop8
I0909 20:37:13.978104  8491 net.cpp:329] drop8 <- fc8
I0909 20:37:13.978112  8491 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:37:13.978119  8491 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:37:13.978126  8491 net.cpp:125] drop8 needs backward computation.
I0909 20:37:13.978133  8491 net.cpp:66] Creating Layer fc9
I0909 20:37:13.978139  8491 net.cpp:329] fc9 <- fc8
I0909 20:37:13.978147  8491 net.cpp:290] fc9 -> fc9
I0909 20:37:13.978520  8491 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:37:13.978531  8491 net.cpp:125] fc9 needs backward computation.
I0909 20:37:13.978540  8491 net.cpp:66] Creating Layer fc10
I0909 20:37:13.978545  8491 net.cpp:329] fc10 <- fc9
I0909 20:37:13.978554  8491 net.cpp:290] fc10 -> fc10
I0909 20:37:13.978566  8491 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:37:13.978576  8491 net.cpp:125] fc10 needs backward computation.
I0909 20:37:13.978584  8491 net.cpp:66] Creating Layer prob
I0909 20:37:13.978588  8491 net.cpp:329] prob <- fc10
I0909 20:37:13.978595  8491 net.cpp:290] prob -> prob
I0909 20:37:13.978605  8491 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:37:13.978611  8491 net.cpp:125] prob needs backward computation.
I0909 20:37:13.978616  8491 net.cpp:156] This network produces output prob
I0909 20:37:13.978629  8491 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:37:13.978637  8491 net.cpp:167] Network initialization done.
I0909 20:37:13.978643  8491 net.cpp:168] Memory required for data: 6183480
Classifying 237 inputs.
Done in 156.61 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:41:13.243190  8639 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:41:13.243330  8639 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:41:13.243340  8639 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:41:13.243496  8639 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:41:13.243562  8639 net.cpp:292] Input 0 -> data
I0909 20:41:13.243589  8639 net.cpp:66] Creating Layer conv1
I0909 20:41:13.243595  8639 net.cpp:329] conv1 <- data
I0909 20:41:13.243603  8639 net.cpp:290] conv1 -> conv1
I0909 20:41:13.244972  8639 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:13.244992  8639 net.cpp:125] conv1 needs backward computation.
I0909 20:41:13.244999  8639 net.cpp:66] Creating Layer relu1
I0909 20:41:13.245005  8639 net.cpp:329] relu1 <- conv1
I0909 20:41:13.245012  8639 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:41:13.245021  8639 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:13.245028  8639 net.cpp:125] relu1 needs backward computation.
I0909 20:41:13.245034  8639 net.cpp:66] Creating Layer pool1
I0909 20:41:13.245039  8639 net.cpp:329] pool1 <- conv1
I0909 20:41:13.245046  8639 net.cpp:290] pool1 -> pool1
I0909 20:41:13.245057  8639 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:13.245064  8639 net.cpp:125] pool1 needs backward computation.
I0909 20:41:13.245070  8639 net.cpp:66] Creating Layer norm1
I0909 20:41:13.245075  8639 net.cpp:329] norm1 <- pool1
I0909 20:41:13.245082  8639 net.cpp:290] norm1 -> norm1
I0909 20:41:13.245096  8639 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:13.245102  8639 net.cpp:125] norm1 needs backward computation.
I0909 20:41:13.245110  8639 net.cpp:66] Creating Layer conv2
I0909 20:41:13.245116  8639 net.cpp:329] conv2 <- norm1
I0909 20:41:13.245123  8639 net.cpp:290] conv2 -> conv2
I0909 20:41:13.254411  8639 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:13.254428  8639 net.cpp:125] conv2 needs backward computation.
I0909 20:41:13.254436  8639 net.cpp:66] Creating Layer relu2
I0909 20:41:13.254441  8639 net.cpp:329] relu2 <- conv2
I0909 20:41:13.254448  8639 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:41:13.254456  8639 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:13.254461  8639 net.cpp:125] relu2 needs backward computation.
I0909 20:41:13.254467  8639 net.cpp:66] Creating Layer pool2
I0909 20:41:13.254472  8639 net.cpp:329] pool2 <- conv2
I0909 20:41:13.254479  8639 net.cpp:290] pool2 -> pool2
I0909 20:41:13.254487  8639 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:41:13.254492  8639 net.cpp:125] pool2 needs backward computation.
I0909 20:41:13.254504  8639 net.cpp:66] Creating Layer fc7
I0909 20:41:13.254510  8639 net.cpp:329] fc7 <- pool2
I0909 20:41:13.254518  8639 net.cpp:290] fc7 -> fc7
I0909 20:41:13.898172  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.898216  8639 net.cpp:125] fc7 needs backward computation.
I0909 20:41:13.898229  8639 net.cpp:66] Creating Layer relu7
I0909 20:41:13.898236  8639 net.cpp:329] relu7 <- fc7
I0909 20:41:13.898246  8639 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:41:13.898254  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.898260  8639 net.cpp:125] relu7 needs backward computation.
I0909 20:41:13.898267  8639 net.cpp:66] Creating Layer drop7
I0909 20:41:13.898273  8639 net.cpp:329] drop7 <- fc7
I0909 20:41:13.898282  8639 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:41:13.898291  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.898298  8639 net.cpp:125] drop7 needs backward computation.
I0909 20:41:13.898306  8639 net.cpp:66] Creating Layer fc8
I0909 20:41:13.898311  8639 net.cpp:329] fc8 <- fc7
I0909 20:41:13.898319  8639 net.cpp:290] fc8 -> fc8
I0909 20:41:13.906110  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.906121  8639 net.cpp:125] fc8 needs backward computation.
I0909 20:41:13.906129  8639 net.cpp:66] Creating Layer relu8
I0909 20:41:13.906134  8639 net.cpp:329] relu8 <- fc8
I0909 20:41:13.906141  8639 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:41:13.906148  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.906153  8639 net.cpp:125] relu8 needs backward computation.
I0909 20:41:13.906160  8639 net.cpp:66] Creating Layer drop8
I0909 20:41:13.906165  8639 net.cpp:329] drop8 <- fc8
I0909 20:41:13.906173  8639 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:41:13.906180  8639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:13.906186  8639 net.cpp:125] drop8 needs backward computation.
I0909 20:41:13.906193  8639 net.cpp:66] Creating Layer fc9
I0909 20:41:13.906199  8639 net.cpp:329] fc9 <- fc8
I0909 20:41:13.906208  8639 net.cpp:290] fc9 -> fc9
I0909 20:41:13.906581  8639 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:41:13.906592  8639 net.cpp:125] fc9 needs backward computation.
I0909 20:41:13.906600  8639 net.cpp:66] Creating Layer fc10
I0909 20:41:13.906605  8639 net.cpp:329] fc10 <- fc9
I0909 20:41:13.906615  8639 net.cpp:290] fc10 -> fc10
I0909 20:41:13.906625  8639 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:13.906635  8639 net.cpp:125] fc10 needs backward computation.
I0909 20:41:13.906641  8639 net.cpp:66] Creating Layer prob
I0909 20:41:13.906646  8639 net.cpp:329] prob <- fc10
I0909 20:41:13.906653  8639 net.cpp:290] prob -> prob
I0909 20:41:13.906662  8639 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:13.906668  8639 net.cpp:125] prob needs backward computation.
I0909 20:41:13.906673  8639 net.cpp:156] This network produces output prob
I0909 20:41:13.906685  8639 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:41:13.906703  8639 net.cpp:167] Network initialization done.
I0909 20:41:13.906709  8639 net.cpp:168] Memory required for data: 6183480
Classifying 350 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:41:24.996055  8642 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:41:24.996192  8642 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:41:24.996201  8642 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:41:24.996345  8642 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:41:24.996408  8642 net.cpp:292] Input 0 -> data
I0909 20:41:24.996433  8642 net.cpp:66] Creating Layer conv1
I0909 20:41:24.996440  8642 net.cpp:329] conv1 <- data
I0909 20:41:24.996448  8642 net.cpp:290] conv1 -> conv1
I0909 20:41:24.997807  8642 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:24.997828  8642 net.cpp:125] conv1 needs backward computation.
I0909 20:41:24.997836  8642 net.cpp:66] Creating Layer relu1
I0909 20:41:24.997841  8642 net.cpp:329] relu1 <- conv1
I0909 20:41:24.997848  8642 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:41:24.997858  8642 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:24.997863  8642 net.cpp:125] relu1 needs backward computation.
I0909 20:41:24.997869  8642 net.cpp:66] Creating Layer pool1
I0909 20:41:24.997874  8642 net.cpp:329] pool1 <- conv1
I0909 20:41:24.997881  8642 net.cpp:290] pool1 -> pool1
I0909 20:41:24.997892  8642 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:24.997898  8642 net.cpp:125] pool1 needs backward computation.
I0909 20:41:24.997905  8642 net.cpp:66] Creating Layer norm1
I0909 20:41:24.997910  8642 net.cpp:329] norm1 <- pool1
I0909 20:41:24.997916  8642 net.cpp:290] norm1 -> norm1
I0909 20:41:24.997925  8642 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:24.997931  8642 net.cpp:125] norm1 needs backward computation.
I0909 20:41:24.997938  8642 net.cpp:66] Creating Layer conv2
I0909 20:41:24.997944  8642 net.cpp:329] conv2 <- norm1
I0909 20:41:24.997951  8642 net.cpp:290] conv2 -> conv2
I0909 20:41:25.006834  8642 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:25.006849  8642 net.cpp:125] conv2 needs backward computation.
I0909 20:41:25.006855  8642 net.cpp:66] Creating Layer relu2
I0909 20:41:25.006861  8642 net.cpp:329] relu2 <- conv2
I0909 20:41:25.006867  8642 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:41:25.006875  8642 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:25.006880  8642 net.cpp:125] relu2 needs backward computation.
I0909 20:41:25.006886  8642 net.cpp:66] Creating Layer pool2
I0909 20:41:25.006891  8642 net.cpp:329] pool2 <- conv2
I0909 20:41:25.006898  8642 net.cpp:290] pool2 -> pool2
I0909 20:41:25.006906  8642 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:41:25.006911  8642 net.cpp:125] pool2 needs backward computation.
I0909 20:41:25.006918  8642 net.cpp:66] Creating Layer fc7
I0909 20:41:25.006924  8642 net.cpp:329] fc7 <- pool2
I0909 20:41:25.006933  8642 net.cpp:290] fc7 -> fc7
I0909 20:41:25.645828  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.645871  8642 net.cpp:125] fc7 needs backward computation.
I0909 20:41:25.645889  8642 net.cpp:66] Creating Layer relu7
I0909 20:41:25.645895  8642 net.cpp:329] relu7 <- fc7
I0909 20:41:25.645903  8642 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:41:25.645913  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.645920  8642 net.cpp:125] relu7 needs backward computation.
I0909 20:41:25.645927  8642 net.cpp:66] Creating Layer drop7
I0909 20:41:25.645932  8642 net.cpp:329] drop7 <- fc7
I0909 20:41:25.645941  8642 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:41:25.645951  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.645957  8642 net.cpp:125] drop7 needs backward computation.
I0909 20:41:25.645966  8642 net.cpp:66] Creating Layer fc8
I0909 20:41:25.645972  8642 net.cpp:329] fc8 <- fc7
I0909 20:41:25.645978  8642 net.cpp:290] fc8 -> fc8
I0909 20:41:25.653748  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.653759  8642 net.cpp:125] fc8 needs backward computation.
I0909 20:41:25.653767  8642 net.cpp:66] Creating Layer relu8
I0909 20:41:25.653772  8642 net.cpp:329] relu8 <- fc8
I0909 20:41:25.653779  8642 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:41:25.653786  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.653802  8642 net.cpp:125] relu8 needs backward computation.
I0909 20:41:25.653810  8642 net.cpp:66] Creating Layer drop8
I0909 20:41:25.653815  8642 net.cpp:329] drop8 <- fc8
I0909 20:41:25.653823  8642 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:41:25.653831  8642 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:25.653836  8642 net.cpp:125] drop8 needs backward computation.
I0909 20:41:25.653844  8642 net.cpp:66] Creating Layer fc9
I0909 20:41:25.653849  8642 net.cpp:329] fc9 <- fc8
I0909 20:41:25.653857  8642 net.cpp:290] fc9 -> fc9
I0909 20:41:25.654232  8642 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:41:25.654242  8642 net.cpp:125] fc9 needs backward computation.
I0909 20:41:25.654249  8642 net.cpp:66] Creating Layer fc10
I0909 20:41:25.654255  8642 net.cpp:329] fc10 <- fc9
I0909 20:41:25.654263  8642 net.cpp:290] fc10 -> fc10
I0909 20:41:25.654275  8642 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:25.654284  8642 net.cpp:125] fc10 needs backward computation.
I0909 20:41:25.654292  8642 net.cpp:66] Creating Layer prob
I0909 20:41:25.654297  8642 net.cpp:329] prob <- fc10
I0909 20:41:25.654304  8642 net.cpp:290] prob -> prob
I0909 20:41:25.654314  8642 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:25.654320  8642 net.cpp:125] prob needs backward computation.
I0909 20:41:25.654325  8642 net.cpp:156] This network produces output prob
I0909 20:41:25.654337  8642 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:41:25.654345  8642 net.cpp:167] Network initialization done.
I0909 20:41:25.654350  8642 net.cpp:168] Memory required for data: 6183480
Classifying 28 inputs.
Done in 18.15 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:41:45.065124  8645 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:41:45.065263  8645 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:41:45.065274  8645 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:41:45.065417  8645 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:41:45.065479  8645 net.cpp:292] Input 0 -> data
I0909 20:41:45.065505  8645 net.cpp:66] Creating Layer conv1
I0909 20:41:45.065538  8645 net.cpp:329] conv1 <- data
I0909 20:41:45.065556  8645 net.cpp:290] conv1 -> conv1
I0909 20:41:45.066910  8645 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:45.066927  8645 net.cpp:125] conv1 needs backward computation.
I0909 20:41:45.066936  8645 net.cpp:66] Creating Layer relu1
I0909 20:41:45.066942  8645 net.cpp:329] relu1 <- conv1
I0909 20:41:45.066949  8645 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:41:45.066957  8645 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:41:45.066963  8645 net.cpp:125] relu1 needs backward computation.
I0909 20:41:45.066970  8645 net.cpp:66] Creating Layer pool1
I0909 20:41:45.066975  8645 net.cpp:329] pool1 <- conv1
I0909 20:41:45.066982  8645 net.cpp:290] pool1 -> pool1
I0909 20:41:45.066992  8645 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:45.066998  8645 net.cpp:125] pool1 needs backward computation.
I0909 20:41:45.067005  8645 net.cpp:66] Creating Layer norm1
I0909 20:41:45.067010  8645 net.cpp:329] norm1 <- pool1
I0909 20:41:45.067018  8645 net.cpp:290] norm1 -> norm1
I0909 20:41:45.067026  8645 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:41:45.067033  8645 net.cpp:125] norm1 needs backward computation.
I0909 20:41:45.067039  8645 net.cpp:66] Creating Layer conv2
I0909 20:41:45.067045  8645 net.cpp:329] conv2 <- norm1
I0909 20:41:45.067052  8645 net.cpp:290] conv2 -> conv2
I0909 20:41:45.076124  8645 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:45.076154  8645 net.cpp:125] conv2 needs backward computation.
I0909 20:41:45.076164  8645 net.cpp:66] Creating Layer relu2
I0909 20:41:45.076169  8645 net.cpp:329] relu2 <- conv2
I0909 20:41:45.076176  8645 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:41:45.076185  8645 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:41:45.076190  8645 net.cpp:125] relu2 needs backward computation.
I0909 20:41:45.076199  8645 net.cpp:66] Creating Layer pool2
I0909 20:41:45.076203  8645 net.cpp:329] pool2 <- conv2
I0909 20:41:45.076210  8645 net.cpp:290] pool2 -> pool2
I0909 20:41:45.076220  8645 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:41:45.076225  8645 net.cpp:125] pool2 needs backward computation.
I0909 20:41:45.076236  8645 net.cpp:66] Creating Layer fc7
I0909 20:41:45.076242  8645 net.cpp:329] fc7 <- pool2
I0909 20:41:45.076251  8645 net.cpp:290] fc7 -> fc7
I0909 20:41:45.718399  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.718441  8645 net.cpp:125] fc7 needs backward computation.
I0909 20:41:45.718454  8645 net.cpp:66] Creating Layer relu7
I0909 20:41:45.718461  8645 net.cpp:329] relu7 <- fc7
I0909 20:41:45.718479  8645 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:41:45.718490  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.718497  8645 net.cpp:125] relu7 needs backward computation.
I0909 20:41:45.718503  8645 net.cpp:66] Creating Layer drop7
I0909 20:41:45.718509  8645 net.cpp:329] drop7 <- fc7
I0909 20:41:45.718516  8645 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:41:45.718528  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.718533  8645 net.cpp:125] drop7 needs backward computation.
I0909 20:41:45.718543  8645 net.cpp:66] Creating Layer fc8
I0909 20:41:45.718549  8645 net.cpp:329] fc8 <- fc7
I0909 20:41:45.718555  8645 net.cpp:290] fc8 -> fc8
I0909 20:41:45.726346  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.726359  8645 net.cpp:125] fc8 needs backward computation.
I0909 20:41:45.726366  8645 net.cpp:66] Creating Layer relu8
I0909 20:41:45.726372  8645 net.cpp:329] relu8 <- fc8
I0909 20:41:45.726378  8645 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:41:45.726385  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.726392  8645 net.cpp:125] relu8 needs backward computation.
I0909 20:41:45.726398  8645 net.cpp:66] Creating Layer drop8
I0909 20:41:45.726403  8645 net.cpp:329] drop8 <- fc8
I0909 20:41:45.726411  8645 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:41:45.726418  8645 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:41:45.726423  8645 net.cpp:125] drop8 needs backward computation.
I0909 20:41:45.726431  8645 net.cpp:66] Creating Layer fc9
I0909 20:41:45.726436  8645 net.cpp:329] fc9 <- fc8
I0909 20:41:45.726444  8645 net.cpp:290] fc9 -> fc9
I0909 20:41:45.726817  8645 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:41:45.726829  8645 net.cpp:125] fc9 needs backward computation.
I0909 20:41:45.726836  8645 net.cpp:66] Creating Layer fc10
I0909 20:41:45.726842  8645 net.cpp:329] fc10 <- fc9
I0909 20:41:45.726850  8645 net.cpp:290] fc10 -> fc10
I0909 20:41:45.726862  8645 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:45.726871  8645 net.cpp:125] fc10 needs backward computation.
I0909 20:41:45.726878  8645 net.cpp:66] Creating Layer prob
I0909 20:41:45.726884  8645 net.cpp:329] prob <- fc10
I0909 20:41:45.726891  8645 net.cpp:290] prob -> prob
I0909 20:41:45.726901  8645 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:41:45.726907  8645 net.cpp:125] prob needs backward computation.
I0909 20:41:45.726912  8645 net.cpp:156] This network produces output prob
I0909 20:41:45.726923  8645 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:41:45.726932  8645 net.cpp:167] Network initialization done.
I0909 20:41:45.726936  8645 net.cpp:168] Memory required for data: 6183480
Classifying 163 inputs.
Done in 97.19 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:43:27.430603  8650 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:43:27.430744  8650 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:43:27.430753  8650 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:43:27.430899  8650 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:43:27.430964  8650 net.cpp:292] Input 0 -> data
I0909 20:43:27.430991  8650 net.cpp:66] Creating Layer conv1
I0909 20:43:27.430999  8650 net.cpp:329] conv1 <- data
I0909 20:43:27.431006  8650 net.cpp:290] conv1 -> conv1
I0909 20:43:27.432368  8650 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:43:27.432384  8650 net.cpp:125] conv1 needs backward computation.
I0909 20:43:27.432394  8650 net.cpp:66] Creating Layer relu1
I0909 20:43:27.432400  8650 net.cpp:329] relu1 <- conv1
I0909 20:43:27.432406  8650 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:43:27.432415  8650 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:43:27.432421  8650 net.cpp:125] relu1 needs backward computation.
I0909 20:43:27.432427  8650 net.cpp:66] Creating Layer pool1
I0909 20:43:27.432433  8650 net.cpp:329] pool1 <- conv1
I0909 20:43:27.432440  8650 net.cpp:290] pool1 -> pool1
I0909 20:43:27.432451  8650 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:43:27.432457  8650 net.cpp:125] pool1 needs backward computation.
I0909 20:43:27.432464  8650 net.cpp:66] Creating Layer norm1
I0909 20:43:27.432469  8650 net.cpp:329] norm1 <- pool1
I0909 20:43:27.432476  8650 net.cpp:290] norm1 -> norm1
I0909 20:43:27.432487  8650 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:43:27.432492  8650 net.cpp:125] norm1 needs backward computation.
I0909 20:43:27.432499  8650 net.cpp:66] Creating Layer conv2
I0909 20:43:27.432505  8650 net.cpp:329] conv2 <- norm1
I0909 20:43:27.432512  8650 net.cpp:290] conv2 -> conv2
I0909 20:43:27.441627  8650 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:43:27.441642  8650 net.cpp:125] conv2 needs backward computation.
I0909 20:43:27.441649  8650 net.cpp:66] Creating Layer relu2
I0909 20:43:27.441660  8650 net.cpp:329] relu2 <- conv2
I0909 20:43:27.441668  8650 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:43:27.441674  8650 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:43:27.441680  8650 net.cpp:125] relu2 needs backward computation.
I0909 20:43:27.441686  8650 net.cpp:66] Creating Layer pool2
I0909 20:43:27.441691  8650 net.cpp:329] pool2 <- conv2
I0909 20:43:27.441699  8650 net.cpp:290] pool2 -> pool2
I0909 20:43:27.441706  8650 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:43:27.441712  8650 net.cpp:125] pool2 needs backward computation.
I0909 20:43:27.441721  8650 net.cpp:66] Creating Layer fc7
I0909 20:43:27.441727  8650 net.cpp:329] fc7 <- pool2
I0909 20:43:27.441735  8650 net.cpp:290] fc7 -> fc7
I0909 20:43:28.083047  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.083092  8650 net.cpp:125] fc7 needs backward computation.
I0909 20:43:28.083106  8650 net.cpp:66] Creating Layer relu7
I0909 20:43:28.083114  8650 net.cpp:329] relu7 <- fc7
I0909 20:43:28.083122  8650 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:43:28.083132  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.083138  8650 net.cpp:125] relu7 needs backward computation.
I0909 20:43:28.083145  8650 net.cpp:66] Creating Layer drop7
I0909 20:43:28.083150  8650 net.cpp:329] drop7 <- fc7
I0909 20:43:28.083158  8650 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:43:28.083169  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.083175  8650 net.cpp:125] drop7 needs backward computation.
I0909 20:43:28.083184  8650 net.cpp:66] Creating Layer fc8
I0909 20:43:28.083189  8650 net.cpp:329] fc8 <- fc7
I0909 20:43:28.083196  8650 net.cpp:290] fc8 -> fc8
I0909 20:43:28.090987  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.091001  8650 net.cpp:125] fc8 needs backward computation.
I0909 20:43:28.091007  8650 net.cpp:66] Creating Layer relu8
I0909 20:43:28.091013  8650 net.cpp:329] relu8 <- fc8
I0909 20:43:28.091019  8650 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:43:28.091027  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.091032  8650 net.cpp:125] relu8 needs backward computation.
I0909 20:43:28.091039  8650 net.cpp:66] Creating Layer drop8
I0909 20:43:28.091044  8650 net.cpp:329] drop8 <- fc8
I0909 20:43:28.091053  8650 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:43:28.091059  8650 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:28.091065  8650 net.cpp:125] drop8 needs backward computation.
I0909 20:43:28.091073  8650 net.cpp:66] Creating Layer fc9
I0909 20:43:28.091078  8650 net.cpp:329] fc9 <- fc8
I0909 20:43:28.091086  8650 net.cpp:290] fc9 -> fc9
I0909 20:43:28.091459  8650 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:43:28.091470  8650 net.cpp:125] fc9 needs backward computation.
I0909 20:43:28.091478  8650 net.cpp:66] Creating Layer fc10
I0909 20:43:28.091485  8650 net.cpp:329] fc10 <- fc9
I0909 20:43:28.091492  8650 net.cpp:290] fc10 -> fc10
I0909 20:43:28.091505  8650 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:43:28.091514  8650 net.cpp:125] fc10 needs backward computation.
I0909 20:43:28.091521  8650 net.cpp:66] Creating Layer prob
I0909 20:43:28.091526  8650 net.cpp:329] prob <- fc10
I0909 20:43:28.091532  8650 net.cpp:290] prob -> prob
I0909 20:43:28.091542  8650 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:43:28.091547  8650 net.cpp:125] prob needs backward computation.
I0909 20:43:28.091552  8650 net.cpp:156] This network produces output prob
I0909 20:43:28.091565  8650 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:43:28.091573  8650 net.cpp:167] Network initialization done.
I0909 20:43:28.091579  8650 net.cpp:168] Memory required for data: 6183480
Classifying 8 inputs.
Done in 5.25 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:43:34.170922  8653 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:43:34.171061  8653 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:43:34.171082  8653 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:43:34.171231  8653 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:43:34.171285  8653 net.cpp:292] Input 0 -> data
I0909 20:43:34.171311  8653 net.cpp:66] Creating Layer conv1
I0909 20:43:34.171319  8653 net.cpp:329] conv1 <- data
I0909 20:43:34.171326  8653 net.cpp:290] conv1 -> conv1
I0909 20:43:34.172726  8653 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:43:34.172745  8653 net.cpp:125] conv1 needs backward computation.
I0909 20:43:34.172755  8653 net.cpp:66] Creating Layer relu1
I0909 20:43:34.172761  8653 net.cpp:329] relu1 <- conv1
I0909 20:43:34.172768  8653 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:43:34.172785  8653 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:43:34.172791  8653 net.cpp:125] relu1 needs backward computation.
I0909 20:43:34.172801  8653 net.cpp:66] Creating Layer pool1
I0909 20:43:34.172807  8653 net.cpp:329] pool1 <- conv1
I0909 20:43:34.172814  8653 net.cpp:290] pool1 -> pool1
I0909 20:43:34.172826  8653 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:43:34.172832  8653 net.cpp:125] pool1 needs backward computation.
I0909 20:43:34.172838  8653 net.cpp:66] Creating Layer norm1
I0909 20:43:34.172844  8653 net.cpp:329] norm1 <- pool1
I0909 20:43:34.172852  8653 net.cpp:290] norm1 -> norm1
I0909 20:43:34.172862  8653 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:43:34.172866  8653 net.cpp:125] norm1 needs backward computation.
I0909 20:43:34.172874  8653 net.cpp:66] Creating Layer conv2
I0909 20:43:34.172880  8653 net.cpp:329] conv2 <- norm1
I0909 20:43:34.172888  8653 net.cpp:290] conv2 -> conv2
I0909 20:43:34.182168  8653 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:43:34.182186  8653 net.cpp:125] conv2 needs backward computation.
I0909 20:43:34.182194  8653 net.cpp:66] Creating Layer relu2
I0909 20:43:34.182199  8653 net.cpp:329] relu2 <- conv2
I0909 20:43:34.182206  8653 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:43:34.182214  8653 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:43:34.182219  8653 net.cpp:125] relu2 needs backward computation.
I0909 20:43:34.182225  8653 net.cpp:66] Creating Layer pool2
I0909 20:43:34.182231  8653 net.cpp:329] pool2 <- conv2
I0909 20:43:34.182237  8653 net.cpp:290] pool2 -> pool2
I0909 20:43:34.182245  8653 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:43:34.182251  8653 net.cpp:125] pool2 needs backward computation.
I0909 20:43:34.182261  8653 net.cpp:66] Creating Layer fc7
I0909 20:43:34.182267  8653 net.cpp:329] fc7 <- pool2
I0909 20:43:34.182274  8653 net.cpp:290] fc7 -> fc7
I0909 20:43:34.840152  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.840193  8653 net.cpp:125] fc7 needs backward computation.
I0909 20:43:34.840208  8653 net.cpp:66] Creating Layer relu7
I0909 20:43:34.840215  8653 net.cpp:329] relu7 <- fc7
I0909 20:43:34.840222  8653 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:43:34.840232  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.840239  8653 net.cpp:125] relu7 needs backward computation.
I0909 20:43:34.840245  8653 net.cpp:66] Creating Layer drop7
I0909 20:43:34.840251  8653 net.cpp:329] drop7 <- fc7
I0909 20:43:34.840258  8653 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:43:34.840270  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.840276  8653 net.cpp:125] drop7 needs backward computation.
I0909 20:43:34.840284  8653 net.cpp:66] Creating Layer fc8
I0909 20:43:34.840289  8653 net.cpp:329] fc8 <- fc7
I0909 20:43:34.840296  8653 net.cpp:290] fc8 -> fc8
I0909 20:43:34.848047  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.848060  8653 net.cpp:125] fc8 needs backward computation.
I0909 20:43:34.848067  8653 net.cpp:66] Creating Layer relu8
I0909 20:43:34.848073  8653 net.cpp:329] relu8 <- fc8
I0909 20:43:34.848078  8653 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:43:34.848085  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.848091  8653 net.cpp:125] relu8 needs backward computation.
I0909 20:43:34.848098  8653 net.cpp:66] Creating Layer drop8
I0909 20:43:34.848103  8653 net.cpp:329] drop8 <- fc8
I0909 20:43:34.848109  8653 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:43:34.848116  8653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:43:34.848122  8653 net.cpp:125] drop8 needs backward computation.
I0909 20:43:34.848129  8653 net.cpp:66] Creating Layer fc9
I0909 20:43:34.848134  8653 net.cpp:329] fc9 <- fc8
I0909 20:43:34.848145  8653 net.cpp:290] fc9 -> fc9
I0909 20:43:34.848515  8653 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:43:34.848526  8653 net.cpp:125] fc9 needs backward computation.
I0909 20:43:34.848533  8653 net.cpp:66] Creating Layer fc10
I0909 20:43:34.848538  8653 net.cpp:329] fc10 <- fc9
I0909 20:43:34.848546  8653 net.cpp:290] fc10 -> fc10
I0909 20:43:34.848558  8653 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:43:34.848577  8653 net.cpp:125] fc10 needs backward computation.
I0909 20:43:34.848583  8653 net.cpp:66] Creating Layer prob
I0909 20:43:34.848589  8653 net.cpp:329] prob <- fc10
I0909 20:43:34.848595  8653 net.cpp:290] prob -> prob
I0909 20:43:34.848604  8653 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:43:34.848610  8653 net.cpp:125] prob needs backward computation.
I0909 20:43:34.848615  8653 net.cpp:156] This network produces output prob
I0909 20:43:34.848628  8653 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:43:34.848635  8653 net.cpp:167] Network initialization done.
I0909 20:43:34.848640  8653 net.cpp:168] Memory required for data: 6183480
Classifying 244 inputs.
Done in 151.47 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:46:11.548223  8663 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:46:11.548364  8663 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:46:11.548373  8663 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:46:11.548521  8663 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:46:11.548589  8663 net.cpp:292] Input 0 -> data
I0909 20:46:11.548614  8663 net.cpp:66] Creating Layer conv1
I0909 20:46:11.548622  8663 net.cpp:329] conv1 <- data
I0909 20:46:11.548630  8663 net.cpp:290] conv1 -> conv1
I0909 20:46:11.550024  8663 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:46:11.550043  8663 net.cpp:125] conv1 needs backward computation.
I0909 20:46:11.550052  8663 net.cpp:66] Creating Layer relu1
I0909 20:46:11.550058  8663 net.cpp:329] relu1 <- conv1
I0909 20:46:11.550065  8663 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:46:11.550075  8663 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:46:11.550081  8663 net.cpp:125] relu1 needs backward computation.
I0909 20:46:11.550087  8663 net.cpp:66] Creating Layer pool1
I0909 20:46:11.550092  8663 net.cpp:329] pool1 <- conv1
I0909 20:46:11.550099  8663 net.cpp:290] pool1 -> pool1
I0909 20:46:11.550112  8663 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:46:11.550117  8663 net.cpp:125] pool1 needs backward computation.
I0909 20:46:11.550123  8663 net.cpp:66] Creating Layer norm1
I0909 20:46:11.550129  8663 net.cpp:329] norm1 <- pool1
I0909 20:46:11.550137  8663 net.cpp:290] norm1 -> norm1
I0909 20:46:11.550145  8663 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:46:11.550151  8663 net.cpp:125] norm1 needs backward computation.
I0909 20:46:11.550158  8663 net.cpp:66] Creating Layer conv2
I0909 20:46:11.550164  8663 net.cpp:329] conv2 <- norm1
I0909 20:46:11.550171  8663 net.cpp:290] conv2 -> conv2
I0909 20:46:11.559281  8663 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:46:11.559296  8663 net.cpp:125] conv2 needs backward computation.
I0909 20:46:11.559303  8663 net.cpp:66] Creating Layer relu2
I0909 20:46:11.559309  8663 net.cpp:329] relu2 <- conv2
I0909 20:46:11.559315  8663 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:46:11.559322  8663 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:46:11.559329  8663 net.cpp:125] relu2 needs backward computation.
I0909 20:46:11.559334  8663 net.cpp:66] Creating Layer pool2
I0909 20:46:11.559340  8663 net.cpp:329] pool2 <- conv2
I0909 20:46:11.559346  8663 net.cpp:290] pool2 -> pool2
I0909 20:46:11.559355  8663 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:46:11.559360  8663 net.cpp:125] pool2 needs backward computation.
I0909 20:46:11.559367  8663 net.cpp:66] Creating Layer fc7
I0909 20:46:11.559373  8663 net.cpp:329] fc7 <- pool2
I0909 20:46:11.559382  8663 net.cpp:290] fc7 -> fc7
I0909 20:46:12.198725  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.198770  8663 net.cpp:125] fc7 needs backward computation.
I0909 20:46:12.198784  8663 net.cpp:66] Creating Layer relu7
I0909 20:46:12.198792  8663 net.cpp:329] relu7 <- fc7
I0909 20:46:12.198801  8663 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:46:12.198809  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.198815  8663 net.cpp:125] relu7 needs backward computation.
I0909 20:46:12.198823  8663 net.cpp:66] Creating Layer drop7
I0909 20:46:12.198828  8663 net.cpp:329] drop7 <- fc7
I0909 20:46:12.198837  8663 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:46:12.198848  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.198853  8663 net.cpp:125] drop7 needs backward computation.
I0909 20:46:12.198861  8663 net.cpp:66] Creating Layer fc8
I0909 20:46:12.198868  8663 net.cpp:329] fc8 <- fc7
I0909 20:46:12.198874  8663 net.cpp:290] fc8 -> fc8
I0909 20:46:12.206661  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.206675  8663 net.cpp:125] fc8 needs backward computation.
I0909 20:46:12.206681  8663 net.cpp:66] Creating Layer relu8
I0909 20:46:12.206686  8663 net.cpp:329] relu8 <- fc8
I0909 20:46:12.206704  8663 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:46:12.206712  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.206717  8663 net.cpp:125] relu8 needs backward computation.
I0909 20:46:12.206723  8663 net.cpp:66] Creating Layer drop8
I0909 20:46:12.206728  8663 net.cpp:329] drop8 <- fc8
I0909 20:46:12.206737  8663 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:46:12.206743  8663 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:46:12.206749  8663 net.cpp:125] drop8 needs backward computation.
I0909 20:46:12.206756  8663 net.cpp:66] Creating Layer fc9
I0909 20:46:12.206763  8663 net.cpp:329] fc9 <- fc8
I0909 20:46:12.206770  8663 net.cpp:290] fc9 -> fc9
I0909 20:46:12.207144  8663 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:46:12.207155  8663 net.cpp:125] fc9 needs backward computation.
I0909 20:46:12.207162  8663 net.cpp:66] Creating Layer fc10
I0909 20:46:12.207168  8663 net.cpp:329] fc10 <- fc9
I0909 20:46:12.207176  8663 net.cpp:290] fc10 -> fc10
I0909 20:46:12.207188  8663 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:46:12.207201  8663 net.cpp:125] fc10 needs backward computation.
I0909 20:46:12.207207  8663 net.cpp:66] Creating Layer prob
I0909 20:46:12.207212  8663 net.cpp:329] prob <- fc10
I0909 20:46:12.207219  8663 net.cpp:290] prob -> prob
I0909 20:46:12.207228  8663 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:46:12.207234  8663 net.cpp:125] prob needs backward computation.
I0909 20:46:12.207239  8663 net.cpp:156] This network produces output prob
I0909 20:46:12.207252  8663 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:46:12.207260  8663 net.cpp:167] Network initialization done.
I0909 20:46:12.207265  8663 net.cpp:168] Memory required for data: 6183480
Classifying 224 inputs.
Done in 144.05 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:48:40.465711  8673 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:48:40.465852  8673 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:48:40.465862  8673 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:48:40.466009  8673 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:48:40.466075  8673 net.cpp:292] Input 0 -> data
I0909 20:48:40.466101  8673 net.cpp:66] Creating Layer conv1
I0909 20:48:40.466109  8673 net.cpp:329] conv1 <- data
I0909 20:48:40.466116  8673 net.cpp:290] conv1 -> conv1
I0909 20:48:40.467478  8673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:48:40.467496  8673 net.cpp:125] conv1 needs backward computation.
I0909 20:48:40.467505  8673 net.cpp:66] Creating Layer relu1
I0909 20:48:40.467511  8673 net.cpp:329] relu1 <- conv1
I0909 20:48:40.467519  8673 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:48:40.467527  8673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:48:40.467533  8673 net.cpp:125] relu1 needs backward computation.
I0909 20:48:40.467540  8673 net.cpp:66] Creating Layer pool1
I0909 20:48:40.467545  8673 net.cpp:329] pool1 <- conv1
I0909 20:48:40.467552  8673 net.cpp:290] pool1 -> pool1
I0909 20:48:40.467563  8673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:48:40.467568  8673 net.cpp:125] pool1 needs backward computation.
I0909 20:48:40.467576  8673 net.cpp:66] Creating Layer norm1
I0909 20:48:40.467581  8673 net.cpp:329] norm1 <- pool1
I0909 20:48:40.467587  8673 net.cpp:290] norm1 -> norm1
I0909 20:48:40.467597  8673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:48:40.467602  8673 net.cpp:125] norm1 needs backward computation.
I0909 20:48:40.467610  8673 net.cpp:66] Creating Layer conv2
I0909 20:48:40.467615  8673 net.cpp:329] conv2 <- norm1
I0909 20:48:40.467622  8673 net.cpp:290] conv2 -> conv2
I0909 20:48:40.476999  8673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:48:40.477030  8673 net.cpp:125] conv2 needs backward computation.
I0909 20:48:40.477040  8673 net.cpp:66] Creating Layer relu2
I0909 20:48:40.477046  8673 net.cpp:329] relu2 <- conv2
I0909 20:48:40.477052  8673 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:48:40.477061  8673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:48:40.477066  8673 net.cpp:125] relu2 needs backward computation.
I0909 20:48:40.477073  8673 net.cpp:66] Creating Layer pool2
I0909 20:48:40.477078  8673 net.cpp:329] pool2 <- conv2
I0909 20:48:40.477085  8673 net.cpp:290] pool2 -> pool2
I0909 20:48:40.477094  8673 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:48:40.477099  8673 net.cpp:125] pool2 needs backward computation.
I0909 20:48:40.477108  8673 net.cpp:66] Creating Layer fc7
I0909 20:48:40.477113  8673 net.cpp:329] fc7 <- pool2
I0909 20:48:40.477124  8673 net.cpp:290] fc7 -> fc7
I0909 20:48:41.117903  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.117949  8673 net.cpp:125] fc7 needs backward computation.
I0909 20:48:41.117964  8673 net.cpp:66] Creating Layer relu7
I0909 20:48:41.117983  8673 net.cpp:329] relu7 <- fc7
I0909 20:48:41.118000  8673 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:48:41.118010  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.118016  8673 net.cpp:125] relu7 needs backward computation.
I0909 20:48:41.118024  8673 net.cpp:66] Creating Layer drop7
I0909 20:48:41.118029  8673 net.cpp:329] drop7 <- fc7
I0909 20:48:41.118037  8673 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:48:41.118049  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.118055  8673 net.cpp:125] drop7 needs backward computation.
I0909 20:48:41.118065  8673 net.cpp:66] Creating Layer fc8
I0909 20:48:41.118072  8673 net.cpp:329] fc8 <- fc7
I0909 20:48:41.118078  8673 net.cpp:290] fc8 -> fc8
I0909 20:48:41.125857  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.125869  8673 net.cpp:125] fc8 needs backward computation.
I0909 20:48:41.125876  8673 net.cpp:66] Creating Layer relu8
I0909 20:48:41.125881  8673 net.cpp:329] relu8 <- fc8
I0909 20:48:41.125888  8673 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:48:41.125895  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.125901  8673 net.cpp:125] relu8 needs backward computation.
I0909 20:48:41.125907  8673 net.cpp:66] Creating Layer drop8
I0909 20:48:41.125912  8673 net.cpp:329] drop8 <- fc8
I0909 20:48:41.125921  8673 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:48:41.125928  8673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:41.125933  8673 net.cpp:125] drop8 needs backward computation.
I0909 20:48:41.125941  8673 net.cpp:66] Creating Layer fc9
I0909 20:48:41.125946  8673 net.cpp:329] fc9 <- fc8
I0909 20:48:41.125954  8673 net.cpp:290] fc9 -> fc9
I0909 20:48:41.126327  8673 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:48:41.126338  8673 net.cpp:125] fc9 needs backward computation.
I0909 20:48:41.126346  8673 net.cpp:66] Creating Layer fc10
I0909 20:48:41.126351  8673 net.cpp:329] fc10 <- fc9
I0909 20:48:41.126360  8673 net.cpp:290] fc10 -> fc10
I0909 20:48:41.126373  8673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:48:41.126381  8673 net.cpp:125] fc10 needs backward computation.
I0909 20:48:41.126389  8673 net.cpp:66] Creating Layer prob
I0909 20:48:41.126394  8673 net.cpp:329] prob <- fc10
I0909 20:48:41.126400  8673 net.cpp:290] prob -> prob
I0909 20:48:41.126410  8673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:48:41.126415  8673 net.cpp:125] prob needs backward computation.
I0909 20:48:41.126420  8673 net.cpp:156] This network produces output prob
I0909 20:48:41.126432  8673 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:48:41.126441  8673 net.cpp:167] Network initialization done.
I0909 20:48:41.126446  8673 net.cpp:168] Memory required for data: 6183480
Classifying 90 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:48:43.725553  8676 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:48:43.725707  8676 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:48:43.725715  8676 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:48:43.725862  8676 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:48:43.725927  8676 net.cpp:292] Input 0 -> data
I0909 20:48:43.725955  8676 net.cpp:66] Creating Layer conv1
I0909 20:48:43.725961  8676 net.cpp:329] conv1 <- data
I0909 20:48:43.725970  8676 net.cpp:290] conv1 -> conv1
I0909 20:48:43.727332  8676 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:48:43.727351  8676 net.cpp:125] conv1 needs backward computation.
I0909 20:48:43.727360  8676 net.cpp:66] Creating Layer relu1
I0909 20:48:43.727366  8676 net.cpp:329] relu1 <- conv1
I0909 20:48:43.727372  8676 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:48:43.727381  8676 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:48:43.727387  8676 net.cpp:125] relu1 needs backward computation.
I0909 20:48:43.727394  8676 net.cpp:66] Creating Layer pool1
I0909 20:48:43.727399  8676 net.cpp:329] pool1 <- conv1
I0909 20:48:43.727406  8676 net.cpp:290] pool1 -> pool1
I0909 20:48:43.727417  8676 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:48:43.727422  8676 net.cpp:125] pool1 needs backward computation.
I0909 20:48:43.727429  8676 net.cpp:66] Creating Layer norm1
I0909 20:48:43.727434  8676 net.cpp:329] norm1 <- pool1
I0909 20:48:43.727447  8676 net.cpp:290] norm1 -> norm1
I0909 20:48:43.727457  8676 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:48:43.727463  8676 net.cpp:125] norm1 needs backward computation.
I0909 20:48:43.727469  8676 net.cpp:66] Creating Layer conv2
I0909 20:48:43.727475  8676 net.cpp:329] conv2 <- norm1
I0909 20:48:43.727483  8676 net.cpp:290] conv2 -> conv2
I0909 20:48:43.736618  8676 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:48:43.736634  8676 net.cpp:125] conv2 needs backward computation.
I0909 20:48:43.736640  8676 net.cpp:66] Creating Layer relu2
I0909 20:48:43.736646  8676 net.cpp:329] relu2 <- conv2
I0909 20:48:43.736654  8676 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:48:43.736660  8676 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:48:43.736665  8676 net.cpp:125] relu2 needs backward computation.
I0909 20:48:43.736672  8676 net.cpp:66] Creating Layer pool2
I0909 20:48:43.736677  8676 net.cpp:329] pool2 <- conv2
I0909 20:48:43.736685  8676 net.cpp:290] pool2 -> pool2
I0909 20:48:43.736692  8676 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:48:43.736697  8676 net.cpp:125] pool2 needs backward computation.
I0909 20:48:43.736706  8676 net.cpp:66] Creating Layer fc7
I0909 20:48:43.736712  8676 net.cpp:329] fc7 <- pool2
I0909 20:48:43.736721  8676 net.cpp:290] fc7 -> fc7
I0909 20:48:44.378170  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.378211  8676 net.cpp:125] fc7 needs backward computation.
I0909 20:48:44.378226  8676 net.cpp:66] Creating Layer relu7
I0909 20:48:44.378233  8676 net.cpp:329] relu7 <- fc7
I0909 20:48:44.378240  8676 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:48:44.378249  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.378255  8676 net.cpp:125] relu7 needs backward computation.
I0909 20:48:44.378262  8676 net.cpp:66] Creating Layer drop7
I0909 20:48:44.378268  8676 net.cpp:329] drop7 <- fc7
I0909 20:48:44.378275  8676 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:48:44.378285  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.378291  8676 net.cpp:125] drop7 needs backward computation.
I0909 20:48:44.378300  8676 net.cpp:66] Creating Layer fc8
I0909 20:48:44.378305  8676 net.cpp:329] fc8 <- fc7
I0909 20:48:44.378312  8676 net.cpp:290] fc8 -> fc8
I0909 20:48:44.385890  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.385902  8676 net.cpp:125] fc8 needs backward computation.
I0909 20:48:44.385910  8676 net.cpp:66] Creating Layer relu8
I0909 20:48:44.385915  8676 net.cpp:329] relu8 <- fc8
I0909 20:48:44.385921  8676 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:48:44.385927  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.385933  8676 net.cpp:125] relu8 needs backward computation.
I0909 20:48:44.385939  8676 net.cpp:66] Creating Layer drop8
I0909 20:48:44.385944  8676 net.cpp:329] drop8 <- fc8
I0909 20:48:44.385952  8676 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:48:44.385959  8676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:48:44.385964  8676 net.cpp:125] drop8 needs backward computation.
I0909 20:48:44.385972  8676 net.cpp:66] Creating Layer fc9
I0909 20:48:44.385977  8676 net.cpp:329] fc9 <- fc8
I0909 20:48:44.385985  8676 net.cpp:290] fc9 -> fc9
I0909 20:48:44.386348  8676 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:48:44.386358  8676 net.cpp:125] fc9 needs backward computation.
I0909 20:48:44.386366  8676 net.cpp:66] Creating Layer fc10
I0909 20:48:44.386371  8676 net.cpp:329] fc10 <- fc9
I0909 20:48:44.386379  8676 net.cpp:290] fc10 -> fc10
I0909 20:48:44.386391  8676 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:48:44.386400  8676 net.cpp:125] fc10 needs backward computation.
I0909 20:48:44.386406  8676 net.cpp:66] Creating Layer prob
I0909 20:48:44.386411  8676 net.cpp:329] prob <- fc10
I0909 20:48:44.386418  8676 net.cpp:290] prob -> prob
I0909 20:48:44.386427  8676 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:48:44.386432  8676 net.cpp:125] prob needs backward computation.
I0909 20:48:44.386437  8676 net.cpp:156] This network produces output prob
I0909 20:48:44.386450  8676 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:48:44.386466  8676 net.cpp:167] Network initialization done.
I0909 20:48:44.386472  8676 net.cpp:168] Memory required for data: 6183480
Classifying 223 inputs.
Done in 150.65 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:51:20.239980  8684 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:51:20.240123  8684 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:51:20.240131  8684 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:51:20.240296  8684 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:51:20.240351  8684 net.cpp:292] Input 0 -> data
I0909 20:51:20.240377  8684 net.cpp:66] Creating Layer conv1
I0909 20:51:20.240397  8684 net.cpp:329] conv1 <- data
I0909 20:51:20.240406  8684 net.cpp:290] conv1 -> conv1
I0909 20:51:20.241808  8684 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:51:20.241828  8684 net.cpp:125] conv1 needs backward computation.
I0909 20:51:20.241837  8684 net.cpp:66] Creating Layer relu1
I0909 20:51:20.241843  8684 net.cpp:329] relu1 <- conv1
I0909 20:51:20.241850  8684 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:51:20.241859  8684 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:51:20.241864  8684 net.cpp:125] relu1 needs backward computation.
I0909 20:51:20.241871  8684 net.cpp:66] Creating Layer pool1
I0909 20:51:20.241878  8684 net.cpp:329] pool1 <- conv1
I0909 20:51:20.241884  8684 net.cpp:290] pool1 -> pool1
I0909 20:51:20.241895  8684 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:51:20.241900  8684 net.cpp:125] pool1 needs backward computation.
I0909 20:51:20.241907  8684 net.cpp:66] Creating Layer norm1
I0909 20:51:20.241914  8684 net.cpp:329] norm1 <- pool1
I0909 20:51:20.241935  8684 net.cpp:290] norm1 -> norm1
I0909 20:51:20.241945  8684 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:51:20.241950  8684 net.cpp:125] norm1 needs backward computation.
I0909 20:51:20.241957  8684 net.cpp:66] Creating Layer conv2
I0909 20:51:20.241962  8684 net.cpp:329] conv2 <- norm1
I0909 20:51:20.241969  8684 net.cpp:290] conv2 -> conv2
I0909 20:51:20.251145  8684 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:51:20.251171  8684 net.cpp:125] conv2 needs backward computation.
I0909 20:51:20.251181  8684 net.cpp:66] Creating Layer relu2
I0909 20:51:20.251188  8684 net.cpp:329] relu2 <- conv2
I0909 20:51:20.251195  8684 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:51:20.251204  8684 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:51:20.251210  8684 net.cpp:125] relu2 needs backward computation.
I0909 20:51:20.251217  8684 net.cpp:66] Creating Layer pool2
I0909 20:51:20.251222  8684 net.cpp:329] pool2 <- conv2
I0909 20:51:20.251230  8684 net.cpp:290] pool2 -> pool2
I0909 20:51:20.251238  8684 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:51:20.251245  8684 net.cpp:125] pool2 needs backward computation.
I0909 20:51:20.251255  8684 net.cpp:66] Creating Layer fc7
I0909 20:51:20.251261  8684 net.cpp:329] fc7 <- pool2
I0909 20:51:20.251269  8684 net.cpp:290] fc7 -> fc7
I0909 20:51:20.891695  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.891741  8684 net.cpp:125] fc7 needs backward computation.
I0909 20:51:20.891754  8684 net.cpp:66] Creating Layer relu7
I0909 20:51:20.891762  8684 net.cpp:329] relu7 <- fc7
I0909 20:51:20.891769  8684 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:51:20.891778  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.891784  8684 net.cpp:125] relu7 needs backward computation.
I0909 20:51:20.891791  8684 net.cpp:66] Creating Layer drop7
I0909 20:51:20.891798  8684 net.cpp:329] drop7 <- fc7
I0909 20:51:20.891805  8684 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:51:20.891816  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.891821  8684 net.cpp:125] drop7 needs backward computation.
I0909 20:51:20.891830  8684 net.cpp:66] Creating Layer fc8
I0909 20:51:20.891836  8684 net.cpp:329] fc8 <- fc7
I0909 20:51:20.891844  8684 net.cpp:290] fc8 -> fc8
I0909 20:51:20.899711  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.899729  8684 net.cpp:125] fc8 needs backward computation.
I0909 20:51:20.899737  8684 net.cpp:66] Creating Layer relu8
I0909 20:51:20.899744  8684 net.cpp:329] relu8 <- fc8
I0909 20:51:20.899751  8684 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:51:20.899759  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.899765  8684 net.cpp:125] relu8 needs backward computation.
I0909 20:51:20.899771  8684 net.cpp:66] Creating Layer drop8
I0909 20:51:20.899776  8684 net.cpp:329] drop8 <- fc8
I0909 20:51:20.899785  8684 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:51:20.899791  8684 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:20.899796  8684 net.cpp:125] drop8 needs backward computation.
I0909 20:51:20.899816  8684 net.cpp:66] Creating Layer fc9
I0909 20:51:20.899821  8684 net.cpp:329] fc9 <- fc8
I0909 20:51:20.899829  8684 net.cpp:290] fc9 -> fc9
I0909 20:51:20.900220  8684 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:51:20.900231  8684 net.cpp:125] fc9 needs backward computation.
I0909 20:51:20.900239  8684 net.cpp:66] Creating Layer fc10
I0909 20:51:20.900245  8684 net.cpp:329] fc10 <- fc9
I0909 20:51:20.900254  8684 net.cpp:290] fc10 -> fc10
I0909 20:51:20.900265  8684 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:51:20.900274  8684 net.cpp:125] fc10 needs backward computation.
I0909 20:51:20.900281  8684 net.cpp:66] Creating Layer prob
I0909 20:51:20.900286  8684 net.cpp:329] prob <- fc10
I0909 20:51:20.900293  8684 net.cpp:290] prob -> prob
I0909 20:51:20.900302  8684 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:51:20.900308  8684 net.cpp:125] prob needs backward computation.
I0909 20:51:20.900313  8684 net.cpp:156] This network produces output prob
I0909 20:51:20.900326  8684 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:51:20.900336  8684 net.cpp:167] Network initialization done.
I0909 20:51:20.900341  8684 net.cpp:168] Memory required for data: 6183480
Classifying 35 inputs.
Done in 22.08 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:51:44.369034  8687 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:51:44.369174  8687 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:51:44.369184  8687 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:51:44.369329  8687 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:51:44.369395  8687 net.cpp:292] Input 0 -> data
I0909 20:51:44.369421  8687 net.cpp:66] Creating Layer conv1
I0909 20:51:44.369427  8687 net.cpp:329] conv1 <- data
I0909 20:51:44.369436  8687 net.cpp:290] conv1 -> conv1
I0909 20:51:44.370826  8687 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:51:44.370846  8687 net.cpp:125] conv1 needs backward computation.
I0909 20:51:44.370854  8687 net.cpp:66] Creating Layer relu1
I0909 20:51:44.370861  8687 net.cpp:329] relu1 <- conv1
I0909 20:51:44.370867  8687 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:51:44.370877  8687 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:51:44.370882  8687 net.cpp:125] relu1 needs backward computation.
I0909 20:51:44.370889  8687 net.cpp:66] Creating Layer pool1
I0909 20:51:44.370894  8687 net.cpp:329] pool1 <- conv1
I0909 20:51:44.370901  8687 net.cpp:290] pool1 -> pool1
I0909 20:51:44.370913  8687 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:51:44.370918  8687 net.cpp:125] pool1 needs backward computation.
I0909 20:51:44.370925  8687 net.cpp:66] Creating Layer norm1
I0909 20:51:44.370931  8687 net.cpp:329] norm1 <- pool1
I0909 20:51:44.370937  8687 net.cpp:290] norm1 -> norm1
I0909 20:51:44.370947  8687 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:51:44.370954  8687 net.cpp:125] norm1 needs backward computation.
I0909 20:51:44.370960  8687 net.cpp:66] Creating Layer conv2
I0909 20:51:44.370966  8687 net.cpp:329] conv2 <- norm1
I0909 20:51:44.370973  8687 net.cpp:290] conv2 -> conv2
I0909 20:51:44.379966  8687 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:51:44.379979  8687 net.cpp:125] conv2 needs backward computation.
I0909 20:51:44.379987  8687 net.cpp:66] Creating Layer relu2
I0909 20:51:44.379992  8687 net.cpp:329] relu2 <- conv2
I0909 20:51:44.379998  8687 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:51:44.380005  8687 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:51:44.380012  8687 net.cpp:125] relu2 needs backward computation.
I0909 20:51:44.380017  8687 net.cpp:66] Creating Layer pool2
I0909 20:51:44.380023  8687 net.cpp:329] pool2 <- conv2
I0909 20:51:44.380029  8687 net.cpp:290] pool2 -> pool2
I0909 20:51:44.380038  8687 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:51:44.380043  8687 net.cpp:125] pool2 needs backward computation.
I0909 20:51:44.380054  8687 net.cpp:66] Creating Layer fc7
I0909 20:51:44.380060  8687 net.cpp:329] fc7 <- pool2
I0909 20:51:44.380067  8687 net.cpp:290] fc7 -> fc7
I0909 20:51:45.024473  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.024526  8687 net.cpp:125] fc7 needs backward computation.
I0909 20:51:45.024540  8687 net.cpp:66] Creating Layer relu7
I0909 20:51:45.024548  8687 net.cpp:329] relu7 <- fc7
I0909 20:51:45.024555  8687 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:51:45.024565  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.024571  8687 net.cpp:125] relu7 needs backward computation.
I0909 20:51:45.024579  8687 net.cpp:66] Creating Layer drop7
I0909 20:51:45.024585  8687 net.cpp:329] drop7 <- fc7
I0909 20:51:45.024592  8687 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:51:45.024603  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.024623  8687 net.cpp:125] drop7 needs backward computation.
I0909 20:51:45.024633  8687 net.cpp:66] Creating Layer fc8
I0909 20:51:45.024639  8687 net.cpp:329] fc8 <- fc7
I0909 20:51:45.024646  8687 net.cpp:290] fc8 -> fc8
I0909 20:51:45.032441  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.032454  8687 net.cpp:125] fc8 needs backward computation.
I0909 20:51:45.032461  8687 net.cpp:66] Creating Layer relu8
I0909 20:51:45.032467  8687 net.cpp:329] relu8 <- fc8
I0909 20:51:45.032474  8687 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:51:45.032480  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.032486  8687 net.cpp:125] relu8 needs backward computation.
I0909 20:51:45.032492  8687 net.cpp:66] Creating Layer drop8
I0909 20:51:45.032498  8687 net.cpp:329] drop8 <- fc8
I0909 20:51:45.032506  8687 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:51:45.032513  8687 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:51:45.032519  8687 net.cpp:125] drop8 needs backward computation.
I0909 20:51:45.032526  8687 net.cpp:66] Creating Layer fc9
I0909 20:51:45.032532  8687 net.cpp:329] fc9 <- fc8
I0909 20:51:45.032541  8687 net.cpp:290] fc9 -> fc9
I0909 20:51:45.032914  8687 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:51:45.032925  8687 net.cpp:125] fc9 needs backward computation.
I0909 20:51:45.032933  8687 net.cpp:66] Creating Layer fc10
I0909 20:51:45.032939  8687 net.cpp:329] fc10 <- fc9
I0909 20:51:45.032948  8687 net.cpp:290] fc10 -> fc10
I0909 20:51:45.032959  8687 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:51:45.032969  8687 net.cpp:125] fc10 needs backward computation.
I0909 20:51:45.032975  8687 net.cpp:66] Creating Layer prob
I0909 20:51:45.032981  8687 net.cpp:329] prob <- fc10
I0909 20:51:45.032989  8687 net.cpp:290] prob -> prob
I0909 20:51:45.032997  8687 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:51:45.033004  8687 net.cpp:125] prob needs backward computation.
I0909 20:51:45.033009  8687 net.cpp:156] This network produces output prob
I0909 20:51:45.033020  8687 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:51:45.033030  8687 net.cpp:167] Network initialization done.
I0909 20:51:45.033035  8687 net.cpp:168] Memory required for data: 6183480
Classifying 74 inputs.
Done in 47.23 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:52:34.773255  8691 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:52:34.773409  8691 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:52:34.773932  8691 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:52:34.774122  8691 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:52:34.774188  8691 net.cpp:292] Input 0 -> data
I0909 20:52:34.774214  8691 net.cpp:66] Creating Layer conv1
I0909 20:52:34.774220  8691 net.cpp:329] conv1 <- data
I0909 20:52:34.774229  8691 net.cpp:290] conv1 -> conv1
I0909 20:52:34.775593  8691 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:52:34.775610  8691 net.cpp:125] conv1 needs backward computation.
I0909 20:52:34.775619  8691 net.cpp:66] Creating Layer relu1
I0909 20:52:34.775625  8691 net.cpp:329] relu1 <- conv1
I0909 20:52:34.775632  8691 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:52:34.775641  8691 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:52:34.775647  8691 net.cpp:125] relu1 needs backward computation.
I0909 20:52:34.775653  8691 net.cpp:66] Creating Layer pool1
I0909 20:52:34.775660  8691 net.cpp:329] pool1 <- conv1
I0909 20:52:34.775665  8691 net.cpp:290] pool1 -> pool1
I0909 20:52:34.775676  8691 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:52:34.775682  8691 net.cpp:125] pool1 needs backward computation.
I0909 20:52:34.775689  8691 net.cpp:66] Creating Layer norm1
I0909 20:52:34.775696  8691 net.cpp:329] norm1 <- pool1
I0909 20:52:34.775702  8691 net.cpp:290] norm1 -> norm1
I0909 20:52:34.775712  8691 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:52:34.775717  8691 net.cpp:125] norm1 needs backward computation.
I0909 20:52:34.775725  8691 net.cpp:66] Creating Layer conv2
I0909 20:52:34.775730  8691 net.cpp:329] conv2 <- norm1
I0909 20:52:34.775738  8691 net.cpp:290] conv2 -> conv2
I0909 20:52:34.784875  8691 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:52:34.784891  8691 net.cpp:125] conv2 needs backward computation.
I0909 20:52:34.784898  8691 net.cpp:66] Creating Layer relu2
I0909 20:52:34.784904  8691 net.cpp:329] relu2 <- conv2
I0909 20:52:34.784910  8691 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:52:34.784919  8691 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:52:34.784924  8691 net.cpp:125] relu2 needs backward computation.
I0909 20:52:34.784929  8691 net.cpp:66] Creating Layer pool2
I0909 20:52:34.784935  8691 net.cpp:329] pool2 <- conv2
I0909 20:52:34.784947  8691 net.cpp:290] pool2 -> pool2
I0909 20:52:34.784955  8691 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:52:34.784961  8691 net.cpp:125] pool2 needs backward computation.
I0909 20:52:34.784968  8691 net.cpp:66] Creating Layer fc7
I0909 20:52:34.784975  8691 net.cpp:329] fc7 <- pool2
I0909 20:52:34.784983  8691 net.cpp:290] fc7 -> fc7
I0909 20:52:35.428165  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.428210  8691 net.cpp:125] fc7 needs backward computation.
I0909 20:52:35.428225  8691 net.cpp:66] Creating Layer relu7
I0909 20:52:35.428232  8691 net.cpp:329] relu7 <- fc7
I0909 20:52:35.428241  8691 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:52:35.428251  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.428256  8691 net.cpp:125] relu7 needs backward computation.
I0909 20:52:35.428263  8691 net.cpp:66] Creating Layer drop7
I0909 20:52:35.428268  8691 net.cpp:329] drop7 <- fc7
I0909 20:52:35.428277  8691 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:52:35.428288  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.428293  8691 net.cpp:125] drop7 needs backward computation.
I0909 20:52:35.428303  8691 net.cpp:66] Creating Layer fc8
I0909 20:52:35.428308  8691 net.cpp:329] fc8 <- fc7
I0909 20:52:35.428315  8691 net.cpp:290] fc8 -> fc8
I0909 20:52:35.436106  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.436120  8691 net.cpp:125] fc8 needs backward computation.
I0909 20:52:35.436126  8691 net.cpp:66] Creating Layer relu8
I0909 20:52:35.436132  8691 net.cpp:329] relu8 <- fc8
I0909 20:52:35.436138  8691 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:52:35.436146  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.436151  8691 net.cpp:125] relu8 needs backward computation.
I0909 20:52:35.436157  8691 net.cpp:66] Creating Layer drop8
I0909 20:52:35.436163  8691 net.cpp:329] drop8 <- fc8
I0909 20:52:35.436171  8691 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:52:35.436178  8691 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:35.436183  8691 net.cpp:125] drop8 needs backward computation.
I0909 20:52:35.436192  8691 net.cpp:66] Creating Layer fc9
I0909 20:52:35.436197  8691 net.cpp:329] fc9 <- fc8
I0909 20:52:35.436204  8691 net.cpp:290] fc9 -> fc9
I0909 20:52:35.436580  8691 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:52:35.436591  8691 net.cpp:125] fc9 needs backward computation.
I0909 20:52:35.436599  8691 net.cpp:66] Creating Layer fc10
I0909 20:52:35.436604  8691 net.cpp:329] fc10 <- fc9
I0909 20:52:35.436614  8691 net.cpp:290] fc10 -> fc10
I0909 20:52:35.436625  8691 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:52:35.436635  8691 net.cpp:125] fc10 needs backward computation.
I0909 20:52:35.436641  8691 net.cpp:66] Creating Layer prob
I0909 20:52:35.436647  8691 net.cpp:329] prob <- fc10
I0909 20:52:35.436653  8691 net.cpp:290] prob -> prob
I0909 20:52:35.436662  8691 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:52:35.436668  8691 net.cpp:125] prob needs backward computation.
I0909 20:52:35.436673  8691 net.cpp:156] This network produces output prob
I0909 20:52:35.436686  8691 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:52:35.436696  8691 net.cpp:167] Network initialization done.
I0909 20:52:35.436700  8691 net.cpp:168] Memory required for data: 6183480
Classifying 14 inputs.
Done in 8.97 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:52:45.463572  8694 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:52:45.463711  8694 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:52:45.463719  8694 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:52:45.463865  8694 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:52:45.463930  8694 net.cpp:292] Input 0 -> data
I0909 20:52:45.463956  8694 net.cpp:66] Creating Layer conv1
I0909 20:52:45.463963  8694 net.cpp:329] conv1 <- data
I0909 20:52:45.463971  8694 net.cpp:290] conv1 -> conv1
I0909 20:52:45.465335  8694 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:52:45.465353  8694 net.cpp:125] conv1 needs backward computation.
I0909 20:52:45.465363  8694 net.cpp:66] Creating Layer relu1
I0909 20:52:45.465368  8694 net.cpp:329] relu1 <- conv1
I0909 20:52:45.465375  8694 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:52:45.465384  8694 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:52:45.465389  8694 net.cpp:125] relu1 needs backward computation.
I0909 20:52:45.465396  8694 net.cpp:66] Creating Layer pool1
I0909 20:52:45.465402  8694 net.cpp:329] pool1 <- conv1
I0909 20:52:45.465409  8694 net.cpp:290] pool1 -> pool1
I0909 20:52:45.465420  8694 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:52:45.465426  8694 net.cpp:125] pool1 needs backward computation.
I0909 20:52:45.465433  8694 net.cpp:66] Creating Layer norm1
I0909 20:52:45.465443  8694 net.cpp:329] norm1 <- pool1
I0909 20:52:45.465451  8694 net.cpp:290] norm1 -> norm1
I0909 20:52:45.465461  8694 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:52:45.465466  8694 net.cpp:125] norm1 needs backward computation.
I0909 20:52:45.465473  8694 net.cpp:66] Creating Layer conv2
I0909 20:52:45.465478  8694 net.cpp:329] conv2 <- norm1
I0909 20:52:45.465486  8694 net.cpp:290] conv2 -> conv2
I0909 20:52:45.474653  8694 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:52:45.474668  8694 net.cpp:125] conv2 needs backward computation.
I0909 20:52:45.474675  8694 net.cpp:66] Creating Layer relu2
I0909 20:52:45.474681  8694 net.cpp:329] relu2 <- conv2
I0909 20:52:45.474688  8694 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:52:45.474694  8694 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:52:45.474700  8694 net.cpp:125] relu2 needs backward computation.
I0909 20:52:45.474706  8694 net.cpp:66] Creating Layer pool2
I0909 20:52:45.474712  8694 net.cpp:329] pool2 <- conv2
I0909 20:52:45.474719  8694 net.cpp:290] pool2 -> pool2
I0909 20:52:45.474726  8694 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:52:45.474732  8694 net.cpp:125] pool2 needs backward computation.
I0909 20:52:45.474741  8694 net.cpp:66] Creating Layer fc7
I0909 20:52:45.474747  8694 net.cpp:329] fc7 <- pool2
I0909 20:52:45.474755  8694 net.cpp:290] fc7 -> fc7
I0909 20:52:46.119155  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.119199  8694 net.cpp:125] fc7 needs backward computation.
I0909 20:52:46.119215  8694 net.cpp:66] Creating Layer relu7
I0909 20:52:46.119221  8694 net.cpp:329] relu7 <- fc7
I0909 20:52:46.119230  8694 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:52:46.119240  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.119246  8694 net.cpp:125] relu7 needs backward computation.
I0909 20:52:46.119252  8694 net.cpp:66] Creating Layer drop7
I0909 20:52:46.119258  8694 net.cpp:329] drop7 <- fc7
I0909 20:52:46.119266  8694 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:52:46.119277  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.119282  8694 net.cpp:125] drop7 needs backward computation.
I0909 20:52:46.119292  8694 net.cpp:66] Creating Layer fc8
I0909 20:52:46.119297  8694 net.cpp:329] fc8 <- fc7
I0909 20:52:46.119304  8694 net.cpp:290] fc8 -> fc8
I0909 20:52:46.127094  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.127107  8694 net.cpp:125] fc8 needs backward computation.
I0909 20:52:46.127115  8694 net.cpp:66] Creating Layer relu8
I0909 20:52:46.127120  8694 net.cpp:329] relu8 <- fc8
I0909 20:52:46.127127  8694 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:52:46.127135  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.127140  8694 net.cpp:125] relu8 needs backward computation.
I0909 20:52:46.127146  8694 net.cpp:66] Creating Layer drop8
I0909 20:52:46.127151  8694 net.cpp:329] drop8 <- fc8
I0909 20:52:46.127159  8694 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:52:46.127167  8694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:52:46.127173  8694 net.cpp:125] drop8 needs backward computation.
I0909 20:52:46.127181  8694 net.cpp:66] Creating Layer fc9
I0909 20:52:46.127187  8694 net.cpp:329] fc9 <- fc8
I0909 20:52:46.127194  8694 net.cpp:290] fc9 -> fc9
I0909 20:52:46.127570  8694 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:52:46.127580  8694 net.cpp:125] fc9 needs backward computation.
I0909 20:52:46.127589  8694 net.cpp:66] Creating Layer fc10
I0909 20:52:46.127594  8694 net.cpp:329] fc10 <- fc9
I0909 20:52:46.127604  8694 net.cpp:290] fc10 -> fc10
I0909 20:52:46.127615  8694 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:52:46.127624  8694 net.cpp:125] fc10 needs backward computation.
I0909 20:52:46.127631  8694 net.cpp:66] Creating Layer prob
I0909 20:52:46.127636  8694 net.cpp:329] prob <- fc10
I0909 20:52:46.127643  8694 net.cpp:290] prob -> prob
I0909 20:52:46.127652  8694 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:52:46.127658  8694 net.cpp:125] prob needs backward computation.
I0909 20:52:46.127663  8694 net.cpp:156] This network produces output prob
I0909 20:52:46.127686  8694 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:52:46.127696  8694 net.cpp:167] Network initialization done.
I0909 20:52:46.127701  8694 net.cpp:168] Memory required for data: 6183480
Classifying 99 inputs.
Done in 63.20 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:53:51.785609  8698 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:53:51.785748  8698 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:53:51.785758  8698 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:53:51.785900  8698 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:53:51.785955  8698 net.cpp:292] Input 0 -> data
I0909 20:53:51.785991  8698 net.cpp:66] Creating Layer conv1
I0909 20:53:51.785998  8698 net.cpp:329] conv1 <- data
I0909 20:53:51.786006  8698 net.cpp:290] conv1 -> conv1
I0909 20:53:51.787328  8698 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:53:51.787345  8698 net.cpp:125] conv1 needs backward computation.
I0909 20:53:51.787354  8698 net.cpp:66] Creating Layer relu1
I0909 20:53:51.787360  8698 net.cpp:329] relu1 <- conv1
I0909 20:53:51.787366  8698 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:53:51.787374  8698 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:53:51.787380  8698 net.cpp:125] relu1 needs backward computation.
I0909 20:53:51.787387  8698 net.cpp:66] Creating Layer pool1
I0909 20:53:51.787392  8698 net.cpp:329] pool1 <- conv1
I0909 20:53:51.787400  8698 net.cpp:290] pool1 -> pool1
I0909 20:53:51.787410  8698 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:53:51.787415  8698 net.cpp:125] pool1 needs backward computation.
I0909 20:53:51.787421  8698 net.cpp:66] Creating Layer norm1
I0909 20:53:51.787427  8698 net.cpp:329] norm1 <- pool1
I0909 20:53:51.787433  8698 net.cpp:290] norm1 -> norm1
I0909 20:53:51.787442  8698 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:53:51.787448  8698 net.cpp:125] norm1 needs backward computation.
I0909 20:53:51.787456  8698 net.cpp:66] Creating Layer conv2
I0909 20:53:51.787461  8698 net.cpp:329] conv2 <- norm1
I0909 20:53:51.787467  8698 net.cpp:290] conv2 -> conv2
I0909 20:53:51.796367  8698 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:53:51.796381  8698 net.cpp:125] conv2 needs backward computation.
I0909 20:53:51.796389  8698 net.cpp:66] Creating Layer relu2
I0909 20:53:51.796394  8698 net.cpp:329] relu2 <- conv2
I0909 20:53:51.796401  8698 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:53:51.796408  8698 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:53:51.796413  8698 net.cpp:125] relu2 needs backward computation.
I0909 20:53:51.796419  8698 net.cpp:66] Creating Layer pool2
I0909 20:53:51.796424  8698 net.cpp:329] pool2 <- conv2
I0909 20:53:51.796432  8698 net.cpp:290] pool2 -> pool2
I0909 20:53:51.796438  8698 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:53:51.796444  8698 net.cpp:125] pool2 needs backward computation.
I0909 20:53:51.796453  8698 net.cpp:66] Creating Layer fc7
I0909 20:53:51.796459  8698 net.cpp:329] fc7 <- pool2
I0909 20:53:51.796466  8698 net.cpp:290] fc7 -> fc7
I0909 20:53:52.437757  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.437800  8698 net.cpp:125] fc7 needs backward computation.
I0909 20:53:52.437814  8698 net.cpp:66] Creating Layer relu7
I0909 20:53:52.437821  8698 net.cpp:329] relu7 <- fc7
I0909 20:53:52.437830  8698 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:53:52.437839  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.437845  8698 net.cpp:125] relu7 needs backward computation.
I0909 20:53:52.437852  8698 net.cpp:66] Creating Layer drop7
I0909 20:53:52.437857  8698 net.cpp:329] drop7 <- fc7
I0909 20:53:52.437866  8698 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:53:52.437877  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.437882  8698 net.cpp:125] drop7 needs backward computation.
I0909 20:53:52.437891  8698 net.cpp:66] Creating Layer fc8
I0909 20:53:52.437897  8698 net.cpp:329] fc8 <- fc7
I0909 20:53:52.437904  8698 net.cpp:290] fc8 -> fc8
I0909 20:53:52.445698  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.445710  8698 net.cpp:125] fc8 needs backward computation.
I0909 20:53:52.445718  8698 net.cpp:66] Creating Layer relu8
I0909 20:53:52.445724  8698 net.cpp:329] relu8 <- fc8
I0909 20:53:52.445730  8698 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:53:52.445737  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.445742  8698 net.cpp:125] relu8 needs backward computation.
I0909 20:53:52.445749  8698 net.cpp:66] Creating Layer drop8
I0909 20:53:52.445755  8698 net.cpp:329] drop8 <- fc8
I0909 20:53:52.445762  8698 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:53:52.445770  8698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:53:52.445785  8698 net.cpp:125] drop8 needs backward computation.
I0909 20:53:52.445793  8698 net.cpp:66] Creating Layer fc9
I0909 20:53:52.445799  8698 net.cpp:329] fc9 <- fc8
I0909 20:53:52.445807  8698 net.cpp:290] fc9 -> fc9
I0909 20:53:52.446182  8698 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:53:52.446192  8698 net.cpp:125] fc9 needs backward computation.
I0909 20:53:52.446200  8698 net.cpp:66] Creating Layer fc10
I0909 20:53:52.446207  8698 net.cpp:329] fc10 <- fc9
I0909 20:53:52.446214  8698 net.cpp:290] fc10 -> fc10
I0909 20:53:52.446226  8698 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:53:52.446235  8698 net.cpp:125] fc10 needs backward computation.
I0909 20:53:52.446243  8698 net.cpp:66] Creating Layer prob
I0909 20:53:52.446248  8698 net.cpp:329] prob <- fc10
I0909 20:53:52.446254  8698 net.cpp:290] prob -> prob
I0909 20:53:52.446264  8698 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:53:52.446269  8698 net.cpp:125] prob needs backward computation.
I0909 20:53:52.446274  8698 net.cpp:156] This network produces output prob
I0909 20:53:52.446287  8698 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:53:52.446295  8698 net.cpp:167] Network initialization done.
I0909 20:53:52.446300  8698 net.cpp:168] Memory required for data: 6183480
Classifying 416 inputs.
Done in 258.42 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 20:58:23.884292  8712 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 20:58:23.884434  8712 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 20:58:23.884443  8712 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 20:58:23.884591  8712 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 20:58:23.893401  8712 net.cpp:292] Input 0 -> data
I0909 20:58:23.893434  8712 net.cpp:66] Creating Layer conv1
I0909 20:58:23.893441  8712 net.cpp:329] conv1 <- data
I0909 20:58:23.893450  8712 net.cpp:290] conv1 -> conv1
I0909 20:58:23.894822  8712 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:58:23.894842  8712 net.cpp:125] conv1 needs backward computation.
I0909 20:58:23.894851  8712 net.cpp:66] Creating Layer relu1
I0909 20:58:23.894857  8712 net.cpp:329] relu1 <- conv1
I0909 20:58:23.894865  8712 net.cpp:280] relu1 -> conv1 (in-place)
I0909 20:58:23.894872  8712 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 20:58:23.894878  8712 net.cpp:125] relu1 needs backward computation.
I0909 20:58:23.894886  8712 net.cpp:66] Creating Layer pool1
I0909 20:58:23.894891  8712 net.cpp:329] pool1 <- conv1
I0909 20:58:23.894897  8712 net.cpp:290] pool1 -> pool1
I0909 20:58:23.894908  8712 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:58:23.894914  8712 net.cpp:125] pool1 needs backward computation.
I0909 20:58:23.894922  8712 net.cpp:66] Creating Layer norm1
I0909 20:58:23.894927  8712 net.cpp:329] norm1 <- pool1
I0909 20:58:23.894933  8712 net.cpp:290] norm1 -> norm1
I0909 20:58:23.894942  8712 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 20:58:23.894948  8712 net.cpp:125] norm1 needs backward computation.
I0909 20:58:23.894956  8712 net.cpp:66] Creating Layer conv2
I0909 20:58:23.894961  8712 net.cpp:329] conv2 <- norm1
I0909 20:58:23.894968  8712 net.cpp:290] conv2 -> conv2
I0909 20:58:23.904088  8712 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:58:23.904103  8712 net.cpp:125] conv2 needs backward computation.
I0909 20:58:23.904109  8712 net.cpp:66] Creating Layer relu2
I0909 20:58:23.904115  8712 net.cpp:329] relu2 <- conv2
I0909 20:58:23.904121  8712 net.cpp:280] relu2 -> conv2 (in-place)
I0909 20:58:23.904129  8712 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 20:58:23.904134  8712 net.cpp:125] relu2 needs backward computation.
I0909 20:58:23.904140  8712 net.cpp:66] Creating Layer pool2
I0909 20:58:23.904146  8712 net.cpp:329] pool2 <- conv2
I0909 20:58:23.904153  8712 net.cpp:290] pool2 -> pool2
I0909 20:58:23.904161  8712 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 20:58:23.904166  8712 net.cpp:125] pool2 needs backward computation.
I0909 20:58:23.904175  8712 net.cpp:66] Creating Layer fc7
I0909 20:58:23.904181  8712 net.cpp:329] fc7 <- pool2
I0909 20:58:23.904189  8712 net.cpp:290] fc7 -> fc7
I0909 20:58:24.545714  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.545760  8712 net.cpp:125] fc7 needs backward computation.
I0909 20:58:24.545775  8712 net.cpp:66] Creating Layer relu7
I0909 20:58:24.545783  8712 net.cpp:329] relu7 <- fc7
I0909 20:58:24.545791  8712 net.cpp:280] relu7 -> fc7 (in-place)
I0909 20:58:24.545800  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.545806  8712 net.cpp:125] relu7 needs backward computation.
I0909 20:58:24.545814  8712 net.cpp:66] Creating Layer drop7
I0909 20:58:24.545819  8712 net.cpp:329] drop7 <- fc7
I0909 20:58:24.545836  8712 net.cpp:280] drop7 -> fc7 (in-place)
I0909 20:58:24.545848  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.545853  8712 net.cpp:125] drop7 needs backward computation.
I0909 20:58:24.545862  8712 net.cpp:66] Creating Layer fc8
I0909 20:58:24.545867  8712 net.cpp:329] fc8 <- fc7
I0909 20:58:24.545876  8712 net.cpp:290] fc8 -> fc8
I0909 20:58:24.553504  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.553520  8712 net.cpp:125] fc8 needs backward computation.
I0909 20:58:24.553527  8712 net.cpp:66] Creating Layer relu8
I0909 20:58:24.553534  8712 net.cpp:329] relu8 <- fc8
I0909 20:58:24.553540  8712 net.cpp:280] relu8 -> fc8 (in-place)
I0909 20:58:24.553547  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.553552  8712 net.cpp:125] relu8 needs backward computation.
I0909 20:58:24.553560  8712 net.cpp:66] Creating Layer drop8
I0909 20:58:24.553565  8712 net.cpp:329] drop8 <- fc8
I0909 20:58:24.553572  8712 net.cpp:280] drop8 -> fc8 (in-place)
I0909 20:58:24.553580  8712 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 20:58:24.553586  8712 net.cpp:125] drop8 needs backward computation.
I0909 20:58:24.553592  8712 net.cpp:66] Creating Layer fc9
I0909 20:58:24.553598  8712 net.cpp:329] fc9 <- fc8
I0909 20:58:24.553608  8712 net.cpp:290] fc9 -> fc9
I0909 20:58:24.553982  8712 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 20:58:24.553993  8712 net.cpp:125] fc9 needs backward computation.
I0909 20:58:24.554002  8712 net.cpp:66] Creating Layer fc10
I0909 20:58:24.554008  8712 net.cpp:329] fc10 <- fc9
I0909 20:58:24.554016  8712 net.cpp:290] fc10 -> fc10
I0909 20:58:24.554028  8712 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:58:24.554038  8712 net.cpp:125] fc10 needs backward computation.
I0909 20:58:24.554044  8712 net.cpp:66] Creating Layer prob
I0909 20:58:24.554049  8712 net.cpp:329] prob <- fc10
I0909 20:58:24.554056  8712 net.cpp:290] prob -> prob
I0909 20:58:24.554065  8712 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 20:58:24.554071  8712 net.cpp:125] prob needs backward computation.
I0909 20:58:24.554076  8712 net.cpp:156] This network produces output prob
I0909 20:58:24.554088  8712 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 20:58:24.554097  8712 net.cpp:167] Network initialization done.
I0909 20:58:24.554102  8712 net.cpp:168] Memory required for data: 6183480
Classifying 197 inputs.
Done in 127.24 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:00:37.519049  8717 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:00:37.519189  8717 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:00:37.519199  8717 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:00:37.519345  8717 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:00:37.519410  8717 net.cpp:292] Input 0 -> data
I0909 21:00:37.519436  8717 net.cpp:66] Creating Layer conv1
I0909 21:00:37.519443  8717 net.cpp:329] conv1 <- data
I0909 21:00:37.519451  8717 net.cpp:290] conv1 -> conv1
I0909 21:00:37.520813  8717 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:00:37.520831  8717 net.cpp:125] conv1 needs backward computation.
I0909 21:00:37.520840  8717 net.cpp:66] Creating Layer relu1
I0909 21:00:37.520846  8717 net.cpp:329] relu1 <- conv1
I0909 21:00:37.520853  8717 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:00:37.520862  8717 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:00:37.520869  8717 net.cpp:125] relu1 needs backward computation.
I0909 21:00:37.520875  8717 net.cpp:66] Creating Layer pool1
I0909 21:00:37.520881  8717 net.cpp:329] pool1 <- conv1
I0909 21:00:37.520889  8717 net.cpp:290] pool1 -> pool1
I0909 21:00:37.520900  8717 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:00:37.520905  8717 net.cpp:125] pool1 needs backward computation.
I0909 21:00:37.520912  8717 net.cpp:66] Creating Layer norm1
I0909 21:00:37.520918  8717 net.cpp:329] norm1 <- pool1
I0909 21:00:37.520925  8717 net.cpp:290] norm1 -> norm1
I0909 21:00:37.520934  8717 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:00:37.520941  8717 net.cpp:125] norm1 needs backward computation.
I0909 21:00:37.520948  8717 net.cpp:66] Creating Layer conv2
I0909 21:00:37.520953  8717 net.cpp:329] conv2 <- norm1
I0909 21:00:37.520961  8717 net.cpp:290] conv2 -> conv2
I0909 21:00:37.530089  8717 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:00:37.530104  8717 net.cpp:125] conv2 needs backward computation.
I0909 21:00:37.530112  8717 net.cpp:66] Creating Layer relu2
I0909 21:00:37.530118  8717 net.cpp:329] relu2 <- conv2
I0909 21:00:37.530124  8717 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:00:37.530133  8717 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:00:37.530138  8717 net.cpp:125] relu2 needs backward computation.
I0909 21:00:37.530144  8717 net.cpp:66] Creating Layer pool2
I0909 21:00:37.530154  8717 net.cpp:329] pool2 <- conv2
I0909 21:00:37.530163  8717 net.cpp:290] pool2 -> pool2
I0909 21:00:37.530170  8717 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:00:37.530176  8717 net.cpp:125] pool2 needs backward computation.
I0909 21:00:37.530185  8717 net.cpp:66] Creating Layer fc7
I0909 21:00:37.530191  8717 net.cpp:329] fc7 <- pool2
I0909 21:00:37.530199  8717 net.cpp:290] fc7 -> fc7
I0909 21:00:38.169802  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.169843  8717 net.cpp:125] fc7 needs backward computation.
I0909 21:00:38.169858  8717 net.cpp:66] Creating Layer relu7
I0909 21:00:38.169865  8717 net.cpp:329] relu7 <- fc7
I0909 21:00:38.169873  8717 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:00:38.169883  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.169889  8717 net.cpp:125] relu7 needs backward computation.
I0909 21:00:38.169896  8717 net.cpp:66] Creating Layer drop7
I0909 21:00:38.169903  8717 net.cpp:329] drop7 <- fc7
I0909 21:00:38.169910  8717 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:00:38.169921  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.169927  8717 net.cpp:125] drop7 needs backward computation.
I0909 21:00:38.169936  8717 net.cpp:66] Creating Layer fc8
I0909 21:00:38.169941  8717 net.cpp:329] fc8 <- fc7
I0909 21:00:38.169950  8717 net.cpp:290] fc8 -> fc8
I0909 21:00:38.177724  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.177737  8717 net.cpp:125] fc8 needs backward computation.
I0909 21:00:38.177744  8717 net.cpp:66] Creating Layer relu8
I0909 21:00:38.177750  8717 net.cpp:329] relu8 <- fc8
I0909 21:00:38.177757  8717 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:00:38.177764  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.177769  8717 net.cpp:125] relu8 needs backward computation.
I0909 21:00:38.177777  8717 net.cpp:66] Creating Layer drop8
I0909 21:00:38.177781  8717 net.cpp:329] drop8 <- fc8
I0909 21:00:38.177789  8717 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:00:38.177796  8717 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:38.177803  8717 net.cpp:125] drop8 needs backward computation.
I0909 21:00:38.177809  8717 net.cpp:66] Creating Layer fc9
I0909 21:00:38.177815  8717 net.cpp:329] fc9 <- fc8
I0909 21:00:38.177825  8717 net.cpp:290] fc9 -> fc9
I0909 21:00:38.178199  8717 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:00:38.178210  8717 net.cpp:125] fc9 needs backward computation.
I0909 21:00:38.178217  8717 net.cpp:66] Creating Layer fc10
I0909 21:00:38.178223  8717 net.cpp:329] fc10 <- fc9
I0909 21:00:38.178231  8717 net.cpp:290] fc10 -> fc10
I0909 21:00:38.178243  8717 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:00:38.178252  8717 net.cpp:125] fc10 needs backward computation.
I0909 21:00:38.178259  8717 net.cpp:66] Creating Layer prob
I0909 21:00:38.178266  8717 net.cpp:329] prob <- fc10
I0909 21:00:38.178272  8717 net.cpp:290] prob -> prob
I0909 21:00:38.178282  8717 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:00:38.178287  8717 net.cpp:125] prob needs backward computation.
I0909 21:00:38.178292  8717 net.cpp:156] This network produces output prob
I0909 21:00:38.178304  8717 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:00:38.178313  8717 net.cpp:167] Network initialization done.
I0909 21:00:38.178318  8717 net.cpp:168] Memory required for data: 6183480
Classifying 650 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:00:49.907249  8720 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:00:49.907402  8720 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:00:49.907412  8720 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:00:49.907559  8720 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:00:49.907613  8720 net.cpp:292] Input 0 -> data
I0909 21:00:49.907639  8720 net.cpp:66] Creating Layer conv1
I0909 21:00:49.907646  8720 net.cpp:329] conv1 <- data
I0909 21:00:49.907655  8720 net.cpp:290] conv1 -> conv1
I0909 21:00:49.909020  8720 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:00:49.909039  8720 net.cpp:125] conv1 needs backward computation.
I0909 21:00:49.909047  8720 net.cpp:66] Creating Layer relu1
I0909 21:00:49.909054  8720 net.cpp:329] relu1 <- conv1
I0909 21:00:49.909060  8720 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:00:49.909075  8720 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:00:49.909080  8720 net.cpp:125] relu1 needs backward computation.
I0909 21:00:49.909087  8720 net.cpp:66] Creating Layer pool1
I0909 21:00:49.909093  8720 net.cpp:329] pool1 <- conv1
I0909 21:00:49.909101  8720 net.cpp:290] pool1 -> pool1
I0909 21:00:49.909111  8720 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:00:49.909117  8720 net.cpp:125] pool1 needs backward computation.
I0909 21:00:49.909123  8720 net.cpp:66] Creating Layer norm1
I0909 21:00:49.909129  8720 net.cpp:329] norm1 <- pool1
I0909 21:00:49.909137  8720 net.cpp:290] norm1 -> norm1
I0909 21:00:49.909147  8720 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:00:49.909152  8720 net.cpp:125] norm1 needs backward computation.
I0909 21:00:49.909159  8720 net.cpp:66] Creating Layer conv2
I0909 21:00:49.909165  8720 net.cpp:329] conv2 <- norm1
I0909 21:00:49.909173  8720 net.cpp:290] conv2 -> conv2
I0909 21:00:49.918273  8720 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:00:49.918288  8720 net.cpp:125] conv2 needs backward computation.
I0909 21:00:49.918295  8720 net.cpp:66] Creating Layer relu2
I0909 21:00:49.918301  8720 net.cpp:329] relu2 <- conv2
I0909 21:00:49.918308  8720 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:00:49.918315  8720 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:00:49.918321  8720 net.cpp:125] relu2 needs backward computation.
I0909 21:00:49.918328  8720 net.cpp:66] Creating Layer pool2
I0909 21:00:49.918333  8720 net.cpp:329] pool2 <- conv2
I0909 21:00:49.918339  8720 net.cpp:290] pool2 -> pool2
I0909 21:00:49.918347  8720 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:00:49.918354  8720 net.cpp:125] pool2 needs backward computation.
I0909 21:00:49.918360  8720 net.cpp:66] Creating Layer fc7
I0909 21:00:49.918366  8720 net.cpp:329] fc7 <- pool2
I0909 21:00:49.918375  8720 net.cpp:290] fc7 -> fc7
I0909 21:00:50.557502  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.557549  8720 net.cpp:125] fc7 needs backward computation.
I0909 21:00:50.557564  8720 net.cpp:66] Creating Layer relu7
I0909 21:00:50.557571  8720 net.cpp:329] relu7 <- fc7
I0909 21:00:50.557579  8720 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:00:50.557590  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.557595  8720 net.cpp:125] relu7 needs backward computation.
I0909 21:00:50.557602  8720 net.cpp:66] Creating Layer drop7
I0909 21:00:50.557608  8720 net.cpp:329] drop7 <- fc7
I0909 21:00:50.557616  8720 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:00:50.557627  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.557633  8720 net.cpp:125] drop7 needs backward computation.
I0909 21:00:50.557641  8720 net.cpp:66] Creating Layer fc8
I0909 21:00:50.557647  8720 net.cpp:329] fc8 <- fc7
I0909 21:00:50.557654  8720 net.cpp:290] fc8 -> fc8
I0909 21:00:50.565390  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.565404  8720 net.cpp:125] fc8 needs backward computation.
I0909 21:00:50.565412  8720 net.cpp:66] Creating Layer relu8
I0909 21:00:50.565417  8720 net.cpp:329] relu8 <- fc8
I0909 21:00:50.565423  8720 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:00:50.565430  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.565435  8720 net.cpp:125] relu8 needs backward computation.
I0909 21:00:50.565443  8720 net.cpp:66] Creating Layer drop8
I0909 21:00:50.565448  8720 net.cpp:329] drop8 <- fc8
I0909 21:00:50.565455  8720 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:00:50.565462  8720 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:00:50.565467  8720 net.cpp:125] drop8 needs backward computation.
I0909 21:00:50.565475  8720 net.cpp:66] Creating Layer fc9
I0909 21:00:50.565480  8720 net.cpp:329] fc9 <- fc8
I0909 21:00:50.565489  8720 net.cpp:290] fc9 -> fc9
I0909 21:00:50.565877  8720 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:00:50.565891  8720 net.cpp:125] fc9 needs backward computation.
I0909 21:00:50.565897  8720 net.cpp:66] Creating Layer fc10
I0909 21:00:50.565903  8720 net.cpp:329] fc10 <- fc9
I0909 21:00:50.565922  8720 net.cpp:290] fc10 -> fc10
I0909 21:00:50.565934  8720 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:00:50.565943  8720 net.cpp:125] fc10 needs backward computation.
I0909 21:00:50.565950  8720 net.cpp:66] Creating Layer prob
I0909 21:00:50.565956  8720 net.cpp:329] prob <- fc10
I0909 21:00:50.565963  8720 net.cpp:290] prob -> prob
I0909 21:00:50.565971  8720 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:00:50.565978  8720 net.cpp:125] prob needs backward computation.
I0909 21:00:50.565982  8720 net.cpp:156] This network produces output prob
I0909 21:00:50.565994  8720 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:00:50.566004  8720 net.cpp:167] Network initialization done.
I0909 21:00:50.566009  8720 net.cpp:168] Memory required for data: 6183480
Classifying 286 inputs.
Done in 175.21 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:03:51.292985  8737 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:03:51.293126  8737 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:03:51.293136  8737 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:03:51.293283  8737 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:03:51.293349  8737 net.cpp:292] Input 0 -> data
I0909 21:03:51.293375  8737 net.cpp:66] Creating Layer conv1
I0909 21:03:51.293381  8737 net.cpp:329] conv1 <- data
I0909 21:03:51.293390  8737 net.cpp:290] conv1 -> conv1
I0909 21:03:51.294811  8737 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:51.294829  8737 net.cpp:125] conv1 needs backward computation.
I0909 21:03:51.294838  8737 net.cpp:66] Creating Layer relu1
I0909 21:03:51.294844  8737 net.cpp:329] relu1 <- conv1
I0909 21:03:51.294852  8737 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:03:51.294859  8737 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:51.294865  8737 net.cpp:125] relu1 needs backward computation.
I0909 21:03:51.294872  8737 net.cpp:66] Creating Layer pool1
I0909 21:03:51.294878  8737 net.cpp:329] pool1 <- conv1
I0909 21:03:51.294884  8737 net.cpp:290] pool1 -> pool1
I0909 21:03:51.294895  8737 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:51.294901  8737 net.cpp:125] pool1 needs backward computation.
I0909 21:03:51.294909  8737 net.cpp:66] Creating Layer norm1
I0909 21:03:51.294914  8737 net.cpp:329] norm1 <- pool1
I0909 21:03:51.294920  8737 net.cpp:290] norm1 -> norm1
I0909 21:03:51.294930  8737 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:51.294935  8737 net.cpp:125] norm1 needs backward computation.
I0909 21:03:51.294944  8737 net.cpp:66] Creating Layer conv2
I0909 21:03:51.294949  8737 net.cpp:329] conv2 <- norm1
I0909 21:03:51.294955  8737 net.cpp:290] conv2 -> conv2
I0909 21:03:51.304211  8737 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:51.304225  8737 net.cpp:125] conv2 needs backward computation.
I0909 21:03:51.304232  8737 net.cpp:66] Creating Layer relu2
I0909 21:03:51.304239  8737 net.cpp:329] relu2 <- conv2
I0909 21:03:51.304244  8737 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:03:51.304251  8737 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:51.304257  8737 net.cpp:125] relu2 needs backward computation.
I0909 21:03:51.304263  8737 net.cpp:66] Creating Layer pool2
I0909 21:03:51.304270  8737 net.cpp:329] pool2 <- conv2
I0909 21:03:51.304275  8737 net.cpp:290] pool2 -> pool2
I0909 21:03:51.304283  8737 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:03:51.304289  8737 net.cpp:125] pool2 needs backward computation.
I0909 21:03:51.304298  8737 net.cpp:66] Creating Layer fc7
I0909 21:03:51.304304  8737 net.cpp:329] fc7 <- pool2
I0909 21:03:51.304311  8737 net.cpp:290] fc7 -> fc7
I0909 21:03:51.947814  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.947860  8737 net.cpp:125] fc7 needs backward computation.
I0909 21:03:51.947875  8737 net.cpp:66] Creating Layer relu7
I0909 21:03:51.947882  8737 net.cpp:329] relu7 <- fc7
I0909 21:03:51.947890  8737 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:03:51.947899  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.947906  8737 net.cpp:125] relu7 needs backward computation.
I0909 21:03:51.947913  8737 net.cpp:66] Creating Layer drop7
I0909 21:03:51.947918  8737 net.cpp:329] drop7 <- fc7
I0909 21:03:51.947926  8737 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:03:51.947937  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.947943  8737 net.cpp:125] drop7 needs backward computation.
I0909 21:03:51.947952  8737 net.cpp:66] Creating Layer fc8
I0909 21:03:51.947957  8737 net.cpp:329] fc8 <- fc7
I0909 21:03:51.947965  8737 net.cpp:290] fc8 -> fc8
I0909 21:03:51.955766  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.955778  8737 net.cpp:125] fc8 needs backward computation.
I0909 21:03:51.955796  8737 net.cpp:66] Creating Layer relu8
I0909 21:03:51.955802  8737 net.cpp:329] relu8 <- fc8
I0909 21:03:51.955809  8737 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:03:51.955816  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.955821  8737 net.cpp:125] relu8 needs backward computation.
I0909 21:03:51.955828  8737 net.cpp:66] Creating Layer drop8
I0909 21:03:51.955833  8737 net.cpp:329] drop8 <- fc8
I0909 21:03:51.955842  8737 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:03:51.955848  8737 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:51.955854  8737 net.cpp:125] drop8 needs backward computation.
I0909 21:03:51.955862  8737 net.cpp:66] Creating Layer fc9
I0909 21:03:51.955868  8737 net.cpp:329] fc9 <- fc8
I0909 21:03:51.955875  8737 net.cpp:290] fc9 -> fc9
I0909 21:03:51.956249  8737 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:03:51.956260  8737 net.cpp:125] fc9 needs backward computation.
I0909 21:03:51.956269  8737 net.cpp:66] Creating Layer fc10
I0909 21:03:51.956274  8737 net.cpp:329] fc10 <- fc9
I0909 21:03:51.956282  8737 net.cpp:290] fc10 -> fc10
I0909 21:03:51.956293  8737 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:51.956303  8737 net.cpp:125] fc10 needs backward computation.
I0909 21:03:51.956310  8737 net.cpp:66] Creating Layer prob
I0909 21:03:51.956315  8737 net.cpp:329] prob <- fc10
I0909 21:03:51.956322  8737 net.cpp:290] prob -> prob
I0909 21:03:51.956331  8737 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:51.956337  8737 net.cpp:125] prob needs backward computation.
I0909 21:03:51.956342  8737 net.cpp:156] This network produces output prob
I0909 21:03:51.956354  8737 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:03:51.956362  8737 net.cpp:167] Network initialization done.
I0909 21:03:51.956367  8737 net.cpp:168] Memory required for data: 6183480
Classifying 50 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:03:53.400405  8740 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:03:53.400547  8740 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:03:53.400557  8740 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:03:53.400704  8740 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:03:53.400769  8740 net.cpp:292] Input 0 -> data
I0909 21:03:53.400796  8740 net.cpp:66] Creating Layer conv1
I0909 21:03:53.400804  8740 net.cpp:329] conv1 <- data
I0909 21:03:53.400811  8740 net.cpp:290] conv1 -> conv1
I0909 21:03:53.402195  8740 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:53.402215  8740 net.cpp:125] conv1 needs backward computation.
I0909 21:03:53.402225  8740 net.cpp:66] Creating Layer relu1
I0909 21:03:53.402230  8740 net.cpp:329] relu1 <- conv1
I0909 21:03:53.402236  8740 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:03:53.402245  8740 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:53.402251  8740 net.cpp:125] relu1 needs backward computation.
I0909 21:03:53.402258  8740 net.cpp:66] Creating Layer pool1
I0909 21:03:53.402263  8740 net.cpp:329] pool1 <- conv1
I0909 21:03:53.402271  8740 net.cpp:290] pool1 -> pool1
I0909 21:03:53.402281  8740 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:53.402287  8740 net.cpp:125] pool1 needs backward computation.
I0909 21:03:53.402294  8740 net.cpp:66] Creating Layer norm1
I0909 21:03:53.402299  8740 net.cpp:329] norm1 <- pool1
I0909 21:03:53.402307  8740 net.cpp:290] norm1 -> norm1
I0909 21:03:53.402315  8740 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:53.402322  8740 net.cpp:125] norm1 needs backward computation.
I0909 21:03:53.402328  8740 net.cpp:66] Creating Layer conv2
I0909 21:03:53.402334  8740 net.cpp:329] conv2 <- norm1
I0909 21:03:53.402341  8740 net.cpp:290] conv2 -> conv2
I0909 21:03:53.411674  8740 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:53.411705  8740 net.cpp:125] conv2 needs backward computation.
I0909 21:03:53.411715  8740 net.cpp:66] Creating Layer relu2
I0909 21:03:53.411721  8740 net.cpp:329] relu2 <- conv2
I0909 21:03:53.411730  8740 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:03:53.411738  8740 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:53.411743  8740 net.cpp:125] relu2 needs backward computation.
I0909 21:03:53.411751  8740 net.cpp:66] Creating Layer pool2
I0909 21:03:53.411756  8740 net.cpp:329] pool2 <- conv2
I0909 21:03:53.411772  8740 net.cpp:290] pool2 -> pool2
I0909 21:03:53.411782  8740 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:03:53.411788  8740 net.cpp:125] pool2 needs backward computation.
I0909 21:03:53.411799  8740 net.cpp:66] Creating Layer fc7
I0909 21:03:53.411805  8740 net.cpp:329] fc7 <- pool2
I0909 21:03:53.411813  8740 net.cpp:290] fc7 -> fc7
I0909 21:03:54.049396  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.049440  8740 net.cpp:125] fc7 needs backward computation.
I0909 21:03:54.049454  8740 net.cpp:66] Creating Layer relu7
I0909 21:03:54.049461  8740 net.cpp:329] relu7 <- fc7
I0909 21:03:54.049468  8740 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:03:54.049478  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.049484  8740 net.cpp:125] relu7 needs backward computation.
I0909 21:03:54.049490  8740 net.cpp:66] Creating Layer drop7
I0909 21:03:54.049496  8740 net.cpp:329] drop7 <- fc7
I0909 21:03:54.049504  8740 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:03:54.049517  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.049525  8740 net.cpp:125] drop7 needs backward computation.
I0909 21:03:54.049532  8740 net.cpp:66] Creating Layer fc8
I0909 21:03:54.049538  8740 net.cpp:329] fc8 <- fc7
I0909 21:03:54.049546  8740 net.cpp:290] fc8 -> fc8
I0909 21:03:54.057318  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.057330  8740 net.cpp:125] fc8 needs backward computation.
I0909 21:03:54.057337  8740 net.cpp:66] Creating Layer relu8
I0909 21:03:54.057343  8740 net.cpp:329] relu8 <- fc8
I0909 21:03:54.057349  8740 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:03:54.057356  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.057363  8740 net.cpp:125] relu8 needs backward computation.
I0909 21:03:54.057369  8740 net.cpp:66] Creating Layer drop8
I0909 21:03:54.057374  8740 net.cpp:329] drop8 <- fc8
I0909 21:03:54.057382  8740 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:03:54.057389  8740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:54.057395  8740 net.cpp:125] drop8 needs backward computation.
I0909 21:03:54.057402  8740 net.cpp:66] Creating Layer fc9
I0909 21:03:54.057407  8740 net.cpp:329] fc9 <- fc8
I0909 21:03:54.057416  8740 net.cpp:290] fc9 -> fc9
I0909 21:03:54.057795  8740 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:03:54.057806  8740 net.cpp:125] fc9 needs backward computation.
I0909 21:03:54.057814  8740 net.cpp:66] Creating Layer fc10
I0909 21:03:54.057821  8740 net.cpp:329] fc10 <- fc9
I0909 21:03:54.057828  8740 net.cpp:290] fc10 -> fc10
I0909 21:03:54.057840  8740 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:54.057849  8740 net.cpp:125] fc10 needs backward computation.
I0909 21:03:54.057855  8740 net.cpp:66] Creating Layer prob
I0909 21:03:54.057862  8740 net.cpp:329] prob <- fc10
I0909 21:03:54.057868  8740 net.cpp:290] prob -> prob
I0909 21:03:54.057878  8740 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:54.057883  8740 net.cpp:125] prob needs backward computation.
I0909 21:03:54.057888  8740 net.cpp:156] This network produces output prob
I0909 21:03:54.057900  8740 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:03:54.057909  8740 net.cpp:167] Network initialization done.
I0909 21:03:54.057914  8740 net.cpp:168] Memory required for data: 6183480
Classifying 100 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:03:56.288069  8743 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:03:56.288220  8743 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:03:56.288229  8743 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:03:56.288375  8743 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:03:56.288430  8743 net.cpp:292] Input 0 -> data
I0909 21:03:56.288456  8743 net.cpp:66] Creating Layer conv1
I0909 21:03:56.288463  8743 net.cpp:329] conv1 <- data
I0909 21:03:56.288471  8743 net.cpp:290] conv1 -> conv1
I0909 21:03:56.289870  8743 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:56.289890  8743 net.cpp:125] conv1 needs backward computation.
I0909 21:03:56.289898  8743 net.cpp:66] Creating Layer relu1
I0909 21:03:56.289904  8743 net.cpp:329] relu1 <- conv1
I0909 21:03:56.289911  8743 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:03:56.289919  8743 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:03:56.289930  8743 net.cpp:125] relu1 needs backward computation.
I0909 21:03:56.289938  8743 net.cpp:66] Creating Layer pool1
I0909 21:03:56.289943  8743 net.cpp:329] pool1 <- conv1
I0909 21:03:56.289950  8743 net.cpp:290] pool1 -> pool1
I0909 21:03:56.289962  8743 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:56.289968  8743 net.cpp:125] pool1 needs backward computation.
I0909 21:03:56.289974  8743 net.cpp:66] Creating Layer norm1
I0909 21:03:56.289979  8743 net.cpp:329] norm1 <- pool1
I0909 21:03:56.289986  8743 net.cpp:290] norm1 -> norm1
I0909 21:03:56.289996  8743 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:03:56.290001  8743 net.cpp:125] norm1 needs backward computation.
I0909 21:03:56.290009  8743 net.cpp:66] Creating Layer conv2
I0909 21:03:56.290015  8743 net.cpp:329] conv2 <- norm1
I0909 21:03:56.290022  8743 net.cpp:290] conv2 -> conv2
I0909 21:03:56.299144  8743 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:56.299159  8743 net.cpp:125] conv2 needs backward computation.
I0909 21:03:56.299165  8743 net.cpp:66] Creating Layer relu2
I0909 21:03:56.299171  8743 net.cpp:329] relu2 <- conv2
I0909 21:03:56.299178  8743 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:03:56.299185  8743 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:03:56.299191  8743 net.cpp:125] relu2 needs backward computation.
I0909 21:03:56.299197  8743 net.cpp:66] Creating Layer pool2
I0909 21:03:56.299202  8743 net.cpp:329] pool2 <- conv2
I0909 21:03:56.299208  8743 net.cpp:290] pool2 -> pool2
I0909 21:03:56.299216  8743 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:03:56.299222  8743 net.cpp:125] pool2 needs backward computation.
I0909 21:03:56.299229  8743 net.cpp:66] Creating Layer fc7
I0909 21:03:56.299235  8743 net.cpp:329] fc7 <- pool2
I0909 21:03:56.299244  8743 net.cpp:290] fc7 -> fc7
I0909 21:03:56.936764  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.936805  8743 net.cpp:125] fc7 needs backward computation.
I0909 21:03:56.936818  8743 net.cpp:66] Creating Layer relu7
I0909 21:03:56.936826  8743 net.cpp:329] relu7 <- fc7
I0909 21:03:56.936833  8743 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:03:56.936843  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.936848  8743 net.cpp:125] relu7 needs backward computation.
I0909 21:03:56.936856  8743 net.cpp:66] Creating Layer drop7
I0909 21:03:56.936861  8743 net.cpp:329] drop7 <- fc7
I0909 21:03:56.936869  8743 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:03:56.936880  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.936887  8743 net.cpp:125] drop7 needs backward computation.
I0909 21:03:56.936894  8743 net.cpp:66] Creating Layer fc8
I0909 21:03:56.936900  8743 net.cpp:329] fc8 <- fc7
I0909 21:03:56.936908  8743 net.cpp:290] fc8 -> fc8
I0909 21:03:56.944681  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.944694  8743 net.cpp:125] fc8 needs backward computation.
I0909 21:03:56.944700  8743 net.cpp:66] Creating Layer relu8
I0909 21:03:56.944706  8743 net.cpp:329] relu8 <- fc8
I0909 21:03:56.944712  8743 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:03:56.944720  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.944725  8743 net.cpp:125] relu8 needs backward computation.
I0909 21:03:56.944732  8743 net.cpp:66] Creating Layer drop8
I0909 21:03:56.944737  8743 net.cpp:329] drop8 <- fc8
I0909 21:03:56.944746  8743 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:03:56.944752  8743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:03:56.944758  8743 net.cpp:125] drop8 needs backward computation.
I0909 21:03:56.944766  8743 net.cpp:66] Creating Layer fc9
I0909 21:03:56.944771  8743 net.cpp:329] fc9 <- fc8
I0909 21:03:56.944779  8743 net.cpp:290] fc9 -> fc9
I0909 21:03:56.945152  8743 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:03:56.945163  8743 net.cpp:125] fc9 needs backward computation.
I0909 21:03:56.945171  8743 net.cpp:66] Creating Layer fc10
I0909 21:03:56.945178  8743 net.cpp:329] fc10 <- fc9
I0909 21:03:56.945185  8743 net.cpp:290] fc10 -> fc10
I0909 21:03:56.945207  8743 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:56.945217  8743 net.cpp:125] fc10 needs backward computation.
I0909 21:03:56.945225  8743 net.cpp:66] Creating Layer prob
I0909 21:03:56.945230  8743 net.cpp:329] prob <- fc10
I0909 21:03:56.945236  8743 net.cpp:290] prob -> prob
I0909 21:03:56.945245  8743 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:03:56.945251  8743 net.cpp:125] prob needs backward computation.
I0909 21:03:56.945257  8743 net.cpp:156] This network produces output prob
I0909 21:03:56.945269  8743 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:03:56.945278  8743 net.cpp:167] Network initialization done.
I0909 21:03:56.945283  8743 net.cpp:168] Memory required for data: 6183480
Classifying 11 inputs.
Done in 6.82 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:04:04.702667  8747 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:04:04.702805  8747 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:04:04.702813  8747 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:04:04.702957  8747 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:04:04.703021  8747 net.cpp:292] Input 0 -> data
I0909 21:04:04.703045  8747 net.cpp:66] Creating Layer conv1
I0909 21:04:04.703052  8747 net.cpp:329] conv1 <- data
I0909 21:04:04.703060  8747 net.cpp:290] conv1 -> conv1
I0909 21:04:04.704383  8747 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:04:04.704401  8747 net.cpp:125] conv1 needs backward computation.
I0909 21:04:04.704411  8747 net.cpp:66] Creating Layer relu1
I0909 21:04:04.704417  8747 net.cpp:329] relu1 <- conv1
I0909 21:04:04.704423  8747 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:04:04.704432  8747 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:04:04.704438  8747 net.cpp:125] relu1 needs backward computation.
I0909 21:04:04.704444  8747 net.cpp:66] Creating Layer pool1
I0909 21:04:04.704450  8747 net.cpp:329] pool1 <- conv1
I0909 21:04:04.704457  8747 net.cpp:290] pool1 -> pool1
I0909 21:04:04.704468  8747 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:04:04.704474  8747 net.cpp:125] pool1 needs backward computation.
I0909 21:04:04.704480  8747 net.cpp:66] Creating Layer norm1
I0909 21:04:04.704486  8747 net.cpp:329] norm1 <- pool1
I0909 21:04:04.704493  8747 net.cpp:290] norm1 -> norm1
I0909 21:04:04.704502  8747 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:04:04.704509  8747 net.cpp:125] norm1 needs backward computation.
I0909 21:04:04.704515  8747 net.cpp:66] Creating Layer conv2
I0909 21:04:04.704520  8747 net.cpp:329] conv2 <- norm1
I0909 21:04:04.704527  8747 net.cpp:290] conv2 -> conv2
I0909 21:04:04.713413  8747 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:04:04.713428  8747 net.cpp:125] conv2 needs backward computation.
I0909 21:04:04.713435  8747 net.cpp:66] Creating Layer relu2
I0909 21:04:04.713441  8747 net.cpp:329] relu2 <- conv2
I0909 21:04:04.713448  8747 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:04:04.713454  8747 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:04:04.713460  8747 net.cpp:125] relu2 needs backward computation.
I0909 21:04:04.713466  8747 net.cpp:66] Creating Layer pool2
I0909 21:04:04.713471  8747 net.cpp:329] pool2 <- conv2
I0909 21:04:04.713479  8747 net.cpp:290] pool2 -> pool2
I0909 21:04:04.713486  8747 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:04:04.713491  8747 net.cpp:125] pool2 needs backward computation.
I0909 21:04:04.713500  8747 net.cpp:66] Creating Layer fc7
I0909 21:04:04.713507  8747 net.cpp:329] fc7 <- pool2
I0909 21:04:04.713518  8747 net.cpp:290] fc7 -> fc7
I0909 21:04:05.353559  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.353603  8747 net.cpp:125] fc7 needs backward computation.
I0909 21:04:05.353618  8747 net.cpp:66] Creating Layer relu7
I0909 21:04:05.353626  8747 net.cpp:329] relu7 <- fc7
I0909 21:04:05.353634  8747 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:04:05.353644  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.353651  8747 net.cpp:125] relu7 needs backward computation.
I0909 21:04:05.353657  8747 net.cpp:66] Creating Layer drop7
I0909 21:04:05.353663  8747 net.cpp:329] drop7 <- fc7
I0909 21:04:05.353672  8747 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:04:05.353682  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.353688  8747 net.cpp:125] drop7 needs backward computation.
I0909 21:04:05.353698  8747 net.cpp:66] Creating Layer fc8
I0909 21:04:05.353703  8747 net.cpp:329] fc8 <- fc7
I0909 21:04:05.353709  8747 net.cpp:290] fc8 -> fc8
I0909 21:04:05.361497  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.361515  8747 net.cpp:125] fc8 needs backward computation.
I0909 21:04:05.361522  8747 net.cpp:66] Creating Layer relu8
I0909 21:04:05.361538  8747 net.cpp:329] relu8 <- fc8
I0909 21:04:05.361546  8747 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:04:05.361553  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.361559  8747 net.cpp:125] relu8 needs backward computation.
I0909 21:04:05.361565  8747 net.cpp:66] Creating Layer drop8
I0909 21:04:05.361570  8747 net.cpp:329] drop8 <- fc8
I0909 21:04:05.361578  8747 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:04:05.361585  8747 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:05.361592  8747 net.cpp:125] drop8 needs backward computation.
I0909 21:04:05.361599  8747 net.cpp:66] Creating Layer fc9
I0909 21:04:05.361604  8747 net.cpp:329] fc9 <- fc8
I0909 21:04:05.361613  8747 net.cpp:290] fc9 -> fc9
I0909 21:04:05.361985  8747 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:04:05.361996  8747 net.cpp:125] fc9 needs backward computation.
I0909 21:04:05.362005  8747 net.cpp:66] Creating Layer fc10
I0909 21:04:05.362010  8747 net.cpp:329] fc10 <- fc9
I0909 21:04:05.362020  8747 net.cpp:290] fc10 -> fc10
I0909 21:04:05.362031  8747 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:04:05.362040  8747 net.cpp:125] fc10 needs backward computation.
I0909 21:04:05.362047  8747 net.cpp:66] Creating Layer prob
I0909 21:04:05.362053  8747 net.cpp:329] prob <- fc10
I0909 21:04:05.362059  8747 net.cpp:290] prob -> prob
I0909 21:04:05.362069  8747 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:04:05.362076  8747 net.cpp:125] prob needs backward computation.
I0909 21:04:05.362079  8747 net.cpp:156] This network produces output prob
I0909 21:04:05.362092  8747 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:04:05.362102  8747 net.cpp:167] Network initialization done.
I0909 21:04:05.362107  8747 net.cpp:168] Memory required for data: 6183480
Classifying 74 inputs.
Done in 46.52 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:04:58.185339  8751 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:04:58.185480  8751 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:04:58.185489  8751 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:04:58.185662  8751 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:04:58.185729  8751 net.cpp:292] Input 0 -> data
I0909 21:04:58.185755  8751 net.cpp:66] Creating Layer conv1
I0909 21:04:58.185761  8751 net.cpp:329] conv1 <- data
I0909 21:04:58.185770  8751 net.cpp:290] conv1 -> conv1
I0909 21:04:58.187131  8751 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:04:58.187150  8751 net.cpp:125] conv1 needs backward computation.
I0909 21:04:58.187160  8751 net.cpp:66] Creating Layer relu1
I0909 21:04:58.187165  8751 net.cpp:329] relu1 <- conv1
I0909 21:04:58.187171  8751 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:04:58.187180  8751 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:04:58.187186  8751 net.cpp:125] relu1 needs backward computation.
I0909 21:04:58.187193  8751 net.cpp:66] Creating Layer pool1
I0909 21:04:58.187199  8751 net.cpp:329] pool1 <- conv1
I0909 21:04:58.187206  8751 net.cpp:290] pool1 -> pool1
I0909 21:04:58.187217  8751 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:04:58.187223  8751 net.cpp:125] pool1 needs backward computation.
I0909 21:04:58.187230  8751 net.cpp:66] Creating Layer norm1
I0909 21:04:58.187237  8751 net.cpp:329] norm1 <- pool1
I0909 21:04:58.187242  8751 net.cpp:290] norm1 -> norm1
I0909 21:04:58.187252  8751 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:04:58.187258  8751 net.cpp:125] norm1 needs backward computation.
I0909 21:04:58.187266  8751 net.cpp:66] Creating Layer conv2
I0909 21:04:58.187271  8751 net.cpp:329] conv2 <- norm1
I0909 21:04:58.187279  8751 net.cpp:290] conv2 -> conv2
I0909 21:04:58.196403  8751 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:04:58.196418  8751 net.cpp:125] conv2 needs backward computation.
I0909 21:04:58.196424  8751 net.cpp:66] Creating Layer relu2
I0909 21:04:58.196430  8751 net.cpp:329] relu2 <- conv2
I0909 21:04:58.196437  8751 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:04:58.196444  8751 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:04:58.196450  8751 net.cpp:125] relu2 needs backward computation.
I0909 21:04:58.196456  8751 net.cpp:66] Creating Layer pool2
I0909 21:04:58.196462  8751 net.cpp:329] pool2 <- conv2
I0909 21:04:58.196468  8751 net.cpp:290] pool2 -> pool2
I0909 21:04:58.196477  8751 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:04:58.196483  8751 net.cpp:125] pool2 needs backward computation.
I0909 21:04:58.196492  8751 net.cpp:66] Creating Layer fc7
I0909 21:04:58.196498  8751 net.cpp:329] fc7 <- pool2
I0909 21:04:58.196506  8751 net.cpp:290] fc7 -> fc7
I0909 21:04:58.841850  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.841895  8751 net.cpp:125] fc7 needs backward computation.
I0909 21:04:58.841920  8751 net.cpp:66] Creating Layer relu7
I0909 21:04:58.841928  8751 net.cpp:329] relu7 <- fc7
I0909 21:04:58.841936  8751 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:04:58.841946  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.841953  8751 net.cpp:125] relu7 needs backward computation.
I0909 21:04:58.841959  8751 net.cpp:66] Creating Layer drop7
I0909 21:04:58.841965  8751 net.cpp:329] drop7 <- fc7
I0909 21:04:58.841974  8751 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:04:58.841984  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.841990  8751 net.cpp:125] drop7 needs backward computation.
I0909 21:04:58.842000  8751 net.cpp:66] Creating Layer fc8
I0909 21:04:58.842005  8751 net.cpp:329] fc8 <- fc7
I0909 21:04:58.842012  8751 net.cpp:290] fc8 -> fc8
I0909 21:04:58.849798  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.849812  8751 net.cpp:125] fc8 needs backward computation.
I0909 21:04:58.849819  8751 net.cpp:66] Creating Layer relu8
I0909 21:04:58.849824  8751 net.cpp:329] relu8 <- fc8
I0909 21:04:58.849831  8751 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:04:58.849838  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.849844  8751 net.cpp:125] relu8 needs backward computation.
I0909 21:04:58.849850  8751 net.cpp:66] Creating Layer drop8
I0909 21:04:58.849856  8751 net.cpp:329] drop8 <- fc8
I0909 21:04:58.849864  8751 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:04:58.849871  8751 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:04:58.849877  8751 net.cpp:125] drop8 needs backward computation.
I0909 21:04:58.849884  8751 net.cpp:66] Creating Layer fc9
I0909 21:04:58.849890  8751 net.cpp:329] fc9 <- fc8
I0909 21:04:58.849899  8751 net.cpp:290] fc9 -> fc9
I0909 21:04:58.850272  8751 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:04:58.850283  8751 net.cpp:125] fc9 needs backward computation.
I0909 21:04:58.850291  8751 net.cpp:66] Creating Layer fc10
I0909 21:04:58.850297  8751 net.cpp:329] fc10 <- fc9
I0909 21:04:58.850306  8751 net.cpp:290] fc10 -> fc10
I0909 21:04:58.850318  8751 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:04:58.850327  8751 net.cpp:125] fc10 needs backward computation.
I0909 21:04:58.850334  8751 net.cpp:66] Creating Layer prob
I0909 21:04:58.850340  8751 net.cpp:329] prob <- fc10
I0909 21:04:58.850347  8751 net.cpp:290] prob -> prob
I0909 21:04:58.850356  8751 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:04:58.850363  8751 net.cpp:125] prob needs backward computation.
I0909 21:04:58.850368  8751 net.cpp:156] This network produces output prob
I0909 21:04:58.850380  8751 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:04:58.850389  8751 net.cpp:167] Network initialization done.
I0909 21:04:58.850394  8751 net.cpp:168] Memory required for data: 6183480
Classifying 11 inputs.
Done in 6.65 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:05:06.575448  8755 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:05:06.575584  8755 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:05:06.575592  8755 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:05:06.575736  8755 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:05:06.575798  8755 net.cpp:292] Input 0 -> data
I0909 21:05:06.575824  8755 net.cpp:66] Creating Layer conv1
I0909 21:05:06.575830  8755 net.cpp:329] conv1 <- data
I0909 21:05:06.575839  8755 net.cpp:290] conv1 -> conv1
I0909 21:05:06.577163  8755 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:05:06.577182  8755 net.cpp:125] conv1 needs backward computation.
I0909 21:05:06.577189  8755 net.cpp:66] Creating Layer relu1
I0909 21:05:06.577195  8755 net.cpp:329] relu1 <- conv1
I0909 21:05:06.577203  8755 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:05:06.577210  8755 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:05:06.577216  8755 net.cpp:125] relu1 needs backward computation.
I0909 21:05:06.577224  8755 net.cpp:66] Creating Layer pool1
I0909 21:05:06.577229  8755 net.cpp:329] pool1 <- conv1
I0909 21:05:06.577235  8755 net.cpp:290] pool1 -> pool1
I0909 21:05:06.577246  8755 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:05:06.577252  8755 net.cpp:125] pool1 needs backward computation.
I0909 21:05:06.577258  8755 net.cpp:66] Creating Layer norm1
I0909 21:05:06.577263  8755 net.cpp:329] norm1 <- pool1
I0909 21:05:06.577270  8755 net.cpp:290] norm1 -> norm1
I0909 21:05:06.577280  8755 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:05:06.577286  8755 net.cpp:125] norm1 needs backward computation.
I0909 21:05:06.577292  8755 net.cpp:66] Creating Layer conv2
I0909 21:05:06.577298  8755 net.cpp:329] conv2 <- norm1
I0909 21:05:06.577304  8755 net.cpp:290] conv2 -> conv2
I0909 21:05:06.586354  8755 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:05:06.586374  8755 net.cpp:125] conv2 needs backward computation.
I0909 21:05:06.586380  8755 net.cpp:66] Creating Layer relu2
I0909 21:05:06.586386  8755 net.cpp:329] relu2 <- conv2
I0909 21:05:06.586393  8755 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:05:06.586400  8755 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:05:06.586405  8755 net.cpp:125] relu2 needs backward computation.
I0909 21:05:06.586411  8755 net.cpp:66] Creating Layer pool2
I0909 21:05:06.586417  8755 net.cpp:329] pool2 <- conv2
I0909 21:05:06.586423  8755 net.cpp:290] pool2 -> pool2
I0909 21:05:06.586431  8755 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:05:06.586436  8755 net.cpp:125] pool2 needs backward computation.
I0909 21:05:06.586443  8755 net.cpp:66] Creating Layer fc7
I0909 21:05:06.586448  8755 net.cpp:329] fc7 <- pool2
I0909 21:05:06.586457  8755 net.cpp:290] fc7 -> fc7
I0909 21:05:07.228149  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.228191  8755 net.cpp:125] fc7 needs backward computation.
I0909 21:05:07.228205  8755 net.cpp:66] Creating Layer relu7
I0909 21:05:07.228212  8755 net.cpp:329] relu7 <- fc7
I0909 21:05:07.228220  8755 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:05:07.228230  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.228236  8755 net.cpp:125] relu7 needs backward computation.
I0909 21:05:07.228245  8755 net.cpp:66] Creating Layer drop7
I0909 21:05:07.228250  8755 net.cpp:329] drop7 <- fc7
I0909 21:05:07.228257  8755 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:05:07.228268  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.228274  8755 net.cpp:125] drop7 needs backward computation.
I0909 21:05:07.228283  8755 net.cpp:66] Creating Layer fc8
I0909 21:05:07.228288  8755 net.cpp:329] fc8 <- fc7
I0909 21:05:07.228296  8755 net.cpp:290] fc8 -> fc8
I0909 21:05:07.236104  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.236124  8755 net.cpp:125] fc8 needs backward computation.
I0909 21:05:07.236131  8755 net.cpp:66] Creating Layer relu8
I0909 21:05:07.236137  8755 net.cpp:329] relu8 <- fc8
I0909 21:05:07.236143  8755 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:05:07.236150  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.236156  8755 net.cpp:125] relu8 needs backward computation.
I0909 21:05:07.236162  8755 net.cpp:66] Creating Layer drop8
I0909 21:05:07.236167  8755 net.cpp:329] drop8 <- fc8
I0909 21:05:07.236176  8755 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:05:07.236183  8755 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:05:07.236188  8755 net.cpp:125] drop8 needs backward computation.
I0909 21:05:07.236196  8755 net.cpp:66] Creating Layer fc9
I0909 21:05:07.236202  8755 net.cpp:329] fc9 <- fc8
I0909 21:05:07.236210  8755 net.cpp:290] fc9 -> fc9
I0909 21:05:07.236583  8755 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:05:07.236594  8755 net.cpp:125] fc9 needs backward computation.
I0909 21:05:07.236603  8755 net.cpp:66] Creating Layer fc10
I0909 21:05:07.236608  8755 net.cpp:329] fc10 <- fc9
I0909 21:05:07.236616  8755 net.cpp:290] fc10 -> fc10
I0909 21:05:07.236629  8755 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:05:07.236637  8755 net.cpp:125] fc10 needs backward computation.
I0909 21:05:07.236644  8755 net.cpp:66] Creating Layer prob
I0909 21:05:07.236650  8755 net.cpp:329] prob <- fc10
I0909 21:05:07.236656  8755 net.cpp:290] prob -> prob
I0909 21:05:07.236665  8755 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:05:07.236671  8755 net.cpp:125] prob needs backward computation.
I0909 21:05:07.236676  8755 net.cpp:156] This network produces output prob
I0909 21:05:07.236690  8755 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:05:07.236698  8755 net.cpp:167] Network initialization done.
I0909 21:05:07.236703  8755 net.cpp:168] Memory required for data: 6183480
Classifying 238 inputs.
Done in 148.16 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:07:40.479627  8760 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:07:40.479780  8760 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:07:40.479789  8760 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:07:40.479936  8760 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:07:40.479990  8760 net.cpp:292] Input 0 -> data
I0909 21:07:40.480015  8760 net.cpp:66] Creating Layer conv1
I0909 21:07:40.480022  8760 net.cpp:329] conv1 <- data
I0909 21:07:40.480031  8760 net.cpp:290] conv1 -> conv1
I0909 21:07:40.481391  8760 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:07:40.481410  8760 net.cpp:125] conv1 needs backward computation.
I0909 21:07:40.481420  8760 net.cpp:66] Creating Layer relu1
I0909 21:07:40.481426  8760 net.cpp:329] relu1 <- conv1
I0909 21:07:40.481431  8760 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:07:40.481446  8760 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:07:40.481451  8760 net.cpp:125] relu1 needs backward computation.
I0909 21:07:40.481458  8760 net.cpp:66] Creating Layer pool1
I0909 21:07:40.481464  8760 net.cpp:329] pool1 <- conv1
I0909 21:07:40.481470  8760 net.cpp:290] pool1 -> pool1
I0909 21:07:40.481482  8760 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:07:40.481487  8760 net.cpp:125] pool1 needs backward computation.
I0909 21:07:40.481494  8760 net.cpp:66] Creating Layer norm1
I0909 21:07:40.481500  8760 net.cpp:329] norm1 <- pool1
I0909 21:07:40.481506  8760 net.cpp:290] norm1 -> norm1
I0909 21:07:40.481539  8760 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:07:40.481546  8760 net.cpp:125] norm1 needs backward computation.
I0909 21:07:40.481554  8760 net.cpp:66] Creating Layer conv2
I0909 21:07:40.481560  8760 net.cpp:329] conv2 <- norm1
I0909 21:07:40.481567  8760 net.cpp:290] conv2 -> conv2
I0909 21:07:40.490711  8760 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:07:40.490725  8760 net.cpp:125] conv2 needs backward computation.
I0909 21:07:40.490733  8760 net.cpp:66] Creating Layer relu2
I0909 21:07:40.490738  8760 net.cpp:329] relu2 <- conv2
I0909 21:07:40.490746  8760 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:07:40.490752  8760 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:07:40.490758  8760 net.cpp:125] relu2 needs backward computation.
I0909 21:07:40.490764  8760 net.cpp:66] Creating Layer pool2
I0909 21:07:40.490769  8760 net.cpp:329] pool2 <- conv2
I0909 21:07:40.490777  8760 net.cpp:290] pool2 -> pool2
I0909 21:07:40.490784  8760 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:07:40.490789  8760 net.cpp:125] pool2 needs backward computation.
I0909 21:07:40.490799  8760 net.cpp:66] Creating Layer fc7
I0909 21:07:40.490805  8760 net.cpp:329] fc7 <- pool2
I0909 21:07:40.490813  8760 net.cpp:290] fc7 -> fc7
I0909 21:07:41.134449  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.134490  8760 net.cpp:125] fc7 needs backward computation.
I0909 21:07:41.134503  8760 net.cpp:66] Creating Layer relu7
I0909 21:07:41.134511  8760 net.cpp:329] relu7 <- fc7
I0909 21:07:41.134520  8760 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:07:41.134528  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.134534  8760 net.cpp:125] relu7 needs backward computation.
I0909 21:07:41.134541  8760 net.cpp:66] Creating Layer drop7
I0909 21:07:41.134547  8760 net.cpp:329] drop7 <- fc7
I0909 21:07:41.134556  8760 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:07:41.134567  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.134572  8760 net.cpp:125] drop7 needs backward computation.
I0909 21:07:41.134580  8760 net.cpp:66] Creating Layer fc8
I0909 21:07:41.134587  8760 net.cpp:329] fc8 <- fc7
I0909 21:07:41.134593  8760 net.cpp:290] fc8 -> fc8
I0909 21:07:41.142382  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.142395  8760 net.cpp:125] fc8 needs backward computation.
I0909 21:07:41.142402  8760 net.cpp:66] Creating Layer relu8
I0909 21:07:41.142408  8760 net.cpp:329] relu8 <- fc8
I0909 21:07:41.142415  8760 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:07:41.142421  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.142427  8760 net.cpp:125] relu8 needs backward computation.
I0909 21:07:41.142433  8760 net.cpp:66] Creating Layer drop8
I0909 21:07:41.142439  8760 net.cpp:329] drop8 <- fc8
I0909 21:07:41.142447  8760 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:07:41.142454  8760 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:07:41.142460  8760 net.cpp:125] drop8 needs backward computation.
I0909 21:07:41.142467  8760 net.cpp:66] Creating Layer fc9
I0909 21:07:41.142472  8760 net.cpp:329] fc9 <- fc8
I0909 21:07:41.142482  8760 net.cpp:290] fc9 -> fc9
I0909 21:07:41.142854  8760 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:07:41.142865  8760 net.cpp:125] fc9 needs backward computation.
I0909 21:07:41.142874  8760 net.cpp:66] Creating Layer fc10
I0909 21:07:41.142889  8760 net.cpp:329] fc10 <- fc9
I0909 21:07:41.142899  8760 net.cpp:290] fc10 -> fc10
I0909 21:07:41.142910  8760 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:07:41.142920  8760 net.cpp:125] fc10 needs backward computation.
I0909 21:07:41.142927  8760 net.cpp:66] Creating Layer prob
I0909 21:07:41.142932  8760 net.cpp:329] prob <- fc10
I0909 21:07:41.142940  8760 net.cpp:290] prob -> prob
I0909 21:07:41.142948  8760 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:07:41.142954  8760 net.cpp:125] prob needs backward computation.
I0909 21:07:41.142959  8760 net.cpp:156] This network produces output prob
I0909 21:07:41.142971  8760 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:07:41.142981  8760 net.cpp:167] Network initialization done.
I0909 21:07:41.142985  8760 net.cpp:168] Memory required for data: 6183480
Classifying 67 inputs.
Done in 42.24 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:08:27.705994  8769 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:08:27.706132  8769 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:08:27.706141  8769 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:08:27.706286  8769 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:08:27.706351  8769 net.cpp:292] Input 0 -> data
I0909 21:08:27.706375  8769 net.cpp:66] Creating Layer conv1
I0909 21:08:27.706382  8769 net.cpp:329] conv1 <- data
I0909 21:08:27.706390  8769 net.cpp:290] conv1 -> conv1
I0909 21:08:27.707728  8769 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:08:27.707746  8769 net.cpp:125] conv1 needs backward computation.
I0909 21:08:27.707754  8769 net.cpp:66] Creating Layer relu1
I0909 21:08:27.707761  8769 net.cpp:329] relu1 <- conv1
I0909 21:08:27.707767  8769 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:08:27.707775  8769 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:08:27.707782  8769 net.cpp:125] relu1 needs backward computation.
I0909 21:08:27.707788  8769 net.cpp:66] Creating Layer pool1
I0909 21:08:27.707793  8769 net.cpp:329] pool1 <- conv1
I0909 21:08:27.707800  8769 net.cpp:290] pool1 -> pool1
I0909 21:08:27.707810  8769 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:08:27.707816  8769 net.cpp:125] pool1 needs backward computation.
I0909 21:08:27.707823  8769 net.cpp:66] Creating Layer norm1
I0909 21:08:27.707828  8769 net.cpp:329] norm1 <- pool1
I0909 21:08:27.707835  8769 net.cpp:290] norm1 -> norm1
I0909 21:08:27.707844  8769 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:08:27.707850  8769 net.cpp:125] norm1 needs backward computation.
I0909 21:08:27.707857  8769 net.cpp:66] Creating Layer conv2
I0909 21:08:27.707862  8769 net.cpp:329] conv2 <- norm1
I0909 21:08:27.707870  8769 net.cpp:290] conv2 -> conv2
I0909 21:08:27.716944  8769 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:08:27.716959  8769 net.cpp:125] conv2 needs backward computation.
I0909 21:08:27.716966  8769 net.cpp:66] Creating Layer relu2
I0909 21:08:27.716972  8769 net.cpp:329] relu2 <- conv2
I0909 21:08:27.716979  8769 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:08:27.716986  8769 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:08:27.716991  8769 net.cpp:125] relu2 needs backward computation.
I0909 21:08:27.716999  8769 net.cpp:66] Creating Layer pool2
I0909 21:08:27.717003  8769 net.cpp:329] pool2 <- conv2
I0909 21:08:27.717010  8769 net.cpp:290] pool2 -> pool2
I0909 21:08:27.717018  8769 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:08:27.717025  8769 net.cpp:125] pool2 needs backward computation.
I0909 21:08:27.717031  8769 net.cpp:66] Creating Layer fc7
I0909 21:08:27.717036  8769 net.cpp:329] fc7 <- pool2
I0909 21:08:27.717046  8769 net.cpp:290] fc7 -> fc7
I0909 21:08:28.355794  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.355840  8769 net.cpp:125] fc7 needs backward computation.
I0909 21:08:28.355854  8769 net.cpp:66] Creating Layer relu7
I0909 21:08:28.355861  8769 net.cpp:329] relu7 <- fc7
I0909 21:08:28.355870  8769 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:08:28.355880  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.355885  8769 net.cpp:125] relu7 needs backward computation.
I0909 21:08:28.355893  8769 net.cpp:66] Creating Layer drop7
I0909 21:08:28.355898  8769 net.cpp:329] drop7 <- fc7
I0909 21:08:28.355906  8769 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:08:28.355917  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.355923  8769 net.cpp:125] drop7 needs backward computation.
I0909 21:08:28.355931  8769 net.cpp:66] Creating Layer fc8
I0909 21:08:28.355937  8769 net.cpp:329] fc8 <- fc7
I0909 21:08:28.355944  8769 net.cpp:290] fc8 -> fc8
I0909 21:08:28.363736  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.363749  8769 net.cpp:125] fc8 needs backward computation.
I0909 21:08:28.363766  8769 net.cpp:66] Creating Layer relu8
I0909 21:08:28.363772  8769 net.cpp:329] relu8 <- fc8
I0909 21:08:28.363780  8769 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:08:28.363786  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.363792  8769 net.cpp:125] relu8 needs backward computation.
I0909 21:08:28.363798  8769 net.cpp:66] Creating Layer drop8
I0909 21:08:28.363803  8769 net.cpp:329] drop8 <- fc8
I0909 21:08:28.363811  8769 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:08:28.363819  8769 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:08:28.363824  8769 net.cpp:125] drop8 needs backward computation.
I0909 21:08:28.363832  8769 net.cpp:66] Creating Layer fc9
I0909 21:08:28.363837  8769 net.cpp:329] fc9 <- fc8
I0909 21:08:28.363845  8769 net.cpp:290] fc9 -> fc9
I0909 21:08:28.364223  8769 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:08:28.364233  8769 net.cpp:125] fc9 needs backward computation.
I0909 21:08:28.364240  8769 net.cpp:66] Creating Layer fc10
I0909 21:08:28.364246  8769 net.cpp:329] fc10 <- fc9
I0909 21:08:28.364255  8769 net.cpp:290] fc10 -> fc10
I0909 21:08:28.364267  8769 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:08:28.364276  8769 net.cpp:125] fc10 needs backward computation.
I0909 21:08:28.364284  8769 net.cpp:66] Creating Layer prob
I0909 21:08:28.364289  8769 net.cpp:329] prob <- fc10
I0909 21:08:28.364295  8769 net.cpp:290] prob -> prob
I0909 21:08:28.364305  8769 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:08:28.364311  8769 net.cpp:125] prob needs backward computation.
I0909 21:08:28.364316  8769 net.cpp:156] This network produces output prob
I0909 21:08:28.364330  8769 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:08:28.364337  8769 net.cpp:167] Network initialization done.
I0909 21:08:28.364342  8769 net.cpp:168] Memory required for data: 6183480
Classifying 115 inputs.
Done in 70.81 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:09:41.461494  8785 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:09:41.461663  8785 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:09:41.461673  8785 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:09:41.461820  8785 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:09:41.461885  8785 net.cpp:292] Input 0 -> data
I0909 21:09:41.461912  8785 net.cpp:66] Creating Layer conv1
I0909 21:09:41.461920  8785 net.cpp:329] conv1 <- data
I0909 21:09:41.461927  8785 net.cpp:290] conv1 -> conv1
I0909 21:09:41.463290  8785 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:09:41.463309  8785 net.cpp:125] conv1 needs backward computation.
I0909 21:09:41.463317  8785 net.cpp:66] Creating Layer relu1
I0909 21:09:41.463323  8785 net.cpp:329] relu1 <- conv1
I0909 21:09:41.463330  8785 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:09:41.463340  8785 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:09:41.463346  8785 net.cpp:125] relu1 needs backward computation.
I0909 21:09:41.463352  8785 net.cpp:66] Creating Layer pool1
I0909 21:09:41.463357  8785 net.cpp:329] pool1 <- conv1
I0909 21:09:41.463364  8785 net.cpp:290] pool1 -> pool1
I0909 21:09:41.463376  8785 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:09:41.463382  8785 net.cpp:125] pool1 needs backward computation.
I0909 21:09:41.463388  8785 net.cpp:66] Creating Layer norm1
I0909 21:09:41.463394  8785 net.cpp:329] norm1 <- pool1
I0909 21:09:41.463402  8785 net.cpp:290] norm1 -> norm1
I0909 21:09:41.463410  8785 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:09:41.463417  8785 net.cpp:125] norm1 needs backward computation.
I0909 21:09:41.463423  8785 net.cpp:66] Creating Layer conv2
I0909 21:09:41.463429  8785 net.cpp:329] conv2 <- norm1
I0909 21:09:41.463438  8785 net.cpp:290] conv2 -> conv2
I0909 21:09:41.472563  8785 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:09:41.472579  8785 net.cpp:125] conv2 needs backward computation.
I0909 21:09:41.472585  8785 net.cpp:66] Creating Layer relu2
I0909 21:09:41.472591  8785 net.cpp:329] relu2 <- conv2
I0909 21:09:41.472599  8785 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:09:41.472605  8785 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:09:41.472611  8785 net.cpp:125] relu2 needs backward computation.
I0909 21:09:41.472617  8785 net.cpp:66] Creating Layer pool2
I0909 21:09:41.472623  8785 net.cpp:329] pool2 <- conv2
I0909 21:09:41.472630  8785 net.cpp:290] pool2 -> pool2
I0909 21:09:41.472637  8785 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:09:41.472643  8785 net.cpp:125] pool2 needs backward computation.
I0909 21:09:41.472653  8785 net.cpp:66] Creating Layer fc7
I0909 21:09:41.472659  8785 net.cpp:329] fc7 <- pool2
I0909 21:09:41.472666  8785 net.cpp:290] fc7 -> fc7
I0909 21:09:42.119774  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.119824  8785 net.cpp:125] fc7 needs backward computation.
I0909 21:09:42.119839  8785 net.cpp:66] Creating Layer relu7
I0909 21:09:42.119846  8785 net.cpp:329] relu7 <- fc7
I0909 21:09:42.119854  8785 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:09:42.119863  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.119869  8785 net.cpp:125] relu7 needs backward computation.
I0909 21:09:42.119876  8785 net.cpp:66] Creating Layer drop7
I0909 21:09:42.119882  8785 net.cpp:329] drop7 <- fc7
I0909 21:09:42.119890  8785 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:09:42.119901  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.119907  8785 net.cpp:125] drop7 needs backward computation.
I0909 21:09:42.119916  8785 net.cpp:66] Creating Layer fc8
I0909 21:09:42.119921  8785 net.cpp:329] fc8 <- fc7
I0909 21:09:42.119930  8785 net.cpp:290] fc8 -> fc8
I0909 21:09:42.127758  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.127770  8785 net.cpp:125] fc8 needs backward computation.
I0909 21:09:42.127778  8785 net.cpp:66] Creating Layer relu8
I0909 21:09:42.127784  8785 net.cpp:329] relu8 <- fc8
I0909 21:09:42.127790  8785 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:09:42.127797  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.127804  8785 net.cpp:125] relu8 needs backward computation.
I0909 21:09:42.127810  8785 net.cpp:66] Creating Layer drop8
I0909 21:09:42.127815  8785 net.cpp:329] drop8 <- fc8
I0909 21:09:42.127823  8785 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:09:42.127830  8785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:09:42.127835  8785 net.cpp:125] drop8 needs backward computation.
I0909 21:09:42.127843  8785 net.cpp:66] Creating Layer fc9
I0909 21:09:42.127848  8785 net.cpp:329] fc9 <- fc8
I0909 21:09:42.127857  8785 net.cpp:290] fc9 -> fc9
I0909 21:09:42.128232  8785 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:09:42.128243  8785 net.cpp:125] fc9 needs backward computation.
I0909 21:09:42.128252  8785 net.cpp:66] Creating Layer fc10
I0909 21:09:42.128257  8785 net.cpp:329] fc10 <- fc9
I0909 21:09:42.128265  8785 net.cpp:290] fc10 -> fc10
I0909 21:09:42.128278  8785 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:09:42.128286  8785 net.cpp:125] fc10 needs backward computation.
I0909 21:09:42.128293  8785 net.cpp:66] Creating Layer prob
I0909 21:09:42.128299  8785 net.cpp:329] prob <- fc10
I0909 21:09:42.128305  8785 net.cpp:290] prob -> prob
I0909 21:09:42.128315  8785 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:09:42.128321  8785 net.cpp:125] prob needs backward computation.
I0909 21:09:42.128326  8785 net.cpp:156] This network produces output prob
I0909 21:09:42.128339  8785 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:09:42.128347  8785 net.cpp:167] Network initialization done.
I0909 21:09:42.128352  8785 net.cpp:168] Memory required for data: 6183480
Classifying 78 inputs.
Done in 48.88 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:10:33.659039  8790 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:10:33.659183  8790 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:10:33.659191  8790 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:10:33.659338  8790 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:10:33.659402  8790 net.cpp:292] Input 0 -> data
I0909 21:10:33.659428  8790 net.cpp:66] Creating Layer conv1
I0909 21:10:33.659435  8790 net.cpp:329] conv1 <- data
I0909 21:10:33.659443  8790 net.cpp:290] conv1 -> conv1
I0909 21:10:33.660804  8790 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:10:33.660822  8790 net.cpp:125] conv1 needs backward computation.
I0909 21:10:33.660831  8790 net.cpp:66] Creating Layer relu1
I0909 21:10:33.660837  8790 net.cpp:329] relu1 <- conv1
I0909 21:10:33.660843  8790 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:10:33.660852  8790 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:10:33.660858  8790 net.cpp:125] relu1 needs backward computation.
I0909 21:10:33.660864  8790 net.cpp:66] Creating Layer pool1
I0909 21:10:33.660869  8790 net.cpp:329] pool1 <- conv1
I0909 21:10:33.660876  8790 net.cpp:290] pool1 -> pool1
I0909 21:10:33.660887  8790 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:10:33.660893  8790 net.cpp:125] pool1 needs backward computation.
I0909 21:10:33.660899  8790 net.cpp:66] Creating Layer norm1
I0909 21:10:33.660905  8790 net.cpp:329] norm1 <- pool1
I0909 21:10:33.660912  8790 net.cpp:290] norm1 -> norm1
I0909 21:10:33.660922  8790 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:10:33.660928  8790 net.cpp:125] norm1 needs backward computation.
I0909 21:10:33.660934  8790 net.cpp:66] Creating Layer conv2
I0909 21:10:33.660939  8790 net.cpp:329] conv2 <- norm1
I0909 21:10:33.660951  8790 net.cpp:290] conv2 -> conv2
I0909 21:10:33.670094  8790 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:10:33.670109  8790 net.cpp:125] conv2 needs backward computation.
I0909 21:10:33.670116  8790 net.cpp:66] Creating Layer relu2
I0909 21:10:33.670122  8790 net.cpp:329] relu2 <- conv2
I0909 21:10:33.670130  8790 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:10:33.670136  8790 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:10:33.670141  8790 net.cpp:125] relu2 needs backward computation.
I0909 21:10:33.670148  8790 net.cpp:66] Creating Layer pool2
I0909 21:10:33.670153  8790 net.cpp:329] pool2 <- conv2
I0909 21:10:33.670161  8790 net.cpp:290] pool2 -> pool2
I0909 21:10:33.670168  8790 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:10:33.670173  8790 net.cpp:125] pool2 needs backward computation.
I0909 21:10:33.670183  8790 net.cpp:66] Creating Layer fc7
I0909 21:10:33.670189  8790 net.cpp:329] fc7 <- pool2
I0909 21:10:33.670197  8790 net.cpp:290] fc7 -> fc7
I0909 21:10:34.354887  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.354931  8790 net.cpp:125] fc7 needs backward computation.
I0909 21:10:34.354945  8790 net.cpp:66] Creating Layer relu7
I0909 21:10:34.354953  8790 net.cpp:329] relu7 <- fc7
I0909 21:10:34.354960  8790 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:10:34.354970  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.354976  8790 net.cpp:125] relu7 needs backward computation.
I0909 21:10:34.354984  8790 net.cpp:66] Creating Layer drop7
I0909 21:10:34.354990  8790 net.cpp:329] drop7 <- fc7
I0909 21:10:34.354996  8790 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:10:34.355008  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.355013  8790 net.cpp:125] drop7 needs backward computation.
I0909 21:10:34.355022  8790 net.cpp:66] Creating Layer fc8
I0909 21:10:34.355028  8790 net.cpp:329] fc8 <- fc7
I0909 21:10:34.355036  8790 net.cpp:290] fc8 -> fc8
I0909 21:10:34.363636  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.363673  8790 net.cpp:125] fc8 needs backward computation.
I0909 21:10:34.363683  8790 net.cpp:66] Creating Layer relu8
I0909 21:10:34.363690  8790 net.cpp:329] relu8 <- fc8
I0909 21:10:34.363698  8790 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:10:34.363708  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.363714  8790 net.cpp:125] relu8 needs backward computation.
I0909 21:10:34.363723  8790 net.cpp:66] Creating Layer drop8
I0909 21:10:34.363728  8790 net.cpp:329] drop8 <- fc8
I0909 21:10:34.363735  8790 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:10:34.363744  8790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:34.363749  8790 net.cpp:125] drop8 needs backward computation.
I0909 21:10:34.363759  8790 net.cpp:66] Creating Layer fc9
I0909 21:10:34.363764  8790 net.cpp:329] fc9 <- fc8
I0909 21:10:34.363772  8790 net.cpp:290] fc9 -> fc9
I0909 21:10:34.364194  8790 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:10:34.364207  8790 net.cpp:125] fc9 needs backward computation.
I0909 21:10:34.364215  8790 net.cpp:66] Creating Layer fc10
I0909 21:10:34.364220  8790 net.cpp:329] fc10 <- fc9
I0909 21:10:34.364229  8790 net.cpp:290] fc10 -> fc10
I0909 21:10:34.364241  8790 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:10:34.364251  8790 net.cpp:125] fc10 needs backward computation.
I0909 21:10:34.364259  8790 net.cpp:66] Creating Layer prob
I0909 21:10:34.364264  8790 net.cpp:329] prob <- fc10
I0909 21:10:34.364271  8790 net.cpp:290] prob -> prob
I0909 21:10:34.364282  8790 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:10:34.364289  8790 net.cpp:125] prob needs backward computation.
I0909 21:10:34.364294  8790 net.cpp:156] This network produces output prob
I0909 21:10:34.364307  8790 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:10:34.364317  8790 net.cpp:167] Network initialization done.
I0909 21:10:34.364322  8790 net.cpp:168] Memory required for data: 6183480
Classifying 25 inputs.
Done in 15.66 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:10:51.178664  8793 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:10:51.178817  8793 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:10:51.178827  8793 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:10:51.178973  8793 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:10:51.179026  8793 net.cpp:292] Input 0 -> data
I0909 21:10:51.179052  8793 net.cpp:66] Creating Layer conv1
I0909 21:10:51.179059  8793 net.cpp:329] conv1 <- data
I0909 21:10:51.179067  8793 net.cpp:290] conv1 -> conv1
I0909 21:10:51.180430  8793 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:10:51.180449  8793 net.cpp:125] conv1 needs backward computation.
I0909 21:10:51.180457  8793 net.cpp:66] Creating Layer relu1
I0909 21:10:51.180469  8793 net.cpp:329] relu1 <- conv1
I0909 21:10:51.180476  8793 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:10:51.180485  8793 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:10:51.180491  8793 net.cpp:125] relu1 needs backward computation.
I0909 21:10:51.180498  8793 net.cpp:66] Creating Layer pool1
I0909 21:10:51.180503  8793 net.cpp:329] pool1 <- conv1
I0909 21:10:51.180510  8793 net.cpp:290] pool1 -> pool1
I0909 21:10:51.180521  8793 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:10:51.180527  8793 net.cpp:125] pool1 needs backward computation.
I0909 21:10:51.180534  8793 net.cpp:66] Creating Layer norm1
I0909 21:10:51.180539  8793 net.cpp:329] norm1 <- pool1
I0909 21:10:51.180546  8793 net.cpp:290] norm1 -> norm1
I0909 21:10:51.180555  8793 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:10:51.180562  8793 net.cpp:125] norm1 needs backward computation.
I0909 21:10:51.180569  8793 net.cpp:66] Creating Layer conv2
I0909 21:10:51.180575  8793 net.cpp:329] conv2 <- norm1
I0909 21:10:51.180583  8793 net.cpp:290] conv2 -> conv2
I0909 21:10:51.189893  8793 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:10:51.189921  8793 net.cpp:125] conv2 needs backward computation.
I0909 21:10:51.189930  8793 net.cpp:66] Creating Layer relu2
I0909 21:10:51.189936  8793 net.cpp:329] relu2 <- conv2
I0909 21:10:51.189944  8793 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:10:51.189952  8793 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:10:51.189959  8793 net.cpp:125] relu2 needs backward computation.
I0909 21:10:51.189965  8793 net.cpp:66] Creating Layer pool2
I0909 21:10:51.189971  8793 net.cpp:329] pool2 <- conv2
I0909 21:10:51.189977  8793 net.cpp:290] pool2 -> pool2
I0909 21:10:51.189986  8793 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:10:51.189992  8793 net.cpp:125] pool2 needs backward computation.
I0909 21:10:51.190003  8793 net.cpp:66] Creating Layer fc7
I0909 21:10:51.190009  8793 net.cpp:329] fc7 <- pool2
I0909 21:10:51.190016  8793 net.cpp:290] fc7 -> fc7
I0909 21:10:51.827890  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.827934  8793 net.cpp:125] fc7 needs backward computation.
I0909 21:10:51.827949  8793 net.cpp:66] Creating Layer relu7
I0909 21:10:51.827956  8793 net.cpp:329] relu7 <- fc7
I0909 21:10:51.827965  8793 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:10:51.827975  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.827980  8793 net.cpp:125] relu7 needs backward computation.
I0909 21:10:51.827987  8793 net.cpp:66] Creating Layer drop7
I0909 21:10:51.827992  8793 net.cpp:329] drop7 <- fc7
I0909 21:10:51.828001  8793 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:10:51.828011  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.828017  8793 net.cpp:125] drop7 needs backward computation.
I0909 21:10:51.828027  8793 net.cpp:66] Creating Layer fc8
I0909 21:10:51.828032  8793 net.cpp:329] fc8 <- fc7
I0909 21:10:51.828039  8793 net.cpp:290] fc8 -> fc8
I0909 21:10:51.835711  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.835722  8793 net.cpp:125] fc8 needs backward computation.
I0909 21:10:51.835729  8793 net.cpp:66] Creating Layer relu8
I0909 21:10:51.835736  8793 net.cpp:329] relu8 <- fc8
I0909 21:10:51.835741  8793 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:10:51.835748  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.835754  8793 net.cpp:125] relu8 needs backward computation.
I0909 21:10:51.835760  8793 net.cpp:66] Creating Layer drop8
I0909 21:10:51.835765  8793 net.cpp:329] drop8 <- fc8
I0909 21:10:51.835773  8793 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:10:51.835780  8793 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:10:51.835786  8793 net.cpp:125] drop8 needs backward computation.
I0909 21:10:51.835793  8793 net.cpp:66] Creating Layer fc9
I0909 21:10:51.835798  8793 net.cpp:329] fc9 <- fc8
I0909 21:10:51.835806  8793 net.cpp:290] fc9 -> fc9
I0909 21:10:51.836171  8793 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:10:51.836181  8793 net.cpp:125] fc9 needs backward computation.
I0909 21:10:51.836199  8793 net.cpp:66] Creating Layer fc10
I0909 21:10:51.836205  8793 net.cpp:329] fc10 <- fc9
I0909 21:10:51.836215  8793 net.cpp:290] fc10 -> fc10
I0909 21:10:51.836226  8793 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:10:51.836236  8793 net.cpp:125] fc10 needs backward computation.
I0909 21:10:51.836241  8793 net.cpp:66] Creating Layer prob
I0909 21:10:51.836247  8793 net.cpp:329] prob <- fc10
I0909 21:10:51.836254  8793 net.cpp:290] prob -> prob
I0909 21:10:51.836263  8793 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:10:51.836269  8793 net.cpp:125] prob needs backward computation.
I0909 21:10:51.836274  8793 net.cpp:156] This network produces output prob
I0909 21:10:51.836287  8793 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:10:51.836294  8793 net.cpp:167] Network initialization done.
I0909 21:10:51.836299  8793 net.cpp:168] Memory required for data: 6183480
Classifying 200 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:11:09.919103  8797 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:11:09.919245  8797 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:11:09.919255  8797 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:11:09.919402  8797 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:11:09.919467  8797 net.cpp:292] Input 0 -> data
I0909 21:11:09.919492  8797 net.cpp:66] Creating Layer conv1
I0909 21:11:09.919499  8797 net.cpp:329] conv1 <- data
I0909 21:11:09.919507  8797 net.cpp:290] conv1 -> conv1
I0909 21:11:09.920872  8797 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:11:09.920891  8797 net.cpp:125] conv1 needs backward computation.
I0909 21:11:09.920899  8797 net.cpp:66] Creating Layer relu1
I0909 21:11:09.920905  8797 net.cpp:329] relu1 <- conv1
I0909 21:11:09.920912  8797 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:11:09.920920  8797 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:11:09.920927  8797 net.cpp:125] relu1 needs backward computation.
I0909 21:11:09.920933  8797 net.cpp:66] Creating Layer pool1
I0909 21:11:09.920939  8797 net.cpp:329] pool1 <- conv1
I0909 21:11:09.920946  8797 net.cpp:290] pool1 -> pool1
I0909 21:11:09.920958  8797 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:11:09.920964  8797 net.cpp:125] pool1 needs backward computation.
I0909 21:11:09.920970  8797 net.cpp:66] Creating Layer norm1
I0909 21:11:09.920976  8797 net.cpp:329] norm1 <- pool1
I0909 21:11:09.920982  8797 net.cpp:290] norm1 -> norm1
I0909 21:11:09.920992  8797 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:11:09.920999  8797 net.cpp:125] norm1 needs backward computation.
I0909 21:11:09.921005  8797 net.cpp:66] Creating Layer conv2
I0909 21:11:09.921011  8797 net.cpp:329] conv2 <- norm1
I0909 21:11:09.921018  8797 net.cpp:290] conv2 -> conv2
I0909 21:11:09.930158  8797 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:11:09.930172  8797 net.cpp:125] conv2 needs backward computation.
I0909 21:11:09.930179  8797 net.cpp:66] Creating Layer relu2
I0909 21:11:09.930186  8797 net.cpp:329] relu2 <- conv2
I0909 21:11:09.930192  8797 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:11:09.930199  8797 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:11:09.930205  8797 net.cpp:125] relu2 needs backward computation.
I0909 21:11:09.930212  8797 net.cpp:66] Creating Layer pool2
I0909 21:11:09.930217  8797 net.cpp:329] pool2 <- conv2
I0909 21:11:09.930223  8797 net.cpp:290] pool2 -> pool2
I0909 21:11:09.930232  8797 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:11:09.930238  8797 net.cpp:125] pool2 needs backward computation.
I0909 21:11:09.930246  8797 net.cpp:66] Creating Layer fc7
I0909 21:11:09.930253  8797 net.cpp:329] fc7 <- pool2
I0909 21:11:09.930259  8797 net.cpp:290] fc7 -> fc7
I0909 21:11:10.572461  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.572506  8797 net.cpp:125] fc7 needs backward computation.
I0909 21:11:10.572520  8797 net.cpp:66] Creating Layer relu7
I0909 21:11:10.572528  8797 net.cpp:329] relu7 <- fc7
I0909 21:11:10.572536  8797 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:11:10.572546  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.572551  8797 net.cpp:125] relu7 needs backward computation.
I0909 21:11:10.572559  8797 net.cpp:66] Creating Layer drop7
I0909 21:11:10.572576  8797 net.cpp:329] drop7 <- fc7
I0909 21:11:10.572583  8797 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:11:10.572594  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.572600  8797 net.cpp:125] drop7 needs backward computation.
I0909 21:11:10.572609  8797 net.cpp:66] Creating Layer fc8
I0909 21:11:10.572615  8797 net.cpp:329] fc8 <- fc7
I0909 21:11:10.572623  8797 net.cpp:290] fc8 -> fc8
I0909 21:11:10.580255  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.580266  8797 net.cpp:125] fc8 needs backward computation.
I0909 21:11:10.580273  8797 net.cpp:66] Creating Layer relu8
I0909 21:11:10.580279  8797 net.cpp:329] relu8 <- fc8
I0909 21:11:10.580286  8797 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:11:10.580292  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.580297  8797 net.cpp:125] relu8 needs backward computation.
I0909 21:11:10.580304  8797 net.cpp:66] Creating Layer drop8
I0909 21:11:10.580309  8797 net.cpp:329] drop8 <- fc8
I0909 21:11:10.580317  8797 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:11:10.580323  8797 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:11:10.580329  8797 net.cpp:125] drop8 needs backward computation.
I0909 21:11:10.580337  8797 net.cpp:66] Creating Layer fc9
I0909 21:11:10.580342  8797 net.cpp:329] fc9 <- fc8
I0909 21:11:10.580350  8797 net.cpp:290] fc9 -> fc9
I0909 21:11:10.580726  8797 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:11:10.580736  8797 net.cpp:125] fc9 needs backward computation.
I0909 21:11:10.580745  8797 net.cpp:66] Creating Layer fc10
I0909 21:11:10.580750  8797 net.cpp:329] fc10 <- fc9
I0909 21:11:10.580760  8797 net.cpp:290] fc10 -> fc10
I0909 21:11:10.580771  8797 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:11:10.580780  8797 net.cpp:125] fc10 needs backward computation.
I0909 21:11:10.580787  8797 net.cpp:66] Creating Layer prob
I0909 21:11:10.580792  8797 net.cpp:329] prob <- fc10
I0909 21:11:10.580799  8797 net.cpp:290] prob -> prob
I0909 21:11:10.580809  8797 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:11:10.580814  8797 net.cpp:125] prob needs backward computation.
I0909 21:11:10.580819  8797 net.cpp:156] This network produces output prob
I0909 21:11:10.580832  8797 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:11:10.580840  8797 net.cpp:167] Network initialization done.
I0909 21:11:10.580847  8797 net.cpp:168] Memory required for data: 6183480
Classifying 441 inputs.
Done in 272.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:15:52.144105  8806 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:15:52.144245  8806 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:15:52.144254  8806 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:15:52.144402  8806 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:15:52.144469  8806 net.cpp:292] Input 0 -> data
I0909 21:15:52.144495  8806 net.cpp:66] Creating Layer conv1
I0909 21:15:52.144501  8806 net.cpp:329] conv1 <- data
I0909 21:15:52.144510  8806 net.cpp:290] conv1 -> conv1
I0909 21:15:52.145918  8806 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:15:52.145937  8806 net.cpp:125] conv1 needs backward computation.
I0909 21:15:52.145946  8806 net.cpp:66] Creating Layer relu1
I0909 21:15:52.145952  8806 net.cpp:329] relu1 <- conv1
I0909 21:15:52.145959  8806 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:15:52.145968  8806 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:15:52.145974  8806 net.cpp:125] relu1 needs backward computation.
I0909 21:15:52.145982  8806 net.cpp:66] Creating Layer pool1
I0909 21:15:52.145987  8806 net.cpp:329] pool1 <- conv1
I0909 21:15:52.145993  8806 net.cpp:290] pool1 -> pool1
I0909 21:15:52.146004  8806 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:15:52.146010  8806 net.cpp:125] pool1 needs backward computation.
I0909 21:15:52.146018  8806 net.cpp:66] Creating Layer norm1
I0909 21:15:52.146023  8806 net.cpp:329] norm1 <- pool1
I0909 21:15:52.146029  8806 net.cpp:290] norm1 -> norm1
I0909 21:15:52.146039  8806 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:15:52.146044  8806 net.cpp:125] norm1 needs backward computation.
I0909 21:15:52.146051  8806 net.cpp:66] Creating Layer conv2
I0909 21:15:52.146057  8806 net.cpp:329] conv2 <- norm1
I0909 21:15:52.146064  8806 net.cpp:290] conv2 -> conv2
I0909 21:15:52.155547  8806 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:15:52.155581  8806 net.cpp:125] conv2 needs backward computation.
I0909 21:15:52.155591  8806 net.cpp:66] Creating Layer relu2
I0909 21:15:52.155597  8806 net.cpp:329] relu2 <- conv2
I0909 21:15:52.155606  8806 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:15:52.155614  8806 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:15:52.155619  8806 net.cpp:125] relu2 needs backward computation.
I0909 21:15:52.155637  8806 net.cpp:66] Creating Layer pool2
I0909 21:15:52.155642  8806 net.cpp:329] pool2 <- conv2
I0909 21:15:52.155649  8806 net.cpp:290] pool2 -> pool2
I0909 21:15:52.155659  8806 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:15:52.155665  8806 net.cpp:125] pool2 needs backward computation.
I0909 21:15:52.155676  8806 net.cpp:66] Creating Layer fc7
I0909 21:15:52.155683  8806 net.cpp:329] fc7 <- pool2
I0909 21:15:52.155690  8806 net.cpp:290] fc7 -> fc7
I0909 21:15:52.798662  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.798710  8806 net.cpp:125] fc7 needs backward computation.
I0909 21:15:52.798725  8806 net.cpp:66] Creating Layer relu7
I0909 21:15:52.798732  8806 net.cpp:329] relu7 <- fc7
I0909 21:15:52.798740  8806 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:15:52.798749  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.798755  8806 net.cpp:125] relu7 needs backward computation.
I0909 21:15:52.798763  8806 net.cpp:66] Creating Layer drop7
I0909 21:15:52.798768  8806 net.cpp:329] drop7 <- fc7
I0909 21:15:52.798776  8806 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:15:52.798787  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.798794  8806 net.cpp:125] drop7 needs backward computation.
I0909 21:15:52.798801  8806 net.cpp:66] Creating Layer fc8
I0909 21:15:52.798807  8806 net.cpp:329] fc8 <- fc7
I0909 21:15:52.798815  8806 net.cpp:290] fc8 -> fc8
I0909 21:15:52.806608  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.806622  8806 net.cpp:125] fc8 needs backward computation.
I0909 21:15:52.806628  8806 net.cpp:66] Creating Layer relu8
I0909 21:15:52.806633  8806 net.cpp:329] relu8 <- fc8
I0909 21:15:52.806640  8806 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:15:52.806648  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.806653  8806 net.cpp:125] relu8 needs backward computation.
I0909 21:15:52.806659  8806 net.cpp:66] Creating Layer drop8
I0909 21:15:52.806665  8806 net.cpp:329] drop8 <- fc8
I0909 21:15:52.806673  8806 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:15:52.806680  8806 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:52.806685  8806 net.cpp:125] drop8 needs backward computation.
I0909 21:15:52.806694  8806 net.cpp:66] Creating Layer fc9
I0909 21:15:52.806699  8806 net.cpp:329] fc9 <- fc8
I0909 21:15:52.806706  8806 net.cpp:290] fc9 -> fc9
I0909 21:15:52.807080  8806 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:15:52.807099  8806 net.cpp:125] fc9 needs backward computation.
I0909 21:15:52.807108  8806 net.cpp:66] Creating Layer fc10
I0909 21:15:52.807114  8806 net.cpp:329] fc10 <- fc9
I0909 21:15:52.807123  8806 net.cpp:290] fc10 -> fc10
I0909 21:15:52.807135  8806 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:15:52.807145  8806 net.cpp:125] fc10 needs backward computation.
I0909 21:15:52.807152  8806 net.cpp:66] Creating Layer prob
I0909 21:15:52.807158  8806 net.cpp:329] prob <- fc10
I0909 21:15:52.807165  8806 net.cpp:290] prob -> prob
I0909 21:15:52.807174  8806 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:15:52.807181  8806 net.cpp:125] prob needs backward computation.
I0909 21:15:52.807186  8806 net.cpp:156] This network produces output prob
I0909 21:15:52.807199  8806 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:15:52.807209  8806 net.cpp:167] Network initialization done.
I0909 21:15:52.807214  8806 net.cpp:168] Memory required for data: 6183480
Classifying 50 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:15:55.138345  8809 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:15:55.138497  8809 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:15:55.138506  8809 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:15:55.138654  8809 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:15:55.138708  8809 net.cpp:292] Input 0 -> data
I0909 21:15:55.138733  8809 net.cpp:66] Creating Layer conv1
I0909 21:15:55.138741  8809 net.cpp:329] conv1 <- data
I0909 21:15:55.138748  8809 net.cpp:290] conv1 -> conv1
I0909 21:15:55.140111  8809 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:15:55.140130  8809 net.cpp:125] conv1 needs backward computation.
I0909 21:15:55.140139  8809 net.cpp:66] Creating Layer relu1
I0909 21:15:55.140146  8809 net.cpp:329] relu1 <- conv1
I0909 21:15:55.140156  8809 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:15:55.140166  8809 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:15:55.140172  8809 net.cpp:125] relu1 needs backward computation.
I0909 21:15:55.140178  8809 net.cpp:66] Creating Layer pool1
I0909 21:15:55.140184  8809 net.cpp:329] pool1 <- conv1
I0909 21:15:55.140192  8809 net.cpp:290] pool1 -> pool1
I0909 21:15:55.140202  8809 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:15:55.140208  8809 net.cpp:125] pool1 needs backward computation.
I0909 21:15:55.140214  8809 net.cpp:66] Creating Layer norm1
I0909 21:15:55.140220  8809 net.cpp:329] norm1 <- pool1
I0909 21:15:55.140226  8809 net.cpp:290] norm1 -> norm1
I0909 21:15:55.140236  8809 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:15:55.140243  8809 net.cpp:125] norm1 needs backward computation.
I0909 21:15:55.140249  8809 net.cpp:66] Creating Layer conv2
I0909 21:15:55.140255  8809 net.cpp:329] conv2 <- norm1
I0909 21:15:55.140262  8809 net.cpp:290] conv2 -> conv2
I0909 21:15:55.149382  8809 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:15:55.149397  8809 net.cpp:125] conv2 needs backward computation.
I0909 21:15:55.149405  8809 net.cpp:66] Creating Layer relu2
I0909 21:15:55.149410  8809 net.cpp:329] relu2 <- conv2
I0909 21:15:55.149416  8809 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:15:55.149425  8809 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:15:55.149430  8809 net.cpp:125] relu2 needs backward computation.
I0909 21:15:55.149436  8809 net.cpp:66] Creating Layer pool2
I0909 21:15:55.149441  8809 net.cpp:329] pool2 <- conv2
I0909 21:15:55.149448  8809 net.cpp:290] pool2 -> pool2
I0909 21:15:55.149456  8809 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:15:55.149461  8809 net.cpp:125] pool2 needs backward computation.
I0909 21:15:55.149471  8809 net.cpp:66] Creating Layer fc7
I0909 21:15:55.149477  8809 net.cpp:329] fc7 <- pool2
I0909 21:15:55.149484  8809 net.cpp:290] fc7 -> fc7
I0909 21:15:55.791345  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.791388  8809 net.cpp:125] fc7 needs backward computation.
I0909 21:15:55.791401  8809 net.cpp:66] Creating Layer relu7
I0909 21:15:55.791409  8809 net.cpp:329] relu7 <- fc7
I0909 21:15:55.791416  8809 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:15:55.791425  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.791431  8809 net.cpp:125] relu7 needs backward computation.
I0909 21:15:55.791438  8809 net.cpp:66] Creating Layer drop7
I0909 21:15:55.791445  8809 net.cpp:329] drop7 <- fc7
I0909 21:15:55.791452  8809 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:15:55.791462  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.791468  8809 net.cpp:125] drop7 needs backward computation.
I0909 21:15:55.791477  8809 net.cpp:66] Creating Layer fc8
I0909 21:15:55.791482  8809 net.cpp:329] fc8 <- fc7
I0909 21:15:55.791489  8809 net.cpp:290] fc8 -> fc8
I0909 21:15:55.799275  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.799288  8809 net.cpp:125] fc8 needs backward computation.
I0909 21:15:55.799295  8809 net.cpp:66] Creating Layer relu8
I0909 21:15:55.799301  8809 net.cpp:329] relu8 <- fc8
I0909 21:15:55.799307  8809 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:15:55.799314  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.799320  8809 net.cpp:125] relu8 needs backward computation.
I0909 21:15:55.799326  8809 net.cpp:66] Creating Layer drop8
I0909 21:15:55.799332  8809 net.cpp:329] drop8 <- fc8
I0909 21:15:55.799340  8809 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:15:55.799346  8809 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:15:55.799352  8809 net.cpp:125] drop8 needs backward computation.
I0909 21:15:55.799360  8809 net.cpp:66] Creating Layer fc9
I0909 21:15:55.799365  8809 net.cpp:329] fc9 <- fc8
I0909 21:15:55.799374  8809 net.cpp:290] fc9 -> fc9
I0909 21:15:55.799747  8809 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:15:55.799758  8809 net.cpp:125] fc9 needs backward computation.
I0909 21:15:55.799767  8809 net.cpp:66] Creating Layer fc10
I0909 21:15:55.799782  8809 net.cpp:329] fc10 <- fc9
I0909 21:15:55.799792  8809 net.cpp:290] fc10 -> fc10
I0909 21:15:55.799803  8809 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:15:55.799815  8809 net.cpp:125] fc10 needs backward computation.
I0909 21:15:55.799823  8809 net.cpp:66] Creating Layer prob
I0909 21:15:55.799828  8809 net.cpp:329] prob <- fc10
I0909 21:15:55.799834  8809 net.cpp:290] prob -> prob
I0909 21:15:55.799844  8809 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:15:55.799849  8809 net.cpp:125] prob needs backward computation.
I0909 21:15:55.799854  8809 net.cpp:156] This network produces output prob
I0909 21:15:55.799867  8809 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:15:55.799875  8809 net.cpp:167] Network initialization done.
I0909 21:15:55.799880  8809 net.cpp:168] Memory required for data: 6183480
Classifying 209 inputs.
Done in 131.73 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:18:12.068184  8821 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:18:12.068325  8821 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:18:12.068334  8821 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:18:12.068481  8821 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:18:12.068547  8821 net.cpp:292] Input 0 -> data
I0909 21:18:12.068572  8821 net.cpp:66] Creating Layer conv1
I0909 21:18:12.068579  8821 net.cpp:329] conv1 <- data
I0909 21:18:12.068588  8821 net.cpp:290] conv1 -> conv1
I0909 21:18:12.070024  8821 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:18:12.070052  8821 net.cpp:125] conv1 needs backward computation.
I0909 21:18:12.070061  8821 net.cpp:66] Creating Layer relu1
I0909 21:18:12.070067  8821 net.cpp:329] relu1 <- conv1
I0909 21:18:12.070075  8821 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:18:12.070083  8821 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:18:12.070089  8821 net.cpp:125] relu1 needs backward computation.
I0909 21:18:12.070096  8821 net.cpp:66] Creating Layer pool1
I0909 21:18:12.070101  8821 net.cpp:329] pool1 <- conv1
I0909 21:18:12.070108  8821 net.cpp:290] pool1 -> pool1
I0909 21:18:12.070119  8821 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:18:12.070125  8821 net.cpp:125] pool1 needs backward computation.
I0909 21:18:12.070132  8821 net.cpp:66] Creating Layer norm1
I0909 21:18:12.070137  8821 net.cpp:329] norm1 <- pool1
I0909 21:18:12.070144  8821 net.cpp:290] norm1 -> norm1
I0909 21:18:12.070153  8821 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:18:12.070159  8821 net.cpp:125] norm1 needs backward computation.
I0909 21:18:12.070166  8821 net.cpp:66] Creating Layer conv2
I0909 21:18:12.070173  8821 net.cpp:329] conv2 <- norm1
I0909 21:18:12.070179  8821 net.cpp:290] conv2 -> conv2
I0909 21:18:12.079404  8821 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:18:12.079430  8821 net.cpp:125] conv2 needs backward computation.
I0909 21:18:12.079439  8821 net.cpp:66] Creating Layer relu2
I0909 21:18:12.079445  8821 net.cpp:329] relu2 <- conv2
I0909 21:18:12.079452  8821 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:18:12.079460  8821 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:18:12.079466  8821 net.cpp:125] relu2 needs backward computation.
I0909 21:18:12.079473  8821 net.cpp:66] Creating Layer pool2
I0909 21:18:12.079478  8821 net.cpp:329] pool2 <- conv2
I0909 21:18:12.079485  8821 net.cpp:290] pool2 -> pool2
I0909 21:18:12.079493  8821 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:18:12.079499  8821 net.cpp:125] pool2 needs backward computation.
I0909 21:18:12.079507  8821 net.cpp:66] Creating Layer fc7
I0909 21:18:12.079512  8821 net.cpp:329] fc7 <- pool2
I0909 21:18:12.079524  8821 net.cpp:290] fc7 -> fc7
I0909 21:18:12.720943  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.720991  8821 net.cpp:125] fc7 needs backward computation.
I0909 21:18:12.721005  8821 net.cpp:66] Creating Layer relu7
I0909 21:18:12.721012  8821 net.cpp:329] relu7 <- fc7
I0909 21:18:12.721020  8821 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:18:12.721030  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.721035  8821 net.cpp:125] relu7 needs backward computation.
I0909 21:18:12.721042  8821 net.cpp:66] Creating Layer drop7
I0909 21:18:12.721048  8821 net.cpp:329] drop7 <- fc7
I0909 21:18:12.721055  8821 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:18:12.721066  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.721072  8821 net.cpp:125] drop7 needs backward computation.
I0909 21:18:12.721081  8821 net.cpp:66] Creating Layer fc8
I0909 21:18:12.721086  8821 net.cpp:329] fc8 <- fc7
I0909 21:18:12.721093  8821 net.cpp:290] fc8 -> fc8
I0909 21:18:12.728763  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.728785  8821 net.cpp:125] fc8 needs backward computation.
I0909 21:18:12.728806  8821 net.cpp:66] Creating Layer relu8
I0909 21:18:12.728811  8821 net.cpp:329] relu8 <- fc8
I0909 21:18:12.728819  8821 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:18:12.728827  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.728832  8821 net.cpp:125] relu8 needs backward computation.
I0909 21:18:12.728839  8821 net.cpp:66] Creating Layer drop8
I0909 21:18:12.728845  8821 net.cpp:329] drop8 <- fc8
I0909 21:18:12.728853  8821 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:18:12.728860  8821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:18:12.728867  8821 net.cpp:125] drop8 needs backward computation.
I0909 21:18:12.728873  8821 net.cpp:66] Creating Layer fc9
I0909 21:18:12.728879  8821 net.cpp:329] fc9 <- fc8
I0909 21:18:12.728888  8821 net.cpp:290] fc9 -> fc9
I0909 21:18:12.729286  8821 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:18:12.729297  8821 net.cpp:125] fc9 needs backward computation.
I0909 21:18:12.729306  8821 net.cpp:66] Creating Layer fc10
I0909 21:18:12.729320  8821 net.cpp:329] fc10 <- fc9
I0909 21:18:12.729327  8821 net.cpp:290] fc10 -> fc10
I0909 21:18:12.729339  8821 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:18:12.729348  8821 net.cpp:125] fc10 needs backward computation.
I0909 21:18:12.729357  8821 net.cpp:66] Creating Layer prob
I0909 21:18:12.729362  8821 net.cpp:329] prob <- fc10
I0909 21:18:12.729368  8821 net.cpp:290] prob -> prob
I0909 21:18:12.729377  8821 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:18:12.729383  8821 net.cpp:125] prob needs backward computation.
I0909 21:18:12.729388  8821 net.cpp:156] This network produces output prob
I0909 21:18:12.729403  8821 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:18:12.729410  8821 net.cpp:167] Network initialization done.
I0909 21:18:12.729415  8821 net.cpp:168] Memory required for data: 6183480
Classifying 97 inputs.
Done in 65.74 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:19:21.305158  8828 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:19:21.305297  8828 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:19:21.305306  8828 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:19:21.305449  8828 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:19:21.305538  8828 net.cpp:292] Input 0 -> data
I0909 21:19:21.305574  8828 net.cpp:66] Creating Layer conv1
I0909 21:19:21.305584  8828 net.cpp:329] conv1 <- data
I0909 21:19:21.305593  8828 net.cpp:290] conv1 -> conv1
I0909 21:19:21.306921  8828 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:19:21.306937  8828 net.cpp:125] conv1 needs backward computation.
I0909 21:19:21.306946  8828 net.cpp:66] Creating Layer relu1
I0909 21:19:21.306952  8828 net.cpp:329] relu1 <- conv1
I0909 21:19:21.306959  8828 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:19:21.306967  8828 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:19:21.306973  8828 net.cpp:125] relu1 needs backward computation.
I0909 21:19:21.306980  8828 net.cpp:66] Creating Layer pool1
I0909 21:19:21.306985  8828 net.cpp:329] pool1 <- conv1
I0909 21:19:21.306993  8828 net.cpp:290] pool1 -> pool1
I0909 21:19:21.307003  8828 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:19:21.307008  8828 net.cpp:125] pool1 needs backward computation.
I0909 21:19:21.307015  8828 net.cpp:66] Creating Layer norm1
I0909 21:19:21.307020  8828 net.cpp:329] norm1 <- pool1
I0909 21:19:21.307028  8828 net.cpp:290] norm1 -> norm1
I0909 21:19:21.307036  8828 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:19:21.307042  8828 net.cpp:125] norm1 needs backward computation.
I0909 21:19:21.307049  8828 net.cpp:66] Creating Layer conv2
I0909 21:19:21.307054  8828 net.cpp:329] conv2 <- norm1
I0909 21:19:21.307062  8828 net.cpp:290] conv2 -> conv2
I0909 21:19:21.316048  8828 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:19:21.316063  8828 net.cpp:125] conv2 needs backward computation.
I0909 21:19:21.316071  8828 net.cpp:66] Creating Layer relu2
I0909 21:19:21.316076  8828 net.cpp:329] relu2 <- conv2
I0909 21:19:21.316082  8828 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:19:21.316089  8828 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:19:21.316095  8828 net.cpp:125] relu2 needs backward computation.
I0909 21:19:21.316102  8828 net.cpp:66] Creating Layer pool2
I0909 21:19:21.316107  8828 net.cpp:329] pool2 <- conv2
I0909 21:19:21.316114  8828 net.cpp:290] pool2 -> pool2
I0909 21:19:21.316123  8828 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:19:21.316128  8828 net.cpp:125] pool2 needs backward computation.
I0909 21:19:21.316136  8828 net.cpp:66] Creating Layer fc7
I0909 21:19:21.316143  8828 net.cpp:329] fc7 <- pool2
I0909 21:19:21.316150  8828 net.cpp:290] fc7 -> fc7
I0909 21:19:21.959419  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.959462  8828 net.cpp:125] fc7 needs backward computation.
I0909 21:19:21.959477  8828 net.cpp:66] Creating Layer relu7
I0909 21:19:21.959486  8828 net.cpp:329] relu7 <- fc7
I0909 21:19:21.959493  8828 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:19:21.959502  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.959508  8828 net.cpp:125] relu7 needs backward computation.
I0909 21:19:21.959517  8828 net.cpp:66] Creating Layer drop7
I0909 21:19:21.959522  8828 net.cpp:329] drop7 <- fc7
I0909 21:19:21.959529  8828 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:19:21.959542  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.959547  8828 net.cpp:125] drop7 needs backward computation.
I0909 21:19:21.959555  8828 net.cpp:66] Creating Layer fc8
I0909 21:19:21.959561  8828 net.cpp:329] fc8 <- fc7
I0909 21:19:21.959568  8828 net.cpp:290] fc8 -> fc8
I0909 21:19:21.967362  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.967375  8828 net.cpp:125] fc8 needs backward computation.
I0909 21:19:21.967382  8828 net.cpp:66] Creating Layer relu8
I0909 21:19:21.967387  8828 net.cpp:329] relu8 <- fc8
I0909 21:19:21.967394  8828 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:19:21.967401  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.967406  8828 net.cpp:125] relu8 needs backward computation.
I0909 21:19:21.967413  8828 net.cpp:66] Creating Layer drop8
I0909 21:19:21.967419  8828 net.cpp:329] drop8 <- fc8
I0909 21:19:21.967427  8828 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:19:21.967434  8828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:21.967440  8828 net.cpp:125] drop8 needs backward computation.
I0909 21:19:21.967448  8828 net.cpp:66] Creating Layer fc9
I0909 21:19:21.967453  8828 net.cpp:329] fc9 <- fc8
I0909 21:19:21.967463  8828 net.cpp:290] fc9 -> fc9
I0909 21:19:21.967835  8828 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:19:21.967846  8828 net.cpp:125] fc9 needs backward computation.
I0909 21:19:21.967854  8828 net.cpp:66] Creating Layer fc10
I0909 21:19:21.967860  8828 net.cpp:329] fc10 <- fc9
I0909 21:19:21.967869  8828 net.cpp:290] fc10 -> fc10
I0909 21:19:21.967880  8828 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:19:21.967890  8828 net.cpp:125] fc10 needs backward computation.
I0909 21:19:21.967897  8828 net.cpp:66] Creating Layer prob
I0909 21:19:21.967902  8828 net.cpp:329] prob <- fc10
I0909 21:19:21.967910  8828 net.cpp:290] prob -> prob
I0909 21:19:21.967918  8828 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:19:21.967924  8828 net.cpp:125] prob needs backward computation.
I0909 21:19:21.967929  8828 net.cpp:156] This network produces output prob
I0909 21:19:21.967941  8828 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:19:21.967950  8828 net.cpp:167] Network initialization done.
I0909 21:19:21.967955  8828 net.cpp:168] Memory required for data: 6183480
Classifying 24 inputs.
Done in 17.69 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:19:42.526315  8831 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:19:42.526456  8831 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:19:42.526464  8831 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:19:42.526612  8831 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:19:42.526676  8831 net.cpp:292] Input 0 -> data
I0909 21:19:42.526702  8831 net.cpp:66] Creating Layer conv1
I0909 21:19:42.526710  8831 net.cpp:329] conv1 <- data
I0909 21:19:42.526717  8831 net.cpp:290] conv1 -> conv1
I0909 21:19:42.528080  8831 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:19:42.528098  8831 net.cpp:125] conv1 needs backward computation.
I0909 21:19:42.528107  8831 net.cpp:66] Creating Layer relu1
I0909 21:19:42.528115  8831 net.cpp:329] relu1 <- conv1
I0909 21:19:42.528120  8831 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:19:42.528130  8831 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:19:42.528136  8831 net.cpp:125] relu1 needs backward computation.
I0909 21:19:42.528142  8831 net.cpp:66] Creating Layer pool1
I0909 21:19:42.528147  8831 net.cpp:329] pool1 <- conv1
I0909 21:19:42.528154  8831 net.cpp:290] pool1 -> pool1
I0909 21:19:42.528165  8831 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:19:42.528172  8831 net.cpp:125] pool1 needs backward computation.
I0909 21:19:42.528178  8831 net.cpp:66] Creating Layer norm1
I0909 21:19:42.528184  8831 net.cpp:329] norm1 <- pool1
I0909 21:19:42.528190  8831 net.cpp:290] norm1 -> norm1
I0909 21:19:42.528200  8831 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:19:42.528206  8831 net.cpp:125] norm1 needs backward computation.
I0909 21:19:42.528213  8831 net.cpp:66] Creating Layer conv2
I0909 21:19:42.528219  8831 net.cpp:329] conv2 <- norm1
I0909 21:19:42.528231  8831 net.cpp:290] conv2 -> conv2
I0909 21:19:42.537395  8831 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:19:42.537410  8831 net.cpp:125] conv2 needs backward computation.
I0909 21:19:42.537416  8831 net.cpp:66] Creating Layer relu2
I0909 21:19:42.537422  8831 net.cpp:329] relu2 <- conv2
I0909 21:19:42.537430  8831 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:19:42.537436  8831 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:19:42.537442  8831 net.cpp:125] relu2 needs backward computation.
I0909 21:19:42.537448  8831 net.cpp:66] Creating Layer pool2
I0909 21:19:42.537454  8831 net.cpp:329] pool2 <- conv2
I0909 21:19:42.537461  8831 net.cpp:290] pool2 -> pool2
I0909 21:19:42.537468  8831 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:19:42.537474  8831 net.cpp:125] pool2 needs backward computation.
I0909 21:19:42.537483  8831 net.cpp:66] Creating Layer fc7
I0909 21:19:42.537489  8831 net.cpp:329] fc7 <- pool2
I0909 21:19:42.537497  8831 net.cpp:290] fc7 -> fc7
I0909 21:19:43.178928  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.178967  8831 net.cpp:125] fc7 needs backward computation.
I0909 21:19:43.178982  8831 net.cpp:66] Creating Layer relu7
I0909 21:19:43.178988  8831 net.cpp:329] relu7 <- fc7
I0909 21:19:43.178997  8831 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:19:43.179005  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.179011  8831 net.cpp:125] relu7 needs backward computation.
I0909 21:19:43.179018  8831 net.cpp:66] Creating Layer drop7
I0909 21:19:43.179024  8831 net.cpp:329] drop7 <- fc7
I0909 21:19:43.179031  8831 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:19:43.179043  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.179049  8831 net.cpp:125] drop7 needs backward computation.
I0909 21:19:43.179057  8831 net.cpp:66] Creating Layer fc8
I0909 21:19:43.179064  8831 net.cpp:329] fc8 <- fc7
I0909 21:19:43.179070  8831 net.cpp:290] fc8 -> fc8
I0909 21:19:43.186857  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.186871  8831 net.cpp:125] fc8 needs backward computation.
I0909 21:19:43.186877  8831 net.cpp:66] Creating Layer relu8
I0909 21:19:43.186883  8831 net.cpp:329] relu8 <- fc8
I0909 21:19:43.186889  8831 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:19:43.186897  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.186902  8831 net.cpp:125] relu8 needs backward computation.
I0909 21:19:43.186909  8831 net.cpp:66] Creating Layer drop8
I0909 21:19:43.186914  8831 net.cpp:329] drop8 <- fc8
I0909 21:19:43.186923  8831 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:19:43.186929  8831 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:19:43.186935  8831 net.cpp:125] drop8 needs backward computation.
I0909 21:19:43.186943  8831 net.cpp:66] Creating Layer fc9
I0909 21:19:43.186949  8831 net.cpp:329] fc9 <- fc8
I0909 21:19:43.186956  8831 net.cpp:290] fc9 -> fc9
I0909 21:19:43.187330  8831 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:19:43.187340  8831 net.cpp:125] fc9 needs backward computation.
I0909 21:19:43.187348  8831 net.cpp:66] Creating Layer fc10
I0909 21:19:43.187355  8831 net.cpp:329] fc10 <- fc9
I0909 21:19:43.187363  8831 net.cpp:290] fc10 -> fc10
I0909 21:19:43.187374  8831 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:19:43.187384  8831 net.cpp:125] fc10 needs backward computation.
I0909 21:19:43.187391  8831 net.cpp:66] Creating Layer prob
I0909 21:19:43.187397  8831 net.cpp:329] prob <- fc10
I0909 21:19:43.187403  8831 net.cpp:290] prob -> prob
I0909 21:19:43.187413  8831 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:19:43.187419  8831 net.cpp:125] prob needs backward computation.
I0909 21:19:43.187424  8831 net.cpp:156] This network produces output prob
I0909 21:19:43.187436  8831 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:19:43.187445  8831 net.cpp:167] Network initialization done.
I0909 21:19:43.187450  8831 net.cpp:168] Memory required for data: 6183480
Classifying 63 inputs.
Done in 39.16 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:20:24.541323  8835 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:20:24.541471  8835 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:20:24.541481  8835 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:20:24.541661  8835 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:20:24.541714  8835 net.cpp:292] Input 0 -> data
I0909 21:20:24.541739  8835 net.cpp:66] Creating Layer conv1
I0909 21:20:24.541746  8835 net.cpp:329] conv1 <- data
I0909 21:20:24.541754  8835 net.cpp:290] conv1 -> conv1
I0909 21:20:24.543090  8835 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:20:24.543108  8835 net.cpp:125] conv1 needs backward computation.
I0909 21:20:24.543118  8835 net.cpp:66] Creating Layer relu1
I0909 21:20:24.543128  8835 net.cpp:329] relu1 <- conv1
I0909 21:20:24.543134  8835 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:20:24.543143  8835 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:20:24.543148  8835 net.cpp:125] relu1 needs backward computation.
I0909 21:20:24.543155  8835 net.cpp:66] Creating Layer pool1
I0909 21:20:24.543160  8835 net.cpp:329] pool1 <- conv1
I0909 21:20:24.543167  8835 net.cpp:290] pool1 -> pool1
I0909 21:20:24.543179  8835 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:20:24.543184  8835 net.cpp:125] pool1 needs backward computation.
I0909 21:20:24.543190  8835 net.cpp:66] Creating Layer norm1
I0909 21:20:24.543195  8835 net.cpp:329] norm1 <- pool1
I0909 21:20:24.543202  8835 net.cpp:290] norm1 -> norm1
I0909 21:20:24.543211  8835 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:20:24.543216  8835 net.cpp:125] norm1 needs backward computation.
I0909 21:20:24.543223  8835 net.cpp:66] Creating Layer conv2
I0909 21:20:24.543229  8835 net.cpp:329] conv2 <- norm1
I0909 21:20:24.543236  8835 net.cpp:290] conv2 -> conv2
I0909 21:20:24.552162  8835 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:20:24.552178  8835 net.cpp:125] conv2 needs backward computation.
I0909 21:20:24.552186  8835 net.cpp:66] Creating Layer relu2
I0909 21:20:24.552191  8835 net.cpp:329] relu2 <- conv2
I0909 21:20:24.552197  8835 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:20:24.552204  8835 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:20:24.552209  8835 net.cpp:125] relu2 needs backward computation.
I0909 21:20:24.552216  8835 net.cpp:66] Creating Layer pool2
I0909 21:20:24.552222  8835 net.cpp:329] pool2 <- conv2
I0909 21:20:24.552227  8835 net.cpp:290] pool2 -> pool2
I0909 21:20:24.552235  8835 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:20:24.552242  8835 net.cpp:125] pool2 needs backward computation.
I0909 21:20:24.552258  8835 net.cpp:66] Creating Layer fc7
I0909 21:20:24.552263  8835 net.cpp:329] fc7 <- pool2
I0909 21:20:24.552270  8835 net.cpp:290] fc7 -> fc7
I0909 21:20:25.195461  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.195503  8835 net.cpp:125] fc7 needs backward computation.
I0909 21:20:25.195518  8835 net.cpp:66] Creating Layer relu7
I0909 21:20:25.195524  8835 net.cpp:329] relu7 <- fc7
I0909 21:20:25.195533  8835 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:20:25.195543  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.195549  8835 net.cpp:125] relu7 needs backward computation.
I0909 21:20:25.195555  8835 net.cpp:66] Creating Layer drop7
I0909 21:20:25.195561  8835 net.cpp:329] drop7 <- fc7
I0909 21:20:25.195569  8835 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:20:25.195580  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.195586  8835 net.cpp:125] drop7 needs backward computation.
I0909 21:20:25.195595  8835 net.cpp:66] Creating Layer fc8
I0909 21:20:25.195600  8835 net.cpp:329] fc8 <- fc7
I0909 21:20:25.195608  8835 net.cpp:290] fc8 -> fc8
I0909 21:20:25.203393  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.203407  8835 net.cpp:125] fc8 needs backward computation.
I0909 21:20:25.203413  8835 net.cpp:66] Creating Layer relu8
I0909 21:20:25.203418  8835 net.cpp:329] relu8 <- fc8
I0909 21:20:25.203425  8835 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:20:25.203433  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.203438  8835 net.cpp:125] relu8 needs backward computation.
I0909 21:20:25.203444  8835 net.cpp:66] Creating Layer drop8
I0909 21:20:25.203449  8835 net.cpp:329] drop8 <- fc8
I0909 21:20:25.203457  8835 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:20:25.203464  8835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:20:25.203470  8835 net.cpp:125] drop8 needs backward computation.
I0909 21:20:25.203477  8835 net.cpp:66] Creating Layer fc9
I0909 21:20:25.203483  8835 net.cpp:329] fc9 <- fc8
I0909 21:20:25.203493  8835 net.cpp:290] fc9 -> fc9
I0909 21:20:25.203866  8835 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:20:25.203876  8835 net.cpp:125] fc9 needs backward computation.
I0909 21:20:25.203896  8835 net.cpp:66] Creating Layer fc10
I0909 21:20:25.203902  8835 net.cpp:329] fc10 <- fc9
I0909 21:20:25.203909  8835 net.cpp:290] fc10 -> fc10
I0909 21:20:25.203922  8835 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:20:25.203932  8835 net.cpp:125] fc10 needs backward computation.
I0909 21:20:25.203938  8835 net.cpp:66] Creating Layer prob
I0909 21:20:25.203943  8835 net.cpp:329] prob <- fc10
I0909 21:20:25.203950  8835 net.cpp:290] prob -> prob
I0909 21:20:25.203959  8835 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:20:25.203965  8835 net.cpp:125] prob needs backward computation.
I0909 21:20:25.203970  8835 net.cpp:156] This network produces output prob
I0909 21:20:25.203982  8835 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:20:25.203990  8835 net.cpp:167] Network initialization done.
I0909 21:20:25.203995  8835 net.cpp:168] Memory required for data: 6183480
Classifying 243 inputs.
Done in 147.24 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:22:57.648931  8840 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:22:57.649072  8840 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:22:57.649081  8840 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:22:57.649230  8840 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:22:57.649294  8840 net.cpp:292] Input 0 -> data
I0909 21:22:57.649320  8840 net.cpp:66] Creating Layer conv1
I0909 21:22:57.649327  8840 net.cpp:329] conv1 <- data
I0909 21:22:57.649335  8840 net.cpp:290] conv1 -> conv1
I0909 21:22:57.650722  8840 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:22:57.650742  8840 net.cpp:125] conv1 needs backward computation.
I0909 21:22:57.650750  8840 net.cpp:66] Creating Layer relu1
I0909 21:22:57.650758  8840 net.cpp:329] relu1 <- conv1
I0909 21:22:57.650763  8840 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:22:57.650773  8840 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:22:57.650779  8840 net.cpp:125] relu1 needs backward computation.
I0909 21:22:57.650785  8840 net.cpp:66] Creating Layer pool1
I0909 21:22:57.650790  8840 net.cpp:329] pool1 <- conv1
I0909 21:22:57.650797  8840 net.cpp:290] pool1 -> pool1
I0909 21:22:57.650809  8840 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:22:57.650815  8840 net.cpp:125] pool1 needs backward computation.
I0909 21:22:57.650820  8840 net.cpp:66] Creating Layer norm1
I0909 21:22:57.650826  8840 net.cpp:329] norm1 <- pool1
I0909 21:22:57.650833  8840 net.cpp:290] norm1 -> norm1
I0909 21:22:57.650842  8840 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:22:57.650848  8840 net.cpp:125] norm1 needs backward computation.
I0909 21:22:57.650856  8840 net.cpp:66] Creating Layer conv2
I0909 21:22:57.650861  8840 net.cpp:329] conv2 <- norm1
I0909 21:22:57.650868  8840 net.cpp:290] conv2 -> conv2
I0909 21:22:57.660006  8840 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:22:57.660022  8840 net.cpp:125] conv2 needs backward computation.
I0909 21:22:57.660028  8840 net.cpp:66] Creating Layer relu2
I0909 21:22:57.660034  8840 net.cpp:329] relu2 <- conv2
I0909 21:22:57.660040  8840 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:22:57.660048  8840 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:22:57.660053  8840 net.cpp:125] relu2 needs backward computation.
I0909 21:22:57.660059  8840 net.cpp:66] Creating Layer pool2
I0909 21:22:57.660065  8840 net.cpp:329] pool2 <- conv2
I0909 21:22:57.660071  8840 net.cpp:290] pool2 -> pool2
I0909 21:22:57.660079  8840 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:22:57.660085  8840 net.cpp:125] pool2 needs backward computation.
I0909 21:22:57.660094  8840 net.cpp:66] Creating Layer fc7
I0909 21:22:57.660100  8840 net.cpp:329] fc7 <- pool2
I0909 21:22:57.660107  8840 net.cpp:290] fc7 -> fc7
I0909 21:22:58.302762  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.302805  8840 net.cpp:125] fc7 needs backward computation.
I0909 21:22:58.302820  8840 net.cpp:66] Creating Layer relu7
I0909 21:22:58.302827  8840 net.cpp:329] relu7 <- fc7
I0909 21:22:58.302834  8840 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:22:58.302845  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.302850  8840 net.cpp:125] relu7 needs backward computation.
I0909 21:22:58.302857  8840 net.cpp:66] Creating Layer drop7
I0909 21:22:58.302862  8840 net.cpp:329] drop7 <- fc7
I0909 21:22:58.302870  8840 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:22:58.302882  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.302887  8840 net.cpp:125] drop7 needs backward computation.
I0909 21:22:58.302896  8840 net.cpp:66] Creating Layer fc8
I0909 21:22:58.302901  8840 net.cpp:329] fc8 <- fc7
I0909 21:22:58.302909  8840 net.cpp:290] fc8 -> fc8
I0909 21:22:58.310724  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.310736  8840 net.cpp:125] fc8 needs backward computation.
I0909 21:22:58.310745  8840 net.cpp:66] Creating Layer relu8
I0909 21:22:58.310750  8840 net.cpp:329] relu8 <- fc8
I0909 21:22:58.310756  8840 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:22:58.310763  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.310768  8840 net.cpp:125] relu8 needs backward computation.
I0909 21:22:58.310775  8840 net.cpp:66] Creating Layer drop8
I0909 21:22:58.310781  8840 net.cpp:329] drop8 <- fc8
I0909 21:22:58.310788  8840 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:22:58.310796  8840 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:22:58.310801  8840 net.cpp:125] drop8 needs backward computation.
I0909 21:22:58.310808  8840 net.cpp:66] Creating Layer fc9
I0909 21:22:58.310813  8840 net.cpp:329] fc9 <- fc8
I0909 21:22:58.310822  8840 net.cpp:290] fc9 -> fc9
I0909 21:22:58.311198  8840 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:22:58.311208  8840 net.cpp:125] fc9 needs backward computation.
I0909 21:22:58.311216  8840 net.cpp:66] Creating Layer fc10
I0909 21:22:58.311223  8840 net.cpp:329] fc10 <- fc9
I0909 21:22:58.311230  8840 net.cpp:290] fc10 -> fc10
I0909 21:22:58.311242  8840 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:22:58.311251  8840 net.cpp:125] fc10 needs backward computation.
I0909 21:22:58.311259  8840 net.cpp:66] Creating Layer prob
I0909 21:22:58.311264  8840 net.cpp:329] prob <- fc10
I0909 21:22:58.311270  8840 net.cpp:290] prob -> prob
I0909 21:22:58.311280  8840 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:22:58.311285  8840 net.cpp:125] prob needs backward computation.
I0909 21:22:58.311291  8840 net.cpp:156] This network produces output prob
I0909 21:22:58.311303  8840 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:22:58.311312  8840 net.cpp:167] Network initialization done.
I0909 21:22:58.311317  8840 net.cpp:168] Memory required for data: 6183480
Classifying 199 inputs.
Done in 124.33 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:25:06.768759  8847 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:25:06.768899  8847 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:25:06.768908  8847 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:25:06.769055  8847 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:25:06.769121  8847 net.cpp:292] Input 0 -> data
I0909 21:25:06.769148  8847 net.cpp:66] Creating Layer conv1
I0909 21:25:06.769155  8847 net.cpp:329] conv1 <- data
I0909 21:25:06.769163  8847 net.cpp:290] conv1 -> conv1
I0909 21:25:06.770542  8847 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:25:06.770561  8847 net.cpp:125] conv1 needs backward computation.
I0909 21:25:06.770570  8847 net.cpp:66] Creating Layer relu1
I0909 21:25:06.770576  8847 net.cpp:329] relu1 <- conv1
I0909 21:25:06.770583  8847 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:25:06.770592  8847 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:25:06.770598  8847 net.cpp:125] relu1 needs backward computation.
I0909 21:25:06.770606  8847 net.cpp:66] Creating Layer pool1
I0909 21:25:06.770611  8847 net.cpp:329] pool1 <- conv1
I0909 21:25:06.770617  8847 net.cpp:290] pool1 -> pool1
I0909 21:25:06.770628  8847 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:25:06.770634  8847 net.cpp:125] pool1 needs backward computation.
I0909 21:25:06.770642  8847 net.cpp:66] Creating Layer norm1
I0909 21:25:06.770647  8847 net.cpp:329] norm1 <- pool1
I0909 21:25:06.770653  8847 net.cpp:290] norm1 -> norm1
I0909 21:25:06.770663  8847 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:25:06.770669  8847 net.cpp:125] norm1 needs backward computation.
I0909 21:25:06.770676  8847 net.cpp:66] Creating Layer conv2
I0909 21:25:06.770683  8847 net.cpp:329] conv2 <- norm1
I0909 21:25:06.770689  8847 net.cpp:290] conv2 -> conv2
I0909 21:25:06.779592  8847 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:25:06.779605  8847 net.cpp:125] conv2 needs backward computation.
I0909 21:25:06.779613  8847 net.cpp:66] Creating Layer relu2
I0909 21:25:06.779618  8847 net.cpp:329] relu2 <- conv2
I0909 21:25:06.779624  8847 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:25:06.779631  8847 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:25:06.779636  8847 net.cpp:125] relu2 needs backward computation.
I0909 21:25:06.779642  8847 net.cpp:66] Creating Layer pool2
I0909 21:25:06.779647  8847 net.cpp:329] pool2 <- conv2
I0909 21:25:06.779654  8847 net.cpp:290] pool2 -> pool2
I0909 21:25:06.779661  8847 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:25:06.779667  8847 net.cpp:125] pool2 needs backward computation.
I0909 21:25:06.779675  8847 net.cpp:66] Creating Layer fc7
I0909 21:25:06.779680  8847 net.cpp:329] fc7 <- pool2
I0909 21:25:06.779693  8847 net.cpp:290] fc7 -> fc7
I0909 21:25:07.417719  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.417764  8847 net.cpp:125] fc7 needs backward computation.
I0909 21:25:07.417778  8847 net.cpp:66] Creating Layer relu7
I0909 21:25:07.417785  8847 net.cpp:329] relu7 <- fc7
I0909 21:25:07.417793  8847 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:25:07.417804  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.417809  8847 net.cpp:125] relu7 needs backward computation.
I0909 21:25:07.417817  8847 net.cpp:66] Creating Layer drop7
I0909 21:25:07.417822  8847 net.cpp:329] drop7 <- fc7
I0909 21:25:07.417830  8847 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:25:07.417841  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.417847  8847 net.cpp:125] drop7 needs backward computation.
I0909 21:25:07.417855  8847 net.cpp:66] Creating Layer fc8
I0909 21:25:07.417861  8847 net.cpp:329] fc8 <- fc7
I0909 21:25:07.417868  8847 net.cpp:290] fc8 -> fc8
I0909 21:25:07.425657  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.425669  8847 net.cpp:125] fc8 needs backward computation.
I0909 21:25:07.425676  8847 net.cpp:66] Creating Layer relu8
I0909 21:25:07.425683  8847 net.cpp:329] relu8 <- fc8
I0909 21:25:07.425688  8847 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:25:07.425695  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.425701  8847 net.cpp:125] relu8 needs backward computation.
I0909 21:25:07.425707  8847 net.cpp:66] Creating Layer drop8
I0909 21:25:07.425714  8847 net.cpp:329] drop8 <- fc8
I0909 21:25:07.425720  8847 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:25:07.425727  8847 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:07.425734  8847 net.cpp:125] drop8 needs backward computation.
I0909 21:25:07.425740  8847 net.cpp:66] Creating Layer fc9
I0909 21:25:07.425746  8847 net.cpp:329] fc9 <- fc8
I0909 21:25:07.425755  8847 net.cpp:290] fc9 -> fc9
I0909 21:25:07.426129  8847 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:25:07.426141  8847 net.cpp:125] fc9 needs backward computation.
I0909 21:25:07.426148  8847 net.cpp:66] Creating Layer fc10
I0909 21:25:07.426153  8847 net.cpp:329] fc10 <- fc9
I0909 21:25:07.426162  8847 net.cpp:290] fc10 -> fc10
I0909 21:25:07.426174  8847 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:25:07.426183  8847 net.cpp:125] fc10 needs backward computation.
I0909 21:25:07.426190  8847 net.cpp:66] Creating Layer prob
I0909 21:25:07.426195  8847 net.cpp:329] prob <- fc10
I0909 21:25:07.426203  8847 net.cpp:290] prob -> prob
I0909 21:25:07.426211  8847 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:25:07.426218  8847 net.cpp:125] prob needs backward computation.
I0909 21:25:07.426223  8847 net.cpp:156] This network produces output prob
I0909 21:25:07.426234  8847 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:25:07.426244  8847 net.cpp:167] Network initialization done.
I0909 21:25:07.426249  8847 net.cpp:168] Memory required for data: 6183480
Classifying 63 inputs.
Done in 38.48 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:25:48.194031  8851 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:25:48.194174  8851 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:25:48.194182  8851 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:25:48.194329  8851 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:25:48.194393  8851 net.cpp:292] Input 0 -> data
I0909 21:25:48.194419  8851 net.cpp:66] Creating Layer conv1
I0909 21:25:48.194427  8851 net.cpp:329] conv1 <- data
I0909 21:25:48.194434  8851 net.cpp:290] conv1 -> conv1
I0909 21:25:48.195797  8851 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:25:48.195816  8851 net.cpp:125] conv1 needs backward computation.
I0909 21:25:48.195824  8851 net.cpp:66] Creating Layer relu1
I0909 21:25:48.195830  8851 net.cpp:329] relu1 <- conv1
I0909 21:25:48.195837  8851 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:25:48.195847  8851 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:25:48.195852  8851 net.cpp:125] relu1 needs backward computation.
I0909 21:25:48.195859  8851 net.cpp:66] Creating Layer pool1
I0909 21:25:48.195864  8851 net.cpp:329] pool1 <- conv1
I0909 21:25:48.195871  8851 net.cpp:290] pool1 -> pool1
I0909 21:25:48.195883  8851 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:25:48.195888  8851 net.cpp:125] pool1 needs backward computation.
I0909 21:25:48.195895  8851 net.cpp:66] Creating Layer norm1
I0909 21:25:48.195901  8851 net.cpp:329] norm1 <- pool1
I0909 21:25:48.195909  8851 net.cpp:290] norm1 -> norm1
I0909 21:25:48.195917  8851 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:25:48.195924  8851 net.cpp:125] norm1 needs backward computation.
I0909 21:25:48.195936  8851 net.cpp:66] Creating Layer conv2
I0909 21:25:48.195942  8851 net.cpp:329] conv2 <- norm1
I0909 21:25:48.195950  8851 net.cpp:290] conv2 -> conv2
I0909 21:25:48.205114  8851 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:25:48.205128  8851 net.cpp:125] conv2 needs backward computation.
I0909 21:25:48.205137  8851 net.cpp:66] Creating Layer relu2
I0909 21:25:48.205142  8851 net.cpp:329] relu2 <- conv2
I0909 21:25:48.205148  8851 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:25:48.205155  8851 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:25:48.205162  8851 net.cpp:125] relu2 needs backward computation.
I0909 21:25:48.205168  8851 net.cpp:66] Creating Layer pool2
I0909 21:25:48.205173  8851 net.cpp:329] pool2 <- conv2
I0909 21:25:48.205180  8851 net.cpp:290] pool2 -> pool2
I0909 21:25:48.205188  8851 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:25:48.205194  8851 net.cpp:125] pool2 needs backward computation.
I0909 21:25:48.205204  8851 net.cpp:66] Creating Layer fc7
I0909 21:25:48.205209  8851 net.cpp:329] fc7 <- pool2
I0909 21:25:48.205216  8851 net.cpp:290] fc7 -> fc7
I0909 21:25:48.848237  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.848284  8851 net.cpp:125] fc7 needs backward computation.
I0909 21:25:48.848829  8851 net.cpp:66] Creating Layer relu7
I0909 21:25:48.848839  8851 net.cpp:329] relu7 <- fc7
I0909 21:25:48.848847  8851 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:25:48.848857  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.848863  8851 net.cpp:125] relu7 needs backward computation.
I0909 21:25:48.848870  8851 net.cpp:66] Creating Layer drop7
I0909 21:25:48.848876  8851 net.cpp:329] drop7 <- fc7
I0909 21:25:48.848884  8851 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:25:48.848896  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.848901  8851 net.cpp:125] drop7 needs backward computation.
I0909 21:25:48.848911  8851 net.cpp:66] Creating Layer fc8
I0909 21:25:48.848917  8851 net.cpp:329] fc8 <- fc7
I0909 21:25:48.848923  8851 net.cpp:290] fc8 -> fc8
I0909 21:25:48.856701  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.856714  8851 net.cpp:125] fc8 needs backward computation.
I0909 21:25:48.856721  8851 net.cpp:66] Creating Layer relu8
I0909 21:25:48.856727  8851 net.cpp:329] relu8 <- fc8
I0909 21:25:48.856734  8851 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:25:48.856741  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.856746  8851 net.cpp:125] relu8 needs backward computation.
I0909 21:25:48.856753  8851 net.cpp:66] Creating Layer drop8
I0909 21:25:48.856758  8851 net.cpp:329] drop8 <- fc8
I0909 21:25:48.856770  8851 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:25:48.856776  8851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:25:48.856782  8851 net.cpp:125] drop8 needs backward computation.
I0909 21:25:48.856789  8851 net.cpp:66] Creating Layer fc9
I0909 21:25:48.856796  8851 net.cpp:329] fc9 <- fc8
I0909 21:25:48.856803  8851 net.cpp:290] fc9 -> fc9
I0909 21:25:48.857177  8851 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:25:48.857187  8851 net.cpp:125] fc9 needs backward computation.
I0909 21:25:48.857197  8851 net.cpp:66] Creating Layer fc10
I0909 21:25:48.857203  8851 net.cpp:329] fc10 <- fc9
I0909 21:25:48.857210  8851 net.cpp:290] fc10 -> fc10
I0909 21:25:48.857223  8851 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:25:48.857233  8851 net.cpp:125] fc10 needs backward computation.
I0909 21:25:48.857239  8851 net.cpp:66] Creating Layer prob
I0909 21:25:48.857244  8851 net.cpp:329] prob <- fc10
I0909 21:25:48.857251  8851 net.cpp:290] prob -> prob
I0909 21:25:48.857260  8851 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:25:48.857266  8851 net.cpp:125] prob needs backward computation.
I0909 21:25:48.857271  8851 net.cpp:156] This network produces output prob
I0909 21:25:48.857285  8851 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:25:48.857292  8851 net.cpp:167] Network initialization done.
I0909 21:25:48.857297  8851 net.cpp:168] Memory required for data: 6183480
Classifying 189 inputs.
Done in 113.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:27:47.029059  8856 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:27:47.029201  8856 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:27:47.029211  8856 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:27:47.029361  8856 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:27:47.029417  8856 net.cpp:292] Input 0 -> data
I0909 21:27:47.029443  8856 net.cpp:66] Creating Layer conv1
I0909 21:27:47.029451  8856 net.cpp:329] conv1 <- data
I0909 21:27:47.029459  8856 net.cpp:290] conv1 -> conv1
I0909 21:27:47.030861  8856 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:27:47.030891  8856 net.cpp:125] conv1 needs backward computation.
I0909 21:27:47.030902  8856 net.cpp:66] Creating Layer relu1
I0909 21:27:47.030908  8856 net.cpp:329] relu1 <- conv1
I0909 21:27:47.030915  8856 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:27:47.030925  8856 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:27:47.030930  8856 net.cpp:125] relu1 needs backward computation.
I0909 21:27:47.030937  8856 net.cpp:66] Creating Layer pool1
I0909 21:27:47.030942  8856 net.cpp:329] pool1 <- conv1
I0909 21:27:47.030949  8856 net.cpp:290] pool1 -> pool1
I0909 21:27:47.030961  8856 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:27:47.030966  8856 net.cpp:125] pool1 needs backward computation.
I0909 21:27:47.030973  8856 net.cpp:66] Creating Layer norm1
I0909 21:27:47.030979  8856 net.cpp:329] norm1 <- pool1
I0909 21:27:47.030987  8856 net.cpp:290] norm1 -> norm1
I0909 21:27:47.030995  8856 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:27:47.031002  8856 net.cpp:125] norm1 needs backward computation.
I0909 21:27:47.031009  8856 net.cpp:66] Creating Layer conv2
I0909 21:27:47.031015  8856 net.cpp:329] conv2 <- norm1
I0909 21:27:47.031023  8856 net.cpp:290] conv2 -> conv2
I0909 21:27:47.040189  8856 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:27:47.040205  8856 net.cpp:125] conv2 needs backward computation.
I0909 21:27:47.040212  8856 net.cpp:66] Creating Layer relu2
I0909 21:27:47.040218  8856 net.cpp:329] relu2 <- conv2
I0909 21:27:47.040225  8856 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:27:47.040232  8856 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:27:47.040237  8856 net.cpp:125] relu2 needs backward computation.
I0909 21:27:47.040243  8856 net.cpp:66] Creating Layer pool2
I0909 21:27:47.040249  8856 net.cpp:329] pool2 <- conv2
I0909 21:27:47.040256  8856 net.cpp:290] pool2 -> pool2
I0909 21:27:47.040264  8856 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:27:47.040271  8856 net.cpp:125] pool2 needs backward computation.
I0909 21:27:47.040277  8856 net.cpp:66] Creating Layer fc7
I0909 21:27:47.040282  8856 net.cpp:329] fc7 <- pool2
I0909 21:27:47.040292  8856 net.cpp:290] fc7 -> fc7
I0909 21:27:47.683883  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.683928  8856 net.cpp:125] fc7 needs backward computation.
I0909 21:27:47.683949  8856 net.cpp:66] Creating Layer relu7
I0909 21:27:47.683958  8856 net.cpp:329] relu7 <- fc7
I0909 21:27:47.683965  8856 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:27:47.683982  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.683989  8856 net.cpp:125] relu7 needs backward computation.
I0909 21:27:47.683995  8856 net.cpp:66] Creating Layer drop7
I0909 21:27:47.684001  8856 net.cpp:329] drop7 <- fc7
I0909 21:27:47.684010  8856 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:27:47.684020  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.684026  8856 net.cpp:125] drop7 needs backward computation.
I0909 21:27:47.684036  8856 net.cpp:66] Creating Layer fc8
I0909 21:27:47.684041  8856 net.cpp:329] fc8 <- fc7
I0909 21:27:47.684047  8856 net.cpp:290] fc8 -> fc8
I0909 21:27:47.691826  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.691838  8856 net.cpp:125] fc8 needs backward computation.
I0909 21:27:47.691845  8856 net.cpp:66] Creating Layer relu8
I0909 21:27:47.691851  8856 net.cpp:329] relu8 <- fc8
I0909 21:27:47.691858  8856 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:27:47.691864  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.691869  8856 net.cpp:125] relu8 needs backward computation.
I0909 21:27:47.691876  8856 net.cpp:66] Creating Layer drop8
I0909 21:27:47.691881  8856 net.cpp:329] drop8 <- fc8
I0909 21:27:47.691890  8856 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:27:47.691897  8856 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:27:47.691903  8856 net.cpp:125] drop8 needs backward computation.
I0909 21:27:47.691910  8856 net.cpp:66] Creating Layer fc9
I0909 21:27:47.691915  8856 net.cpp:329] fc9 <- fc8
I0909 21:27:47.691925  8856 net.cpp:290] fc9 -> fc9
I0909 21:27:47.692307  8856 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:27:47.692318  8856 net.cpp:125] fc9 needs backward computation.
I0909 21:27:47.692327  8856 net.cpp:66] Creating Layer fc10
I0909 21:27:47.692332  8856 net.cpp:329] fc10 <- fc9
I0909 21:27:47.692343  8856 net.cpp:290] fc10 -> fc10
I0909 21:27:47.692355  8856 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:27:47.692365  8856 net.cpp:125] fc10 needs backward computation.
I0909 21:27:47.692373  8856 net.cpp:66] Creating Layer prob
I0909 21:27:47.692378  8856 net.cpp:329] prob <- fc10
I0909 21:27:47.692384  8856 net.cpp:290] prob -> prob
I0909 21:27:47.692394  8856 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:27:47.692400  8856 net.cpp:125] prob needs backward computation.
I0909 21:27:47.692405  8856 net.cpp:156] This network produces output prob
I0909 21:27:47.692417  8856 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:27:47.692426  8856 net.cpp:167] Network initialization done.
I0909 21:27:47.692431  8856 net.cpp:168] Memory required for data: 6183480
Classifying 125 inputs.
Done in 74.33 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:29:05.518285  8866 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:29:05.518427  8866 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:29:05.518436  8866 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:29:05.518584  8866 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:29:05.518651  8866 net.cpp:292] Input 0 -> data
I0909 21:29:05.518676  8866 net.cpp:66] Creating Layer conv1
I0909 21:29:05.518683  8866 net.cpp:329] conv1 <- data
I0909 21:29:05.518692  8866 net.cpp:290] conv1 -> conv1
I0909 21:29:05.520056  8866 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:29:05.520073  8866 net.cpp:125] conv1 needs backward computation.
I0909 21:29:05.520082  8866 net.cpp:66] Creating Layer relu1
I0909 21:29:05.520088  8866 net.cpp:329] relu1 <- conv1
I0909 21:29:05.520095  8866 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:29:05.520104  8866 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:29:05.520110  8866 net.cpp:125] relu1 needs backward computation.
I0909 21:29:05.520117  8866 net.cpp:66] Creating Layer pool1
I0909 21:29:05.520123  8866 net.cpp:329] pool1 <- conv1
I0909 21:29:05.520129  8866 net.cpp:290] pool1 -> pool1
I0909 21:29:05.520140  8866 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:29:05.520146  8866 net.cpp:125] pool1 needs backward computation.
I0909 21:29:05.520153  8866 net.cpp:66] Creating Layer norm1
I0909 21:29:05.520159  8866 net.cpp:329] norm1 <- pool1
I0909 21:29:05.520165  8866 net.cpp:290] norm1 -> norm1
I0909 21:29:05.520175  8866 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:29:05.520181  8866 net.cpp:125] norm1 needs backward computation.
I0909 21:29:05.520190  8866 net.cpp:66] Creating Layer conv2
I0909 21:29:05.520195  8866 net.cpp:329] conv2 <- norm1
I0909 21:29:05.520202  8866 net.cpp:290] conv2 -> conv2
I0909 21:29:05.529489  8866 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:29:05.529531  8866 net.cpp:125] conv2 needs backward computation.
I0909 21:29:05.529543  8866 net.cpp:66] Creating Layer relu2
I0909 21:29:05.529549  8866 net.cpp:329] relu2 <- conv2
I0909 21:29:05.529557  8866 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:29:05.529567  8866 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:29:05.529572  8866 net.cpp:125] relu2 needs backward computation.
I0909 21:29:05.529579  8866 net.cpp:66] Creating Layer pool2
I0909 21:29:05.529585  8866 net.cpp:329] pool2 <- conv2
I0909 21:29:05.529592  8866 net.cpp:290] pool2 -> pool2
I0909 21:29:05.529602  8866 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:29:05.529608  8866 net.cpp:125] pool2 needs backward computation.
I0909 21:29:05.529615  8866 net.cpp:66] Creating Layer fc7
I0909 21:29:05.529621  8866 net.cpp:329] fc7 <- pool2
I0909 21:29:05.529633  8866 net.cpp:290] fc7 -> fc7
I0909 21:29:06.171169  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.171212  8866 net.cpp:125] fc7 needs backward computation.
I0909 21:29:06.171226  8866 net.cpp:66] Creating Layer relu7
I0909 21:29:06.171233  8866 net.cpp:329] relu7 <- fc7
I0909 21:29:06.171241  8866 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:29:06.171252  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.171257  8866 net.cpp:125] relu7 needs backward computation.
I0909 21:29:06.171264  8866 net.cpp:66] Creating Layer drop7
I0909 21:29:06.171269  8866 net.cpp:329] drop7 <- fc7
I0909 21:29:06.171277  8866 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:29:06.171288  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.171294  8866 net.cpp:125] drop7 needs backward computation.
I0909 21:29:06.171303  8866 net.cpp:66] Creating Layer fc8
I0909 21:29:06.171308  8866 net.cpp:329] fc8 <- fc7
I0909 21:29:06.171326  8866 net.cpp:290] fc8 -> fc8
I0909 21:29:06.178886  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.178899  8866 net.cpp:125] fc8 needs backward computation.
I0909 21:29:06.178905  8866 net.cpp:66] Creating Layer relu8
I0909 21:29:06.178911  8866 net.cpp:329] relu8 <- fc8
I0909 21:29:06.178917  8866 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:29:06.178925  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.178930  8866 net.cpp:125] relu8 needs backward computation.
I0909 21:29:06.178936  8866 net.cpp:66] Creating Layer drop8
I0909 21:29:06.178942  8866 net.cpp:329] drop8 <- fc8
I0909 21:29:06.178951  8866 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:29:06.178957  8866 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:29:06.178962  8866 net.cpp:125] drop8 needs backward computation.
I0909 21:29:06.178969  8866 net.cpp:66] Creating Layer fc9
I0909 21:29:06.178975  8866 net.cpp:329] fc9 <- fc8
I0909 21:29:06.178983  8866 net.cpp:290] fc9 -> fc9
I0909 21:29:06.179348  8866 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:29:06.179358  8866 net.cpp:125] fc9 needs backward computation.
I0909 21:29:06.179366  8866 net.cpp:66] Creating Layer fc10
I0909 21:29:06.179373  8866 net.cpp:329] fc10 <- fc9
I0909 21:29:06.179380  8866 net.cpp:290] fc10 -> fc10
I0909 21:29:06.179391  8866 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:29:06.179401  8866 net.cpp:125] fc10 needs backward computation.
I0909 21:29:06.179409  8866 net.cpp:66] Creating Layer prob
I0909 21:29:06.179414  8866 net.cpp:329] prob <- fc10
I0909 21:29:06.179419  8866 net.cpp:290] prob -> prob
I0909 21:29:06.179430  8866 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:29:06.179435  8866 net.cpp:125] prob needs backward computation.
I0909 21:29:06.179440  8866 net.cpp:156] This network produces output prob
I0909 21:29:06.179452  8866 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:29:06.179461  8866 net.cpp:167] Network initialization done.
I0909 21:29:06.179466  8866 net.cpp:168] Memory required for data: 6183480
Classifying 381 inputs.
Done in 252.10 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:33:57.534584  8875 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:33:57.534843  8875 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:33:57.534862  8875 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:33:57.535192  8875 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:33:57.535315  8875 net.cpp:292] Input 0 -> data
I0909 21:33:57.535363  8875 net.cpp:66] Creating Layer conv1
I0909 21:33:57.535378  8875 net.cpp:329] conv1 <- data
I0909 21:33:57.535394  8875 net.cpp:290] conv1 -> conv1
I0909 21:33:57.626252  8875 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:33:57.626291  8875 net.cpp:125] conv1 needs backward computation.
I0909 21:33:57.626303  8875 net.cpp:66] Creating Layer relu1
I0909 21:33:57.626310  8875 net.cpp:329] relu1 <- conv1
I0909 21:33:57.626318  8875 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:33:57.626328  8875 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:33:57.626334  8875 net.cpp:125] relu1 needs backward computation.
I0909 21:33:57.626343  8875 net.cpp:66] Creating Layer pool1
I0909 21:33:57.626348  8875 net.cpp:329] pool1 <- conv1
I0909 21:33:57.626355  8875 net.cpp:290] pool1 -> pool1
I0909 21:33:57.626366  8875 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:33:57.626373  8875 net.cpp:125] pool1 needs backward computation.
I0909 21:33:57.626380  8875 net.cpp:66] Creating Layer norm1
I0909 21:33:57.626386  8875 net.cpp:329] norm1 <- pool1
I0909 21:33:57.626394  8875 net.cpp:290] norm1 -> norm1
I0909 21:33:57.626404  8875 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:33:57.626410  8875 net.cpp:125] norm1 needs backward computation.
I0909 21:33:57.626418  8875 net.cpp:66] Creating Layer conv2
I0909 21:33:57.626425  8875 net.cpp:329] conv2 <- norm1
I0909 21:33:57.626431  8875 net.cpp:290] conv2 -> conv2
I0909 21:33:57.635632  8875 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:33:57.635649  8875 net.cpp:125] conv2 needs backward computation.
I0909 21:33:57.635658  8875 net.cpp:66] Creating Layer relu2
I0909 21:33:57.635663  8875 net.cpp:329] relu2 <- conv2
I0909 21:33:57.635670  8875 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:33:57.635679  8875 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:33:57.635684  8875 net.cpp:125] relu2 needs backward computation.
I0909 21:33:57.635690  8875 net.cpp:66] Creating Layer pool2
I0909 21:33:57.635696  8875 net.cpp:329] pool2 <- conv2
I0909 21:33:57.635704  8875 net.cpp:290] pool2 -> pool2
I0909 21:33:57.635711  8875 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:33:57.635717  8875 net.cpp:125] pool2 needs backward computation.
I0909 21:33:57.635736  8875 net.cpp:66] Creating Layer fc7
I0909 21:33:57.635742  8875 net.cpp:329] fc7 <- pool2
I0909 21:33:57.635751  8875 net.cpp:290] fc7 -> fc7
I0909 21:33:58.277477  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.277535  8875 net.cpp:125] fc7 needs backward computation.
I0909 21:33:58.277560  8875 net.cpp:66] Creating Layer relu7
I0909 21:33:58.277567  8875 net.cpp:329] relu7 <- fc7
I0909 21:33:58.277576  8875 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:33:58.277585  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.277591  8875 net.cpp:125] relu7 needs backward computation.
I0909 21:33:58.277598  8875 net.cpp:66] Creating Layer drop7
I0909 21:33:58.277603  8875 net.cpp:329] drop7 <- fc7
I0909 21:33:58.277611  8875 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:33:58.277621  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.277627  8875 net.cpp:125] drop7 needs backward computation.
I0909 21:33:58.277637  8875 net.cpp:66] Creating Layer fc8
I0909 21:33:58.277642  8875 net.cpp:329] fc8 <- fc7
I0909 21:33:58.277648  8875 net.cpp:290] fc8 -> fc8
I0909 21:33:58.285300  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.285312  8875 net.cpp:125] fc8 needs backward computation.
I0909 21:33:58.285320  8875 net.cpp:66] Creating Layer relu8
I0909 21:33:58.285326  8875 net.cpp:329] relu8 <- fc8
I0909 21:33:58.285331  8875 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:33:58.285338  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.285344  8875 net.cpp:125] relu8 needs backward computation.
I0909 21:33:58.285351  8875 net.cpp:66] Creating Layer drop8
I0909 21:33:58.285356  8875 net.cpp:329] drop8 <- fc8
I0909 21:33:58.285363  8875 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:33:58.285372  8875 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:33:58.285377  8875 net.cpp:125] drop8 needs backward computation.
I0909 21:33:58.285384  8875 net.cpp:66] Creating Layer fc9
I0909 21:33:58.285389  8875 net.cpp:329] fc9 <- fc8
I0909 21:33:58.285398  8875 net.cpp:290] fc9 -> fc9
I0909 21:33:58.285783  8875 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:33:58.285794  8875 net.cpp:125] fc9 needs backward computation.
I0909 21:33:58.285802  8875 net.cpp:66] Creating Layer fc10
I0909 21:33:58.285809  8875 net.cpp:329] fc10 <- fc9
I0909 21:33:58.285816  8875 net.cpp:290] fc10 -> fc10
I0909 21:33:58.285828  8875 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:33:58.285837  8875 net.cpp:125] fc10 needs backward computation.
I0909 21:33:58.285845  8875 net.cpp:66] Creating Layer prob
I0909 21:33:58.285851  8875 net.cpp:329] prob <- fc10
I0909 21:33:58.285856  8875 net.cpp:290] prob -> prob
I0909 21:33:58.285866  8875 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:33:58.285871  8875 net.cpp:125] prob needs backward computation.
I0909 21:33:58.285876  8875 net.cpp:156] This network produces output prob
I0909 21:33:58.285889  8875 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:33:58.285897  8875 net.cpp:167] Network initialization done.
I0909 21:33:58.285902  8875 net.cpp:168] Memory required for data: 6183480
Classifying 57 inputs.
Done in 37.80 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:34:47.332154  8880 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:34:47.332291  8880 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:34:47.332300  8880 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:34:47.332450  8880 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:34:47.332515  8880 net.cpp:292] Input 0 -> data
I0909 21:34:47.332542  8880 net.cpp:66] Creating Layer conv1
I0909 21:34:47.332550  8880 net.cpp:329] conv1 <- data
I0909 21:34:47.332557  8880 net.cpp:290] conv1 -> conv1
I0909 21:34:47.333945  8880 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:34:47.333964  8880 net.cpp:125] conv1 needs backward computation.
I0909 21:34:47.333974  8880 net.cpp:66] Creating Layer relu1
I0909 21:34:47.333981  8880 net.cpp:329] relu1 <- conv1
I0909 21:34:47.333986  8880 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:34:47.333995  8880 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:34:47.334002  8880 net.cpp:125] relu1 needs backward computation.
I0909 21:34:47.334008  8880 net.cpp:66] Creating Layer pool1
I0909 21:34:47.334013  8880 net.cpp:329] pool1 <- conv1
I0909 21:34:47.334020  8880 net.cpp:290] pool1 -> pool1
I0909 21:34:47.334031  8880 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:34:47.334038  8880 net.cpp:125] pool1 needs backward computation.
I0909 21:34:47.334044  8880 net.cpp:66] Creating Layer norm1
I0909 21:34:47.334049  8880 net.cpp:329] norm1 <- pool1
I0909 21:34:47.334055  8880 net.cpp:290] norm1 -> norm1
I0909 21:34:47.334065  8880 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:34:47.334076  8880 net.cpp:125] norm1 needs backward computation.
I0909 21:34:47.334084  8880 net.cpp:66] Creating Layer conv2
I0909 21:34:47.334089  8880 net.cpp:329] conv2 <- norm1
I0909 21:34:47.334097  8880 net.cpp:290] conv2 -> conv2
I0909 21:34:47.343196  8880 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:34:47.343211  8880 net.cpp:125] conv2 needs backward computation.
I0909 21:34:47.343219  8880 net.cpp:66] Creating Layer relu2
I0909 21:34:47.343224  8880 net.cpp:329] relu2 <- conv2
I0909 21:34:47.343230  8880 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:34:47.343237  8880 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:34:47.343242  8880 net.cpp:125] relu2 needs backward computation.
I0909 21:34:47.343250  8880 net.cpp:66] Creating Layer pool2
I0909 21:34:47.343255  8880 net.cpp:329] pool2 <- conv2
I0909 21:34:47.343261  8880 net.cpp:290] pool2 -> pool2
I0909 21:34:47.343269  8880 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:34:47.343274  8880 net.cpp:125] pool2 needs backward computation.
I0909 21:34:47.343283  8880 net.cpp:66] Creating Layer fc7
I0909 21:34:47.343289  8880 net.cpp:329] fc7 <- pool2
I0909 21:34:47.343297  8880 net.cpp:290] fc7 -> fc7
I0909 21:34:47.987263  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.987306  8880 net.cpp:125] fc7 needs backward computation.
I0909 21:34:47.987320  8880 net.cpp:66] Creating Layer relu7
I0909 21:34:47.987329  8880 net.cpp:329] relu7 <- fc7
I0909 21:34:47.987335  8880 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:34:47.987345  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.987351  8880 net.cpp:125] relu7 needs backward computation.
I0909 21:34:47.987360  8880 net.cpp:66] Creating Layer drop7
I0909 21:34:47.987365  8880 net.cpp:329] drop7 <- fc7
I0909 21:34:47.987372  8880 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:34:47.987383  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.987388  8880 net.cpp:125] drop7 needs backward computation.
I0909 21:34:47.987397  8880 net.cpp:66] Creating Layer fc8
I0909 21:34:47.987403  8880 net.cpp:329] fc8 <- fc7
I0909 21:34:47.987411  8880 net.cpp:290] fc8 -> fc8
I0909 21:34:47.995193  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.995204  8880 net.cpp:125] fc8 needs backward computation.
I0909 21:34:47.995211  8880 net.cpp:66] Creating Layer relu8
I0909 21:34:47.995218  8880 net.cpp:329] relu8 <- fc8
I0909 21:34:47.995223  8880 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:34:47.995230  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.995236  8880 net.cpp:125] relu8 needs backward computation.
I0909 21:34:47.995242  8880 net.cpp:66] Creating Layer drop8
I0909 21:34:47.995247  8880 net.cpp:329] drop8 <- fc8
I0909 21:34:47.995255  8880 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:34:47.995262  8880 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:47.995268  8880 net.cpp:125] drop8 needs backward computation.
I0909 21:34:47.995275  8880 net.cpp:66] Creating Layer fc9
I0909 21:34:47.995281  8880 net.cpp:329] fc9 <- fc8
I0909 21:34:47.995290  8880 net.cpp:290] fc9 -> fc9
I0909 21:34:47.995662  8880 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:34:47.995673  8880 net.cpp:125] fc9 needs backward computation.
I0909 21:34:47.995682  8880 net.cpp:66] Creating Layer fc10
I0909 21:34:47.995687  8880 net.cpp:329] fc10 <- fc9
I0909 21:34:47.995695  8880 net.cpp:290] fc10 -> fc10
I0909 21:34:47.995707  8880 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:34:47.995717  8880 net.cpp:125] fc10 needs backward computation.
I0909 21:34:47.995723  8880 net.cpp:66] Creating Layer prob
I0909 21:34:47.995728  8880 net.cpp:329] prob <- fc10
I0909 21:34:47.995734  8880 net.cpp:290] prob -> prob
I0909 21:34:47.995743  8880 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:34:47.995749  8880 net.cpp:125] prob needs backward computation.
I0909 21:34:47.995754  8880 net.cpp:156] This network produces output prob
I0909 21:34:47.995767  8880 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:34:47.995775  8880 net.cpp:167] Network initialization done.
I0909 21:34:47.995791  8880 net.cpp:168] Memory required for data: 6183480
Classifying 10 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 133, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:34:48.872243  8883 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:34:48.872380  8883 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:34:48.872388  8883 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:34:48.872534  8883 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:34:48.872598  8883 net.cpp:292] Input 0 -> data
I0909 21:34:48.872623  8883 net.cpp:66] Creating Layer conv1
I0909 21:34:48.872630  8883 net.cpp:329] conv1 <- data
I0909 21:34:48.872638  8883 net.cpp:290] conv1 -> conv1
I0909 21:34:48.873996  8883 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:34:48.874013  8883 net.cpp:125] conv1 needs backward computation.
I0909 21:34:48.874022  8883 net.cpp:66] Creating Layer relu1
I0909 21:34:48.874028  8883 net.cpp:329] relu1 <- conv1
I0909 21:34:48.874035  8883 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:34:48.874043  8883 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:34:48.874049  8883 net.cpp:125] relu1 needs backward computation.
I0909 21:34:48.874055  8883 net.cpp:66] Creating Layer pool1
I0909 21:34:48.874060  8883 net.cpp:329] pool1 <- conv1
I0909 21:34:48.874068  8883 net.cpp:290] pool1 -> pool1
I0909 21:34:48.874078  8883 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:34:48.874084  8883 net.cpp:125] pool1 needs backward computation.
I0909 21:34:48.874090  8883 net.cpp:66] Creating Layer norm1
I0909 21:34:48.874096  8883 net.cpp:329] norm1 <- pool1
I0909 21:34:48.874102  8883 net.cpp:290] norm1 -> norm1
I0909 21:34:48.874111  8883 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:34:48.874117  8883 net.cpp:125] norm1 needs backward computation.
I0909 21:34:48.874125  8883 net.cpp:66] Creating Layer conv2
I0909 21:34:48.874130  8883 net.cpp:329] conv2 <- norm1
I0909 21:34:48.874136  8883 net.cpp:290] conv2 -> conv2
I0909 21:34:48.882980  8883 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:34:48.882994  8883 net.cpp:125] conv2 needs backward computation.
I0909 21:34:48.883002  8883 net.cpp:66] Creating Layer relu2
I0909 21:34:48.883008  8883 net.cpp:329] relu2 <- conv2
I0909 21:34:48.883013  8883 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:34:48.883020  8883 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:34:48.883025  8883 net.cpp:125] relu2 needs backward computation.
I0909 21:34:48.883031  8883 net.cpp:66] Creating Layer pool2
I0909 21:34:48.883036  8883 net.cpp:329] pool2 <- conv2
I0909 21:34:48.883044  8883 net.cpp:290] pool2 -> pool2
I0909 21:34:48.883050  8883 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:34:48.883056  8883 net.cpp:125] pool2 needs backward computation.
I0909 21:34:48.883064  8883 net.cpp:66] Creating Layer fc7
I0909 21:34:48.883070  8883 net.cpp:329] fc7 <- pool2
I0909 21:34:48.883077  8883 net.cpp:290] fc7 -> fc7
I0909 21:34:49.525902  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.525946  8883 net.cpp:125] fc7 needs backward computation.
I0909 21:34:49.525960  8883 net.cpp:66] Creating Layer relu7
I0909 21:34:49.525969  8883 net.cpp:329] relu7 <- fc7
I0909 21:34:49.525976  8883 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:34:49.525985  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.525991  8883 net.cpp:125] relu7 needs backward computation.
I0909 21:34:49.525998  8883 net.cpp:66] Creating Layer drop7
I0909 21:34:49.526003  8883 net.cpp:329] drop7 <- fc7
I0909 21:34:49.526011  8883 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:34:49.526023  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.526028  8883 net.cpp:125] drop7 needs backward computation.
I0909 21:34:49.526037  8883 net.cpp:66] Creating Layer fc8
I0909 21:34:49.526043  8883 net.cpp:329] fc8 <- fc7
I0909 21:34:49.526051  8883 net.cpp:290] fc8 -> fc8
I0909 21:34:49.533859  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.533870  8883 net.cpp:125] fc8 needs backward computation.
I0909 21:34:49.533877  8883 net.cpp:66] Creating Layer relu8
I0909 21:34:49.533884  8883 net.cpp:329] relu8 <- fc8
I0909 21:34:49.533890  8883 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:34:49.533897  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.533902  8883 net.cpp:125] relu8 needs backward computation.
I0909 21:34:49.533921  8883 net.cpp:66] Creating Layer drop8
I0909 21:34:49.533926  8883 net.cpp:329] drop8 <- fc8
I0909 21:34:49.533934  8883 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:34:49.533941  8883 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:34:49.533947  8883 net.cpp:125] drop8 needs backward computation.
I0909 21:34:49.533954  8883 net.cpp:66] Creating Layer fc9
I0909 21:34:49.533960  8883 net.cpp:329] fc9 <- fc8
I0909 21:34:49.533968  8883 net.cpp:290] fc9 -> fc9
I0909 21:34:49.534342  8883 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:34:49.534353  8883 net.cpp:125] fc9 needs backward computation.
I0909 21:34:49.534360  8883 net.cpp:66] Creating Layer fc10
I0909 21:34:49.534366  8883 net.cpp:329] fc10 <- fc9
I0909 21:34:49.534374  8883 net.cpp:290] fc10 -> fc10
I0909 21:34:49.534386  8883 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:34:49.534395  8883 net.cpp:125] fc10 needs backward computation.
I0909 21:34:49.534402  8883 net.cpp:66] Creating Layer prob
I0909 21:34:49.534407  8883 net.cpp:329] prob <- fc10
I0909 21:34:49.534415  8883 net.cpp:290] prob -> prob
I0909 21:34:49.534423  8883 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:34:49.534430  8883 net.cpp:125] prob needs backward computation.
I0909 21:34:49.534435  8883 net.cpp:156] This network produces output prob
I0909 21:34:49.534446  8883 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:34:49.534456  8883 net.cpp:167] Network initialization done.
I0909 21:34:49.534461  8883 net.cpp:168] Memory required for data: 6183480
Classifying 459 inputs.
Done in 298.86 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:40:03.688179  8909 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:40:03.688318  8909 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:40:03.688326  8909 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:40:03.688473  8909 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:40:03.688537  8909 net.cpp:292] Input 0 -> data
I0909 21:40:03.688563  8909 net.cpp:66] Creating Layer conv1
I0909 21:40:03.688570  8909 net.cpp:329] conv1 <- data
I0909 21:40:03.688577  8909 net.cpp:290] conv1 -> conv1
I0909 21:40:03.689939  8909 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:40:03.689957  8909 net.cpp:125] conv1 needs backward computation.
I0909 21:40:03.689966  8909 net.cpp:66] Creating Layer relu1
I0909 21:40:03.689972  8909 net.cpp:329] relu1 <- conv1
I0909 21:40:03.689980  8909 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:40:03.689987  8909 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:40:03.689993  8909 net.cpp:125] relu1 needs backward computation.
I0909 21:40:03.689999  8909 net.cpp:66] Creating Layer pool1
I0909 21:40:03.690006  8909 net.cpp:329] pool1 <- conv1
I0909 21:40:03.690011  8909 net.cpp:290] pool1 -> pool1
I0909 21:40:03.690022  8909 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:40:03.690028  8909 net.cpp:125] pool1 needs backward computation.
I0909 21:40:03.690034  8909 net.cpp:66] Creating Layer norm1
I0909 21:40:03.690040  8909 net.cpp:329] norm1 <- pool1
I0909 21:40:03.690047  8909 net.cpp:290] norm1 -> norm1
I0909 21:40:03.690055  8909 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:40:03.690062  8909 net.cpp:125] norm1 needs backward computation.
I0909 21:40:03.690068  8909 net.cpp:66] Creating Layer conv2
I0909 21:40:03.690073  8909 net.cpp:329] conv2 <- norm1
I0909 21:40:03.690080  8909 net.cpp:290] conv2 -> conv2
I0909 21:40:03.698950  8909 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:40:03.698963  8909 net.cpp:125] conv2 needs backward computation.
I0909 21:40:03.698971  8909 net.cpp:66] Creating Layer relu2
I0909 21:40:03.698976  8909 net.cpp:329] relu2 <- conv2
I0909 21:40:03.698982  8909 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:40:03.698988  8909 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:40:03.698993  8909 net.cpp:125] relu2 needs backward computation.
I0909 21:40:03.699000  8909 net.cpp:66] Creating Layer pool2
I0909 21:40:03.699005  8909 net.cpp:329] pool2 <- conv2
I0909 21:40:03.699012  8909 net.cpp:290] pool2 -> pool2
I0909 21:40:03.699019  8909 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:40:03.699024  8909 net.cpp:125] pool2 needs backward computation.
I0909 21:40:03.699033  8909 net.cpp:66] Creating Layer fc7
I0909 21:40:03.699039  8909 net.cpp:329] fc7 <- pool2
I0909 21:40:03.699046  8909 net.cpp:290] fc7 -> fc7
I0909 21:40:04.457885  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.457928  8909 net.cpp:125] fc7 needs backward computation.
I0909 21:40:04.457942  8909 net.cpp:66] Creating Layer relu7
I0909 21:40:04.457949  8909 net.cpp:329] relu7 <- fc7
I0909 21:40:04.457957  8909 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:40:04.457976  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.457983  8909 net.cpp:125] relu7 needs backward computation.
I0909 21:40:04.457989  8909 net.cpp:66] Creating Layer drop7
I0909 21:40:04.457995  8909 net.cpp:329] drop7 <- fc7
I0909 21:40:04.458003  8909 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:40:04.458014  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.458019  8909 net.cpp:125] drop7 needs backward computation.
I0909 21:40:04.458027  8909 net.cpp:66] Creating Layer fc8
I0909 21:40:04.458032  8909 net.cpp:329] fc8 <- fc7
I0909 21:40:04.458039  8909 net.cpp:290] fc8 -> fc8
I0909 21:40:04.466980  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.466994  8909 net.cpp:125] fc8 needs backward computation.
I0909 21:40:04.467000  8909 net.cpp:66] Creating Layer relu8
I0909 21:40:04.467005  8909 net.cpp:329] relu8 <- fc8
I0909 21:40:04.467011  8909 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:40:04.467018  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.467023  8909 net.cpp:125] relu8 needs backward computation.
I0909 21:40:04.467031  8909 net.cpp:66] Creating Layer drop8
I0909 21:40:04.467036  8909 net.cpp:329] drop8 <- fc8
I0909 21:40:04.467042  8909 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:40:04.467049  8909 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:40:04.467054  8909 net.cpp:125] drop8 needs backward computation.
I0909 21:40:04.467062  8909 net.cpp:66] Creating Layer fc9
I0909 21:40:04.467067  8909 net.cpp:329] fc9 <- fc8
I0909 21:40:04.467075  8909 net.cpp:290] fc9 -> fc9
I0909 21:40:04.467502  8909 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:40:04.467512  8909 net.cpp:125] fc9 needs backward computation.
I0909 21:40:04.467520  8909 net.cpp:66] Creating Layer fc10
I0909 21:40:04.467526  8909 net.cpp:329] fc10 <- fc9
I0909 21:40:04.467535  8909 net.cpp:290] fc10 -> fc10
I0909 21:40:04.467545  8909 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:40:04.467555  8909 net.cpp:125] fc10 needs backward computation.
I0909 21:40:04.467561  8909 net.cpp:66] Creating Layer prob
I0909 21:40:04.467566  8909 net.cpp:329] prob <- fc10
I0909 21:40:04.467572  8909 net.cpp:290] prob -> prob
I0909 21:40:04.467581  8909 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:40:04.467587  8909 net.cpp:125] prob needs backward computation.
I0909 21:40:04.467592  8909 net.cpp:156] This network produces output prob
I0909 21:40:04.467603  8909 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:40:04.467612  8909 net.cpp:167] Network initialization done.
I0909 21:40:04.467617  8909 net.cpp:168] Memory required for data: 6183480
Classifying 159 inputs.
Done in 101.78 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:41:50.166584  8914 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:41:50.166721  8914 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:41:50.166730  8914 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:41:50.166873  8914 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:41:50.166937  8914 net.cpp:292] Input 0 -> data
I0909 21:41:50.166962  8914 net.cpp:66] Creating Layer conv1
I0909 21:41:50.166970  8914 net.cpp:329] conv1 <- data
I0909 21:41:50.166977  8914 net.cpp:290] conv1 -> conv1
I0909 21:41:50.168303  8914 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:41:50.168321  8914 net.cpp:125] conv1 needs backward computation.
I0909 21:41:50.168330  8914 net.cpp:66] Creating Layer relu1
I0909 21:41:50.168336  8914 net.cpp:329] relu1 <- conv1
I0909 21:41:50.168342  8914 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:41:50.168351  8914 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:41:50.168357  8914 net.cpp:125] relu1 needs backward computation.
I0909 21:41:50.168365  8914 net.cpp:66] Creating Layer pool1
I0909 21:41:50.168370  8914 net.cpp:329] pool1 <- conv1
I0909 21:41:50.168376  8914 net.cpp:290] pool1 -> pool1
I0909 21:41:50.168387  8914 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:41:50.168392  8914 net.cpp:125] pool1 needs backward computation.
I0909 21:41:50.168400  8914 net.cpp:66] Creating Layer norm1
I0909 21:41:50.168404  8914 net.cpp:329] norm1 <- pool1
I0909 21:41:50.168411  8914 net.cpp:290] norm1 -> norm1
I0909 21:41:50.168421  8914 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:41:50.168427  8914 net.cpp:125] norm1 needs backward computation.
I0909 21:41:50.168434  8914 net.cpp:66] Creating Layer conv2
I0909 21:41:50.168439  8914 net.cpp:329] conv2 <- norm1
I0909 21:41:50.168447  8914 net.cpp:290] conv2 -> conv2
I0909 21:41:50.177510  8914 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:41:50.177531  8914 net.cpp:125] conv2 needs backward computation.
I0909 21:41:50.177538  8914 net.cpp:66] Creating Layer relu2
I0909 21:41:50.177543  8914 net.cpp:329] relu2 <- conv2
I0909 21:41:50.177556  8914 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:41:50.177563  8914 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:41:50.177569  8914 net.cpp:125] relu2 needs backward computation.
I0909 21:41:50.177575  8914 net.cpp:66] Creating Layer pool2
I0909 21:41:50.177582  8914 net.cpp:329] pool2 <- conv2
I0909 21:41:50.177588  8914 net.cpp:290] pool2 -> pool2
I0909 21:41:50.177597  8914 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:41:50.177602  8914 net.cpp:125] pool2 needs backward computation.
I0909 21:41:50.177610  8914 net.cpp:66] Creating Layer fc7
I0909 21:41:50.177618  8914 net.cpp:329] fc7 <- pool2
I0909 21:41:50.177624  8914 net.cpp:290] fc7 -> fc7
I0909 21:41:50.819022  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.819067  8914 net.cpp:125] fc7 needs backward computation.
I0909 21:41:50.819082  8914 net.cpp:66] Creating Layer relu7
I0909 21:41:50.819089  8914 net.cpp:329] relu7 <- fc7
I0909 21:41:50.819097  8914 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:41:50.819108  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.819113  8914 net.cpp:125] relu7 needs backward computation.
I0909 21:41:50.819120  8914 net.cpp:66] Creating Layer drop7
I0909 21:41:50.819125  8914 net.cpp:329] drop7 <- fc7
I0909 21:41:50.819134  8914 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:41:50.819145  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.819151  8914 net.cpp:125] drop7 needs backward computation.
I0909 21:41:50.819159  8914 net.cpp:66] Creating Layer fc8
I0909 21:41:50.819165  8914 net.cpp:329] fc8 <- fc7
I0909 21:41:50.819172  8914 net.cpp:290] fc8 -> fc8
I0909 21:41:50.826756  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.826767  8914 net.cpp:125] fc8 needs backward computation.
I0909 21:41:50.826774  8914 net.cpp:66] Creating Layer relu8
I0909 21:41:50.826779  8914 net.cpp:329] relu8 <- fc8
I0909 21:41:50.826786  8914 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:41:50.826792  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.826798  8914 net.cpp:125] relu8 needs backward computation.
I0909 21:41:50.826804  8914 net.cpp:66] Creating Layer drop8
I0909 21:41:50.826809  8914 net.cpp:329] drop8 <- fc8
I0909 21:41:50.826817  8914 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:41:50.826824  8914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:41:50.826829  8914 net.cpp:125] drop8 needs backward computation.
I0909 21:41:50.826838  8914 net.cpp:66] Creating Layer fc9
I0909 21:41:50.826843  8914 net.cpp:329] fc9 <- fc8
I0909 21:41:50.826850  8914 net.cpp:290] fc9 -> fc9
I0909 21:41:50.827215  8914 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:41:50.827226  8914 net.cpp:125] fc9 needs backward computation.
I0909 21:41:50.827234  8914 net.cpp:66] Creating Layer fc10
I0909 21:41:50.827239  8914 net.cpp:329] fc10 <- fc9
I0909 21:41:50.827247  8914 net.cpp:290] fc10 -> fc10
I0909 21:41:50.827260  8914 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:41:50.827268  8914 net.cpp:125] fc10 needs backward computation.
I0909 21:41:50.827275  8914 net.cpp:66] Creating Layer prob
I0909 21:41:50.827280  8914 net.cpp:329] prob <- fc10
I0909 21:41:50.827286  8914 net.cpp:290] prob -> prob
I0909 21:41:50.827296  8914 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:41:50.827301  8914 net.cpp:125] prob needs backward computation.
I0909 21:41:50.827306  8914 net.cpp:156] This network produces output prob
I0909 21:41:50.827318  8914 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:41:50.827327  8914 net.cpp:167] Network initialization done.
I0909 21:41:50.827332  8914 net.cpp:168] Memory required for data: 6183480
Classifying 368 inputs.
Done in 230.78 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:45:48.041856  8924 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:45:48.041993  8924 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:45:48.042014  8924 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:45:48.042160  8924 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:45:48.042212  8924 net.cpp:292] Input 0 -> data
I0909 21:45:48.042237  8924 net.cpp:66] Creating Layer conv1
I0909 21:45:48.042244  8924 net.cpp:329] conv1 <- data
I0909 21:45:48.042253  8924 net.cpp:290] conv1 -> conv1
I0909 21:45:48.043576  8924 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:45:48.043593  8924 net.cpp:125] conv1 needs backward computation.
I0909 21:45:48.043602  8924 net.cpp:66] Creating Layer relu1
I0909 21:45:48.043608  8924 net.cpp:329] relu1 <- conv1
I0909 21:45:48.043614  8924 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:45:48.043622  8924 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:45:48.043628  8924 net.cpp:125] relu1 needs backward computation.
I0909 21:45:48.043640  8924 net.cpp:66] Creating Layer pool1
I0909 21:45:48.043645  8924 net.cpp:329] pool1 <- conv1
I0909 21:45:48.043653  8924 net.cpp:290] pool1 -> pool1
I0909 21:45:48.043663  8924 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:45:48.043669  8924 net.cpp:125] pool1 needs backward computation.
I0909 21:45:48.043675  8924 net.cpp:66] Creating Layer norm1
I0909 21:45:48.043680  8924 net.cpp:329] norm1 <- pool1
I0909 21:45:48.043687  8924 net.cpp:290] norm1 -> norm1
I0909 21:45:48.043696  8924 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:45:48.043702  8924 net.cpp:125] norm1 needs backward computation.
I0909 21:45:48.043709  8924 net.cpp:66] Creating Layer conv2
I0909 21:45:48.043715  8924 net.cpp:329] conv2 <- norm1
I0909 21:45:48.043721  8924 net.cpp:290] conv2 -> conv2
I0909 21:45:48.052670  8924 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:45:48.052685  8924 net.cpp:125] conv2 needs backward computation.
I0909 21:45:48.052691  8924 net.cpp:66] Creating Layer relu2
I0909 21:45:48.052697  8924 net.cpp:329] relu2 <- conv2
I0909 21:45:48.052703  8924 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:45:48.052711  8924 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:45:48.052716  8924 net.cpp:125] relu2 needs backward computation.
I0909 21:45:48.052722  8924 net.cpp:66] Creating Layer pool2
I0909 21:45:48.052727  8924 net.cpp:329] pool2 <- conv2
I0909 21:45:48.052733  8924 net.cpp:290] pool2 -> pool2
I0909 21:45:48.052742  8924 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:45:48.052747  8924 net.cpp:125] pool2 needs backward computation.
I0909 21:45:48.052757  8924 net.cpp:66] Creating Layer fc7
I0909 21:45:48.052762  8924 net.cpp:329] fc7 <- pool2
I0909 21:45:48.052769  8924 net.cpp:290] fc7 -> fc7
I0909 21:45:48.690989  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.691031  8924 net.cpp:125] fc7 needs backward computation.
I0909 21:45:48.691045  8924 net.cpp:66] Creating Layer relu7
I0909 21:45:48.691052  8924 net.cpp:329] relu7 <- fc7
I0909 21:45:48.691061  8924 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:45:48.691071  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.691076  8924 net.cpp:125] relu7 needs backward computation.
I0909 21:45:48.691082  8924 net.cpp:66] Creating Layer drop7
I0909 21:45:48.691088  8924 net.cpp:329] drop7 <- fc7
I0909 21:45:48.691095  8924 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:45:48.691107  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.691112  8924 net.cpp:125] drop7 needs backward computation.
I0909 21:45:48.691120  8924 net.cpp:66] Creating Layer fc8
I0909 21:45:48.691125  8924 net.cpp:329] fc8 <- fc7
I0909 21:45:48.691133  8924 net.cpp:290] fc8 -> fc8
I0909 21:45:48.698727  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.698740  8924 net.cpp:125] fc8 needs backward computation.
I0909 21:45:48.698746  8924 net.cpp:66] Creating Layer relu8
I0909 21:45:48.698751  8924 net.cpp:329] relu8 <- fc8
I0909 21:45:48.698758  8924 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:45:48.698765  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.698770  8924 net.cpp:125] relu8 needs backward computation.
I0909 21:45:48.698776  8924 net.cpp:66] Creating Layer drop8
I0909 21:45:48.698781  8924 net.cpp:329] drop8 <- fc8
I0909 21:45:48.698789  8924 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:45:48.698796  8924 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:45:48.698801  8924 net.cpp:125] drop8 needs backward computation.
I0909 21:45:48.698808  8924 net.cpp:66] Creating Layer fc9
I0909 21:45:48.698813  8924 net.cpp:329] fc9 <- fc8
I0909 21:45:48.698822  8924 net.cpp:290] fc9 -> fc9
I0909 21:45:48.699183  8924 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:45:48.699193  8924 net.cpp:125] fc9 needs backward computation.
I0909 21:45:48.699201  8924 net.cpp:66] Creating Layer fc10
I0909 21:45:48.699208  8924 net.cpp:329] fc10 <- fc9
I0909 21:45:48.699215  8924 net.cpp:290] fc10 -> fc10
I0909 21:45:48.699226  8924 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:45:48.699235  8924 net.cpp:125] fc10 needs backward computation.
I0909 21:45:48.699254  8924 net.cpp:66] Creating Layer prob
I0909 21:45:48.699259  8924 net.cpp:329] prob <- fc10
I0909 21:45:48.699266  8924 net.cpp:290] prob -> prob
I0909 21:45:48.699275  8924 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:45:48.699280  8924 net.cpp:125] prob needs backward computation.
I0909 21:45:48.699285  8924 net.cpp:156] This network produces output prob
I0909 21:45:48.699297  8924 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:45:48.699306  8924 net.cpp:167] Network initialization done.
I0909 21:45:48.699311  8924 net.cpp:168] Memory required for data: 6183480
Classifying 146 inputs.
Done in 86.59 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:47:18.753944  8929 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:47:18.754086  8929 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:47:18.754096  8929 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:47:18.754241  8929 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:47:18.754307  8929 net.cpp:292] Input 0 -> data
I0909 21:47:18.754333  8929 net.cpp:66] Creating Layer conv1
I0909 21:47:18.754340  8929 net.cpp:329] conv1 <- data
I0909 21:47:18.754348  8929 net.cpp:290] conv1 -> conv1
I0909 21:47:18.755710  8929 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:47:18.755728  8929 net.cpp:125] conv1 needs backward computation.
I0909 21:47:18.755738  8929 net.cpp:66] Creating Layer relu1
I0909 21:47:18.755743  8929 net.cpp:329] relu1 <- conv1
I0909 21:47:18.755750  8929 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:47:18.755759  8929 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:47:18.755764  8929 net.cpp:125] relu1 needs backward computation.
I0909 21:47:18.755771  8929 net.cpp:66] Creating Layer pool1
I0909 21:47:18.755776  8929 net.cpp:329] pool1 <- conv1
I0909 21:47:18.755784  8929 net.cpp:290] pool1 -> pool1
I0909 21:47:18.755795  8929 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:47:18.755800  8929 net.cpp:125] pool1 needs backward computation.
I0909 21:47:18.755807  8929 net.cpp:66] Creating Layer norm1
I0909 21:47:18.755812  8929 net.cpp:329] norm1 <- pool1
I0909 21:47:18.755820  8929 net.cpp:290] norm1 -> norm1
I0909 21:47:18.755830  8929 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:47:18.755834  8929 net.cpp:125] norm1 needs backward computation.
I0909 21:47:18.755842  8929 net.cpp:66] Creating Layer conv2
I0909 21:47:18.755847  8929 net.cpp:329] conv2 <- norm1
I0909 21:47:18.755856  8929 net.cpp:290] conv2 -> conv2
I0909 21:47:18.765144  8929 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:47:18.765179  8929 net.cpp:125] conv2 needs backward computation.
I0909 21:47:18.765189  8929 net.cpp:66] Creating Layer relu2
I0909 21:47:18.765197  8929 net.cpp:329] relu2 <- conv2
I0909 21:47:18.765203  8929 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:47:18.765213  8929 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:47:18.765218  8929 net.cpp:125] relu2 needs backward computation.
I0909 21:47:18.765225  8929 net.cpp:66] Creating Layer pool2
I0909 21:47:18.765230  8929 net.cpp:329] pool2 <- conv2
I0909 21:47:18.765238  8929 net.cpp:290] pool2 -> pool2
I0909 21:47:18.765246  8929 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:47:18.765252  8929 net.cpp:125] pool2 needs backward computation.
I0909 21:47:18.765264  8929 net.cpp:66] Creating Layer fc7
I0909 21:47:18.765269  8929 net.cpp:329] fc7 <- pool2
I0909 21:47:18.765277  8929 net.cpp:290] fc7 -> fc7
I0909 21:47:19.404301  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.404346  8929 net.cpp:125] fc7 needs backward computation.
I0909 21:47:19.404361  8929 net.cpp:66] Creating Layer relu7
I0909 21:47:19.404368  8929 net.cpp:329] relu7 <- fc7
I0909 21:47:19.404376  8929 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:47:19.404386  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.404392  8929 net.cpp:125] relu7 needs backward computation.
I0909 21:47:19.404398  8929 net.cpp:66] Creating Layer drop7
I0909 21:47:19.404403  8929 net.cpp:329] drop7 <- fc7
I0909 21:47:19.404412  8929 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:47:19.404422  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.404428  8929 net.cpp:125] drop7 needs backward computation.
I0909 21:47:19.404436  8929 net.cpp:66] Creating Layer fc8
I0909 21:47:19.404443  8929 net.cpp:329] fc8 <- fc7
I0909 21:47:19.404449  8929 net.cpp:290] fc8 -> fc8
I0909 21:47:19.412067  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.412081  8929 net.cpp:125] fc8 needs backward computation.
I0909 21:47:19.412088  8929 net.cpp:66] Creating Layer relu8
I0909 21:47:19.412093  8929 net.cpp:329] relu8 <- fc8
I0909 21:47:19.412101  8929 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:47:19.412117  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.412123  8929 net.cpp:125] relu8 needs backward computation.
I0909 21:47:19.412130  8929 net.cpp:66] Creating Layer drop8
I0909 21:47:19.412135  8929 net.cpp:329] drop8 <- fc8
I0909 21:47:19.412143  8929 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:47:19.412150  8929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:19.412156  8929 net.cpp:125] drop8 needs backward computation.
I0909 21:47:19.412163  8929 net.cpp:66] Creating Layer fc9
I0909 21:47:19.412169  8929 net.cpp:329] fc9 <- fc8
I0909 21:47:19.412178  8929 net.cpp:290] fc9 -> fc9
I0909 21:47:19.412550  8929 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:47:19.412561  8929 net.cpp:125] fc9 needs backward computation.
I0909 21:47:19.412569  8929 net.cpp:66] Creating Layer fc10
I0909 21:47:19.412575  8929 net.cpp:329] fc10 <- fc9
I0909 21:47:19.412583  8929 net.cpp:290] fc10 -> fc10
I0909 21:47:19.412595  8929 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:47:19.412605  8929 net.cpp:125] fc10 needs backward computation.
I0909 21:47:19.412611  8929 net.cpp:66] Creating Layer prob
I0909 21:47:19.412616  8929 net.cpp:329] prob <- fc10
I0909 21:47:19.412622  8929 net.cpp:290] prob -> prob
I0909 21:47:19.412632  8929 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:47:19.412638  8929 net.cpp:125] prob needs backward computation.
I0909 21:47:19.412643  8929 net.cpp:156] This network produces output prob
I0909 21:47:19.412655  8929 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:47:19.412663  8929 net.cpp:167] Network initialization done.
I0909 21:47:19.412668  8929 net.cpp:168] Memory required for data: 6183480
Classifying 15 inputs.
Done in 9.35 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:47:29.745950  8932 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:47:29.746091  8932 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:47:29.746099  8932 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:47:29.746247  8932 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:47:29.746312  8932 net.cpp:292] Input 0 -> data
I0909 21:47:29.746340  8932 net.cpp:66] Creating Layer conv1
I0909 21:47:29.746346  8932 net.cpp:329] conv1 <- data
I0909 21:47:29.746354  8932 net.cpp:290] conv1 -> conv1
I0909 21:47:29.747716  8932 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:47:29.747735  8932 net.cpp:125] conv1 needs backward computation.
I0909 21:47:29.747745  8932 net.cpp:66] Creating Layer relu1
I0909 21:47:29.747750  8932 net.cpp:329] relu1 <- conv1
I0909 21:47:29.747757  8932 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:47:29.747766  8932 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:47:29.747771  8932 net.cpp:125] relu1 needs backward computation.
I0909 21:47:29.747778  8932 net.cpp:66] Creating Layer pool1
I0909 21:47:29.747783  8932 net.cpp:329] pool1 <- conv1
I0909 21:47:29.747791  8932 net.cpp:290] pool1 -> pool1
I0909 21:47:29.747802  8932 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:47:29.747807  8932 net.cpp:125] pool1 needs backward computation.
I0909 21:47:29.747814  8932 net.cpp:66] Creating Layer norm1
I0909 21:47:29.747819  8932 net.cpp:329] norm1 <- pool1
I0909 21:47:29.747825  8932 net.cpp:290] norm1 -> norm1
I0909 21:47:29.747835  8932 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:47:29.747841  8932 net.cpp:125] norm1 needs backward computation.
I0909 21:47:29.747848  8932 net.cpp:66] Creating Layer conv2
I0909 21:47:29.747854  8932 net.cpp:329] conv2 <- norm1
I0909 21:47:29.747861  8932 net.cpp:290] conv2 -> conv2
I0909 21:47:29.757001  8932 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:47:29.757016  8932 net.cpp:125] conv2 needs backward computation.
I0909 21:47:29.757024  8932 net.cpp:66] Creating Layer relu2
I0909 21:47:29.757030  8932 net.cpp:329] relu2 <- conv2
I0909 21:47:29.757036  8932 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:47:29.757043  8932 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:47:29.757048  8932 net.cpp:125] relu2 needs backward computation.
I0909 21:47:29.757055  8932 net.cpp:66] Creating Layer pool2
I0909 21:47:29.757061  8932 net.cpp:329] pool2 <- conv2
I0909 21:47:29.757066  8932 net.cpp:290] pool2 -> pool2
I0909 21:47:29.757074  8932 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:47:29.757081  8932 net.cpp:125] pool2 needs backward computation.
I0909 21:47:29.757089  8932 net.cpp:66] Creating Layer fc7
I0909 21:47:29.757096  8932 net.cpp:329] fc7 <- pool2
I0909 21:47:29.757102  8932 net.cpp:290] fc7 -> fc7
I0909 21:47:30.400444  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.400491  8932 net.cpp:125] fc7 needs backward computation.
I0909 21:47:30.400506  8932 net.cpp:66] Creating Layer relu7
I0909 21:47:30.400512  8932 net.cpp:329] relu7 <- fc7
I0909 21:47:30.400532  8932 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:47:30.400542  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.400548  8932 net.cpp:125] relu7 needs backward computation.
I0909 21:47:30.400555  8932 net.cpp:66] Creating Layer drop7
I0909 21:47:30.400562  8932 net.cpp:329] drop7 <- fc7
I0909 21:47:30.400568  8932 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:47:30.400580  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.400586  8932 net.cpp:125] drop7 needs backward computation.
I0909 21:47:30.400594  8932 net.cpp:66] Creating Layer fc8
I0909 21:47:30.400599  8932 net.cpp:329] fc8 <- fc7
I0909 21:47:30.400607  8932 net.cpp:290] fc8 -> fc8
I0909 21:47:30.408406  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.408418  8932 net.cpp:125] fc8 needs backward computation.
I0909 21:47:30.408427  8932 net.cpp:66] Creating Layer relu8
I0909 21:47:30.408432  8932 net.cpp:329] relu8 <- fc8
I0909 21:47:30.408438  8932 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:47:30.408445  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.408450  8932 net.cpp:125] relu8 needs backward computation.
I0909 21:47:30.408457  8932 net.cpp:66] Creating Layer drop8
I0909 21:47:30.408463  8932 net.cpp:329] drop8 <- fc8
I0909 21:47:30.408469  8932 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:47:30.408476  8932 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:47:30.408483  8932 net.cpp:125] drop8 needs backward computation.
I0909 21:47:30.408489  8932 net.cpp:66] Creating Layer fc9
I0909 21:47:30.408495  8932 net.cpp:329] fc9 <- fc8
I0909 21:47:30.408504  8932 net.cpp:290] fc9 -> fc9
I0909 21:47:30.408876  8932 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:47:30.408887  8932 net.cpp:125] fc9 needs backward computation.
I0909 21:47:30.408895  8932 net.cpp:66] Creating Layer fc10
I0909 21:47:30.408901  8932 net.cpp:329] fc10 <- fc9
I0909 21:47:30.408910  8932 net.cpp:290] fc10 -> fc10
I0909 21:47:30.408921  8932 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:47:30.408931  8932 net.cpp:125] fc10 needs backward computation.
I0909 21:47:30.408937  8932 net.cpp:66] Creating Layer prob
I0909 21:47:30.408942  8932 net.cpp:329] prob <- fc10
I0909 21:47:30.408949  8932 net.cpp:290] prob -> prob
I0909 21:47:30.408958  8932 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:47:30.408963  8932 net.cpp:125] prob needs backward computation.
I0909 21:47:30.408969  8932 net.cpp:156] This network produces output prob
I0909 21:47:30.408982  8932 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:47:30.408989  8932 net.cpp:167] Network initialization done.
I0909 21:47:30.408994  8932 net.cpp:168] Memory required for data: 6183480
Classifying 288 inputs.
Done in 194.65 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:51:10.905689  8944 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:51:10.905840  8944 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:51:10.905848  8944 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:51:10.917331  8944 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:51:10.917408  8944 net.cpp:292] Input 0 -> data
I0909 21:51:10.917436  8944 net.cpp:66] Creating Layer conv1
I0909 21:51:10.917443  8944 net.cpp:329] conv1 <- data
I0909 21:51:10.917451  8944 net.cpp:290] conv1 -> conv1
I0909 21:51:10.918855  8944 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:51:10.918875  8944 net.cpp:125] conv1 needs backward computation.
I0909 21:51:10.918884  8944 net.cpp:66] Creating Layer relu1
I0909 21:51:10.918890  8944 net.cpp:329] relu1 <- conv1
I0909 21:51:10.918897  8944 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:51:10.918905  8944 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:51:10.918911  8944 net.cpp:125] relu1 needs backward computation.
I0909 21:51:10.918918  8944 net.cpp:66] Creating Layer pool1
I0909 21:51:10.918923  8944 net.cpp:329] pool1 <- conv1
I0909 21:51:10.918931  8944 net.cpp:290] pool1 -> pool1
I0909 21:51:10.918941  8944 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:51:10.918947  8944 net.cpp:125] pool1 needs backward computation.
I0909 21:51:10.918953  8944 net.cpp:66] Creating Layer norm1
I0909 21:51:10.918959  8944 net.cpp:329] norm1 <- pool1
I0909 21:51:10.918966  8944 net.cpp:290] norm1 -> norm1
I0909 21:51:10.918975  8944 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:51:10.918982  8944 net.cpp:125] norm1 needs backward computation.
I0909 21:51:10.918988  8944 net.cpp:66] Creating Layer conv2
I0909 21:51:10.918994  8944 net.cpp:329] conv2 <- norm1
I0909 21:51:10.919001  8944 net.cpp:290] conv2 -> conv2
I0909 21:51:10.928177  8944 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:51:10.928200  8944 net.cpp:125] conv2 needs backward computation.
I0909 21:51:10.928216  8944 net.cpp:66] Creating Layer relu2
I0909 21:51:10.928222  8944 net.cpp:329] relu2 <- conv2
I0909 21:51:10.928230  8944 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:51:10.928238  8944 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:51:10.928243  8944 net.cpp:125] relu2 needs backward computation.
I0909 21:51:10.928251  8944 net.cpp:66] Creating Layer pool2
I0909 21:51:10.928256  8944 net.cpp:329] pool2 <- conv2
I0909 21:51:10.928262  8944 net.cpp:290] pool2 -> pool2
I0909 21:51:10.928271  8944 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:51:10.928277  8944 net.cpp:125] pool2 needs backward computation.
I0909 21:51:10.928287  8944 net.cpp:66] Creating Layer fc7
I0909 21:51:10.928292  8944 net.cpp:329] fc7 <- pool2
I0909 21:51:10.928300  8944 net.cpp:290] fc7 -> fc7
I0909 21:51:11.573716  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.573760  8944 net.cpp:125] fc7 needs backward computation.
I0909 21:51:11.573776  8944 net.cpp:66] Creating Layer relu7
I0909 21:51:11.573783  8944 net.cpp:329] relu7 <- fc7
I0909 21:51:11.573792  8944 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:51:11.573802  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.573808  8944 net.cpp:125] relu7 needs backward computation.
I0909 21:51:11.573815  8944 net.cpp:66] Creating Layer drop7
I0909 21:51:11.573822  8944 net.cpp:329] drop7 <- fc7
I0909 21:51:11.573829  8944 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:51:11.573842  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.573848  8944 net.cpp:125] drop7 needs backward computation.
I0909 21:51:11.573856  8944 net.cpp:66] Creating Layer fc8
I0909 21:51:11.573863  8944 net.cpp:329] fc8 <- fc7
I0909 21:51:11.573869  8944 net.cpp:290] fc8 -> fc8
I0909 21:51:11.581971  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.581997  8944 net.cpp:125] fc8 needs backward computation.
I0909 21:51:11.582006  8944 net.cpp:66] Creating Layer relu8
I0909 21:51:11.582012  8944 net.cpp:329] relu8 <- fc8
I0909 21:51:11.582020  8944 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:51:11.582029  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.582036  8944 net.cpp:125] relu8 needs backward computation.
I0909 21:51:11.582042  8944 net.cpp:66] Creating Layer drop8
I0909 21:51:11.582047  8944 net.cpp:329] drop8 <- fc8
I0909 21:51:11.582056  8944 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:51:11.582063  8944 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:11.582069  8944 net.cpp:125] drop8 needs backward computation.
I0909 21:51:11.582077  8944 net.cpp:66] Creating Layer fc9
I0909 21:51:11.582083  8944 net.cpp:329] fc9 <- fc8
I0909 21:51:11.582094  8944 net.cpp:290] fc9 -> fc9
I0909 21:51:11.582470  8944 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:51:11.582481  8944 net.cpp:125] fc9 needs backward computation.
I0909 21:51:11.582489  8944 net.cpp:66] Creating Layer fc10
I0909 21:51:11.582495  8944 net.cpp:329] fc10 <- fc9
I0909 21:51:11.582504  8944 net.cpp:290] fc10 -> fc10
I0909 21:51:11.582516  8944 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:51:11.582525  8944 net.cpp:125] fc10 needs backward computation.
I0909 21:51:11.582532  8944 net.cpp:66] Creating Layer prob
I0909 21:51:11.582538  8944 net.cpp:329] prob <- fc10
I0909 21:51:11.582545  8944 net.cpp:290] prob -> prob
I0909 21:51:11.582554  8944 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:51:11.582561  8944 net.cpp:125] prob needs backward computation.
I0909 21:51:11.582566  8944 net.cpp:156] This network produces output prob
I0909 21:51:11.582579  8944 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:51:11.582588  8944 net.cpp:167] Network initialization done.
I0909 21:51:11.582594  8944 net.cpp:168] Memory required for data: 6183480
Classifying 28 inputs.
Done in 17.35 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:51:32.345815  8948 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:51:32.345957  8948 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:51:32.345978  8948 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:51:32.346125  8948 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:51:32.346181  8948 net.cpp:292] Input 0 -> data
I0909 21:51:32.346207  8948 net.cpp:66] Creating Layer conv1
I0909 21:51:32.346215  8948 net.cpp:329] conv1 <- data
I0909 21:51:32.346222  8948 net.cpp:290] conv1 -> conv1
I0909 21:51:32.347585  8948 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:51:32.347602  8948 net.cpp:125] conv1 needs backward computation.
I0909 21:51:32.347611  8948 net.cpp:66] Creating Layer relu1
I0909 21:51:32.347617  8948 net.cpp:329] relu1 <- conv1
I0909 21:51:32.347625  8948 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:51:32.347633  8948 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:51:32.347645  8948 net.cpp:125] relu1 needs backward computation.
I0909 21:51:32.347651  8948 net.cpp:66] Creating Layer pool1
I0909 21:51:32.347656  8948 net.cpp:329] pool1 <- conv1
I0909 21:51:32.347663  8948 net.cpp:290] pool1 -> pool1
I0909 21:51:32.347674  8948 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:51:32.347681  8948 net.cpp:125] pool1 needs backward computation.
I0909 21:51:32.347687  8948 net.cpp:66] Creating Layer norm1
I0909 21:51:32.347692  8948 net.cpp:329] norm1 <- pool1
I0909 21:51:32.347699  8948 net.cpp:290] norm1 -> norm1
I0909 21:51:32.347709  8948 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:51:32.347714  8948 net.cpp:125] norm1 needs backward computation.
I0909 21:51:32.347723  8948 net.cpp:66] Creating Layer conv2
I0909 21:51:32.347728  8948 net.cpp:329] conv2 <- norm1
I0909 21:51:32.347735  8948 net.cpp:290] conv2 -> conv2
I0909 21:51:32.356871  8948 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:51:32.356902  8948 net.cpp:125] conv2 needs backward computation.
I0909 21:51:32.356912  8948 net.cpp:66] Creating Layer relu2
I0909 21:51:32.356919  8948 net.cpp:329] relu2 <- conv2
I0909 21:51:32.356926  8948 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:51:32.356935  8948 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:51:32.356940  8948 net.cpp:125] relu2 needs backward computation.
I0909 21:51:32.356946  8948 net.cpp:66] Creating Layer pool2
I0909 21:51:32.356952  8948 net.cpp:329] pool2 <- conv2
I0909 21:51:32.356959  8948 net.cpp:290] pool2 -> pool2
I0909 21:51:32.356968  8948 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:51:32.356973  8948 net.cpp:125] pool2 needs backward computation.
I0909 21:51:32.356981  8948 net.cpp:66] Creating Layer fc7
I0909 21:51:32.356987  8948 net.cpp:329] fc7 <- pool2
I0909 21:51:32.356999  8948 net.cpp:290] fc7 -> fc7
I0909 21:51:32.999524  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:32.999570  8948 net.cpp:125] fc7 needs backward computation.
I0909 21:51:32.999585  8948 net.cpp:66] Creating Layer relu7
I0909 21:51:32.999593  8948 net.cpp:329] relu7 <- fc7
I0909 21:51:32.999600  8948 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:51:32.999610  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:32.999616  8948 net.cpp:125] relu7 needs backward computation.
I0909 21:51:32.999624  8948 net.cpp:66] Creating Layer drop7
I0909 21:51:32.999629  8948 net.cpp:329] drop7 <- fc7
I0909 21:51:32.999637  8948 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:51:32.999647  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:32.999654  8948 net.cpp:125] drop7 needs backward computation.
I0909 21:51:32.999662  8948 net.cpp:66] Creating Layer fc8
I0909 21:51:32.999667  8948 net.cpp:329] fc8 <- fc7
I0909 21:51:32.999675  8948 net.cpp:290] fc8 -> fc8
I0909 21:51:33.007505  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:33.007519  8948 net.cpp:125] fc8 needs backward computation.
I0909 21:51:33.007526  8948 net.cpp:66] Creating Layer relu8
I0909 21:51:33.007532  8948 net.cpp:329] relu8 <- fc8
I0909 21:51:33.007539  8948 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:51:33.007545  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:33.007551  8948 net.cpp:125] relu8 needs backward computation.
I0909 21:51:33.007557  8948 net.cpp:66] Creating Layer drop8
I0909 21:51:33.007562  8948 net.cpp:329] drop8 <- fc8
I0909 21:51:33.007570  8948 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:51:33.007577  8948 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:51:33.007583  8948 net.cpp:125] drop8 needs backward computation.
I0909 21:51:33.007591  8948 net.cpp:66] Creating Layer fc9
I0909 21:51:33.007596  8948 net.cpp:329] fc9 <- fc8
I0909 21:51:33.007604  8948 net.cpp:290] fc9 -> fc9
I0909 21:51:33.007978  8948 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:51:33.007988  8948 net.cpp:125] fc9 needs backward computation.
I0909 21:51:33.007997  8948 net.cpp:66] Creating Layer fc10
I0909 21:51:33.008002  8948 net.cpp:329] fc10 <- fc9
I0909 21:51:33.008010  8948 net.cpp:290] fc10 -> fc10
I0909 21:51:33.008023  8948 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:51:33.008043  8948 net.cpp:125] fc10 needs backward computation.
I0909 21:51:33.008049  8948 net.cpp:66] Creating Layer prob
I0909 21:51:33.008055  8948 net.cpp:329] prob <- fc10
I0909 21:51:33.008061  8948 net.cpp:290] prob -> prob
I0909 21:51:33.008071  8948 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:51:33.008076  8948 net.cpp:125] prob needs backward computation.
I0909 21:51:33.008082  8948 net.cpp:156] This network produces output prob
I0909 21:51:33.008095  8948 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:51:33.008103  8948 net.cpp:167] Network initialization done.
I0909 21:51:33.008108  8948 net.cpp:168] Memory required for data: 6183480
Classifying 91 inputs.
Done in 63.13 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:52:39.059384  8952 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:52:39.059525  8952 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:52:39.059535  8952 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:52:39.059681  8952 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:52:39.059747  8952 net.cpp:292] Input 0 -> data
I0909 21:52:39.059774  8952 net.cpp:66] Creating Layer conv1
I0909 21:52:39.059782  8952 net.cpp:329] conv1 <- data
I0909 21:52:39.059789  8952 net.cpp:290] conv1 -> conv1
I0909 21:52:39.061149  8952 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:52:39.061168  8952 net.cpp:125] conv1 needs backward computation.
I0909 21:52:39.061177  8952 net.cpp:66] Creating Layer relu1
I0909 21:52:39.061183  8952 net.cpp:329] relu1 <- conv1
I0909 21:52:39.061189  8952 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:52:39.061198  8952 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:52:39.061204  8952 net.cpp:125] relu1 needs backward computation.
I0909 21:52:39.061211  8952 net.cpp:66] Creating Layer pool1
I0909 21:52:39.061216  8952 net.cpp:329] pool1 <- conv1
I0909 21:52:39.061223  8952 net.cpp:290] pool1 -> pool1
I0909 21:52:39.061234  8952 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:52:39.061240  8952 net.cpp:125] pool1 needs backward computation.
I0909 21:52:39.061247  8952 net.cpp:66] Creating Layer norm1
I0909 21:52:39.061252  8952 net.cpp:329] norm1 <- pool1
I0909 21:52:39.061259  8952 net.cpp:290] norm1 -> norm1
I0909 21:52:39.061269  8952 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:52:39.061275  8952 net.cpp:125] norm1 needs backward computation.
I0909 21:52:39.061281  8952 net.cpp:66] Creating Layer conv2
I0909 21:52:39.061287  8952 net.cpp:329] conv2 <- norm1
I0909 21:52:39.061295  8952 net.cpp:290] conv2 -> conv2
I0909 21:52:39.070457  8952 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:52:39.070472  8952 net.cpp:125] conv2 needs backward computation.
I0909 21:52:39.070479  8952 net.cpp:66] Creating Layer relu2
I0909 21:52:39.070485  8952 net.cpp:329] relu2 <- conv2
I0909 21:52:39.070492  8952 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:52:39.070499  8952 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:52:39.070504  8952 net.cpp:125] relu2 needs backward computation.
I0909 21:52:39.070511  8952 net.cpp:66] Creating Layer pool2
I0909 21:52:39.070516  8952 net.cpp:329] pool2 <- conv2
I0909 21:52:39.070523  8952 net.cpp:290] pool2 -> pool2
I0909 21:52:39.070531  8952 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:52:39.070538  8952 net.cpp:125] pool2 needs backward computation.
I0909 21:52:39.070545  8952 net.cpp:66] Creating Layer fc7
I0909 21:52:39.070551  8952 net.cpp:329] fc7 <- pool2
I0909 21:52:39.070559  8952 net.cpp:290] fc7 -> fc7
I0909 21:52:39.715114  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.715160  8952 net.cpp:125] fc7 needs backward computation.
I0909 21:52:39.715174  8952 net.cpp:66] Creating Layer relu7
I0909 21:52:39.715183  8952 net.cpp:329] relu7 <- fc7
I0909 21:52:39.715190  8952 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:52:39.715200  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.715206  8952 net.cpp:125] relu7 needs backward computation.
I0909 21:52:39.715214  8952 net.cpp:66] Creating Layer drop7
I0909 21:52:39.715219  8952 net.cpp:329] drop7 <- fc7
I0909 21:52:39.715226  8952 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:52:39.715237  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.715244  8952 net.cpp:125] drop7 needs backward computation.
I0909 21:52:39.715251  8952 net.cpp:66] Creating Layer fc8
I0909 21:52:39.715257  8952 net.cpp:329] fc8 <- fc7
I0909 21:52:39.715265  8952 net.cpp:290] fc8 -> fc8
I0909 21:52:39.723049  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.723062  8952 net.cpp:125] fc8 needs backward computation.
I0909 21:52:39.723069  8952 net.cpp:66] Creating Layer relu8
I0909 21:52:39.723075  8952 net.cpp:329] relu8 <- fc8
I0909 21:52:39.723091  8952 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:52:39.723098  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.723104  8952 net.cpp:125] relu8 needs backward computation.
I0909 21:52:39.723111  8952 net.cpp:66] Creating Layer drop8
I0909 21:52:39.723116  8952 net.cpp:329] drop8 <- fc8
I0909 21:52:39.723124  8952 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:52:39.723131  8952 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:52:39.723136  8952 net.cpp:125] drop8 needs backward computation.
I0909 21:52:39.723145  8952 net.cpp:66] Creating Layer fc9
I0909 21:52:39.723150  8952 net.cpp:329] fc9 <- fc8
I0909 21:52:39.723157  8952 net.cpp:290] fc9 -> fc9
I0909 21:52:39.723533  8952 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:52:39.723543  8952 net.cpp:125] fc9 needs backward computation.
I0909 21:52:39.723552  8952 net.cpp:66] Creating Layer fc10
I0909 21:52:39.723557  8952 net.cpp:329] fc10 <- fc9
I0909 21:52:39.723567  8952 net.cpp:290] fc10 -> fc10
I0909 21:52:39.723578  8952 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:52:39.723587  8952 net.cpp:125] fc10 needs backward computation.
I0909 21:52:39.723594  8952 net.cpp:66] Creating Layer prob
I0909 21:52:39.723600  8952 net.cpp:329] prob <- fc10
I0909 21:52:39.723606  8952 net.cpp:290] prob -> prob
I0909 21:52:39.723615  8952 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:52:39.723621  8952 net.cpp:125] prob needs backward computation.
I0909 21:52:39.723626  8952 net.cpp:156] This network produces output prob
I0909 21:52:39.723639  8952 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:52:39.723646  8952 net.cpp:167] Network initialization done.
I0909 21:52:39.723651  8952 net.cpp:168] Memory required for data: 6183480
Classifying 155 inputs.
Done in 97.33 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:54:20.673688  8957 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:54:20.673828  8957 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:54:20.673837  8957 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:54:20.673985  8957 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:54:20.674051  8957 net.cpp:292] Input 0 -> data
I0909 21:54:20.674077  8957 net.cpp:66] Creating Layer conv1
I0909 21:54:20.674083  8957 net.cpp:329] conv1 <- data
I0909 21:54:20.674092  8957 net.cpp:290] conv1 -> conv1
I0909 21:54:20.675640  8957 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:54:20.675659  8957 net.cpp:125] conv1 needs backward computation.
I0909 21:54:20.675668  8957 net.cpp:66] Creating Layer relu1
I0909 21:54:20.675674  8957 net.cpp:329] relu1 <- conv1
I0909 21:54:20.675680  8957 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:54:20.675689  8957 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:54:20.675695  8957 net.cpp:125] relu1 needs backward computation.
I0909 21:54:20.675703  8957 net.cpp:66] Creating Layer pool1
I0909 21:54:20.675707  8957 net.cpp:329] pool1 <- conv1
I0909 21:54:20.675714  8957 net.cpp:290] pool1 -> pool1
I0909 21:54:20.675725  8957 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:54:20.675731  8957 net.cpp:125] pool1 needs backward computation.
I0909 21:54:20.675739  8957 net.cpp:66] Creating Layer norm1
I0909 21:54:20.675743  8957 net.cpp:329] norm1 <- pool1
I0909 21:54:20.675750  8957 net.cpp:290] norm1 -> norm1
I0909 21:54:20.675760  8957 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:54:20.675766  8957 net.cpp:125] norm1 needs backward computation.
I0909 21:54:20.675773  8957 net.cpp:66] Creating Layer conv2
I0909 21:54:20.675779  8957 net.cpp:329] conv2 <- norm1
I0909 21:54:20.675786  8957 net.cpp:290] conv2 -> conv2
I0909 21:54:20.686408  8957 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:54:20.686425  8957 net.cpp:125] conv2 needs backward computation.
I0909 21:54:20.686432  8957 net.cpp:66] Creating Layer relu2
I0909 21:54:20.686439  8957 net.cpp:329] relu2 <- conv2
I0909 21:54:20.686445  8957 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:54:20.686452  8957 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:54:20.686457  8957 net.cpp:125] relu2 needs backward computation.
I0909 21:54:20.686463  8957 net.cpp:66] Creating Layer pool2
I0909 21:54:20.686470  8957 net.cpp:329] pool2 <- conv2
I0909 21:54:20.686475  8957 net.cpp:290] pool2 -> pool2
I0909 21:54:20.686483  8957 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:54:20.686489  8957 net.cpp:125] pool2 needs backward computation.
I0909 21:54:20.686496  8957 net.cpp:66] Creating Layer fc7
I0909 21:54:20.686501  8957 net.cpp:329] fc7 <- pool2
I0909 21:54:20.686511  8957 net.cpp:290] fc7 -> fc7
I0909 21:54:21.327116  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.327158  8957 net.cpp:125] fc7 needs backward computation.
I0909 21:54:21.327183  8957 net.cpp:66] Creating Layer relu7
I0909 21:54:21.327191  8957 net.cpp:329] relu7 <- fc7
I0909 21:54:21.327199  8957 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:54:21.327208  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.327214  8957 net.cpp:125] relu7 needs backward computation.
I0909 21:54:21.327221  8957 net.cpp:66] Creating Layer drop7
I0909 21:54:21.327227  8957 net.cpp:329] drop7 <- fc7
I0909 21:54:21.327234  8957 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:54:21.327245  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.327251  8957 net.cpp:125] drop7 needs backward computation.
I0909 21:54:21.327260  8957 net.cpp:66] Creating Layer fc8
I0909 21:54:21.327265  8957 net.cpp:329] fc8 <- fc7
I0909 21:54:21.327272  8957 net.cpp:290] fc8 -> fc8
I0909 21:54:21.335104  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.335119  8957 net.cpp:125] fc8 needs backward computation.
I0909 21:54:21.335125  8957 net.cpp:66] Creating Layer relu8
I0909 21:54:21.335131  8957 net.cpp:329] relu8 <- fc8
I0909 21:54:21.335139  8957 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:54:21.335145  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.335151  8957 net.cpp:125] relu8 needs backward computation.
I0909 21:54:21.335158  8957 net.cpp:66] Creating Layer drop8
I0909 21:54:21.335163  8957 net.cpp:329] drop8 <- fc8
I0909 21:54:21.335171  8957 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:54:21.335178  8957 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:54:21.335185  8957 net.cpp:125] drop8 needs backward computation.
I0909 21:54:21.335191  8957 net.cpp:66] Creating Layer fc9
I0909 21:54:21.335197  8957 net.cpp:329] fc9 <- fc8
I0909 21:54:21.335206  8957 net.cpp:290] fc9 -> fc9
I0909 21:54:21.335579  8957 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:54:21.335590  8957 net.cpp:125] fc9 needs backward computation.
I0909 21:54:21.335598  8957 net.cpp:66] Creating Layer fc10
I0909 21:54:21.335603  8957 net.cpp:329] fc10 <- fc9
I0909 21:54:21.335613  8957 net.cpp:290] fc10 -> fc10
I0909 21:54:21.335623  8957 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:54:21.335633  8957 net.cpp:125] fc10 needs backward computation.
I0909 21:54:21.335639  8957 net.cpp:66] Creating Layer prob
I0909 21:54:21.335645  8957 net.cpp:329] prob <- fc10
I0909 21:54:21.335651  8957 net.cpp:290] prob -> prob
I0909 21:54:21.335661  8957 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:54:21.335667  8957 net.cpp:125] prob needs backward computation.
I0909 21:54:21.335672  8957 net.cpp:156] This network produces output prob
I0909 21:54:21.335685  8957 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:54:21.335693  8957 net.cpp:167] Network initialization done.
I0909 21:54:21.335698  8957 net.cpp:168] Memory required for data: 6183480
Classifying 182 inputs.
Done in 112.55 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:56:18.119652  8965 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:56:18.119788  8965 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:56:18.119797  8965 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:56:18.119941  8965 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:56:18.120005  8965 net.cpp:292] Input 0 -> data
I0909 21:56:18.120031  8965 net.cpp:66] Creating Layer conv1
I0909 21:56:18.120038  8965 net.cpp:329] conv1 <- data
I0909 21:56:18.120046  8965 net.cpp:290] conv1 -> conv1
I0909 21:56:18.121368  8965 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:56:18.121386  8965 net.cpp:125] conv1 needs backward computation.
I0909 21:56:18.121394  8965 net.cpp:66] Creating Layer relu1
I0909 21:56:18.121400  8965 net.cpp:329] relu1 <- conv1
I0909 21:56:18.121407  8965 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:56:18.121415  8965 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:56:18.121422  8965 net.cpp:125] relu1 needs backward computation.
I0909 21:56:18.121428  8965 net.cpp:66] Creating Layer pool1
I0909 21:56:18.121433  8965 net.cpp:329] pool1 <- conv1
I0909 21:56:18.121439  8965 net.cpp:290] pool1 -> pool1
I0909 21:56:18.121450  8965 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:56:18.121456  8965 net.cpp:125] pool1 needs backward computation.
I0909 21:56:18.121462  8965 net.cpp:66] Creating Layer norm1
I0909 21:56:18.121469  8965 net.cpp:329] norm1 <- pool1
I0909 21:56:18.121474  8965 net.cpp:290] norm1 -> norm1
I0909 21:56:18.121484  8965 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:56:18.121490  8965 net.cpp:125] norm1 needs backward computation.
I0909 21:56:18.121496  8965 net.cpp:66] Creating Layer conv2
I0909 21:56:18.121502  8965 net.cpp:329] conv2 <- norm1
I0909 21:56:18.121510  8965 net.cpp:290] conv2 -> conv2
I0909 21:56:18.130421  8965 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:56:18.130442  8965 net.cpp:125] conv2 needs backward computation.
I0909 21:56:18.130450  8965 net.cpp:66] Creating Layer relu2
I0909 21:56:18.130455  8965 net.cpp:329] relu2 <- conv2
I0909 21:56:18.130462  8965 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:56:18.130470  8965 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:56:18.130475  8965 net.cpp:125] relu2 needs backward computation.
I0909 21:56:18.130481  8965 net.cpp:66] Creating Layer pool2
I0909 21:56:18.130486  8965 net.cpp:329] pool2 <- conv2
I0909 21:56:18.130492  8965 net.cpp:290] pool2 -> pool2
I0909 21:56:18.130501  8965 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:56:18.130506  8965 net.cpp:125] pool2 needs backward computation.
I0909 21:56:18.130512  8965 net.cpp:66] Creating Layer fc7
I0909 21:56:18.130517  8965 net.cpp:329] fc7 <- pool2
I0909 21:56:18.130527  8965 net.cpp:290] fc7 -> fc7
I0909 21:56:18.769603  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.769644  8965 net.cpp:125] fc7 needs backward computation.
I0909 21:56:18.769657  8965 net.cpp:66] Creating Layer relu7
I0909 21:56:18.769665  8965 net.cpp:329] relu7 <- fc7
I0909 21:56:18.769671  8965 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:56:18.769681  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.769687  8965 net.cpp:125] relu7 needs backward computation.
I0909 21:56:18.769695  8965 net.cpp:66] Creating Layer drop7
I0909 21:56:18.769700  8965 net.cpp:329] drop7 <- fc7
I0909 21:56:18.769706  8965 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:56:18.769717  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.769723  8965 net.cpp:125] drop7 needs backward computation.
I0909 21:56:18.769731  8965 net.cpp:66] Creating Layer fc8
I0909 21:56:18.769737  8965 net.cpp:329] fc8 <- fc7
I0909 21:56:18.769743  8965 net.cpp:290] fc8 -> fc8
I0909 21:56:18.777412  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.777436  8965 net.cpp:125] fc8 needs backward computation.
I0909 21:56:18.777443  8965 net.cpp:66] Creating Layer relu8
I0909 21:56:18.777448  8965 net.cpp:329] relu8 <- fc8
I0909 21:56:18.777456  8965 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:56:18.777462  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.777468  8965 net.cpp:125] relu8 needs backward computation.
I0909 21:56:18.777474  8965 net.cpp:66] Creating Layer drop8
I0909 21:56:18.777479  8965 net.cpp:329] drop8 <- fc8
I0909 21:56:18.777487  8965 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:56:18.777501  8965 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:56:18.777508  8965 net.cpp:125] drop8 needs backward computation.
I0909 21:56:18.777523  8965 net.cpp:66] Creating Layer fc9
I0909 21:56:18.777530  8965 net.cpp:329] fc9 <- fc8
I0909 21:56:18.777546  8965 net.cpp:290] fc9 -> fc9
I0909 21:56:18.777928  8965 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:56:18.777940  8965 net.cpp:125] fc9 needs backward computation.
I0909 21:56:18.777948  8965 net.cpp:66] Creating Layer fc10
I0909 21:56:18.777953  8965 net.cpp:329] fc10 <- fc9
I0909 21:56:18.777962  8965 net.cpp:290] fc10 -> fc10
I0909 21:56:18.777973  8965 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:56:18.777983  8965 net.cpp:125] fc10 needs backward computation.
I0909 21:56:18.777990  8965 net.cpp:66] Creating Layer prob
I0909 21:56:18.777995  8965 net.cpp:329] prob <- fc10
I0909 21:56:18.778002  8965 net.cpp:290] prob -> prob
I0909 21:56:18.778012  8965 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:56:18.778018  8965 net.cpp:125] prob needs backward computation.
I0909 21:56:18.778023  8965 net.cpp:156] This network produces output prob
I0909 21:56:18.778035  8965 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:56:18.778044  8965 net.cpp:167] Network initialization done.
I0909 21:56:18.778049  8965 net.cpp:168] Memory required for data: 6183480
Classifying 103 inputs.
Done in 65.60 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 21:57:26.901353  8969 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 21:57:26.901506  8969 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 21:57:26.901537  8969 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 21:57:26.901690  8969 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 21:57:26.901743  8969 net.cpp:292] Input 0 -> data
I0909 21:57:26.901769  8969 net.cpp:66] Creating Layer conv1
I0909 21:57:26.901777  8969 net.cpp:329] conv1 <- data
I0909 21:57:26.901784  8969 net.cpp:290] conv1 -> conv1
I0909 21:57:26.903146  8969 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:57:26.903164  8969 net.cpp:125] conv1 needs backward computation.
I0909 21:57:26.903173  8969 net.cpp:66] Creating Layer relu1
I0909 21:57:26.903179  8969 net.cpp:329] relu1 <- conv1
I0909 21:57:26.903187  8969 net.cpp:280] relu1 -> conv1 (in-place)
I0909 21:57:26.903203  8969 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 21:57:26.903209  8969 net.cpp:125] relu1 needs backward computation.
I0909 21:57:26.903216  8969 net.cpp:66] Creating Layer pool1
I0909 21:57:26.903221  8969 net.cpp:329] pool1 <- conv1
I0909 21:57:26.903228  8969 net.cpp:290] pool1 -> pool1
I0909 21:57:26.903239  8969 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:57:26.903245  8969 net.cpp:125] pool1 needs backward computation.
I0909 21:57:26.903252  8969 net.cpp:66] Creating Layer norm1
I0909 21:57:26.903257  8969 net.cpp:329] norm1 <- pool1
I0909 21:57:26.903264  8969 net.cpp:290] norm1 -> norm1
I0909 21:57:26.903275  8969 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 21:57:26.903280  8969 net.cpp:125] norm1 needs backward computation.
I0909 21:57:26.903287  8969 net.cpp:66] Creating Layer conv2
I0909 21:57:26.903293  8969 net.cpp:329] conv2 <- norm1
I0909 21:57:26.903301  8969 net.cpp:290] conv2 -> conv2
I0909 21:57:26.912415  8969 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:57:26.912430  8969 net.cpp:125] conv2 needs backward computation.
I0909 21:57:26.912436  8969 net.cpp:66] Creating Layer relu2
I0909 21:57:26.912442  8969 net.cpp:329] relu2 <- conv2
I0909 21:57:26.912449  8969 net.cpp:280] relu2 -> conv2 (in-place)
I0909 21:57:26.912456  8969 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 21:57:26.912461  8969 net.cpp:125] relu2 needs backward computation.
I0909 21:57:26.912467  8969 net.cpp:66] Creating Layer pool2
I0909 21:57:26.912473  8969 net.cpp:329] pool2 <- conv2
I0909 21:57:26.912479  8969 net.cpp:290] pool2 -> pool2
I0909 21:57:26.912487  8969 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 21:57:26.912493  8969 net.cpp:125] pool2 needs backward computation.
I0909 21:57:26.912503  8969 net.cpp:66] Creating Layer fc7
I0909 21:57:26.912508  8969 net.cpp:329] fc7 <- pool2
I0909 21:57:26.912515  8969 net.cpp:290] fc7 -> fc7
I0909 21:57:27.556325  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.556370  8969 net.cpp:125] fc7 needs backward computation.
I0909 21:57:27.556385  8969 net.cpp:66] Creating Layer relu7
I0909 21:57:27.556392  8969 net.cpp:329] relu7 <- fc7
I0909 21:57:27.556401  8969 net.cpp:280] relu7 -> fc7 (in-place)
I0909 21:57:27.556411  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.556416  8969 net.cpp:125] relu7 needs backward computation.
I0909 21:57:27.556423  8969 net.cpp:66] Creating Layer drop7
I0909 21:57:27.556429  8969 net.cpp:329] drop7 <- fc7
I0909 21:57:27.556437  8969 net.cpp:280] drop7 -> fc7 (in-place)
I0909 21:57:27.556448  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.556454  8969 net.cpp:125] drop7 needs backward computation.
I0909 21:57:27.556463  8969 net.cpp:66] Creating Layer fc8
I0909 21:57:27.556468  8969 net.cpp:329] fc8 <- fc7
I0909 21:57:27.556475  8969 net.cpp:290] fc8 -> fc8
I0909 21:57:27.564282  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.564296  8969 net.cpp:125] fc8 needs backward computation.
I0909 21:57:27.564302  8969 net.cpp:66] Creating Layer relu8
I0909 21:57:27.564307  8969 net.cpp:329] relu8 <- fc8
I0909 21:57:27.564314  8969 net.cpp:280] relu8 -> fc8 (in-place)
I0909 21:57:27.564321  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.564327  8969 net.cpp:125] relu8 needs backward computation.
I0909 21:57:27.564333  8969 net.cpp:66] Creating Layer drop8
I0909 21:57:27.564339  8969 net.cpp:329] drop8 <- fc8
I0909 21:57:27.564347  8969 net.cpp:280] drop8 -> fc8 (in-place)
I0909 21:57:27.564353  8969 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 21:57:27.564359  8969 net.cpp:125] drop8 needs backward computation.
I0909 21:57:27.564368  8969 net.cpp:66] Creating Layer fc9
I0909 21:57:27.564373  8969 net.cpp:329] fc9 <- fc8
I0909 21:57:27.564380  8969 net.cpp:290] fc9 -> fc9
I0909 21:57:27.564757  8969 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 21:57:27.564767  8969 net.cpp:125] fc9 needs backward computation.
I0909 21:57:27.564776  8969 net.cpp:66] Creating Layer fc10
I0909 21:57:27.564781  8969 net.cpp:329] fc10 <- fc9
I0909 21:57:27.564800  8969 net.cpp:290] fc10 -> fc10
I0909 21:57:27.564813  8969 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:57:27.564822  8969 net.cpp:125] fc10 needs backward computation.
I0909 21:57:27.564829  8969 net.cpp:66] Creating Layer prob
I0909 21:57:27.564834  8969 net.cpp:329] prob <- fc10
I0909 21:57:27.564841  8969 net.cpp:290] prob -> prob
I0909 21:57:27.564851  8969 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 21:57:27.564857  8969 net.cpp:125] prob needs backward computation.
I0909 21:57:27.564862  8969 net.cpp:156] This network produces output prob
I0909 21:57:27.564873  8969 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 21:57:27.564882  8969 net.cpp:167] Network initialization done.
I0909 21:57:27.564887  8969 net.cpp:168] Memory required for data: 6183480
Classifying 517 inputs.
Done in 531.65 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:07:15.866256  8989 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:07:15.866401  8989 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:07:15.866411  8989 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:07:15.866621  8989 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:07:15.866684  8989 net.cpp:292] Input 0 -> data
I0909 22:07:15.866711  8989 net.cpp:66] Creating Layer conv1
I0909 22:07:15.866719  8989 net.cpp:329] conv1 <- data
I0909 22:07:15.866727  8989 net.cpp:290] conv1 -> conv1
I0909 22:07:15.916503  8989 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:07:15.916544  8989 net.cpp:125] conv1 needs backward computation.
I0909 22:07:15.916558  8989 net.cpp:66] Creating Layer relu1
I0909 22:07:15.916564  8989 net.cpp:329] relu1 <- conv1
I0909 22:07:15.916571  8989 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:07:15.916589  8989 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:07:15.916596  8989 net.cpp:125] relu1 needs backward computation.
I0909 22:07:15.916604  8989 net.cpp:66] Creating Layer pool1
I0909 22:07:15.916610  8989 net.cpp:329] pool1 <- conv1
I0909 22:07:15.916617  8989 net.cpp:290] pool1 -> pool1
I0909 22:07:15.916637  8989 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:07:15.916643  8989 net.cpp:125] pool1 needs backward computation.
I0909 22:07:15.916651  8989 net.cpp:66] Creating Layer norm1
I0909 22:07:15.916656  8989 net.cpp:329] norm1 <- pool1
I0909 22:07:15.916663  8989 net.cpp:290] norm1 -> norm1
I0909 22:07:15.916673  8989 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:07:15.916679  8989 net.cpp:125] norm1 needs backward computation.
I0909 22:07:15.916687  8989 net.cpp:66] Creating Layer conv2
I0909 22:07:15.916693  8989 net.cpp:329] conv2 <- norm1
I0909 22:07:15.916702  8989 net.cpp:290] conv2 -> conv2
I0909 22:07:15.925832  8989 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:07:15.925848  8989 net.cpp:125] conv2 needs backward computation.
I0909 22:07:15.925855  8989 net.cpp:66] Creating Layer relu2
I0909 22:07:15.925861  8989 net.cpp:329] relu2 <- conv2
I0909 22:07:15.925868  8989 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:07:15.925875  8989 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:07:15.925881  8989 net.cpp:125] relu2 needs backward computation.
I0909 22:07:15.925887  8989 net.cpp:66] Creating Layer pool2
I0909 22:07:15.925894  8989 net.cpp:329] pool2 <- conv2
I0909 22:07:15.925900  8989 net.cpp:290] pool2 -> pool2
I0909 22:07:15.925907  8989 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:07:15.925914  8989 net.cpp:125] pool2 needs backward computation.
I0909 22:07:15.925921  8989 net.cpp:66] Creating Layer fc7
I0909 22:07:15.925927  8989 net.cpp:329] fc7 <- pool2
I0909 22:07:15.925936  8989 net.cpp:290] fc7 -> fc7
I0909 22:07:16.567801  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.567843  8989 net.cpp:125] fc7 needs backward computation.
I0909 22:07:16.567857  8989 net.cpp:66] Creating Layer relu7
I0909 22:07:16.567864  8989 net.cpp:329] relu7 <- fc7
I0909 22:07:16.567873  8989 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:07:16.567883  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.567888  8989 net.cpp:125] relu7 needs backward computation.
I0909 22:07:16.567895  8989 net.cpp:66] Creating Layer drop7
I0909 22:07:16.567900  8989 net.cpp:329] drop7 <- fc7
I0909 22:07:16.567909  8989 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:07:16.567919  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.567924  8989 net.cpp:125] drop7 needs backward computation.
I0909 22:07:16.567934  8989 net.cpp:66] Creating Layer fc8
I0909 22:07:16.567939  8989 net.cpp:329] fc8 <- fc7
I0909 22:07:16.567946  8989 net.cpp:290] fc8 -> fc8
I0909 22:07:16.575749  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.575762  8989 net.cpp:125] fc8 needs backward computation.
I0909 22:07:16.575780  8989 net.cpp:66] Creating Layer relu8
I0909 22:07:16.575786  8989 net.cpp:329] relu8 <- fc8
I0909 22:07:16.575793  8989 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:07:16.575800  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.575806  8989 net.cpp:125] relu8 needs backward computation.
I0909 22:07:16.575813  8989 net.cpp:66] Creating Layer drop8
I0909 22:07:16.575819  8989 net.cpp:329] drop8 <- fc8
I0909 22:07:16.575825  8989 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:07:16.575834  8989 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:07:16.575839  8989 net.cpp:125] drop8 needs backward computation.
I0909 22:07:16.575846  8989 net.cpp:66] Creating Layer fc9
I0909 22:07:16.575851  8989 net.cpp:329] fc9 <- fc8
I0909 22:07:16.575860  8989 net.cpp:290] fc9 -> fc9
I0909 22:07:16.576233  8989 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:07:16.576244  8989 net.cpp:125] fc9 needs backward computation.
I0909 22:07:16.576252  8989 net.cpp:66] Creating Layer fc10
I0909 22:07:16.576257  8989 net.cpp:329] fc10 <- fc9
I0909 22:07:16.576267  8989 net.cpp:290] fc10 -> fc10
I0909 22:07:16.576277  8989 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:07:16.576287  8989 net.cpp:125] fc10 needs backward computation.
I0909 22:07:16.576294  8989 net.cpp:66] Creating Layer prob
I0909 22:07:16.576299  8989 net.cpp:329] prob <- fc10
I0909 22:07:16.576306  8989 net.cpp:290] prob -> prob
I0909 22:07:16.576315  8989 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:07:16.576321  8989 net.cpp:125] prob needs backward computation.
I0909 22:07:16.576326  8989 net.cpp:156] This network produces output prob
I0909 22:07:16.576340  8989 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:07:16.576349  8989 net.cpp:167] Network initialization done.
I0909 22:07:16.576354  8989 net.cpp:168] Memory required for data: 6183480
Classifying 193 inputs.
Done in 126.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:09:30.748560  9013 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:09:30.748706  9013 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:09:30.748716  9013 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:09:30.748865  9013 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:09:30.748934  9013 net.cpp:292] Input 0 -> data
I0909 22:09:30.748960  9013 net.cpp:66] Creating Layer conv1
I0909 22:09:30.748966  9013 net.cpp:329] conv1 <- data
I0909 22:09:30.748975  9013 net.cpp:290] conv1 -> conv1
I0909 22:09:30.750360  9013 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:09:30.750381  9013 net.cpp:125] conv1 needs backward computation.
I0909 22:09:30.750391  9013 net.cpp:66] Creating Layer relu1
I0909 22:09:30.750396  9013 net.cpp:329] relu1 <- conv1
I0909 22:09:30.750403  9013 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:09:30.750412  9013 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:09:30.750418  9013 net.cpp:125] relu1 needs backward computation.
I0909 22:09:30.750426  9013 net.cpp:66] Creating Layer pool1
I0909 22:09:30.750432  9013 net.cpp:329] pool1 <- conv1
I0909 22:09:30.750438  9013 net.cpp:290] pool1 -> pool1
I0909 22:09:30.750449  9013 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:09:30.750455  9013 net.cpp:125] pool1 needs backward computation.
I0909 22:09:30.750462  9013 net.cpp:66] Creating Layer norm1
I0909 22:09:30.750468  9013 net.cpp:329] norm1 <- pool1
I0909 22:09:30.750474  9013 net.cpp:290] norm1 -> norm1
I0909 22:09:30.750483  9013 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:09:30.750489  9013 net.cpp:125] norm1 needs backward computation.
I0909 22:09:30.750497  9013 net.cpp:66] Creating Layer conv2
I0909 22:09:30.750504  9013 net.cpp:329] conv2 <- norm1
I0909 22:09:30.750510  9013 net.cpp:290] conv2 -> conv2
I0909 22:09:30.759603  9013 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:09:30.759618  9013 net.cpp:125] conv2 needs backward computation.
I0909 22:09:30.759624  9013 net.cpp:66] Creating Layer relu2
I0909 22:09:30.759629  9013 net.cpp:329] relu2 <- conv2
I0909 22:09:30.759636  9013 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:09:30.759644  9013 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:09:30.759649  9013 net.cpp:125] relu2 needs backward computation.
I0909 22:09:30.759655  9013 net.cpp:66] Creating Layer pool2
I0909 22:09:30.759660  9013 net.cpp:329] pool2 <- conv2
I0909 22:09:30.759667  9013 net.cpp:290] pool2 -> pool2
I0909 22:09:30.759675  9013 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:09:30.759681  9013 net.cpp:125] pool2 needs backward computation.
I0909 22:09:30.759688  9013 net.cpp:66] Creating Layer fc7
I0909 22:09:30.759693  9013 net.cpp:329] fc7 <- pool2
I0909 22:09:30.759703  9013 net.cpp:290] fc7 -> fc7
I0909 22:09:31.396900  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.396956  9013 net.cpp:125] fc7 needs backward computation.
I0909 22:09:31.396970  9013 net.cpp:66] Creating Layer relu7
I0909 22:09:31.396977  9013 net.cpp:329] relu7 <- fc7
I0909 22:09:31.396986  9013 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:09:31.396996  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.397001  9013 net.cpp:125] relu7 needs backward computation.
I0909 22:09:31.397008  9013 net.cpp:66] Creating Layer drop7
I0909 22:09:31.397013  9013 net.cpp:329] drop7 <- fc7
I0909 22:09:31.397022  9013 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:09:31.397032  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.397037  9013 net.cpp:125] drop7 needs backward computation.
I0909 22:09:31.397045  9013 net.cpp:66] Creating Layer fc8
I0909 22:09:31.397050  9013 net.cpp:329] fc8 <- fc7
I0909 22:09:31.397058  9013 net.cpp:290] fc8 -> fc8
I0909 22:09:31.404609  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.404621  9013 net.cpp:125] fc8 needs backward computation.
I0909 22:09:31.404628  9013 net.cpp:66] Creating Layer relu8
I0909 22:09:31.404633  9013 net.cpp:329] relu8 <- fc8
I0909 22:09:31.404640  9013 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:09:31.404647  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.404652  9013 net.cpp:125] relu8 needs backward computation.
I0909 22:09:31.404659  9013 net.cpp:66] Creating Layer drop8
I0909 22:09:31.404664  9013 net.cpp:329] drop8 <- fc8
I0909 22:09:31.404671  9013 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:09:31.404678  9013 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:31.404685  9013 net.cpp:125] drop8 needs backward computation.
I0909 22:09:31.404691  9013 net.cpp:66] Creating Layer fc9
I0909 22:09:31.404696  9013 net.cpp:329] fc9 <- fc8
I0909 22:09:31.404705  9013 net.cpp:290] fc9 -> fc9
I0909 22:09:31.405067  9013 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:09:31.405077  9013 net.cpp:125] fc9 needs backward computation.
I0909 22:09:31.405086  9013 net.cpp:66] Creating Layer fc10
I0909 22:09:31.405091  9013 net.cpp:329] fc10 <- fc9
I0909 22:09:31.405099  9013 net.cpp:290] fc10 -> fc10
I0909 22:09:31.405110  9013 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:09:31.405119  9013 net.cpp:125] fc10 needs backward computation.
I0909 22:09:31.405127  9013 net.cpp:66] Creating Layer prob
I0909 22:09:31.405132  9013 net.cpp:329] prob <- fc10
I0909 22:09:31.405138  9013 net.cpp:290] prob -> prob
I0909 22:09:31.405148  9013 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:09:31.405153  9013 net.cpp:125] prob needs backward computation.
I0909 22:09:31.405158  9013 net.cpp:156] This network produces output prob
I0909 22:09:31.405170  9013 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:09:31.405179  9013 net.cpp:167] Network initialization done.
I0909 22:09:31.405184  9013 net.cpp:168] Memory required for data: 6183480
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 157, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 119, in main
    inputs.append(caffe.io.load_image(im_f))
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py", line 23, in load_image
    img = skimage.img_as_float(skimage.io.imread(filename)).astype(np.float32)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 287, in img_as_float
    return convert(image, np.float64, force_copy)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 99, in convert
    raise ValueError("can not convert %s to %s." % (dtypeobj_in, dtypeobj))
ValueError: can not convert object to float64.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:09:40.996803  9017 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:09:40.996955  9017 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:09:40.996964  9017 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:09:40.997123  9017 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:09:40.997179  9017 net.cpp:292] Input 0 -> data
I0909 22:09:40.997205  9017 net.cpp:66] Creating Layer conv1
I0909 22:09:40.997211  9017 net.cpp:329] conv1 <- data
I0909 22:09:40.997220  9017 net.cpp:290] conv1 -> conv1
I0909 22:09:40.998606  9017 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:09:40.998626  9017 net.cpp:125] conv1 needs backward computation.
I0909 22:09:40.998636  9017 net.cpp:66] Creating Layer relu1
I0909 22:09:40.998641  9017 net.cpp:329] relu1 <- conv1
I0909 22:09:40.998648  9017 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:09:40.998657  9017 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:09:40.998663  9017 net.cpp:125] relu1 needs backward computation.
I0909 22:09:40.998671  9017 net.cpp:66] Creating Layer pool1
I0909 22:09:40.998680  9017 net.cpp:329] pool1 <- conv1
I0909 22:09:40.998688  9017 net.cpp:290] pool1 -> pool1
I0909 22:09:40.998700  9017 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:09:40.998706  9017 net.cpp:125] pool1 needs backward computation.
I0909 22:09:40.998713  9017 net.cpp:66] Creating Layer norm1
I0909 22:09:40.998718  9017 net.cpp:329] norm1 <- pool1
I0909 22:09:40.998725  9017 net.cpp:290] norm1 -> norm1
I0909 22:09:40.998735  9017 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:09:40.998741  9017 net.cpp:125] norm1 needs backward computation.
I0909 22:09:40.998749  9017 net.cpp:66] Creating Layer conv2
I0909 22:09:40.998754  9017 net.cpp:329] conv2 <- norm1
I0909 22:09:40.998761  9017 net.cpp:290] conv2 -> conv2
I0909 22:09:41.007872  9017 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:09:41.007886  9017 net.cpp:125] conv2 needs backward computation.
I0909 22:09:41.007894  9017 net.cpp:66] Creating Layer relu2
I0909 22:09:41.007899  9017 net.cpp:329] relu2 <- conv2
I0909 22:09:41.007906  9017 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:09:41.007915  9017 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:09:41.007920  9017 net.cpp:125] relu2 needs backward computation.
I0909 22:09:41.007926  9017 net.cpp:66] Creating Layer pool2
I0909 22:09:41.007931  9017 net.cpp:329] pool2 <- conv2
I0909 22:09:41.007938  9017 net.cpp:290] pool2 -> pool2
I0909 22:09:41.007946  9017 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:09:41.007952  9017 net.cpp:125] pool2 needs backward computation.
I0909 22:09:41.007959  9017 net.cpp:66] Creating Layer fc7
I0909 22:09:41.007964  9017 net.cpp:329] fc7 <- pool2
I0909 22:09:41.007974  9017 net.cpp:290] fc7 -> fc7
I0909 22:09:41.647605  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.647650  9017 net.cpp:125] fc7 needs backward computation.
I0909 22:09:41.647662  9017 net.cpp:66] Creating Layer relu7
I0909 22:09:41.647670  9017 net.cpp:329] relu7 <- fc7
I0909 22:09:41.647678  9017 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:09:41.647687  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.647693  9017 net.cpp:125] relu7 needs backward computation.
I0909 22:09:41.647701  9017 net.cpp:66] Creating Layer drop7
I0909 22:09:41.647706  9017 net.cpp:329] drop7 <- fc7
I0909 22:09:41.647713  9017 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:09:41.647723  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.647729  9017 net.cpp:125] drop7 needs backward computation.
I0909 22:09:41.647737  9017 net.cpp:66] Creating Layer fc8
I0909 22:09:41.647743  9017 net.cpp:329] fc8 <- fc7
I0909 22:09:41.647750  9017 net.cpp:290] fc8 -> fc8
I0909 22:09:41.655463  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.655477  9017 net.cpp:125] fc8 needs backward computation.
I0909 22:09:41.655483  9017 net.cpp:66] Creating Layer relu8
I0909 22:09:41.655489  9017 net.cpp:329] relu8 <- fc8
I0909 22:09:41.655495  9017 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:09:41.655503  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.655508  9017 net.cpp:125] relu8 needs backward computation.
I0909 22:09:41.655515  9017 net.cpp:66] Creating Layer drop8
I0909 22:09:41.655520  9017 net.cpp:329] drop8 <- fc8
I0909 22:09:41.655529  9017 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:09:41.655535  9017 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:09:41.655541  9017 net.cpp:125] drop8 needs backward computation.
I0909 22:09:41.655549  9017 net.cpp:66] Creating Layer fc9
I0909 22:09:41.655555  9017 net.cpp:329] fc9 <- fc8
I0909 22:09:41.655562  9017 net.cpp:290] fc9 -> fc9
I0909 22:09:41.655936  9017 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:09:41.655946  9017 net.cpp:125] fc9 needs backward computation.
I0909 22:09:41.655954  9017 net.cpp:66] Creating Layer fc10
I0909 22:09:41.655959  9017 net.cpp:329] fc10 <- fc9
I0909 22:09:41.655971  9017 net.cpp:290] fc10 -> fc10
I0909 22:09:41.655982  9017 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:09:41.655992  9017 net.cpp:125] fc10 needs backward computation.
I0909 22:09:41.656009  9017 net.cpp:66] Creating Layer prob
I0909 22:09:41.656015  9017 net.cpp:329] prob <- fc10
I0909 22:09:41.656023  9017 net.cpp:290] prob -> prob
I0909 22:09:41.656031  9017 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:09:41.656038  9017 net.cpp:125] prob needs backward computation.
I0909 22:09:41.656043  9017 net.cpp:156] This network produces output prob
I0909 22:09:41.656055  9017 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:09:41.656064  9017 net.cpp:167] Network initialization done.
I0909 22:09:41.656069  9017 net.cpp:168] Memory required for data: 6183480
Classifying 46 inputs.
Done in 29.02 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:10:12.958837  9020 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:10:12.958979  9020 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:10:12.958988  9020 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:10:12.959137  9020 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:10:12.959203  9020 net.cpp:292] Input 0 -> data
I0909 22:10:12.959230  9020 net.cpp:66] Creating Layer conv1
I0909 22:10:12.959238  9020 net.cpp:329] conv1 <- data
I0909 22:10:12.959245  9020 net.cpp:290] conv1 -> conv1
I0909 22:10:12.960634  9020 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:10:12.960654  9020 net.cpp:125] conv1 needs backward computation.
I0909 22:10:12.960664  9020 net.cpp:66] Creating Layer relu1
I0909 22:10:12.960669  9020 net.cpp:329] relu1 <- conv1
I0909 22:10:12.960676  9020 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:10:12.960685  9020 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:10:12.960691  9020 net.cpp:125] relu1 needs backward computation.
I0909 22:10:12.960698  9020 net.cpp:66] Creating Layer pool1
I0909 22:10:12.960705  9020 net.cpp:329] pool1 <- conv1
I0909 22:10:12.960711  9020 net.cpp:290] pool1 -> pool1
I0909 22:10:12.960722  9020 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:10:12.960728  9020 net.cpp:125] pool1 needs backward computation.
I0909 22:10:12.960736  9020 net.cpp:66] Creating Layer norm1
I0909 22:10:12.960741  9020 net.cpp:329] norm1 <- pool1
I0909 22:10:12.960748  9020 net.cpp:290] norm1 -> norm1
I0909 22:10:12.960758  9020 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:10:12.960764  9020 net.cpp:125] norm1 needs backward computation.
I0909 22:10:12.960772  9020 net.cpp:66] Creating Layer conv2
I0909 22:10:12.960778  9020 net.cpp:329] conv2 <- norm1
I0909 22:10:12.960785  9020 net.cpp:290] conv2 -> conv2
I0909 22:10:12.969912  9020 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:10:12.969928  9020 net.cpp:125] conv2 needs backward computation.
I0909 22:10:12.969934  9020 net.cpp:66] Creating Layer relu2
I0909 22:10:12.969940  9020 net.cpp:329] relu2 <- conv2
I0909 22:10:12.969948  9020 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:10:12.969954  9020 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:10:12.969960  9020 net.cpp:125] relu2 needs backward computation.
I0909 22:10:12.969967  9020 net.cpp:66] Creating Layer pool2
I0909 22:10:12.969972  9020 net.cpp:329] pool2 <- conv2
I0909 22:10:12.969979  9020 net.cpp:290] pool2 -> pool2
I0909 22:10:12.969987  9020 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:10:12.969993  9020 net.cpp:125] pool2 needs backward computation.
I0909 22:10:12.970002  9020 net.cpp:66] Creating Layer fc7
I0909 22:10:12.970008  9020 net.cpp:329] fc7 <- pool2
I0909 22:10:12.970016  9020 net.cpp:290] fc7 -> fc7
I0909 22:10:13.609370  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.609412  9020 net.cpp:125] fc7 needs backward computation.
I0909 22:10:13.609427  9020 net.cpp:66] Creating Layer relu7
I0909 22:10:13.609436  9020 net.cpp:329] relu7 <- fc7
I0909 22:10:13.609443  9020 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:10:13.609453  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.609459  9020 net.cpp:125] relu7 needs backward computation.
I0909 22:10:13.609467  9020 net.cpp:66] Creating Layer drop7
I0909 22:10:13.609472  9020 net.cpp:329] drop7 <- fc7
I0909 22:10:13.609480  9020 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:10:13.609490  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.609496  9020 net.cpp:125] drop7 needs backward computation.
I0909 22:10:13.609505  9020 net.cpp:66] Creating Layer fc8
I0909 22:10:13.609514  9020 net.cpp:329] fc8 <- fc7
I0909 22:10:13.609524  9020 net.cpp:290] fc8 -> fc8
I0909 22:10:13.617310  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.617324  9020 net.cpp:125] fc8 needs backward computation.
I0909 22:10:13.617331  9020 net.cpp:66] Creating Layer relu8
I0909 22:10:13.617337  9020 net.cpp:329] relu8 <- fc8
I0909 22:10:13.617343  9020 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:10:13.617352  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.617367  9020 net.cpp:125] relu8 needs backward computation.
I0909 22:10:13.617374  9020 net.cpp:66] Creating Layer drop8
I0909 22:10:13.617379  9020 net.cpp:329] drop8 <- fc8
I0909 22:10:13.617388  9020 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:10:13.617394  9020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:10:13.617400  9020 net.cpp:125] drop8 needs backward computation.
I0909 22:10:13.617408  9020 net.cpp:66] Creating Layer fc9
I0909 22:10:13.617413  9020 net.cpp:329] fc9 <- fc8
I0909 22:10:13.617421  9020 net.cpp:290] fc9 -> fc9
I0909 22:10:13.617820  9020 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:10:13.617832  9020 net.cpp:125] fc9 needs backward computation.
I0909 22:10:13.617841  9020 net.cpp:66] Creating Layer fc10
I0909 22:10:13.617847  9020 net.cpp:329] fc10 <- fc9
I0909 22:10:13.617856  9020 net.cpp:290] fc10 -> fc10
I0909 22:10:13.617868  9020 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:10:13.617877  9020 net.cpp:125] fc10 needs backward computation.
I0909 22:10:13.617885  9020 net.cpp:66] Creating Layer prob
I0909 22:10:13.617890  9020 net.cpp:329] prob <- fc10
I0909 22:10:13.617897  9020 net.cpp:290] prob -> prob
I0909 22:10:13.617907  9020 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:10:13.617913  9020 net.cpp:125] prob needs backward computation.
I0909 22:10:13.617918  9020 net.cpp:156] This network produces output prob
I0909 22:10:13.617930  9020 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:10:13.617938  9020 net.cpp:167] Network initialization done.
I0909 22:10:13.617944  9020 net.cpp:168] Memory required for data: 6183480
Classifying 201 inputs.
Done in 129.01 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:12:27.493862  9025 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:12:27.494005  9025 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:12:27.494014  9025 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:12:27.494164  9025 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:12:27.494231  9025 net.cpp:292] Input 0 -> data
I0909 22:12:27.494258  9025 net.cpp:66] Creating Layer conv1
I0909 22:12:27.494266  9025 net.cpp:329] conv1 <- data
I0909 22:12:27.494273  9025 net.cpp:290] conv1 -> conv1
I0909 22:12:27.495640  9025 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:12:27.495658  9025 net.cpp:125] conv1 needs backward computation.
I0909 22:12:27.495668  9025 net.cpp:66] Creating Layer relu1
I0909 22:12:27.495674  9025 net.cpp:329] relu1 <- conv1
I0909 22:12:27.495682  9025 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:12:27.495690  9025 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:12:27.495697  9025 net.cpp:125] relu1 needs backward computation.
I0909 22:12:27.495703  9025 net.cpp:66] Creating Layer pool1
I0909 22:12:27.495709  9025 net.cpp:329] pool1 <- conv1
I0909 22:12:27.495717  9025 net.cpp:290] pool1 -> pool1
I0909 22:12:27.495728  9025 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:12:27.495734  9025 net.cpp:125] pool1 needs backward computation.
I0909 22:12:27.495741  9025 net.cpp:66] Creating Layer norm1
I0909 22:12:27.495746  9025 net.cpp:329] norm1 <- pool1
I0909 22:12:27.495754  9025 net.cpp:290] norm1 -> norm1
I0909 22:12:27.495764  9025 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:12:27.495769  9025 net.cpp:125] norm1 needs backward computation.
I0909 22:12:27.495777  9025 net.cpp:66] Creating Layer conv2
I0909 22:12:27.495784  9025 net.cpp:329] conv2 <- norm1
I0909 22:12:27.495790  9025 net.cpp:290] conv2 -> conv2
I0909 22:12:27.504747  9025 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:12:27.504762  9025 net.cpp:125] conv2 needs backward computation.
I0909 22:12:27.504770  9025 net.cpp:66] Creating Layer relu2
I0909 22:12:27.504776  9025 net.cpp:329] relu2 <- conv2
I0909 22:12:27.504782  9025 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:12:27.504789  9025 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:12:27.504796  9025 net.cpp:125] relu2 needs backward computation.
I0909 22:12:27.504801  9025 net.cpp:66] Creating Layer pool2
I0909 22:12:27.504806  9025 net.cpp:329] pool2 <- conv2
I0909 22:12:27.504813  9025 net.cpp:290] pool2 -> pool2
I0909 22:12:27.504822  9025 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:12:27.504827  9025 net.cpp:125] pool2 needs backward computation.
I0909 22:12:27.504833  9025 net.cpp:66] Creating Layer fc7
I0909 22:12:27.504839  9025 net.cpp:329] fc7 <- pool2
I0909 22:12:27.504848  9025 net.cpp:290] fc7 -> fc7
I0909 22:12:28.145613  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.145654  9025 net.cpp:125] fc7 needs backward computation.
I0909 22:12:28.145668  9025 net.cpp:66] Creating Layer relu7
I0909 22:12:28.145675  9025 net.cpp:329] relu7 <- fc7
I0909 22:12:28.145696  9025 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:12:28.145707  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.145714  9025 net.cpp:125] relu7 needs backward computation.
I0909 22:12:28.145721  9025 net.cpp:66] Creating Layer drop7
I0909 22:12:28.145726  9025 net.cpp:329] drop7 <- fc7
I0909 22:12:28.145735  9025 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:12:28.145745  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.145751  9025 net.cpp:125] drop7 needs backward computation.
I0909 22:12:28.145761  9025 net.cpp:66] Creating Layer fc8
I0909 22:12:28.145766  9025 net.cpp:329] fc8 <- fc7
I0909 22:12:28.145773  9025 net.cpp:290] fc8 -> fc8
I0909 22:12:28.153552  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.153563  9025 net.cpp:125] fc8 needs backward computation.
I0909 22:12:28.153571  9025 net.cpp:66] Creating Layer relu8
I0909 22:12:28.153576  9025 net.cpp:329] relu8 <- fc8
I0909 22:12:28.153584  9025 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:12:28.153590  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.153596  9025 net.cpp:125] relu8 needs backward computation.
I0909 22:12:28.153602  9025 net.cpp:66] Creating Layer drop8
I0909 22:12:28.153609  9025 net.cpp:329] drop8 <- fc8
I0909 22:12:28.153616  9025 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:12:28.153623  9025 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:12:28.153630  9025 net.cpp:125] drop8 needs backward computation.
I0909 22:12:28.153636  9025 net.cpp:66] Creating Layer fc9
I0909 22:12:28.153642  9025 net.cpp:329] fc9 <- fc8
I0909 22:12:28.153650  9025 net.cpp:290] fc9 -> fc9
I0909 22:12:28.154023  9025 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:12:28.154034  9025 net.cpp:125] fc9 needs backward computation.
I0909 22:12:28.154042  9025 net.cpp:66] Creating Layer fc10
I0909 22:12:28.154048  9025 net.cpp:329] fc10 <- fc9
I0909 22:12:28.154057  9025 net.cpp:290] fc10 -> fc10
I0909 22:12:28.154068  9025 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:12:28.154078  9025 net.cpp:125] fc10 needs backward computation.
I0909 22:12:28.154085  9025 net.cpp:66] Creating Layer prob
I0909 22:12:28.154091  9025 net.cpp:329] prob <- fc10
I0909 22:12:28.154098  9025 net.cpp:290] prob -> prob
I0909 22:12:28.154108  9025 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:12:28.154114  9025 net.cpp:125] prob needs backward computation.
I0909 22:12:28.154119  9025 net.cpp:156] This network produces output prob
I0909 22:12:28.154131  9025 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:12:28.154140  9025 net.cpp:167] Network initialization done.
I0909 22:12:28.154146  9025 net.cpp:168] Memory required for data: 6183480
Classifying 275 inputs.
Done in 514.47 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:22:10.917647  9049 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:22:10.917806  9049 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:22:10.917815  9049 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:22:10.918046  9049 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:22:10.918110  9049 net.cpp:292] Input 0 -> data
I0909 22:22:10.918138  9049 net.cpp:66] Creating Layer conv1
I0909 22:22:10.918144  9049 net.cpp:329] conv1 <- data
I0909 22:22:10.918154  9049 net.cpp:290] conv1 -> conv1
I0909 22:22:10.967913  9049 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:22:10.967939  9049 net.cpp:125] conv1 needs backward computation.
I0909 22:22:10.967949  9049 net.cpp:66] Creating Layer relu1
I0909 22:22:10.967955  9049 net.cpp:329] relu1 <- conv1
I0909 22:22:10.967962  9049 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:22:10.967972  9049 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:22:10.967978  9049 net.cpp:125] relu1 needs backward computation.
I0909 22:22:10.967984  9049 net.cpp:66] Creating Layer pool1
I0909 22:22:10.967989  9049 net.cpp:329] pool1 <- conv1
I0909 22:22:10.967996  9049 net.cpp:290] pool1 -> pool1
I0909 22:22:10.968008  9049 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:22:10.968014  9049 net.cpp:125] pool1 needs backward computation.
I0909 22:22:10.968020  9049 net.cpp:66] Creating Layer norm1
I0909 22:22:10.968025  9049 net.cpp:329] norm1 <- pool1
I0909 22:22:10.968032  9049 net.cpp:290] norm1 -> norm1
I0909 22:22:10.968041  9049 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:22:10.968047  9049 net.cpp:125] norm1 needs backward computation.
I0909 22:22:10.968055  9049 net.cpp:66] Creating Layer conv2
I0909 22:22:10.968060  9049 net.cpp:329] conv2 <- norm1
I0909 22:22:10.968068  9049 net.cpp:290] conv2 -> conv2
I0909 22:22:10.977174  9049 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:22:10.977188  9049 net.cpp:125] conv2 needs backward computation.
I0909 22:22:10.977195  9049 net.cpp:66] Creating Layer relu2
I0909 22:22:10.977207  9049 net.cpp:329] relu2 <- conv2
I0909 22:22:10.977215  9049 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:22:10.977221  9049 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:22:10.977227  9049 net.cpp:125] relu2 needs backward computation.
I0909 22:22:10.977233  9049 net.cpp:66] Creating Layer pool2
I0909 22:22:10.977239  9049 net.cpp:329] pool2 <- conv2
I0909 22:22:10.977246  9049 net.cpp:290] pool2 -> pool2
I0909 22:22:10.977253  9049 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:22:10.977259  9049 net.cpp:125] pool2 needs backward computation.
I0909 22:22:10.977267  9049 net.cpp:66] Creating Layer fc7
I0909 22:22:10.977272  9049 net.cpp:329] fc7 <- pool2
I0909 22:22:10.977282  9049 net.cpp:290] fc7 -> fc7
I0909 22:22:11.621847  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.621893  9049 net.cpp:125] fc7 needs backward computation.
I0909 22:22:11.621912  9049 net.cpp:66] Creating Layer relu7
I0909 22:22:11.621918  9049 net.cpp:329] relu7 <- fc7
I0909 22:22:11.621927  9049 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:22:11.621937  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.621942  9049 net.cpp:125] relu7 needs backward computation.
I0909 22:22:11.621949  9049 net.cpp:66] Creating Layer drop7
I0909 22:22:11.621955  9049 net.cpp:329] drop7 <- fc7
I0909 22:22:11.621963  9049 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:22:11.621974  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.621980  9049 net.cpp:125] drop7 needs backward computation.
I0909 22:22:11.621989  9049 net.cpp:66] Creating Layer fc8
I0909 22:22:11.621994  9049 net.cpp:329] fc8 <- fc7
I0909 22:22:11.622001  9049 net.cpp:290] fc8 -> fc8
I0909 22:22:11.629787  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.629799  9049 net.cpp:125] fc8 needs backward computation.
I0909 22:22:11.629806  9049 net.cpp:66] Creating Layer relu8
I0909 22:22:11.629812  9049 net.cpp:329] relu8 <- fc8
I0909 22:22:11.629819  9049 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:22:11.629825  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.629832  9049 net.cpp:125] relu8 needs backward computation.
I0909 22:22:11.629837  9049 net.cpp:66] Creating Layer drop8
I0909 22:22:11.629843  9049 net.cpp:329] drop8 <- fc8
I0909 22:22:11.629850  9049 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:22:11.629858  9049 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:11.629863  9049 net.cpp:125] drop8 needs backward computation.
I0909 22:22:11.629870  9049 net.cpp:66] Creating Layer fc9
I0909 22:22:11.629876  9049 net.cpp:329] fc9 <- fc8
I0909 22:22:11.629884  9049 net.cpp:290] fc9 -> fc9
I0909 22:22:11.630256  9049 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:22:11.630267  9049 net.cpp:125] fc9 needs backward computation.
I0909 22:22:11.630275  9049 net.cpp:66] Creating Layer fc10
I0909 22:22:11.630280  9049 net.cpp:329] fc10 <- fc9
I0909 22:22:11.630290  9049 net.cpp:290] fc10 -> fc10
I0909 22:22:11.630300  9049 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:22:11.630311  9049 net.cpp:125] fc10 needs backward computation.
I0909 22:22:11.630317  9049 net.cpp:66] Creating Layer prob
I0909 22:22:11.630322  9049 net.cpp:329] prob <- fc10
I0909 22:22:11.630329  9049 net.cpp:290] prob -> prob
I0909 22:22:11.630338  9049 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:22:11.630344  9049 net.cpp:125] prob needs backward computation.
I0909 22:22:11.630349  9049 net.cpp:156] This network produces output prob
I0909 22:22:11.630363  9049 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:22:11.630372  9049 net.cpp:167] Network initialization done.
I0909 22:22:11.630377  9049 net.cpp:168] Memory required for data: 6183480
Classifying 54 inputs.
Done in 34.14 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:22:49.763649  9053 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:22:49.763790  9053 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:22:49.763811  9053 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:22:49.763958  9053 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:22:49.764014  9053 net.cpp:292] Input 0 -> data
I0909 22:22:49.764039  9053 net.cpp:66] Creating Layer conv1
I0909 22:22:49.764046  9053 net.cpp:329] conv1 <- data
I0909 22:22:49.764055  9053 net.cpp:290] conv1 -> conv1
I0909 22:22:49.765416  9053 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:22:49.765434  9053 net.cpp:125] conv1 needs backward computation.
I0909 22:22:49.765444  9053 net.cpp:66] Creating Layer relu1
I0909 22:22:49.765450  9053 net.cpp:329] relu1 <- conv1
I0909 22:22:49.765457  9053 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:22:49.765465  9053 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:22:49.765471  9053 net.cpp:125] relu1 needs backward computation.
I0909 22:22:49.765482  9053 net.cpp:66] Creating Layer pool1
I0909 22:22:49.765490  9053 net.cpp:329] pool1 <- conv1
I0909 22:22:49.765496  9053 net.cpp:290] pool1 -> pool1
I0909 22:22:49.765507  9053 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:22:49.765533  9053 net.cpp:125] pool1 needs backward computation.
I0909 22:22:49.765542  9053 net.cpp:66] Creating Layer norm1
I0909 22:22:49.765547  9053 net.cpp:329] norm1 <- pool1
I0909 22:22:49.765553  9053 net.cpp:290] norm1 -> norm1
I0909 22:22:49.765563  9053 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:22:49.765569  9053 net.cpp:125] norm1 needs backward computation.
I0909 22:22:49.765578  9053 net.cpp:66] Creating Layer conv2
I0909 22:22:49.765586  9053 net.cpp:329] conv2 <- norm1
I0909 22:22:49.765594  9053 net.cpp:290] conv2 -> conv2
I0909 22:22:49.774693  9053 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:22:49.774708  9053 net.cpp:125] conv2 needs backward computation.
I0909 22:22:49.774715  9053 net.cpp:66] Creating Layer relu2
I0909 22:22:49.774720  9053 net.cpp:329] relu2 <- conv2
I0909 22:22:49.774727  9053 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:22:49.774734  9053 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:22:49.774740  9053 net.cpp:125] relu2 needs backward computation.
I0909 22:22:49.774746  9053 net.cpp:66] Creating Layer pool2
I0909 22:22:49.774752  9053 net.cpp:329] pool2 <- conv2
I0909 22:22:49.774759  9053 net.cpp:290] pool2 -> pool2
I0909 22:22:49.774766  9053 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:22:49.774772  9053 net.cpp:125] pool2 needs backward computation.
I0909 22:22:49.774781  9053 net.cpp:66] Creating Layer fc7
I0909 22:22:49.774787  9053 net.cpp:329] fc7 <- pool2
I0909 22:22:49.774796  9053 net.cpp:290] fc7 -> fc7
I0909 22:22:50.420881  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.420927  9053 net.cpp:125] fc7 needs backward computation.
I0909 22:22:50.420941  9053 net.cpp:66] Creating Layer relu7
I0909 22:22:50.420948  9053 net.cpp:329] relu7 <- fc7
I0909 22:22:50.420956  9053 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:22:50.420966  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.420972  9053 net.cpp:125] relu7 needs backward computation.
I0909 22:22:50.420979  9053 net.cpp:66] Creating Layer drop7
I0909 22:22:50.420985  9053 net.cpp:329] drop7 <- fc7
I0909 22:22:50.420994  9053 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:22:50.421005  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.421010  9053 net.cpp:125] drop7 needs backward computation.
I0909 22:22:50.421020  9053 net.cpp:66] Creating Layer fc8
I0909 22:22:50.421025  9053 net.cpp:329] fc8 <- fc7
I0909 22:22:50.421032  9053 net.cpp:290] fc8 -> fc8
I0909 22:22:50.428819  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.428832  9053 net.cpp:125] fc8 needs backward computation.
I0909 22:22:50.428838  9053 net.cpp:66] Creating Layer relu8
I0909 22:22:50.428844  9053 net.cpp:329] relu8 <- fc8
I0909 22:22:50.428850  9053 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:22:50.428858  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.428864  9053 net.cpp:125] relu8 needs backward computation.
I0909 22:22:50.428869  9053 net.cpp:66] Creating Layer drop8
I0909 22:22:50.428875  9053 net.cpp:329] drop8 <- fc8
I0909 22:22:50.428884  9053 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:22:50.428890  9053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:22:50.428896  9053 net.cpp:125] drop8 needs backward computation.
I0909 22:22:50.428903  9053 net.cpp:66] Creating Layer fc9
I0909 22:22:50.428910  9053 net.cpp:329] fc9 <- fc8
I0909 22:22:50.428918  9053 net.cpp:290] fc9 -> fc9
I0909 22:22:50.429291  9053 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:22:50.429302  9053 net.cpp:125] fc9 needs backward computation.
I0909 22:22:50.429311  9053 net.cpp:66] Creating Layer fc10
I0909 22:22:50.429316  9053 net.cpp:329] fc10 <- fc9
I0909 22:22:50.429324  9053 net.cpp:290] fc10 -> fc10
I0909 22:22:50.429337  9053 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:22:50.429358  9053 net.cpp:125] fc10 needs backward computation.
I0909 22:22:50.429364  9053 net.cpp:66] Creating Layer prob
I0909 22:22:50.429370  9053 net.cpp:329] prob <- fc10
I0909 22:22:50.429378  9053 net.cpp:290] prob -> prob
I0909 22:22:50.429386  9053 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:22:50.429393  9053 net.cpp:125] prob needs backward computation.
I0909 22:22:50.429397  9053 net.cpp:156] This network produces output prob
I0909 22:22:50.429410  9053 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:22:50.429419  9053 net.cpp:167] Network initialization done.
I0909 22:22:50.429424  9053 net.cpp:168] Memory required for data: 6183480
Classifying 95 inputs.
Done in 57.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:23:50.433809  9057 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:23:50.433951  9057 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:23:50.433960  9057 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:23:50.434110  9057 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:23:50.434177  9057 net.cpp:292] Input 0 -> data
I0909 22:23:50.434205  9057 net.cpp:66] Creating Layer conv1
I0909 22:23:50.434211  9057 net.cpp:329] conv1 <- data
I0909 22:23:50.434219  9057 net.cpp:290] conv1 -> conv1
I0909 22:23:50.435582  9057 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:23:50.435600  9057 net.cpp:125] conv1 needs backward computation.
I0909 22:23:50.435608  9057 net.cpp:66] Creating Layer relu1
I0909 22:23:50.435614  9057 net.cpp:329] relu1 <- conv1
I0909 22:23:50.435621  9057 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:23:50.435629  9057 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:23:50.435636  9057 net.cpp:125] relu1 needs backward computation.
I0909 22:23:50.435642  9057 net.cpp:66] Creating Layer pool1
I0909 22:23:50.435647  9057 net.cpp:329] pool1 <- conv1
I0909 22:23:50.435653  9057 net.cpp:290] pool1 -> pool1
I0909 22:23:50.435665  9057 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:23:50.435670  9057 net.cpp:125] pool1 needs backward computation.
I0909 22:23:50.435678  9057 net.cpp:66] Creating Layer norm1
I0909 22:23:50.435683  9057 net.cpp:329] norm1 <- pool1
I0909 22:23:50.435689  9057 net.cpp:290] norm1 -> norm1
I0909 22:23:50.435698  9057 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:23:50.435704  9057 net.cpp:125] norm1 needs backward computation.
I0909 22:23:50.435711  9057 net.cpp:66] Creating Layer conv2
I0909 22:23:50.435716  9057 net.cpp:329] conv2 <- norm1
I0909 22:23:50.435724  9057 net.cpp:290] conv2 -> conv2
I0909 22:23:50.444624  9057 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:23:50.444638  9057 net.cpp:125] conv2 needs backward computation.
I0909 22:23:50.444645  9057 net.cpp:66] Creating Layer relu2
I0909 22:23:50.444651  9057 net.cpp:329] relu2 <- conv2
I0909 22:23:50.444658  9057 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:23:50.444664  9057 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:23:50.444670  9057 net.cpp:125] relu2 needs backward computation.
I0909 22:23:50.444676  9057 net.cpp:66] Creating Layer pool2
I0909 22:23:50.444681  9057 net.cpp:329] pool2 <- conv2
I0909 22:23:50.444689  9057 net.cpp:290] pool2 -> pool2
I0909 22:23:50.444696  9057 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:23:50.444701  9057 net.cpp:125] pool2 needs backward computation.
I0909 22:23:50.444710  9057 net.cpp:66] Creating Layer fc7
I0909 22:23:50.444716  9057 net.cpp:329] fc7 <- pool2
I0909 22:23:50.444723  9057 net.cpp:290] fc7 -> fc7
I0909 22:23:51.088101  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.088141  9057 net.cpp:125] fc7 needs backward computation.
I0909 22:23:51.088155  9057 net.cpp:66] Creating Layer relu7
I0909 22:23:51.088162  9057 net.cpp:329] relu7 <- fc7
I0909 22:23:51.088171  9057 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:23:51.088181  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.088186  9057 net.cpp:125] relu7 needs backward computation.
I0909 22:23:51.088192  9057 net.cpp:66] Creating Layer drop7
I0909 22:23:51.088197  9057 net.cpp:329] drop7 <- fc7
I0909 22:23:51.088206  9057 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:23:51.088215  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.088222  9057 net.cpp:125] drop7 needs backward computation.
I0909 22:23:51.088230  9057 net.cpp:66] Creating Layer fc8
I0909 22:23:51.088235  9057 net.cpp:329] fc8 <- fc7
I0909 22:23:51.088243  9057 net.cpp:290] fc8 -> fc8
I0909 22:23:51.095808  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.095820  9057 net.cpp:125] fc8 needs backward computation.
I0909 22:23:51.095827  9057 net.cpp:66] Creating Layer relu8
I0909 22:23:51.095832  9057 net.cpp:329] relu8 <- fc8
I0909 22:23:51.095849  9057 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:23:51.095857  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.095862  9057 net.cpp:125] relu8 needs backward computation.
I0909 22:23:51.095868  9057 net.cpp:66] Creating Layer drop8
I0909 22:23:51.095873  9057 net.cpp:329] drop8 <- fc8
I0909 22:23:51.095881  9057 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:23:51.095888  9057 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:23:51.095893  9057 net.cpp:125] drop8 needs backward computation.
I0909 22:23:51.095901  9057 net.cpp:66] Creating Layer fc9
I0909 22:23:51.095906  9057 net.cpp:329] fc9 <- fc8
I0909 22:23:51.095914  9057 net.cpp:290] fc9 -> fc9
I0909 22:23:51.096279  9057 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:23:51.096289  9057 net.cpp:125] fc9 needs backward computation.
I0909 22:23:51.096297  9057 net.cpp:66] Creating Layer fc10
I0909 22:23:51.096303  9057 net.cpp:329] fc10 <- fc9
I0909 22:23:51.096312  9057 net.cpp:290] fc10 -> fc10
I0909 22:23:51.096323  9057 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:23:51.096331  9057 net.cpp:125] fc10 needs backward computation.
I0909 22:23:51.096338  9057 net.cpp:66] Creating Layer prob
I0909 22:23:51.096343  9057 net.cpp:329] prob <- fc10
I0909 22:23:51.096350  9057 net.cpp:290] prob -> prob
I0909 22:23:51.096359  9057 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:23:51.096364  9057 net.cpp:125] prob needs backward computation.
I0909 22:23:51.096369  9057 net.cpp:156] This network produces output prob
I0909 22:23:51.096381  9057 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:23:51.096390  9057 net.cpp:167] Network initialization done.
I0909 22:23:51.096395  9057 net.cpp:168] Memory required for data: 6183480
Classifying 172 inputs.
Done in 104.23 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:25:38.630584  9063 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:25:38.630725  9063 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:25:38.630735  9063 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:25:38.630882  9063 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:25:38.630949  9063 net.cpp:292] Input 0 -> data
I0909 22:25:38.630975  9063 net.cpp:66] Creating Layer conv1
I0909 22:25:38.630982  9063 net.cpp:329] conv1 <- data
I0909 22:25:38.630990  9063 net.cpp:290] conv1 -> conv1
I0909 22:25:38.632355  9063 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:25:38.632374  9063 net.cpp:125] conv1 needs backward computation.
I0909 22:25:38.632382  9063 net.cpp:66] Creating Layer relu1
I0909 22:25:38.632390  9063 net.cpp:329] relu1 <- conv1
I0909 22:25:38.632395  9063 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:25:38.632405  9063 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:25:38.632411  9063 net.cpp:125] relu1 needs backward computation.
I0909 22:25:38.632417  9063 net.cpp:66] Creating Layer pool1
I0909 22:25:38.632422  9063 net.cpp:329] pool1 <- conv1
I0909 22:25:38.632429  9063 net.cpp:290] pool1 -> pool1
I0909 22:25:38.632441  9063 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:25:38.632447  9063 net.cpp:125] pool1 needs backward computation.
I0909 22:25:38.632453  9063 net.cpp:66] Creating Layer norm1
I0909 22:25:38.632459  9063 net.cpp:329] norm1 <- pool1
I0909 22:25:38.632465  9063 net.cpp:290] norm1 -> norm1
I0909 22:25:38.632475  9063 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:25:38.632482  9063 net.cpp:125] norm1 needs backward computation.
I0909 22:25:38.632489  9063 net.cpp:66] Creating Layer conv2
I0909 22:25:38.632495  9063 net.cpp:329] conv2 <- norm1
I0909 22:25:38.632503  9063 net.cpp:290] conv2 -> conv2
I0909 22:25:38.641757  9063 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:25:38.641782  9063 net.cpp:125] conv2 needs backward computation.
I0909 22:25:38.641791  9063 net.cpp:66] Creating Layer relu2
I0909 22:25:38.641798  9063 net.cpp:329] relu2 <- conv2
I0909 22:25:38.641803  9063 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:25:38.641813  9063 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:25:38.641818  9063 net.cpp:125] relu2 needs backward computation.
I0909 22:25:38.641824  9063 net.cpp:66] Creating Layer pool2
I0909 22:25:38.641829  9063 net.cpp:329] pool2 <- conv2
I0909 22:25:38.641836  9063 net.cpp:290] pool2 -> pool2
I0909 22:25:38.641845  9063 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:25:38.641851  9063 net.cpp:125] pool2 needs backward computation.
I0909 22:25:38.641859  9063 net.cpp:66] Creating Layer fc7
I0909 22:25:38.641865  9063 net.cpp:329] fc7 <- pool2
I0909 22:25:38.641875  9063 net.cpp:290] fc7 -> fc7
I0909 22:25:39.287856  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.287902  9063 net.cpp:125] fc7 needs backward computation.
I0909 22:25:39.287916  9063 net.cpp:66] Creating Layer relu7
I0909 22:25:39.287935  9063 net.cpp:329] relu7 <- fc7
I0909 22:25:39.287943  9063 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:25:39.287953  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.287960  9063 net.cpp:125] relu7 needs backward computation.
I0909 22:25:39.287966  9063 net.cpp:66] Creating Layer drop7
I0909 22:25:39.287972  9063 net.cpp:329] drop7 <- fc7
I0909 22:25:39.287979  9063 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:25:39.287992  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.287997  9063 net.cpp:125] drop7 needs backward computation.
I0909 22:25:39.288005  9063 net.cpp:66] Creating Layer fc8
I0909 22:25:39.288012  9063 net.cpp:329] fc8 <- fc7
I0909 22:25:39.288018  9063 net.cpp:290] fc8 -> fc8
I0909 22:25:39.295832  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.295845  9063 net.cpp:125] fc8 needs backward computation.
I0909 22:25:39.295852  9063 net.cpp:66] Creating Layer relu8
I0909 22:25:39.295857  9063 net.cpp:329] relu8 <- fc8
I0909 22:25:39.295864  9063 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:25:39.295871  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.295876  9063 net.cpp:125] relu8 needs backward computation.
I0909 22:25:39.295883  9063 net.cpp:66] Creating Layer drop8
I0909 22:25:39.295888  9063 net.cpp:329] drop8 <- fc8
I0909 22:25:39.295897  9063 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:25:39.295903  9063 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:25:39.295909  9063 net.cpp:125] drop8 needs backward computation.
I0909 22:25:39.295917  9063 net.cpp:66] Creating Layer fc9
I0909 22:25:39.295922  9063 net.cpp:329] fc9 <- fc8
I0909 22:25:39.295930  9063 net.cpp:290] fc9 -> fc9
I0909 22:25:39.296308  9063 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:25:39.296319  9063 net.cpp:125] fc9 needs backward computation.
I0909 22:25:39.296326  9063 net.cpp:66] Creating Layer fc10
I0909 22:25:39.296332  9063 net.cpp:329] fc10 <- fc9
I0909 22:25:39.296340  9063 net.cpp:290] fc10 -> fc10
I0909 22:25:39.296352  9063 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:25:39.296362  9063 net.cpp:125] fc10 needs backward computation.
I0909 22:25:39.296370  9063 net.cpp:66] Creating Layer prob
I0909 22:25:39.296375  9063 net.cpp:329] prob <- fc10
I0909 22:25:39.296381  9063 net.cpp:290] prob -> prob
I0909 22:25:39.296391  9063 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:25:39.296397  9063 net.cpp:125] prob needs backward computation.
I0909 22:25:39.296402  9063 net.cpp:156] This network produces output prob
I0909 22:25:39.296414  9063 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:25:39.296423  9063 net.cpp:167] Network initialization done.
I0909 22:25:39.296428  9063 net.cpp:168] Memory required for data: 6183480
Classifying 219 inputs.
Done in 142.50 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 22:28:08.758347  9069 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 22:28:08.758487  9069 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 22:28:08.758497  9069 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 22:28:08.758646  9069 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 22:28:08.758712  9069 net.cpp:292] Input 0 -> data
I0909 22:28:08.758738  9069 net.cpp:66] Creating Layer conv1
I0909 22:28:08.758744  9069 net.cpp:329] conv1 <- data
I0909 22:28:08.758752  9069 net.cpp:290] conv1 -> conv1
I0909 22:28:08.760115  9069 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:28:08.760133  9069 net.cpp:125] conv1 needs backward computation.
I0909 22:28:08.760143  9069 net.cpp:66] Creating Layer relu1
I0909 22:28:08.760149  9069 net.cpp:329] relu1 <- conv1
I0909 22:28:08.760156  9069 net.cpp:280] relu1 -> conv1 (in-place)
I0909 22:28:08.760164  9069 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 22:28:08.760170  9069 net.cpp:125] relu1 needs backward computation.
I0909 22:28:08.760177  9069 net.cpp:66] Creating Layer pool1
I0909 22:28:08.760182  9069 net.cpp:329] pool1 <- conv1
I0909 22:28:08.760190  9069 net.cpp:290] pool1 -> pool1
I0909 22:28:08.760200  9069 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:28:08.760206  9069 net.cpp:125] pool1 needs backward computation.
I0909 22:28:08.760213  9069 net.cpp:66] Creating Layer norm1
I0909 22:28:08.760218  9069 net.cpp:329] norm1 <- pool1
I0909 22:28:08.760226  9069 net.cpp:290] norm1 -> norm1
I0909 22:28:08.760234  9069 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 22:28:08.760241  9069 net.cpp:125] norm1 needs backward computation.
I0909 22:28:08.760248  9069 net.cpp:66] Creating Layer conv2
I0909 22:28:08.760253  9069 net.cpp:329] conv2 <- norm1
I0909 22:28:08.760260  9069 net.cpp:290] conv2 -> conv2
I0909 22:28:08.769383  9069 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:28:08.769398  9069 net.cpp:125] conv2 needs backward computation.
I0909 22:28:08.769410  9069 net.cpp:66] Creating Layer relu2
I0909 22:28:08.769417  9069 net.cpp:329] relu2 <- conv2
I0909 22:28:08.769423  9069 net.cpp:280] relu2 -> conv2 (in-place)
I0909 22:28:08.769431  9069 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 22:28:08.769436  9069 net.cpp:125] relu2 needs backward computation.
I0909 22:28:08.769443  9069 net.cpp:66] Creating Layer pool2
I0909 22:28:08.769448  9069 net.cpp:329] pool2 <- conv2
I0909 22:28:08.769455  9069 net.cpp:290] pool2 -> pool2
I0909 22:28:08.769464  9069 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 22:28:08.769469  9069 net.cpp:125] pool2 needs backward computation.
I0909 22:28:08.769479  9069 net.cpp:66] Creating Layer fc7
I0909 22:28:08.769484  9069 net.cpp:329] fc7 <- pool2
I0909 22:28:08.769492  9069 net.cpp:290] fc7 -> fc7
I0909 22:28:09.411555  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.411602  9069 net.cpp:125] fc7 needs backward computation.
I0909 22:28:09.411617  9069 net.cpp:66] Creating Layer relu7
I0909 22:28:09.411628  9069 net.cpp:329] relu7 <- fc7
I0909 22:28:09.411636  9069 net.cpp:280] relu7 -> fc7 (in-place)
I0909 22:28:09.411648  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.411653  9069 net.cpp:125] relu7 needs backward computation.
I0909 22:28:09.411660  9069 net.cpp:66] Creating Layer drop7
I0909 22:28:09.411665  9069 net.cpp:329] drop7 <- fc7
I0909 22:28:09.411674  9069 net.cpp:280] drop7 -> fc7 (in-place)
I0909 22:28:09.411685  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.411691  9069 net.cpp:125] drop7 needs backward computation.
I0909 22:28:09.411700  9069 net.cpp:66] Creating Layer fc8
I0909 22:28:09.411705  9069 net.cpp:329] fc8 <- fc7
I0909 22:28:09.411712  9069 net.cpp:290] fc8 -> fc8
I0909 22:28:09.419509  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.419523  9069 net.cpp:125] fc8 needs backward computation.
I0909 22:28:09.419529  9069 net.cpp:66] Creating Layer relu8
I0909 22:28:09.419535  9069 net.cpp:329] relu8 <- fc8
I0909 22:28:09.419543  9069 net.cpp:280] relu8 -> fc8 (in-place)
I0909 22:28:09.419548  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.419554  9069 net.cpp:125] relu8 needs backward computation.
I0909 22:28:09.419561  9069 net.cpp:66] Creating Layer drop8
I0909 22:28:09.419566  9069 net.cpp:329] drop8 <- fc8
I0909 22:28:09.419574  9069 net.cpp:280] drop8 -> fc8 (in-place)
I0909 22:28:09.419581  9069 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 22:28:09.419587  9069 net.cpp:125] drop8 needs backward computation.
I0909 22:28:09.419595  9069 net.cpp:66] Creating Layer fc9
I0909 22:28:09.419600  9069 net.cpp:329] fc9 <- fc8
I0909 22:28:09.419608  9069 net.cpp:290] fc9 -> fc9
I0909 22:28:09.419986  9069 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 22:28:09.419996  9069 net.cpp:125] fc9 needs backward computation.
I0909 22:28:09.420003  9069 net.cpp:66] Creating Layer fc10
I0909 22:28:09.420009  9069 net.cpp:329] fc10 <- fc9
I0909 22:28:09.420018  9069 net.cpp:290] fc10 -> fc10
I0909 22:28:09.420030  9069 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:28:09.420039  9069 net.cpp:125] fc10 needs backward computation.
I0909 22:28:09.420047  9069 net.cpp:66] Creating Layer prob
I0909 22:28:09.420052  9069 net.cpp:329] prob <- fc10
I0909 22:28:09.420058  9069 net.cpp:290] prob -> prob
I0909 22:28:09.420068  9069 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 22:28:09.420073  9069 net.cpp:125] prob needs backward computation.
I0909 22:28:09.420078  9069 net.cpp:156] This network produces output prob
I0909 22:28:09.420090  9069 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 22:28:09.420099  9069 net.cpp:167] Network initialization done.
I0909 22:28:09.420104  9069 net.cpp:168] Memory required for data: 6183480
Classifying 346 inputs.
Done in 211.47 s.

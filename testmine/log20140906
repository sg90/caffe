WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:06:10.226467 31654 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:06:10.226624 31654 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:06:10.226632 31654 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:06:10.226788 31654 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:06:10.226840 31654 net.cpp:292] Input 0 -> data
I0905 17:06:10.226865 31654 net.cpp:66] Creating Layer conv1
I0905 17:06:10.226871 31654 net.cpp:329] conv1 <- data
I0905 17:06:10.226878 31654 net.cpp:290] conv1 -> conv1
I0905 17:06:10.228196 31654 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:06:10.228214 31654 net.cpp:125] conv1 needs backward computation.
I0905 17:06:10.228229 31654 net.cpp:66] Creating Layer relu1
I0905 17:06:10.228235 31654 net.cpp:329] relu1 <- conv1
I0905 17:06:10.228241 31654 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:06:10.228250 31654 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:06:10.228255 31654 net.cpp:125] relu1 needs backward computation.
I0905 17:06:10.228262 31654 net.cpp:66] Creating Layer pool1
I0905 17:06:10.228267 31654 net.cpp:329] pool1 <- conv1
I0905 17:06:10.228274 31654 net.cpp:290] pool1 -> pool1
I0905 17:06:10.228286 31654 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:06:10.228291 31654 net.cpp:125] pool1 needs backward computation.
I0905 17:06:10.228297 31654 net.cpp:66] Creating Layer norm1
I0905 17:06:10.228303 31654 net.cpp:329] norm1 <- pool1
I0905 17:06:10.228309 31654 net.cpp:290] norm1 -> norm1
I0905 17:06:10.228319 31654 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:06:10.228325 31654 net.cpp:125] norm1 needs backward computation.
I0905 17:06:10.228332 31654 net.cpp:66] Creating Layer conv2
I0905 17:06:10.228338 31654 net.cpp:329] conv2 <- norm1
I0905 17:06:10.228344 31654 net.cpp:290] conv2 -> conv2
I0905 17:06:10.237709 31654 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:06:10.237740 31654 net.cpp:125] conv2 needs backward computation.
I0905 17:06:10.237748 31654 net.cpp:66] Creating Layer relu2
I0905 17:06:10.237754 31654 net.cpp:329] relu2 <- conv2
I0905 17:06:10.237761 31654 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:06:10.237771 31654 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:06:10.237776 31654 net.cpp:125] relu2 needs backward computation.
I0905 17:06:10.237788 31654 net.cpp:66] Creating Layer pool2
I0905 17:06:10.237793 31654 net.cpp:329] pool2 <- conv2
I0905 17:06:10.237800 31654 net.cpp:290] pool2 -> pool2
I0905 17:06:10.237810 31654 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:06:10.237815 31654 net.cpp:125] pool2 needs backward computation.
I0905 17:06:10.237823 31654 net.cpp:66] Creating Layer fc7
I0905 17:06:10.237829 31654 net.cpp:329] fc7 <- pool2
I0905 17:06:10.237836 31654 net.cpp:290] fc7 -> fc7
I0905 17:06:10.871584 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.871628 31654 net.cpp:125] fc7 needs backward computation.
I0905 17:06:10.871640 31654 net.cpp:66] Creating Layer relu7
I0905 17:06:10.871647 31654 net.cpp:329] relu7 <- fc7
I0905 17:06:10.871656 31654 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:06:10.871666 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.871672 31654 net.cpp:125] relu7 needs backward computation.
I0905 17:06:10.871680 31654 net.cpp:66] Creating Layer drop7
I0905 17:06:10.871685 31654 net.cpp:329] drop7 <- fc7
I0905 17:06:10.871691 31654 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:06:10.871701 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.871707 31654 net.cpp:125] drop7 needs backward computation.
I0905 17:06:10.871716 31654 net.cpp:66] Creating Layer fc8
I0905 17:06:10.871721 31654 net.cpp:329] fc8 <- fc7
I0905 17:06:10.871731 31654 net.cpp:290] fc8 -> fc8
I0905 17:06:10.879344 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.879356 31654 net.cpp:125] fc8 needs backward computation.
I0905 17:06:10.879364 31654 net.cpp:66] Creating Layer relu8
I0905 17:06:10.879369 31654 net.cpp:329] relu8 <- fc8
I0905 17:06:10.879376 31654 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:06:10.879384 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.879390 31654 net.cpp:125] relu8 needs backward computation.
I0905 17:06:10.879396 31654 net.cpp:66] Creating Layer drop8
I0905 17:06:10.879401 31654 net.cpp:329] drop8 <- fc8
I0905 17:06:10.879407 31654 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:06:10.879415 31654 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:06:10.879420 31654 net.cpp:125] drop8 needs backward computation.
I0905 17:06:10.879428 31654 net.cpp:66] Creating Layer fc9
I0905 17:06:10.879434 31654 net.cpp:329] fc9 <- fc8
I0905 17:06:10.879441 31654 net.cpp:290] fc9 -> fc9
I0905 17:06:10.879806 31654 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:06:10.879827 31654 net.cpp:125] fc9 needs backward computation.
I0905 17:06:10.879834 31654 net.cpp:66] Creating Layer fc10
I0905 17:06:10.879840 31654 net.cpp:329] fc10 <- fc9
I0905 17:06:10.879848 31654 net.cpp:290] fc10 -> fc10
I0905 17:06:10.879860 31654 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:06:10.879868 31654 net.cpp:125] fc10 needs backward computation.
I0905 17:06:10.879874 31654 net.cpp:66] Creating Layer prob
I0905 17:06:10.879879 31654 net.cpp:329] prob <- fc10
I0905 17:06:10.879887 31654 net.cpp:290] prob -> prob
I0905 17:06:10.879896 31654 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:06:10.879902 31654 net.cpp:125] prob needs backward computation.
I0905 17:06:10.879907 31654 net.cpp:156] This network produces output prob
I0905 17:06:10.879920 31654 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:06:10.879928 31654 net.cpp:167] Network initialization done.
I0905 17:06:10.879933 31654 net.cpp:168] Memory required for data: 6183480
Classifying 919 inputs.
Done in 565.04 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 19 is out of bounds for axis 0 with size 19
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:15:58.582005 31785 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:15:58.582149 31785 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:15:58.582159 31785 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:15:58.582301 31785 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:15:58.582360 31785 net.cpp:292] Input 0 -> data
I0905 17:15:58.582386 31785 net.cpp:66] Creating Layer conv1
I0905 17:15:58.582393 31785 net.cpp:329] conv1 <- data
I0905 17:15:58.582401 31785 net.cpp:290] conv1 -> conv1
I0905 17:15:58.607363 31785 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:15:58.607388 31785 net.cpp:125] conv1 needs backward computation.
I0905 17:15:58.607398 31785 net.cpp:66] Creating Layer relu1
I0905 17:15:58.607404 31785 net.cpp:329] relu1 <- conv1
I0905 17:15:58.607411 31785 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:15:58.607420 31785 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:15:58.607425 31785 net.cpp:125] relu1 needs backward computation.
I0905 17:15:58.607432 31785 net.cpp:66] Creating Layer pool1
I0905 17:15:58.607439 31785 net.cpp:329] pool1 <- conv1
I0905 17:15:58.607445 31785 net.cpp:290] pool1 -> pool1
I0905 17:15:58.607455 31785 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:15:58.607461 31785 net.cpp:125] pool1 needs backward computation.
I0905 17:15:58.607468 31785 net.cpp:66] Creating Layer norm1
I0905 17:15:58.607473 31785 net.cpp:329] norm1 <- pool1
I0905 17:15:58.607481 31785 net.cpp:290] norm1 -> norm1
I0905 17:15:58.607489 31785 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:15:58.607496 31785 net.cpp:125] norm1 needs backward computation.
I0905 17:15:58.607509 31785 net.cpp:66] Creating Layer conv2
I0905 17:15:58.607516 31785 net.cpp:329] conv2 <- norm1
I0905 17:15:58.607522 31785 net.cpp:290] conv2 -> conv2
I0905 17:15:58.616358 31785 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:15:58.616372 31785 net.cpp:125] conv2 needs backward computation.
I0905 17:15:58.616379 31785 net.cpp:66] Creating Layer relu2
I0905 17:15:58.616385 31785 net.cpp:329] relu2 <- conv2
I0905 17:15:58.616391 31785 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:15:58.616399 31785 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:15:58.616405 31785 net.cpp:125] relu2 needs backward computation.
I0905 17:15:58.616410 31785 net.cpp:66] Creating Layer pool2
I0905 17:15:58.616415 31785 net.cpp:329] pool2 <- conv2
I0905 17:15:58.616422 31785 net.cpp:290] pool2 -> pool2
I0905 17:15:58.616430 31785 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:15:58.616436 31785 net.cpp:125] pool2 needs backward computation.
I0905 17:15:58.616444 31785 net.cpp:66] Creating Layer fc7
I0905 17:15:58.616451 31785 net.cpp:329] fc7 <- pool2
I0905 17:15:58.616457 31785 net.cpp:290] fc7 -> fc7
I0905 17:15:59.245034 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.245076 31785 net.cpp:125] fc7 needs backward computation.
I0905 17:15:59.245090 31785 net.cpp:66] Creating Layer relu7
I0905 17:15:59.245096 31785 net.cpp:329] relu7 <- fc7
I0905 17:15:59.245106 31785 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:15:59.245116 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.245121 31785 net.cpp:125] relu7 needs backward computation.
I0905 17:15:59.245129 31785 net.cpp:66] Creating Layer drop7
I0905 17:15:59.245134 31785 net.cpp:329] drop7 <- fc7
I0905 17:15:59.245141 31785 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:15:59.245165 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.245172 31785 net.cpp:125] drop7 needs backward computation.
I0905 17:15:59.245180 31785 net.cpp:66] Creating Layer fc8
I0905 17:15:59.245187 31785 net.cpp:329] fc8 <- fc7
I0905 17:15:59.245195 31785 net.cpp:290] fc8 -> fc8
I0905 17:15:59.252962 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.252975 31785 net.cpp:125] fc8 needs backward computation.
I0905 17:15:59.252982 31785 net.cpp:66] Creating Layer relu8
I0905 17:15:59.252989 31785 net.cpp:329] relu8 <- fc8
I0905 17:15:59.252996 31785 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:15:59.253003 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.253010 31785 net.cpp:125] relu8 needs backward computation.
I0905 17:15:59.253016 31785 net.cpp:66] Creating Layer drop8
I0905 17:15:59.253021 31785 net.cpp:329] drop8 <- fc8
I0905 17:15:59.253028 31785 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:15:59.253034 31785 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:15:59.253041 31785 net.cpp:125] drop8 needs backward computation.
I0905 17:15:59.253049 31785 net.cpp:66] Creating Layer fc9
I0905 17:15:59.253056 31785 net.cpp:329] fc9 <- fc8
I0905 17:15:59.253062 31785 net.cpp:290] fc9 -> fc9
I0905 17:15:59.253435 31785 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:15:59.253448 31785 net.cpp:125] fc9 needs backward computation.
I0905 17:15:59.253456 31785 net.cpp:66] Creating Layer fc10
I0905 17:15:59.253461 31785 net.cpp:329] fc10 <- fc9
I0905 17:15:59.253470 31785 net.cpp:290] fc10 -> fc10
I0905 17:15:59.253482 31785 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:15:59.253490 31785 net.cpp:125] fc10 needs backward computation.
I0905 17:15:59.253497 31785 net.cpp:66] Creating Layer prob
I0905 17:15:59.253502 31785 net.cpp:329] prob <- fc10
I0905 17:15:59.253510 31785 net.cpp:290] prob -> prob
I0905 17:15:59.253520 31785 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:15:59.253526 31785 net.cpp:125] prob needs backward computation.
I0905 17:15:59.253531 31785 net.cpp:156] This network produces output prob
I0905 17:15:59.253543 31785 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:15:59.253552 31785 net.cpp:167] Network initialization done.
I0905 17:15:59.253557 31785 net.cpp:168] Memory required for data: 6183480
Classifying 544 inputs.
Done in 525.02 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 44 is out of bounds for axis 0 with size 44
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:25:31.674401 31804 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:25:31.674561 31804 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:25:31.674571 31804 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:25:31.674720 31804 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:25:31.674778 31804 net.cpp:292] Input 0 -> data
I0905 17:25:31.674804 31804 net.cpp:66] Creating Layer conv1
I0905 17:25:31.674811 31804 net.cpp:329] conv1 <- data
I0905 17:25:31.674819 31804 net.cpp:290] conv1 -> conv1
I0905 17:25:31.699758 31804 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:25:31.699784 31804 net.cpp:125] conv1 needs backward computation.
I0905 17:25:31.699794 31804 net.cpp:66] Creating Layer relu1
I0905 17:25:31.699800 31804 net.cpp:329] relu1 <- conv1
I0905 17:25:31.699807 31804 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:25:31.699815 31804 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:25:31.699821 31804 net.cpp:125] relu1 needs backward computation.
I0905 17:25:31.699828 31804 net.cpp:66] Creating Layer pool1
I0905 17:25:31.699834 31804 net.cpp:329] pool1 <- conv1
I0905 17:25:31.699841 31804 net.cpp:290] pool1 -> pool1
I0905 17:25:31.699852 31804 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:25:31.699858 31804 net.cpp:125] pool1 needs backward computation.
I0905 17:25:31.699865 31804 net.cpp:66] Creating Layer norm1
I0905 17:25:31.699870 31804 net.cpp:329] norm1 <- pool1
I0905 17:25:31.699877 31804 net.cpp:290] norm1 -> norm1
I0905 17:25:31.699887 31804 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:25:31.699892 31804 net.cpp:125] norm1 needs backward computation.
I0905 17:25:31.699900 31804 net.cpp:66] Creating Layer conv2
I0905 17:25:31.699906 31804 net.cpp:329] conv2 <- norm1
I0905 17:25:31.699913 31804 net.cpp:290] conv2 -> conv2
I0905 17:25:31.709025 31804 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:25:31.709041 31804 net.cpp:125] conv2 needs backward computation.
I0905 17:25:31.709048 31804 net.cpp:66] Creating Layer relu2
I0905 17:25:31.709054 31804 net.cpp:329] relu2 <- conv2
I0905 17:25:31.709061 31804 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:25:31.709074 31804 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:25:31.709079 31804 net.cpp:125] relu2 needs backward computation.
I0905 17:25:31.709086 31804 net.cpp:66] Creating Layer pool2
I0905 17:25:31.709092 31804 net.cpp:329] pool2 <- conv2
I0905 17:25:31.709099 31804 net.cpp:290] pool2 -> pool2
I0905 17:25:31.709106 31804 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:25:31.709112 31804 net.cpp:125] pool2 needs backward computation.
I0905 17:25:31.709121 31804 net.cpp:66] Creating Layer fc7
I0905 17:25:31.709127 31804 net.cpp:329] fc7 <- pool2
I0905 17:25:31.709134 31804 net.cpp:290] fc7 -> fc7
I0905 17:25:32.341789 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.341840 31804 net.cpp:125] fc7 needs backward computation.
I0905 17:25:32.341855 31804 net.cpp:66] Creating Layer relu7
I0905 17:25:32.341866 31804 net.cpp:329] relu7 <- fc7
I0905 17:25:32.341876 31804 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:25:32.341887 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.341892 31804 net.cpp:125] relu7 needs backward computation.
I0905 17:25:32.341902 31804 net.cpp:66] Creating Layer drop7
I0905 17:25:32.341907 31804 net.cpp:329] drop7 <- fc7
I0905 17:25:32.341913 31804 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:25:32.341924 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.341929 31804 net.cpp:125] drop7 needs backward computation.
I0905 17:25:32.341938 31804 net.cpp:66] Creating Layer fc8
I0905 17:25:32.341943 31804 net.cpp:329] fc8 <- fc7
I0905 17:25:32.341953 31804 net.cpp:290] fc8 -> fc8
I0905 17:25:32.349758 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.349771 31804 net.cpp:125] fc8 needs backward computation.
I0905 17:25:32.349778 31804 net.cpp:66] Creating Layer relu8
I0905 17:25:32.349784 31804 net.cpp:329] relu8 <- fc8
I0905 17:25:32.349792 31804 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:25:32.349799 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.349805 31804 net.cpp:125] relu8 needs backward computation.
I0905 17:25:32.349812 31804 net.cpp:66] Creating Layer drop8
I0905 17:25:32.349817 31804 net.cpp:329] drop8 <- fc8
I0905 17:25:32.349824 31804 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:25:32.349831 31804 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:25:32.349836 31804 net.cpp:125] drop8 needs backward computation.
I0905 17:25:32.349845 31804 net.cpp:66] Creating Layer fc9
I0905 17:25:32.349851 31804 net.cpp:329] fc9 <- fc8
I0905 17:25:32.349858 31804 net.cpp:290] fc9 -> fc9
I0905 17:25:32.350230 31804 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:25:32.350242 31804 net.cpp:125] fc9 needs backward computation.
I0905 17:25:32.350250 31804 net.cpp:66] Creating Layer fc10
I0905 17:25:32.350256 31804 net.cpp:329] fc10 <- fc9
I0905 17:25:32.350265 31804 net.cpp:290] fc10 -> fc10
I0905 17:25:32.350276 31804 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:25:32.350285 31804 net.cpp:125] fc10 needs backward computation.
I0905 17:25:32.350291 31804 net.cpp:66] Creating Layer prob
I0905 17:25:32.350296 31804 net.cpp:329] prob <- fc10
I0905 17:25:32.350306 31804 net.cpp:290] prob -> prob
I0905 17:25:32.350314 31804 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:25:32.350321 31804 net.cpp:125] prob needs backward computation.
I0905 17:25:32.350327 31804 net.cpp:156] This network produces output prob
I0905 17:25:32.350338 31804 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:25:32.350347 31804 net.cpp:167] Network initialization done.
I0905 17:25:32.350353 31804 net.cpp:168] Memory required for data: 6183480
Classifying 78 inputs.
Done in 48.30 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:26:26.181818 31808 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:26:26.181953 31808 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:26:26.181962 31808 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:26:26.182117 31808 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:26:26.182168 31808 net.cpp:292] Input 0 -> data
I0905 17:26:26.182193 31808 net.cpp:66] Creating Layer conv1
I0905 17:26:26.182200 31808 net.cpp:329] conv1 <- data
I0905 17:26:26.182209 31808 net.cpp:290] conv1 -> conv1
I0905 17:26:26.183529 31808 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:26:26.183547 31808 net.cpp:125] conv1 needs backward computation.
I0905 17:26:26.183555 31808 net.cpp:66] Creating Layer relu1
I0905 17:26:26.183562 31808 net.cpp:329] relu1 <- conv1
I0905 17:26:26.183568 31808 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:26:26.183576 31808 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:26:26.183583 31808 net.cpp:125] relu1 needs backward computation.
I0905 17:26:26.183588 31808 net.cpp:66] Creating Layer pool1
I0905 17:26:26.183598 31808 net.cpp:329] pool1 <- conv1
I0905 17:26:26.183605 31808 net.cpp:290] pool1 -> pool1
I0905 17:26:26.183616 31808 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:26:26.183622 31808 net.cpp:125] pool1 needs backward computation.
I0905 17:26:26.183629 31808 net.cpp:66] Creating Layer norm1
I0905 17:26:26.183634 31808 net.cpp:329] norm1 <- pool1
I0905 17:26:26.183640 31808 net.cpp:290] norm1 -> norm1
I0905 17:26:26.183650 31808 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:26:26.183655 31808 net.cpp:125] norm1 needs backward computation.
I0905 17:26:26.183662 31808 net.cpp:66] Creating Layer conv2
I0905 17:26:26.183668 31808 net.cpp:329] conv2 <- norm1
I0905 17:26:26.183676 31808 net.cpp:290] conv2 -> conv2
I0905 17:26:26.192584 31808 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:26:26.192597 31808 net.cpp:125] conv2 needs backward computation.
I0905 17:26:26.192605 31808 net.cpp:66] Creating Layer relu2
I0905 17:26:26.192610 31808 net.cpp:329] relu2 <- conv2
I0905 17:26:26.192616 31808 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:26:26.192623 31808 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:26:26.192630 31808 net.cpp:125] relu2 needs backward computation.
I0905 17:26:26.192637 31808 net.cpp:66] Creating Layer pool2
I0905 17:26:26.192643 31808 net.cpp:329] pool2 <- conv2
I0905 17:26:26.192649 31808 net.cpp:290] pool2 -> pool2
I0905 17:26:26.192657 31808 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:26:26.192663 31808 net.cpp:125] pool2 needs backward computation.
I0905 17:26:26.192670 31808 net.cpp:66] Creating Layer fc7
I0905 17:26:26.192675 31808 net.cpp:329] fc7 <- pool2
I0905 17:26:26.192682 31808 net.cpp:290] fc7 -> fc7
I0905 17:26:26.852005 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.852051 31808 net.cpp:125] fc7 needs backward computation.
I0905 17:26:26.852064 31808 net.cpp:66] Creating Layer relu7
I0905 17:26:26.852072 31808 net.cpp:329] relu7 <- fc7
I0905 17:26:26.852082 31808 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:26:26.852092 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.852097 31808 net.cpp:125] relu7 needs backward computation.
I0905 17:26:26.852104 31808 net.cpp:66] Creating Layer drop7
I0905 17:26:26.852109 31808 net.cpp:329] drop7 <- fc7
I0905 17:26:26.852116 31808 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:26:26.852128 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.852133 31808 net.cpp:125] drop7 needs backward computation.
I0905 17:26:26.852141 31808 net.cpp:66] Creating Layer fc8
I0905 17:26:26.852147 31808 net.cpp:329] fc8 <- fc7
I0905 17:26:26.852156 31808 net.cpp:290] fc8 -> fc8
I0905 17:26:26.860283 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.860296 31808 net.cpp:125] fc8 needs backward computation.
I0905 17:26:26.860302 31808 net.cpp:66] Creating Layer relu8
I0905 17:26:26.860308 31808 net.cpp:329] relu8 <- fc8
I0905 17:26:26.860316 31808 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:26:26.860323 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.860328 31808 net.cpp:125] relu8 needs backward computation.
I0905 17:26:26.860335 31808 net.cpp:66] Creating Layer drop8
I0905 17:26:26.860340 31808 net.cpp:329] drop8 <- fc8
I0905 17:26:26.860347 31808 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:26:26.860353 31808 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:26:26.860358 31808 net.cpp:125] drop8 needs backward computation.
I0905 17:26:26.860368 31808 net.cpp:66] Creating Layer fc9
I0905 17:26:26.860373 31808 net.cpp:329] fc9 <- fc8
I0905 17:26:26.860379 31808 net.cpp:290] fc9 -> fc9
I0905 17:26:26.860757 31808 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:26:26.860769 31808 net.cpp:125] fc9 needs backward computation.
I0905 17:26:26.860777 31808 net.cpp:66] Creating Layer fc10
I0905 17:26:26.860782 31808 net.cpp:329] fc10 <- fc9
I0905 17:26:26.860791 31808 net.cpp:290] fc10 -> fc10
I0905 17:26:26.860802 31808 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:26:26.860810 31808 net.cpp:125] fc10 needs backward computation.
I0905 17:26:26.860817 31808 net.cpp:66] Creating Layer prob
I0905 17:26:26.860832 31808 net.cpp:329] prob <- fc10
I0905 17:26:26.860841 31808 net.cpp:290] prob -> prob
I0905 17:26:26.860849 31808 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:26:26.860855 31808 net.cpp:125] prob needs backward computation.
I0905 17:26:26.860860 31808 net.cpp:156] This network produces output prob
I0905 17:26:26.860872 31808 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:26:26.860880 31808 net.cpp:167] Network initialization done.
I0905 17:26:26.860885 31808 net.cpp:168] Memory required for data: 6183480
Classifying 240 inputs.
Done in 139.57 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 40 is out of bounds for axis 0 with size 40
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:28:50.706029 31815 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:28:50.706166 31815 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:28:50.706176 31815 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:28:50.706318 31815 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:28:50.706382 31815 net.cpp:292] Input 0 -> data
I0905 17:28:50.706408 31815 net.cpp:66] Creating Layer conv1
I0905 17:28:50.706415 31815 net.cpp:329] conv1 <- data
I0905 17:28:50.706423 31815 net.cpp:290] conv1 -> conv1
I0905 17:28:50.707747 31815 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:28:50.707764 31815 net.cpp:125] conv1 needs backward computation.
I0905 17:28:50.707773 31815 net.cpp:66] Creating Layer relu1
I0905 17:28:50.707779 31815 net.cpp:329] relu1 <- conv1
I0905 17:28:50.707787 31815 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:28:50.707794 31815 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:28:50.707800 31815 net.cpp:125] relu1 needs backward computation.
I0905 17:28:50.707808 31815 net.cpp:66] Creating Layer pool1
I0905 17:28:50.707813 31815 net.cpp:329] pool1 <- conv1
I0905 17:28:50.707819 31815 net.cpp:290] pool1 -> pool1
I0905 17:28:50.707829 31815 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:28:50.707835 31815 net.cpp:125] pool1 needs backward computation.
I0905 17:28:50.707842 31815 net.cpp:66] Creating Layer norm1
I0905 17:28:50.707847 31815 net.cpp:329] norm1 <- pool1
I0905 17:28:50.707854 31815 net.cpp:290] norm1 -> norm1
I0905 17:28:50.707864 31815 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:28:50.707870 31815 net.cpp:125] norm1 needs backward computation.
I0905 17:28:50.707876 31815 net.cpp:66] Creating Layer conv2
I0905 17:28:50.707882 31815 net.cpp:329] conv2 <- norm1
I0905 17:28:50.707890 31815 net.cpp:290] conv2 -> conv2
I0905 17:28:50.716815 31815 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:28:50.716830 31815 net.cpp:125] conv2 needs backward computation.
I0905 17:28:50.716837 31815 net.cpp:66] Creating Layer relu2
I0905 17:28:50.716843 31815 net.cpp:329] relu2 <- conv2
I0905 17:28:50.716850 31815 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:28:50.716856 31815 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:28:50.716862 31815 net.cpp:125] relu2 needs backward computation.
I0905 17:28:50.716868 31815 net.cpp:66] Creating Layer pool2
I0905 17:28:50.716873 31815 net.cpp:329] pool2 <- conv2
I0905 17:28:50.716881 31815 net.cpp:290] pool2 -> pool2
I0905 17:28:50.716888 31815 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:28:50.716893 31815 net.cpp:125] pool2 needs backward computation.
I0905 17:28:50.716902 31815 net.cpp:66] Creating Layer fc7
I0905 17:28:50.716908 31815 net.cpp:329] fc7 <- pool2
I0905 17:28:50.716915 31815 net.cpp:290] fc7 -> fc7
I0905 17:28:51.345965 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.346012 31815 net.cpp:125] fc7 needs backward computation.
I0905 17:28:51.346025 31815 net.cpp:66] Creating Layer relu7
I0905 17:28:51.346034 31815 net.cpp:329] relu7 <- fc7
I0905 17:28:51.346042 31815 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:28:51.346053 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.346060 31815 net.cpp:125] relu7 needs backward computation.
I0905 17:28:51.346066 31815 net.cpp:66] Creating Layer drop7
I0905 17:28:51.346072 31815 net.cpp:329] drop7 <- fc7
I0905 17:28:51.346079 31815 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:28:51.346091 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.346096 31815 net.cpp:125] drop7 needs backward computation.
I0905 17:28:51.346106 31815 net.cpp:66] Creating Layer fc8
I0905 17:28:51.346110 31815 net.cpp:329] fc8 <- fc7
I0905 17:28:51.346119 31815 net.cpp:290] fc8 -> fc8
I0905 17:28:51.353901 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.353914 31815 net.cpp:125] fc8 needs backward computation.
I0905 17:28:51.353930 31815 net.cpp:66] Creating Layer relu8
I0905 17:28:51.353936 31815 net.cpp:329] relu8 <- fc8
I0905 17:28:51.353945 31815 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:28:51.353952 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.353958 31815 net.cpp:125] relu8 needs backward computation.
I0905 17:28:51.353965 31815 net.cpp:66] Creating Layer drop8
I0905 17:28:51.353971 31815 net.cpp:329] drop8 <- fc8
I0905 17:28:51.353976 31815 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:28:51.353983 31815 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:28:51.353989 31815 net.cpp:125] drop8 needs backward computation.
I0905 17:28:51.353998 31815 net.cpp:66] Creating Layer fc9
I0905 17:28:51.354003 31815 net.cpp:329] fc9 <- fc8
I0905 17:28:51.354010 31815 net.cpp:290] fc9 -> fc9
I0905 17:28:51.354382 31815 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:28:51.354393 31815 net.cpp:125] fc9 needs backward computation.
I0905 17:28:51.354401 31815 net.cpp:66] Creating Layer fc10
I0905 17:28:51.354408 31815 net.cpp:329] fc10 <- fc9
I0905 17:28:51.354415 31815 net.cpp:290] fc10 -> fc10
I0905 17:28:51.354426 31815 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:28:51.354434 31815 net.cpp:125] fc10 needs backward computation.
I0905 17:28:51.354440 31815 net.cpp:66] Creating Layer prob
I0905 17:28:51.354446 31815 net.cpp:329] prob <- fc10
I0905 17:28:51.354454 31815 net.cpp:290] prob -> prob
I0905 17:28:51.354464 31815 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:28:51.354468 31815 net.cpp:125] prob needs backward computation.
I0905 17:28:51.354473 31815 net.cpp:156] This network produces output prob
I0905 17:28:51.354486 31815 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:28:51.354495 31815 net.cpp:167] Network initialization done.
I0905 17:28:51.354500 31815 net.cpp:168] Memory required for data: 6183480
Classifying 366 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 134, in main
    np.concatenate((predictions, classifier.predict(inputs[reminder + i * 100 : reminder + (i + 1) * 100], not args.center_only)))
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 83, in predict
    dtype=np.float32)
MemoryError
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:33:08.531769 31824 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:33:08.531918 31824 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:33:08.531926 31824 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:33:08.532071 31824 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:33:08.532130 31824 net.cpp:292] Input 0 -> data
I0905 17:33:08.532156 31824 net.cpp:66] Creating Layer conv1
I0905 17:33:08.532165 31824 net.cpp:329] conv1 <- data
I0905 17:33:08.532172 31824 net.cpp:290] conv1 -> conv1
I0905 17:33:08.557095 31824 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:33:08.557119 31824 net.cpp:125] conv1 needs backward computation.
I0905 17:33:08.557129 31824 net.cpp:66] Creating Layer relu1
I0905 17:33:08.557135 31824 net.cpp:329] relu1 <- conv1
I0905 17:33:08.557142 31824 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:33:08.557152 31824 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:33:08.557157 31824 net.cpp:125] relu1 needs backward computation.
I0905 17:33:08.557164 31824 net.cpp:66] Creating Layer pool1
I0905 17:33:08.557169 31824 net.cpp:329] pool1 <- conv1
I0905 17:33:08.557176 31824 net.cpp:290] pool1 -> pool1
I0905 17:33:08.557188 31824 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:33:08.557193 31824 net.cpp:125] pool1 needs backward computation.
I0905 17:33:08.557199 31824 net.cpp:66] Creating Layer norm1
I0905 17:33:08.557205 31824 net.cpp:329] norm1 <- pool1
I0905 17:33:08.557212 31824 net.cpp:290] norm1 -> norm1
I0905 17:33:08.557221 31824 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:33:08.557227 31824 net.cpp:125] norm1 needs backward computation.
I0905 17:33:08.557235 31824 net.cpp:66] Creating Layer conv2
I0905 17:33:08.557240 31824 net.cpp:329] conv2 <- norm1
I0905 17:33:08.557247 31824 net.cpp:290] conv2 -> conv2
I0905 17:33:08.566135 31824 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:33:08.566153 31824 net.cpp:125] conv2 needs backward computation.
I0905 17:33:08.566159 31824 net.cpp:66] Creating Layer relu2
I0905 17:33:08.566165 31824 net.cpp:329] relu2 <- conv2
I0905 17:33:08.566172 31824 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:33:08.566179 31824 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:33:08.566184 31824 net.cpp:125] relu2 needs backward computation.
I0905 17:33:08.566190 31824 net.cpp:66] Creating Layer pool2
I0905 17:33:08.566202 31824 net.cpp:329] pool2 <- conv2
I0905 17:33:08.566210 31824 net.cpp:290] pool2 -> pool2
I0905 17:33:08.566218 31824 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:33:08.566223 31824 net.cpp:125] pool2 needs backward computation.
I0905 17:33:08.566233 31824 net.cpp:66] Creating Layer fc7
I0905 17:33:08.566238 31824 net.cpp:329] fc7 <- pool2
I0905 17:33:08.566246 31824 net.cpp:290] fc7 -> fc7
I0905 17:33:09.193904 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.193951 31824 net.cpp:125] fc7 needs backward computation.
I0905 17:33:09.193964 31824 net.cpp:66] Creating Layer relu7
I0905 17:33:09.193971 31824 net.cpp:329] relu7 <- fc7
I0905 17:33:09.193981 31824 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:33:09.193991 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.193997 31824 net.cpp:125] relu7 needs backward computation.
I0905 17:33:09.194005 31824 net.cpp:66] Creating Layer drop7
I0905 17:33:09.194010 31824 net.cpp:329] drop7 <- fc7
I0905 17:33:09.194016 31824 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:33:09.194027 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.194033 31824 net.cpp:125] drop7 needs backward computation.
I0905 17:33:09.194042 31824 net.cpp:66] Creating Layer fc8
I0905 17:33:09.194047 31824 net.cpp:329] fc8 <- fc7
I0905 17:33:09.194056 31824 net.cpp:290] fc8 -> fc8
I0905 17:33:09.201614 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.201627 31824 net.cpp:125] fc8 needs backward computation.
I0905 17:33:09.201633 31824 net.cpp:66] Creating Layer relu8
I0905 17:33:09.201639 31824 net.cpp:329] relu8 <- fc8
I0905 17:33:09.201647 31824 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:33:09.201654 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.201659 31824 net.cpp:125] relu8 needs backward computation.
I0905 17:33:09.201666 31824 net.cpp:66] Creating Layer drop8
I0905 17:33:09.201671 31824 net.cpp:329] drop8 <- fc8
I0905 17:33:09.201678 31824 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:33:09.201685 31824 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:09.201690 31824 net.cpp:125] drop8 needs backward computation.
I0905 17:33:09.201699 31824 net.cpp:66] Creating Layer fc9
I0905 17:33:09.201705 31824 net.cpp:329] fc9 <- fc8
I0905 17:33:09.201712 31824 net.cpp:290] fc9 -> fc9
I0905 17:33:09.202075 31824 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:33:09.202087 31824 net.cpp:125] fc9 needs backward computation.
I0905 17:33:09.202095 31824 net.cpp:66] Creating Layer fc10
I0905 17:33:09.202101 31824 net.cpp:329] fc10 <- fc9
I0905 17:33:09.202110 31824 net.cpp:290] fc10 -> fc10
I0905 17:33:09.202121 31824 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:33:09.202129 31824 net.cpp:125] fc10 needs backward computation.
I0905 17:33:09.202136 31824 net.cpp:66] Creating Layer prob
I0905 17:33:09.202142 31824 net.cpp:329] prob <- fc10
I0905 17:33:09.202149 31824 net.cpp:290] prob -> prob
I0905 17:33:09.202159 31824 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:33:09.202165 31824 net.cpp:125] prob needs backward computation.
I0905 17:33:09.202170 31824 net.cpp:156] This network produces output prob
I0905 17:33:09.202183 31824 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:33:09.202191 31824 net.cpp:167] Network initialization done.
I0905 17:33:09.202196 31824 net.cpp:168] Memory required for data: 6183480
Classifying 22 inputs.
Done in 13.95 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:33:26.688029 31827 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:33:26.688177 31827 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:33:26.688187 31827 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:33:26.688339 31827 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:33:26.688400 31827 net.cpp:292] Input 0 -> data
I0905 17:33:26.688427 31827 net.cpp:66] Creating Layer conv1
I0905 17:33:26.688433 31827 net.cpp:329] conv1 <- data
I0905 17:33:26.688441 31827 net.cpp:290] conv1 -> conv1
I0905 17:33:26.689798 31827 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:33:26.689816 31827 net.cpp:125] conv1 needs backward computation.
I0905 17:33:26.689826 31827 net.cpp:66] Creating Layer relu1
I0905 17:33:26.689831 31827 net.cpp:329] relu1 <- conv1
I0905 17:33:26.689838 31827 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:33:26.689846 31827 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:33:26.689852 31827 net.cpp:125] relu1 needs backward computation.
I0905 17:33:26.689859 31827 net.cpp:66] Creating Layer pool1
I0905 17:33:26.689864 31827 net.cpp:329] pool1 <- conv1
I0905 17:33:26.689872 31827 net.cpp:290] pool1 -> pool1
I0905 17:33:26.689882 31827 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:33:26.689888 31827 net.cpp:125] pool1 needs backward computation.
I0905 17:33:26.689900 31827 net.cpp:66] Creating Layer norm1
I0905 17:33:26.689906 31827 net.cpp:329] norm1 <- pool1
I0905 17:33:26.689913 31827 net.cpp:290] norm1 -> norm1
I0905 17:33:26.689923 31827 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:33:26.689929 31827 net.cpp:125] norm1 needs backward computation.
I0905 17:33:26.689936 31827 net.cpp:66] Creating Layer conv2
I0905 17:33:26.689941 31827 net.cpp:329] conv2 <- norm1
I0905 17:33:26.689949 31827 net.cpp:290] conv2 -> conv2
I0905 17:33:26.698851 31827 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:33:26.698865 31827 net.cpp:125] conv2 needs backward computation.
I0905 17:33:26.698873 31827 net.cpp:66] Creating Layer relu2
I0905 17:33:26.698878 31827 net.cpp:329] relu2 <- conv2
I0905 17:33:26.698884 31827 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:33:26.698891 31827 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:33:26.698897 31827 net.cpp:125] relu2 needs backward computation.
I0905 17:33:26.698904 31827 net.cpp:66] Creating Layer pool2
I0905 17:33:26.698909 31827 net.cpp:329] pool2 <- conv2
I0905 17:33:26.698915 31827 net.cpp:290] pool2 -> pool2
I0905 17:33:26.698922 31827 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:33:26.698928 31827 net.cpp:125] pool2 needs backward computation.
I0905 17:33:26.698937 31827 net.cpp:66] Creating Layer fc7
I0905 17:33:26.698943 31827 net.cpp:329] fc7 <- pool2
I0905 17:33:26.698951 31827 net.cpp:290] fc7 -> fc7
I0905 17:33:27.332007 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.332056 31827 net.cpp:125] fc7 needs backward computation.
I0905 17:33:27.332070 31827 net.cpp:66] Creating Layer relu7
I0905 17:33:27.332077 31827 net.cpp:329] relu7 <- fc7
I0905 17:33:27.332087 31827 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:33:27.332098 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.332103 31827 net.cpp:125] relu7 needs backward computation.
I0905 17:33:27.332110 31827 net.cpp:66] Creating Layer drop7
I0905 17:33:27.332116 31827 net.cpp:329] drop7 <- fc7
I0905 17:33:27.332123 31827 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:33:27.332134 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.332140 31827 net.cpp:125] drop7 needs backward computation.
I0905 17:33:27.332149 31827 net.cpp:66] Creating Layer fc8
I0905 17:33:27.332154 31827 net.cpp:329] fc8 <- fc7
I0905 17:33:27.332164 31827 net.cpp:290] fc8 -> fc8
I0905 17:33:27.339931 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.339943 31827 net.cpp:125] fc8 needs backward computation.
I0905 17:33:27.339951 31827 net.cpp:66] Creating Layer relu8
I0905 17:33:27.339956 31827 net.cpp:329] relu8 <- fc8
I0905 17:33:27.339967 31827 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:33:27.339974 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.339979 31827 net.cpp:125] relu8 needs backward computation.
I0905 17:33:27.339987 31827 net.cpp:66] Creating Layer drop8
I0905 17:33:27.339993 31827 net.cpp:329] drop8 <- fc8
I0905 17:33:27.339998 31827 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:33:27.340005 31827 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:33:27.340011 31827 net.cpp:125] drop8 needs backward computation.
I0905 17:33:27.340019 31827 net.cpp:66] Creating Layer fc9
I0905 17:33:27.340025 31827 net.cpp:329] fc9 <- fc8
I0905 17:33:27.340033 31827 net.cpp:290] fc9 -> fc9
I0905 17:33:27.340405 31827 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:33:27.340417 31827 net.cpp:125] fc9 needs backward computation.
I0905 17:33:27.340425 31827 net.cpp:66] Creating Layer fc10
I0905 17:33:27.340431 31827 net.cpp:329] fc10 <- fc9
I0905 17:33:27.340440 31827 net.cpp:290] fc10 -> fc10
I0905 17:33:27.340451 31827 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:33:27.340459 31827 net.cpp:125] fc10 needs backward computation.
I0905 17:33:27.340466 31827 net.cpp:66] Creating Layer prob
I0905 17:33:27.340472 31827 net.cpp:329] prob <- fc10
I0905 17:33:27.340479 31827 net.cpp:290] prob -> prob
I0905 17:33:27.340489 31827 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:33:27.340495 31827 net.cpp:125] prob needs backward computation.
I0905 17:33:27.340510 31827 net.cpp:156] This network produces output prob
I0905 17:33:27.340523 31827 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:33:27.340533 31827 net.cpp:167] Network initialization done.
I0905 17:33:27.340538 31827 net.cpp:168] Memory required for data: 6183480
Classifying 308 inputs.
Done in 317.68 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 8 is out of bounds for axis 0 with size 8
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:39:15.351290 31851 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:39:15.351438 31851 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:39:15.351445 31851 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:39:15.351588 31851 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:39:15.351647 31851 net.cpp:292] Input 0 -> data
I0905 17:39:15.351672 31851 net.cpp:66] Creating Layer conv1
I0905 17:39:15.351680 31851 net.cpp:329] conv1 <- data
I0905 17:39:15.351687 31851 net.cpp:290] conv1 -> conv1
I0905 17:39:15.376541 31851 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:39:15.376566 31851 net.cpp:125] conv1 needs backward computation.
I0905 17:39:15.376575 31851 net.cpp:66] Creating Layer relu1
I0905 17:39:15.376581 31851 net.cpp:329] relu1 <- conv1
I0905 17:39:15.376588 31851 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:39:15.376597 31851 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:39:15.376602 31851 net.cpp:125] relu1 needs backward computation.
I0905 17:39:15.376610 31851 net.cpp:66] Creating Layer pool1
I0905 17:39:15.376616 31851 net.cpp:329] pool1 <- conv1
I0905 17:39:15.376621 31851 net.cpp:290] pool1 -> pool1
I0905 17:39:15.376632 31851 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:39:15.376638 31851 net.cpp:125] pool1 needs backward computation.
I0905 17:39:15.376644 31851 net.cpp:66] Creating Layer norm1
I0905 17:39:15.376651 31851 net.cpp:329] norm1 <- pool1
I0905 17:39:15.376657 31851 net.cpp:290] norm1 -> norm1
I0905 17:39:15.376665 31851 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:39:15.376672 31851 net.cpp:125] norm1 needs backward computation.
I0905 17:39:15.376678 31851 net.cpp:66] Creating Layer conv2
I0905 17:39:15.376684 31851 net.cpp:329] conv2 <- norm1
I0905 17:39:15.376691 31851 net.cpp:290] conv2 -> conv2
I0905 17:39:15.385527 31851 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:39:15.385541 31851 net.cpp:125] conv2 needs backward computation.
I0905 17:39:15.385548 31851 net.cpp:66] Creating Layer relu2
I0905 17:39:15.385553 31851 net.cpp:329] relu2 <- conv2
I0905 17:39:15.385560 31851 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:39:15.385567 31851 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:39:15.385572 31851 net.cpp:125] relu2 needs backward computation.
I0905 17:39:15.385581 31851 net.cpp:66] Creating Layer pool2
I0905 17:39:15.385587 31851 net.cpp:329] pool2 <- conv2
I0905 17:39:15.385594 31851 net.cpp:290] pool2 -> pool2
I0905 17:39:15.385602 31851 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:39:15.385607 31851 net.cpp:125] pool2 needs backward computation.
I0905 17:39:15.385617 31851 net.cpp:66] Creating Layer fc7
I0905 17:39:15.385622 31851 net.cpp:329] fc7 <- pool2
I0905 17:39:15.385628 31851 net.cpp:290] fc7 -> fc7
I0905 17:39:16.013674 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.013718 31851 net.cpp:125] fc7 needs backward computation.
I0905 17:39:16.013731 31851 net.cpp:66] Creating Layer relu7
I0905 17:39:16.013738 31851 net.cpp:329] relu7 <- fc7
I0905 17:39:16.013748 31851 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:39:16.013758 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.013764 31851 net.cpp:125] relu7 needs backward computation.
I0905 17:39:16.013772 31851 net.cpp:66] Creating Layer drop7
I0905 17:39:16.013777 31851 net.cpp:329] drop7 <- fc7
I0905 17:39:16.013783 31851 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:39:16.013794 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.013799 31851 net.cpp:125] drop7 needs backward computation.
I0905 17:39:16.013808 31851 net.cpp:66] Creating Layer fc8
I0905 17:39:16.013813 31851 net.cpp:329] fc8 <- fc7
I0905 17:39:16.013823 31851 net.cpp:290] fc8 -> fc8
I0905 17:39:16.021580 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.021592 31851 net.cpp:125] fc8 needs backward computation.
I0905 17:39:16.021600 31851 net.cpp:66] Creating Layer relu8
I0905 17:39:16.021605 31851 net.cpp:329] relu8 <- fc8
I0905 17:39:16.021613 31851 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:39:16.021620 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.021637 31851 net.cpp:125] relu8 needs backward computation.
I0905 17:39:16.021644 31851 net.cpp:66] Creating Layer drop8
I0905 17:39:16.021649 31851 net.cpp:329] drop8 <- fc8
I0905 17:39:16.021656 31851 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:39:16.021663 31851 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:16.021668 31851 net.cpp:125] drop8 needs backward computation.
I0905 17:39:16.021677 31851 net.cpp:66] Creating Layer fc9
I0905 17:39:16.021682 31851 net.cpp:329] fc9 <- fc8
I0905 17:39:16.021689 31851 net.cpp:290] fc9 -> fc9
I0905 17:39:16.022063 31851 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:39:16.022075 31851 net.cpp:125] fc9 needs backward computation.
I0905 17:39:16.022083 31851 net.cpp:66] Creating Layer fc10
I0905 17:39:16.022089 31851 net.cpp:329] fc10 <- fc9
I0905 17:39:16.022097 31851 net.cpp:290] fc10 -> fc10
I0905 17:39:16.022109 31851 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:39:16.022116 31851 net.cpp:125] fc10 needs backward computation.
I0905 17:39:16.022122 31851 net.cpp:66] Creating Layer prob
I0905 17:39:16.022128 31851 net.cpp:329] prob <- fc10
I0905 17:39:16.022136 31851 net.cpp:290] prob -> prob
I0905 17:39:16.022145 31851 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:39:16.022151 31851 net.cpp:125] prob needs backward computation.
I0905 17:39:16.022156 31851 net.cpp:156] This network produces output prob
I0905 17:39:16.022168 31851 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:39:16.022176 31851 net.cpp:167] Network initialization done.
I0905 17:39:16.022182 31851 net.cpp:168] Memory required for data: 6183480
Classifying 16 inputs.
Done in 9.73 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:39:27.978488 31855 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:39:27.978627 31855 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:39:27.978636 31855 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:39:27.978782 31855 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:39:27.978847 31855 net.cpp:292] Input 0 -> data
I0905 17:39:27.978871 31855 net.cpp:66] Creating Layer conv1
I0905 17:39:27.978878 31855 net.cpp:329] conv1 <- data
I0905 17:39:27.978886 31855 net.cpp:290] conv1 -> conv1
I0905 17:39:27.980245 31855 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:39:27.980263 31855 net.cpp:125] conv1 needs backward computation.
I0905 17:39:27.980273 31855 net.cpp:66] Creating Layer relu1
I0905 17:39:27.980278 31855 net.cpp:329] relu1 <- conv1
I0905 17:39:27.980284 31855 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:39:27.980293 31855 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:39:27.980299 31855 net.cpp:125] relu1 needs backward computation.
I0905 17:39:27.980306 31855 net.cpp:66] Creating Layer pool1
I0905 17:39:27.980311 31855 net.cpp:329] pool1 <- conv1
I0905 17:39:27.980319 31855 net.cpp:290] pool1 -> pool1
I0905 17:39:27.980329 31855 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:39:27.980335 31855 net.cpp:125] pool1 needs backward computation.
I0905 17:39:27.980342 31855 net.cpp:66] Creating Layer norm1
I0905 17:39:27.980347 31855 net.cpp:329] norm1 <- pool1
I0905 17:39:27.980355 31855 net.cpp:290] norm1 -> norm1
I0905 17:39:27.980363 31855 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:39:27.980370 31855 net.cpp:125] norm1 needs backward computation.
I0905 17:39:27.980376 31855 net.cpp:66] Creating Layer conv2
I0905 17:39:27.980382 31855 net.cpp:329] conv2 <- norm1
I0905 17:39:27.980389 31855 net.cpp:290] conv2 -> conv2
I0905 17:39:27.989476 31855 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:39:27.989491 31855 net.cpp:125] conv2 needs backward computation.
I0905 17:39:27.989498 31855 net.cpp:66] Creating Layer relu2
I0905 17:39:27.989505 31855 net.cpp:329] relu2 <- conv2
I0905 17:39:27.989511 31855 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:39:27.989517 31855 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:39:27.989523 31855 net.cpp:125] relu2 needs backward computation.
I0905 17:39:27.989533 31855 net.cpp:66] Creating Layer pool2
I0905 17:39:27.989539 31855 net.cpp:329] pool2 <- conv2
I0905 17:39:27.989545 31855 net.cpp:290] pool2 -> pool2
I0905 17:39:27.989553 31855 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:39:27.989559 31855 net.cpp:125] pool2 needs backward computation.
I0905 17:39:27.989567 31855 net.cpp:66] Creating Layer fc7
I0905 17:39:27.989572 31855 net.cpp:329] fc7 <- pool2
I0905 17:39:27.989586 31855 net.cpp:290] fc7 -> fc7
I0905 17:39:28.622294 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.622339 31855 net.cpp:125] fc7 needs backward computation.
I0905 17:39:28.622350 31855 net.cpp:66] Creating Layer relu7
I0905 17:39:28.622357 31855 net.cpp:329] relu7 <- fc7
I0905 17:39:28.622366 31855 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:39:28.622386 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.622392 31855 net.cpp:125] relu7 needs backward computation.
I0905 17:39:28.622400 31855 net.cpp:66] Creating Layer drop7
I0905 17:39:28.622406 31855 net.cpp:329] drop7 <- fc7
I0905 17:39:28.622411 31855 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:39:28.622421 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.622427 31855 net.cpp:125] drop7 needs backward computation.
I0905 17:39:28.622436 31855 net.cpp:66] Creating Layer fc8
I0905 17:39:28.622441 31855 net.cpp:329] fc8 <- fc7
I0905 17:39:28.622449 31855 net.cpp:290] fc8 -> fc8
I0905 17:39:28.630005 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.630018 31855 net.cpp:125] fc8 needs backward computation.
I0905 17:39:28.630024 31855 net.cpp:66] Creating Layer relu8
I0905 17:39:28.630030 31855 net.cpp:329] relu8 <- fc8
I0905 17:39:28.630038 31855 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:39:28.630044 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.630050 31855 net.cpp:125] relu8 needs backward computation.
I0905 17:39:28.630056 31855 net.cpp:66] Creating Layer drop8
I0905 17:39:28.630061 31855 net.cpp:329] drop8 <- fc8
I0905 17:39:28.630067 31855 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:39:28.630074 31855 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:39:28.630079 31855 net.cpp:125] drop8 needs backward computation.
I0905 17:39:28.630089 31855 net.cpp:66] Creating Layer fc9
I0905 17:39:28.630094 31855 net.cpp:329] fc9 <- fc8
I0905 17:39:28.630100 31855 net.cpp:290] fc9 -> fc9
I0905 17:39:28.630463 31855 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:39:28.630475 31855 net.cpp:125] fc9 needs backward computation.
I0905 17:39:28.630482 31855 net.cpp:66] Creating Layer fc10
I0905 17:39:28.630487 31855 net.cpp:329] fc10 <- fc9
I0905 17:39:28.630496 31855 net.cpp:290] fc10 -> fc10
I0905 17:39:28.630506 31855 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:39:28.630514 31855 net.cpp:125] fc10 needs backward computation.
I0905 17:39:28.630520 31855 net.cpp:66] Creating Layer prob
I0905 17:39:28.630527 31855 net.cpp:329] prob <- fc10
I0905 17:39:28.630533 31855 net.cpp:290] prob -> prob
I0905 17:39:28.630542 31855 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:39:28.630548 31855 net.cpp:125] prob needs backward computation.
I0905 17:39:28.630553 31855 net.cpp:156] This network produces output prob
I0905 17:39:28.630565 31855 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:39:28.630573 31855 net.cpp:167] Network initialization done.
I0905 17:39:28.630578 31855 net.cpp:168] Memory required for data: 6183480
Classifying 305 inputs.
Done in 183.15 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 5 is out of bounds for axis 0 with size 5
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:42:39.588510 31869 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:42:39.588644 31869 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:42:39.588654 31869 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:42:39.588794 31869 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:42:39.588856 31869 net.cpp:292] Input 0 -> data
I0905 17:42:39.588882 31869 net.cpp:66] Creating Layer conv1
I0905 17:42:39.588889 31869 net.cpp:329] conv1 <- data
I0905 17:42:39.588897 31869 net.cpp:290] conv1 -> conv1
I0905 17:42:39.590256 31869 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:42:39.590275 31869 net.cpp:125] conv1 needs backward computation.
I0905 17:42:39.590283 31869 net.cpp:66] Creating Layer relu1
I0905 17:42:39.590289 31869 net.cpp:329] relu1 <- conv1
I0905 17:42:39.590296 31869 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:42:39.590304 31869 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:42:39.590311 31869 net.cpp:125] relu1 needs backward computation.
I0905 17:42:39.590317 31869 net.cpp:66] Creating Layer pool1
I0905 17:42:39.590322 31869 net.cpp:329] pool1 <- conv1
I0905 17:42:39.590328 31869 net.cpp:290] pool1 -> pool1
I0905 17:42:39.590339 31869 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:42:39.590344 31869 net.cpp:125] pool1 needs backward computation.
I0905 17:42:39.590351 31869 net.cpp:66] Creating Layer norm1
I0905 17:42:39.590356 31869 net.cpp:329] norm1 <- pool1
I0905 17:42:39.590363 31869 net.cpp:290] norm1 -> norm1
I0905 17:42:39.590373 31869 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:42:39.590378 31869 net.cpp:125] norm1 needs backward computation.
I0905 17:42:39.590385 31869 net.cpp:66] Creating Layer conv2
I0905 17:42:39.590390 31869 net.cpp:329] conv2 <- norm1
I0905 17:42:39.590402 31869 net.cpp:290] conv2 -> conv2
I0905 17:42:39.599272 31869 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:42:39.599287 31869 net.cpp:125] conv2 needs backward computation.
I0905 17:42:39.599294 31869 net.cpp:66] Creating Layer relu2
I0905 17:42:39.599299 31869 net.cpp:329] relu2 <- conv2
I0905 17:42:39.599305 31869 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:42:39.599313 31869 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:42:39.599318 31869 net.cpp:125] relu2 needs backward computation.
I0905 17:42:39.599324 31869 net.cpp:66] Creating Layer pool2
I0905 17:42:39.599329 31869 net.cpp:329] pool2 <- conv2
I0905 17:42:39.599336 31869 net.cpp:290] pool2 -> pool2
I0905 17:42:39.599344 31869 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:42:39.599349 31869 net.cpp:125] pool2 needs backward computation.
I0905 17:42:39.599359 31869 net.cpp:66] Creating Layer fc7
I0905 17:42:39.599364 31869 net.cpp:329] fc7 <- pool2
I0905 17:42:39.599370 31869 net.cpp:290] fc7 -> fc7
I0905 17:42:40.245232 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.245277 31869 net.cpp:125] fc7 needs backward computation.
I0905 17:42:40.245290 31869 net.cpp:66] Creating Layer relu7
I0905 17:42:40.245297 31869 net.cpp:329] relu7 <- fc7
I0905 17:42:40.245306 31869 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:42:40.245316 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.245322 31869 net.cpp:125] relu7 needs backward computation.
I0905 17:42:40.245331 31869 net.cpp:66] Creating Layer drop7
I0905 17:42:40.245335 31869 net.cpp:329] drop7 <- fc7
I0905 17:42:40.245342 31869 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:42:40.245352 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.245358 31869 net.cpp:125] drop7 needs backward computation.
I0905 17:42:40.245367 31869 net.cpp:66] Creating Layer fc8
I0905 17:42:40.245371 31869 net.cpp:329] fc8 <- fc7
I0905 17:42:40.245380 31869 net.cpp:290] fc8 -> fc8
I0905 17:42:40.253353 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.253366 31869 net.cpp:125] fc8 needs backward computation.
I0905 17:42:40.253372 31869 net.cpp:66] Creating Layer relu8
I0905 17:42:40.253378 31869 net.cpp:329] relu8 <- fc8
I0905 17:42:40.253386 31869 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:42:40.253393 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.253399 31869 net.cpp:125] relu8 needs backward computation.
I0905 17:42:40.253406 31869 net.cpp:66] Creating Layer drop8
I0905 17:42:40.253411 31869 net.cpp:329] drop8 <- fc8
I0905 17:42:40.253417 31869 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:42:40.253423 31869 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:42:40.253429 31869 net.cpp:125] drop8 needs backward computation.
I0905 17:42:40.253437 31869 net.cpp:66] Creating Layer fc9
I0905 17:42:40.253443 31869 net.cpp:329] fc9 <- fc8
I0905 17:42:40.253450 31869 net.cpp:290] fc9 -> fc9
I0905 17:42:40.253840 31869 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:42:40.253854 31869 net.cpp:125] fc9 needs backward computation.
I0905 17:42:40.253862 31869 net.cpp:66] Creating Layer fc10
I0905 17:42:40.253868 31869 net.cpp:329] fc10 <- fc9
I0905 17:42:40.253876 31869 net.cpp:290] fc10 -> fc10
I0905 17:42:40.253888 31869 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:42:40.253896 31869 net.cpp:125] fc10 needs backward computation.
I0905 17:42:40.253903 31869 net.cpp:66] Creating Layer prob
I0905 17:42:40.253908 31869 net.cpp:329] prob <- fc10
I0905 17:42:40.253916 31869 net.cpp:290] prob -> prob
I0905 17:42:40.253926 31869 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:42:40.253931 31869 net.cpp:125] prob needs backward computation.
I0905 17:42:40.253937 31869 net.cpp:156] This network produces output prob
I0905 17:42:40.253950 31869 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:42:40.253958 31869 net.cpp:167] Network initialization done.
I0905 17:42:40.253963 31869 net.cpp:168] Memory required for data: 6183480
Classifying 139 inputs.
Done in 93.42 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 39 is out of bounds for axis 0 with size 39
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:44:17.119575 31874 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:44:17.119710 31874 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:44:17.119719 31874 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:44:17.119861 31874 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:44:17.119914 31874 net.cpp:292] Input 0 -> data
I0905 17:44:17.119938 31874 net.cpp:66] Creating Layer conv1
I0905 17:44:17.119956 31874 net.cpp:329] conv1 <- data
I0905 17:44:17.119963 31874 net.cpp:290] conv1 -> conv1
I0905 17:44:17.121284 31874 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:44:17.121301 31874 net.cpp:125] conv1 needs backward computation.
I0905 17:44:17.121310 31874 net.cpp:66] Creating Layer relu1
I0905 17:44:17.121316 31874 net.cpp:329] relu1 <- conv1
I0905 17:44:17.121322 31874 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:44:17.121331 31874 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:44:17.121336 31874 net.cpp:125] relu1 needs backward computation.
I0905 17:44:17.121343 31874 net.cpp:66] Creating Layer pool1
I0905 17:44:17.121348 31874 net.cpp:329] pool1 <- conv1
I0905 17:44:17.121354 31874 net.cpp:290] pool1 -> pool1
I0905 17:44:17.121366 31874 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:44:17.121371 31874 net.cpp:125] pool1 needs backward computation.
I0905 17:44:17.121377 31874 net.cpp:66] Creating Layer norm1
I0905 17:44:17.121383 31874 net.cpp:329] norm1 <- pool1
I0905 17:44:17.121389 31874 net.cpp:290] norm1 -> norm1
I0905 17:44:17.121399 31874 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:44:17.121404 31874 net.cpp:125] norm1 needs backward computation.
I0905 17:44:17.121412 31874 net.cpp:66] Creating Layer conv2
I0905 17:44:17.121417 31874 net.cpp:329] conv2 <- norm1
I0905 17:44:17.121424 31874 net.cpp:290] conv2 -> conv2
I0905 17:44:17.130333 31874 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:44:17.130347 31874 net.cpp:125] conv2 needs backward computation.
I0905 17:44:17.130354 31874 net.cpp:66] Creating Layer relu2
I0905 17:44:17.130359 31874 net.cpp:329] relu2 <- conv2
I0905 17:44:17.130367 31874 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:44:17.130373 31874 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:44:17.130378 31874 net.cpp:125] relu2 needs backward computation.
I0905 17:44:17.130386 31874 net.cpp:66] Creating Layer pool2
I0905 17:44:17.130393 31874 net.cpp:329] pool2 <- conv2
I0905 17:44:17.130398 31874 net.cpp:290] pool2 -> pool2
I0905 17:44:17.130406 31874 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:44:17.130411 31874 net.cpp:125] pool2 needs backward computation.
I0905 17:44:17.130419 31874 net.cpp:66] Creating Layer fc7
I0905 17:44:17.130424 31874 net.cpp:329] fc7 <- pool2
I0905 17:44:17.130430 31874 net.cpp:290] fc7 -> fc7
I0905 17:44:17.757050 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.757093 31874 net.cpp:125] fc7 needs backward computation.
I0905 17:44:17.757107 31874 net.cpp:66] Creating Layer relu7
I0905 17:44:17.757113 31874 net.cpp:329] relu7 <- fc7
I0905 17:44:17.757122 31874 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:44:17.757133 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.757138 31874 net.cpp:125] relu7 needs backward computation.
I0905 17:44:17.757145 31874 net.cpp:66] Creating Layer drop7
I0905 17:44:17.757150 31874 net.cpp:329] drop7 <- fc7
I0905 17:44:17.757158 31874 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:44:17.757169 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.757174 31874 net.cpp:125] drop7 needs backward computation.
I0905 17:44:17.757182 31874 net.cpp:66] Creating Layer fc8
I0905 17:44:17.757187 31874 net.cpp:329] fc8 <- fc7
I0905 17:44:17.757196 31874 net.cpp:290] fc8 -> fc8
I0905 17:44:17.764969 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.764982 31874 net.cpp:125] fc8 needs backward computation.
I0905 17:44:17.764989 31874 net.cpp:66] Creating Layer relu8
I0905 17:44:17.764996 31874 net.cpp:329] relu8 <- fc8
I0905 17:44:17.765003 31874 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:44:17.765010 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.765015 31874 net.cpp:125] relu8 needs backward computation.
I0905 17:44:17.765022 31874 net.cpp:66] Creating Layer drop8
I0905 17:44:17.765027 31874 net.cpp:329] drop8 <- fc8
I0905 17:44:17.765033 31874 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:44:17.765040 31874 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:44:17.765045 31874 net.cpp:125] drop8 needs backward computation.
I0905 17:44:17.765064 31874 net.cpp:66] Creating Layer fc9
I0905 17:44:17.765071 31874 net.cpp:329] fc9 <- fc8
I0905 17:44:17.765079 31874 net.cpp:290] fc9 -> fc9
I0905 17:44:17.765452 31874 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:44:17.765465 31874 net.cpp:125] fc9 needs backward computation.
I0905 17:44:17.765471 31874 net.cpp:66] Creating Layer fc10
I0905 17:44:17.765477 31874 net.cpp:329] fc10 <- fc9
I0905 17:44:17.765486 31874 net.cpp:290] fc10 -> fc10
I0905 17:44:17.765497 31874 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:44:17.765504 31874 net.cpp:125] fc10 needs backward computation.
I0905 17:44:17.765511 31874 net.cpp:66] Creating Layer prob
I0905 17:44:17.765517 31874 net.cpp:329] prob <- fc10
I0905 17:44:17.765524 31874 net.cpp:290] prob -> prob
I0905 17:44:17.765534 31874 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:44:17.765540 31874 net.cpp:125] prob needs backward computation.
I0905 17:44:17.765545 31874 net.cpp:156] This network produces output prob
I0905 17:44:17.765558 31874 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:44:17.765565 31874 net.cpp:167] Network initialization done.
I0905 17:44:17.765570 31874 net.cpp:168] Memory required for data: 6183480
Classifying 362 inputs.
Done in 219.64 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 62 is out of bounds for axis 0 with size 62
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:48:13.489087 31881 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:48:13.489238 31881 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:48:13.489246 31881 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:48:13.489390 31881 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:48:13.489452 31881 net.cpp:292] Input 0 -> data
I0905 17:48:13.489477 31881 net.cpp:66] Creating Layer conv1
I0905 17:48:13.489485 31881 net.cpp:329] conv1 <- data
I0905 17:48:13.489492 31881 net.cpp:290] conv1 -> conv1
I0905 17:48:13.490875 31881 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:48:13.490892 31881 net.cpp:125] conv1 needs backward computation.
I0905 17:48:13.490901 31881 net.cpp:66] Creating Layer relu1
I0905 17:48:13.490907 31881 net.cpp:329] relu1 <- conv1
I0905 17:48:13.490913 31881 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:48:13.490922 31881 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:48:13.490927 31881 net.cpp:125] relu1 needs backward computation.
I0905 17:48:13.490934 31881 net.cpp:66] Creating Layer pool1
I0905 17:48:13.490939 31881 net.cpp:329] pool1 <- conv1
I0905 17:48:13.490947 31881 net.cpp:290] pool1 -> pool1
I0905 17:48:13.490957 31881 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:48:13.490962 31881 net.cpp:125] pool1 needs backward computation.
I0905 17:48:13.490969 31881 net.cpp:66] Creating Layer norm1
I0905 17:48:13.490974 31881 net.cpp:329] norm1 <- pool1
I0905 17:48:13.490981 31881 net.cpp:290] norm1 -> norm1
I0905 17:48:13.490990 31881 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:48:13.490996 31881 net.cpp:125] norm1 needs backward computation.
I0905 17:48:13.491003 31881 net.cpp:66] Creating Layer conv2
I0905 17:48:13.491008 31881 net.cpp:329] conv2 <- norm1
I0905 17:48:13.491015 31881 net.cpp:290] conv2 -> conv2
I0905 17:48:13.499868 31881 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:48:13.499883 31881 net.cpp:125] conv2 needs backward computation.
I0905 17:48:13.499891 31881 net.cpp:66] Creating Layer relu2
I0905 17:48:13.499896 31881 net.cpp:329] relu2 <- conv2
I0905 17:48:13.499902 31881 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:48:13.499909 31881 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:48:13.499914 31881 net.cpp:125] relu2 needs backward computation.
I0905 17:48:13.499922 31881 net.cpp:66] Creating Layer pool2
I0905 17:48:13.499928 31881 net.cpp:329] pool2 <- conv2
I0905 17:48:13.499934 31881 net.cpp:290] pool2 -> pool2
I0905 17:48:13.499943 31881 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:48:13.499948 31881 net.cpp:125] pool2 needs backward computation.
I0905 17:48:13.499954 31881 net.cpp:66] Creating Layer fc7
I0905 17:48:13.499959 31881 net.cpp:329] fc7 <- pool2
I0905 17:48:13.499966 31881 net.cpp:290] fc7 -> fc7
I0905 17:48:14.132918 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.132961 31881 net.cpp:125] fc7 needs backward computation.
I0905 17:48:14.132972 31881 net.cpp:66] Creating Layer relu7
I0905 17:48:14.132979 31881 net.cpp:329] relu7 <- fc7
I0905 17:48:14.132987 31881 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:48:14.132997 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.133003 31881 net.cpp:125] relu7 needs backward computation.
I0905 17:48:14.133021 31881 net.cpp:66] Creating Layer drop7
I0905 17:48:14.133028 31881 net.cpp:329] drop7 <- fc7
I0905 17:48:14.133033 31881 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:48:14.133044 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.133049 31881 net.cpp:125] drop7 needs backward computation.
I0905 17:48:14.133057 31881 net.cpp:66] Creating Layer fc8
I0905 17:48:14.133062 31881 net.cpp:329] fc8 <- fc7
I0905 17:48:14.133071 31881 net.cpp:290] fc8 -> fc8
I0905 17:48:14.140637 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.140650 31881 net.cpp:125] fc8 needs backward computation.
I0905 17:48:14.140656 31881 net.cpp:66] Creating Layer relu8
I0905 17:48:14.140662 31881 net.cpp:329] relu8 <- fc8
I0905 17:48:14.140669 31881 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:48:14.140676 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.140682 31881 net.cpp:125] relu8 needs backward computation.
I0905 17:48:14.140688 31881 net.cpp:66] Creating Layer drop8
I0905 17:48:14.140693 31881 net.cpp:329] drop8 <- fc8
I0905 17:48:14.140699 31881 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:48:14.140707 31881 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:48:14.140712 31881 net.cpp:125] drop8 needs backward computation.
I0905 17:48:14.140719 31881 net.cpp:66] Creating Layer fc9
I0905 17:48:14.140725 31881 net.cpp:329] fc9 <- fc8
I0905 17:48:14.140732 31881 net.cpp:290] fc9 -> fc9
I0905 17:48:14.141094 31881 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:48:14.141105 31881 net.cpp:125] fc9 needs backward computation.
I0905 17:48:14.141113 31881 net.cpp:66] Creating Layer fc10
I0905 17:48:14.141119 31881 net.cpp:329] fc10 <- fc9
I0905 17:48:14.141126 31881 net.cpp:290] fc10 -> fc10
I0905 17:48:14.141139 31881 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:48:14.141145 31881 net.cpp:125] fc10 needs backward computation.
I0905 17:48:14.141151 31881 net.cpp:66] Creating Layer prob
I0905 17:48:14.141157 31881 net.cpp:329] prob <- fc10
I0905 17:48:14.141165 31881 net.cpp:290] prob -> prob
I0905 17:48:14.141175 31881 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:48:14.141180 31881 net.cpp:125] prob needs backward computation.
I0905 17:48:14.141185 31881 net.cpp:156] This network produces output prob
I0905 17:48:14.141197 31881 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:48:14.141206 31881 net.cpp:167] Network initialization done.
I0905 17:48:14.141211 31881 net.cpp:168] Memory required for data: 6183480
Classifying 423 inputs.
Done in 266.16 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 23 is out of bounds for axis 0 with size 23
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:52:58.719993 31892 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:52:58.720142 31892 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:52:58.720151 31892 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:52:58.720296 31892 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:52:58.720357 31892 net.cpp:292] Input 0 -> data
I0905 17:52:58.720382 31892 net.cpp:66] Creating Layer conv1
I0905 17:52:58.720389 31892 net.cpp:329] conv1 <- data
I0905 17:52:58.720396 31892 net.cpp:290] conv1 -> conv1
I0905 17:52:58.721762 31892 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:52:58.721781 31892 net.cpp:125] conv1 needs backward computation.
I0905 17:52:58.721788 31892 net.cpp:66] Creating Layer relu1
I0905 17:52:58.721794 31892 net.cpp:329] relu1 <- conv1
I0905 17:52:58.721801 31892 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:52:58.721810 31892 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:52:58.721815 31892 net.cpp:125] relu1 needs backward computation.
I0905 17:52:58.721822 31892 net.cpp:66] Creating Layer pool1
I0905 17:52:58.721828 31892 net.cpp:329] pool1 <- conv1
I0905 17:52:58.721834 31892 net.cpp:290] pool1 -> pool1
I0905 17:52:58.721845 31892 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:52:58.721850 31892 net.cpp:125] pool1 needs backward computation.
I0905 17:52:58.721858 31892 net.cpp:66] Creating Layer norm1
I0905 17:52:58.721863 31892 net.cpp:329] norm1 <- pool1
I0905 17:52:58.721869 31892 net.cpp:290] norm1 -> norm1
I0905 17:52:58.721879 31892 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:52:58.721884 31892 net.cpp:125] norm1 needs backward computation.
I0905 17:52:58.721891 31892 net.cpp:66] Creating Layer conv2
I0905 17:52:58.721897 31892 net.cpp:329] conv2 <- norm1
I0905 17:52:58.721904 31892 net.cpp:290] conv2 -> conv2
I0905 17:52:58.730736 31892 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:52:58.730754 31892 net.cpp:125] conv2 needs backward computation.
I0905 17:52:58.730762 31892 net.cpp:66] Creating Layer relu2
I0905 17:52:58.730767 31892 net.cpp:329] relu2 <- conv2
I0905 17:52:58.730773 31892 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:52:58.730780 31892 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:52:58.730785 31892 net.cpp:125] relu2 needs backward computation.
I0905 17:52:58.730794 31892 net.cpp:66] Creating Layer pool2
I0905 17:52:58.730799 31892 net.cpp:329] pool2 <- conv2
I0905 17:52:58.730806 31892 net.cpp:290] pool2 -> pool2
I0905 17:52:58.730813 31892 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:52:58.730819 31892 net.cpp:125] pool2 needs backward computation.
I0905 17:52:58.730826 31892 net.cpp:66] Creating Layer fc7
I0905 17:52:58.730831 31892 net.cpp:329] fc7 <- pool2
I0905 17:52:58.730839 31892 net.cpp:290] fc7 -> fc7
I0905 17:52:59.358824 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.358868 31892 net.cpp:125] fc7 needs backward computation.
I0905 17:52:59.358881 31892 net.cpp:66] Creating Layer relu7
I0905 17:52:59.358888 31892 net.cpp:329] relu7 <- fc7
I0905 17:52:59.358898 31892 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:52:59.358908 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.358914 31892 net.cpp:125] relu7 needs backward computation.
I0905 17:52:59.358922 31892 net.cpp:66] Creating Layer drop7
I0905 17:52:59.358927 31892 net.cpp:329] drop7 <- fc7
I0905 17:52:59.358933 31892 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:52:59.358944 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.358950 31892 net.cpp:125] drop7 needs backward computation.
I0905 17:52:59.358958 31892 net.cpp:66] Creating Layer fc8
I0905 17:52:59.358964 31892 net.cpp:329] fc8 <- fc7
I0905 17:52:59.358973 31892 net.cpp:290] fc8 -> fc8
I0905 17:52:59.366582 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.366595 31892 net.cpp:125] fc8 needs backward computation.
I0905 17:52:59.366608 31892 net.cpp:66] Creating Layer relu8
I0905 17:52:59.366613 31892 net.cpp:329] relu8 <- fc8
I0905 17:52:59.366622 31892 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:52:59.366629 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.366641 31892 net.cpp:125] relu8 needs backward computation.
I0905 17:52:59.366647 31892 net.cpp:66] Creating Layer drop8
I0905 17:52:59.366653 31892 net.cpp:329] drop8 <- fc8
I0905 17:52:59.366659 31892 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:52:59.366665 31892 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:52:59.366672 31892 net.cpp:125] drop8 needs backward computation.
I0905 17:52:59.366679 31892 net.cpp:66] Creating Layer fc9
I0905 17:52:59.366684 31892 net.cpp:329] fc9 <- fc8
I0905 17:52:59.366691 31892 net.cpp:290] fc9 -> fc9
I0905 17:52:59.367053 31892 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:52:59.367065 31892 net.cpp:125] fc9 needs backward computation.
I0905 17:52:59.367074 31892 net.cpp:66] Creating Layer fc10
I0905 17:52:59.367079 31892 net.cpp:329] fc10 <- fc9
I0905 17:52:59.367086 31892 net.cpp:290] fc10 -> fc10
I0905 17:52:59.367097 31892 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:52:59.367105 31892 net.cpp:125] fc10 needs backward computation.
I0905 17:52:59.367111 31892 net.cpp:66] Creating Layer prob
I0905 17:52:59.367116 31892 net.cpp:329] prob <- fc10
I0905 17:52:59.367125 31892 net.cpp:290] prob -> prob
I0905 17:52:59.367133 31892 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:52:59.367140 31892 net.cpp:125] prob needs backward computation.
I0905 17:52:59.367144 31892 net.cpp:156] This network produces output prob
I0905 17:52:59.367156 31892 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:52:59.367164 31892 net.cpp:167] Network initialization done.
I0905 17:52:59.367169 31892 net.cpp:168] Memory required for data: 6183480
Classifying 18 inputs.
Done in 11.23 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:53:13.341261 31895 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:53:13.341408 31895 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:53:13.341416 31895 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:53:13.341559 31895 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:53:13.341637 31895 net.cpp:292] Input 0 -> data
I0905 17:53:13.341675 31895 net.cpp:66] Creating Layer conv1
I0905 17:53:13.341682 31895 net.cpp:329] conv1 <- data
I0905 17:53:13.341691 31895 net.cpp:290] conv1 -> conv1
I0905 17:53:13.343034 31895 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:53:13.343051 31895 net.cpp:125] conv1 needs backward computation.
I0905 17:53:13.343060 31895 net.cpp:66] Creating Layer relu1
I0905 17:53:13.343066 31895 net.cpp:329] relu1 <- conv1
I0905 17:53:13.343073 31895 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:53:13.343086 31895 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:53:13.343092 31895 net.cpp:125] relu1 needs backward computation.
I0905 17:53:13.343099 31895 net.cpp:66] Creating Layer pool1
I0905 17:53:13.343106 31895 net.cpp:329] pool1 <- conv1
I0905 17:53:13.343112 31895 net.cpp:290] pool1 -> pool1
I0905 17:53:13.343123 31895 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:53:13.343129 31895 net.cpp:125] pool1 needs backward computation.
I0905 17:53:13.343137 31895 net.cpp:66] Creating Layer norm1
I0905 17:53:13.343142 31895 net.cpp:329] norm1 <- pool1
I0905 17:53:13.343148 31895 net.cpp:290] norm1 -> norm1
I0905 17:53:13.343158 31895 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:53:13.343163 31895 net.cpp:125] norm1 needs backward computation.
I0905 17:53:13.343170 31895 net.cpp:66] Creating Layer conv2
I0905 17:53:13.343176 31895 net.cpp:329] conv2 <- norm1
I0905 17:53:13.343183 31895 net.cpp:290] conv2 -> conv2
I0905 17:53:13.352092 31895 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:53:13.352108 31895 net.cpp:125] conv2 needs backward computation.
I0905 17:53:13.352114 31895 net.cpp:66] Creating Layer relu2
I0905 17:53:13.352119 31895 net.cpp:329] relu2 <- conv2
I0905 17:53:13.352126 31895 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:53:13.352133 31895 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:53:13.352139 31895 net.cpp:125] relu2 needs backward computation.
I0905 17:53:13.352145 31895 net.cpp:66] Creating Layer pool2
I0905 17:53:13.352151 31895 net.cpp:329] pool2 <- conv2
I0905 17:53:13.352157 31895 net.cpp:290] pool2 -> pool2
I0905 17:53:13.352165 31895 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:53:13.352171 31895 net.cpp:125] pool2 needs backward computation.
I0905 17:53:13.352180 31895 net.cpp:66] Creating Layer fc7
I0905 17:53:13.352186 31895 net.cpp:329] fc7 <- pool2
I0905 17:53:13.352193 31895 net.cpp:290] fc7 -> fc7
I0905 17:53:13.982576 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.982624 31895 net.cpp:125] fc7 needs backward computation.
I0905 17:53:13.982636 31895 net.cpp:66] Creating Layer relu7
I0905 17:53:13.982643 31895 net.cpp:329] relu7 <- fc7
I0905 17:53:13.982653 31895 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:53:13.982663 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.982669 31895 net.cpp:125] relu7 needs backward computation.
I0905 17:53:13.982677 31895 net.cpp:66] Creating Layer drop7
I0905 17:53:13.982682 31895 net.cpp:329] drop7 <- fc7
I0905 17:53:13.982689 31895 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:53:13.982700 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.982707 31895 net.cpp:125] drop7 needs backward computation.
I0905 17:53:13.982715 31895 net.cpp:66] Creating Layer fc8
I0905 17:53:13.982720 31895 net.cpp:329] fc8 <- fc7
I0905 17:53:13.982730 31895 net.cpp:290] fc8 -> fc8
I0905 17:53:13.990500 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.990512 31895 net.cpp:125] fc8 needs backward computation.
I0905 17:53:13.990520 31895 net.cpp:66] Creating Layer relu8
I0905 17:53:13.990525 31895 net.cpp:329] relu8 <- fc8
I0905 17:53:13.990533 31895 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:53:13.990541 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.990547 31895 net.cpp:125] relu8 needs backward computation.
I0905 17:53:13.990555 31895 net.cpp:66] Creating Layer drop8
I0905 17:53:13.990560 31895 net.cpp:329] drop8 <- fc8
I0905 17:53:13.990566 31895 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:53:13.990572 31895 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:53:13.990578 31895 net.cpp:125] drop8 needs backward computation.
I0905 17:53:13.990587 31895 net.cpp:66] Creating Layer fc9
I0905 17:53:13.990593 31895 net.cpp:329] fc9 <- fc8
I0905 17:53:13.990600 31895 net.cpp:290] fc9 -> fc9
I0905 17:53:13.990975 31895 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:53:13.990988 31895 net.cpp:125] fc9 needs backward computation.
I0905 17:53:13.990996 31895 net.cpp:66] Creating Layer fc10
I0905 17:53:13.991001 31895 net.cpp:329] fc10 <- fc9
I0905 17:53:13.991021 31895 net.cpp:290] fc10 -> fc10
I0905 17:53:13.991034 31895 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:53:13.991041 31895 net.cpp:125] fc10 needs backward computation.
I0905 17:53:13.991049 31895 net.cpp:66] Creating Layer prob
I0905 17:53:13.991053 31895 net.cpp:329] prob <- fc10
I0905 17:53:13.991062 31895 net.cpp:290] prob -> prob
I0905 17:53:13.991071 31895 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:53:13.991077 31895 net.cpp:125] prob needs backward computation.
I0905 17:53:13.991082 31895 net.cpp:156] This network produces output prob
I0905 17:53:13.991096 31895 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:53:13.991103 31895 net.cpp:167] Network initialization done.
I0905 17:53:13.991109 31895 net.cpp:168] Memory required for data: 6183480
Classifying 204 inputs.
Done in 121.74 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 4 is out of bounds for axis 0 with size 4
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:55:23.127615 31901 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:55:23.127753 31901 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:55:23.127761 31901 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:55:23.127904 31901 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:55:23.127967 31901 net.cpp:292] Input 0 -> data
I0905 17:55:23.127992 31901 net.cpp:66] Creating Layer conv1
I0905 17:55:23.128000 31901 net.cpp:329] conv1 <- data
I0905 17:55:23.128007 31901 net.cpp:290] conv1 -> conv1
I0905 17:55:23.129328 31901 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:55:23.129348 31901 net.cpp:125] conv1 needs backward computation.
I0905 17:55:23.129355 31901 net.cpp:66] Creating Layer relu1
I0905 17:55:23.129361 31901 net.cpp:329] relu1 <- conv1
I0905 17:55:23.129369 31901 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:55:23.129376 31901 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:55:23.129382 31901 net.cpp:125] relu1 needs backward computation.
I0905 17:55:23.129389 31901 net.cpp:66] Creating Layer pool1
I0905 17:55:23.129395 31901 net.cpp:329] pool1 <- conv1
I0905 17:55:23.129400 31901 net.cpp:290] pool1 -> pool1
I0905 17:55:23.129411 31901 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:55:23.129417 31901 net.cpp:125] pool1 needs backward computation.
I0905 17:55:23.129423 31901 net.cpp:66] Creating Layer norm1
I0905 17:55:23.129428 31901 net.cpp:329] norm1 <- pool1
I0905 17:55:23.129436 31901 net.cpp:290] norm1 -> norm1
I0905 17:55:23.129444 31901 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:55:23.129449 31901 net.cpp:125] norm1 needs backward computation.
I0905 17:55:23.129456 31901 net.cpp:66] Creating Layer conv2
I0905 17:55:23.129462 31901 net.cpp:329] conv2 <- norm1
I0905 17:55:23.129468 31901 net.cpp:290] conv2 -> conv2
I0905 17:55:23.138380 31901 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:55:23.138394 31901 net.cpp:125] conv2 needs backward computation.
I0905 17:55:23.138401 31901 net.cpp:66] Creating Layer relu2
I0905 17:55:23.138406 31901 net.cpp:329] relu2 <- conv2
I0905 17:55:23.138413 31901 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:55:23.138419 31901 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:55:23.138425 31901 net.cpp:125] relu2 needs backward computation.
I0905 17:55:23.138433 31901 net.cpp:66] Creating Layer pool2
I0905 17:55:23.138439 31901 net.cpp:329] pool2 <- conv2
I0905 17:55:23.138445 31901 net.cpp:290] pool2 -> pool2
I0905 17:55:23.138453 31901 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:55:23.138458 31901 net.cpp:125] pool2 needs backward computation.
I0905 17:55:23.138465 31901 net.cpp:66] Creating Layer fc7
I0905 17:55:23.138470 31901 net.cpp:329] fc7 <- pool2
I0905 17:55:23.138478 31901 net.cpp:290] fc7 -> fc7
I0905 17:55:23.767309 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.767369 31901 net.cpp:125] fc7 needs backward computation.
I0905 17:55:23.767387 31901 net.cpp:66] Creating Layer relu7
I0905 17:55:23.767395 31901 net.cpp:329] relu7 <- fc7
I0905 17:55:23.767405 31901 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:55:23.767421 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.767427 31901 net.cpp:125] relu7 needs backward computation.
I0905 17:55:23.767434 31901 net.cpp:66] Creating Layer drop7
I0905 17:55:23.767439 31901 net.cpp:329] drop7 <- fc7
I0905 17:55:23.767446 31901 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:55:23.767457 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.767462 31901 net.cpp:125] drop7 needs backward computation.
I0905 17:55:23.767470 31901 net.cpp:66] Creating Layer fc8
I0905 17:55:23.767484 31901 net.cpp:329] fc8 <- fc7
I0905 17:55:23.767494 31901 net.cpp:290] fc8 -> fc8
I0905 17:55:23.775905 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.775959 31901 net.cpp:125] fc8 needs backward computation.
I0905 17:55:23.776005 31901 net.cpp:66] Creating Layer relu8
I0905 17:55:23.776044 31901 net.cpp:329] relu8 <- fc8
I0905 17:55:23.776067 31901 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:55:23.776077 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.776083 31901 net.cpp:125] relu8 needs backward computation.
I0905 17:55:23.776089 31901 net.cpp:66] Creating Layer drop8
I0905 17:55:23.776095 31901 net.cpp:329] drop8 <- fc8
I0905 17:55:23.776101 31901 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:55:23.776108 31901 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:55:23.776114 31901 net.cpp:125] drop8 needs backward computation.
I0905 17:55:23.776124 31901 net.cpp:66] Creating Layer fc9
I0905 17:55:23.776139 31901 net.cpp:329] fc9 <- fc8
I0905 17:55:23.776185 31901 net.cpp:290] fc9 -> fc9
I0905 17:55:23.776888 31901 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:55:23.776903 31901 net.cpp:125] fc9 needs backward computation.
I0905 17:55:23.776912 31901 net.cpp:66] Creating Layer fc10
I0905 17:55:23.776917 31901 net.cpp:329] fc10 <- fc9
I0905 17:55:23.776934 31901 net.cpp:290] fc10 -> fc10
I0905 17:55:23.776999 31901 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:55:23.777040 31901 net.cpp:125] fc10 needs backward computation.
I0905 17:55:23.777067 31901 net.cpp:66] Creating Layer prob
I0905 17:55:23.777075 31901 net.cpp:329] prob <- fc10
I0905 17:55:23.777082 31901 net.cpp:290] prob -> prob
I0905 17:55:23.777093 31901 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:55:23.777099 31901 net.cpp:125] prob needs backward computation.
I0905 17:55:23.777104 31901 net.cpp:156] This network produces output prob
I0905 17:55:23.777117 31901 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:55:23.777127 31901 net.cpp:167] Network initialization done.
I0905 17:55:23.777130 31901 net.cpp:168] Memory required for data: 6183480
Classifying 85 inputs.
Done in 53.52 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:56:23.330459 31905 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:56:23.330596 31905 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:56:23.330605 31905 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:56:23.330749 31905 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:56:23.330811 31905 net.cpp:292] Input 0 -> data
I0905 17:56:23.330835 31905 net.cpp:66] Creating Layer conv1
I0905 17:56:23.330842 31905 net.cpp:329] conv1 <- data
I0905 17:56:23.330850 31905 net.cpp:290] conv1 -> conv1
I0905 17:56:23.332170 31905 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:56:23.332186 31905 net.cpp:125] conv1 needs backward computation.
I0905 17:56:23.332195 31905 net.cpp:66] Creating Layer relu1
I0905 17:56:23.332201 31905 net.cpp:329] relu1 <- conv1
I0905 17:56:23.332207 31905 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:56:23.332216 31905 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:56:23.332221 31905 net.cpp:125] relu1 needs backward computation.
I0905 17:56:23.332228 31905 net.cpp:66] Creating Layer pool1
I0905 17:56:23.332233 31905 net.cpp:329] pool1 <- conv1
I0905 17:56:23.332240 31905 net.cpp:290] pool1 -> pool1
I0905 17:56:23.332250 31905 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:56:23.332257 31905 net.cpp:125] pool1 needs backward computation.
I0905 17:56:23.332262 31905 net.cpp:66] Creating Layer norm1
I0905 17:56:23.332268 31905 net.cpp:329] norm1 <- pool1
I0905 17:56:23.332274 31905 net.cpp:290] norm1 -> norm1
I0905 17:56:23.332283 31905 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:56:23.332289 31905 net.cpp:125] norm1 needs backward computation.
I0905 17:56:23.332296 31905 net.cpp:66] Creating Layer conv2
I0905 17:56:23.332303 31905 net.cpp:329] conv2 <- norm1
I0905 17:56:23.332309 31905 net.cpp:290] conv2 -> conv2
I0905 17:56:23.341186 31905 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:56:23.341200 31905 net.cpp:125] conv2 needs backward computation.
I0905 17:56:23.341207 31905 net.cpp:66] Creating Layer relu2
I0905 17:56:23.341212 31905 net.cpp:329] relu2 <- conv2
I0905 17:56:23.341219 31905 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:56:23.341225 31905 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:56:23.341231 31905 net.cpp:125] relu2 needs backward computation.
I0905 17:56:23.341239 31905 net.cpp:66] Creating Layer pool2
I0905 17:56:23.341244 31905 net.cpp:329] pool2 <- conv2
I0905 17:56:23.341251 31905 net.cpp:290] pool2 -> pool2
I0905 17:56:23.341259 31905 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:56:23.341264 31905 net.cpp:125] pool2 needs backward computation.
I0905 17:56:23.341276 31905 net.cpp:66] Creating Layer fc7
I0905 17:56:23.341281 31905 net.cpp:329] fc7 <- pool2
I0905 17:56:23.341289 31905 net.cpp:290] fc7 -> fc7
I0905 17:56:23.970156 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.970201 31905 net.cpp:125] fc7 needs backward computation.
I0905 17:56:23.970213 31905 net.cpp:66] Creating Layer relu7
I0905 17:56:23.970221 31905 net.cpp:329] relu7 <- fc7
I0905 17:56:23.970230 31905 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:56:23.970240 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.970245 31905 net.cpp:125] relu7 needs backward computation.
I0905 17:56:23.970253 31905 net.cpp:66] Creating Layer drop7
I0905 17:56:23.970258 31905 net.cpp:329] drop7 <- fc7
I0905 17:56:23.970264 31905 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:56:23.970275 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.970281 31905 net.cpp:125] drop7 needs backward computation.
I0905 17:56:23.970289 31905 net.cpp:66] Creating Layer fc8
I0905 17:56:23.970294 31905 net.cpp:329] fc8 <- fc7
I0905 17:56:23.970304 31905 net.cpp:290] fc8 -> fc8
I0905 17:56:23.977869 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.977881 31905 net.cpp:125] fc8 needs backward computation.
I0905 17:56:23.977888 31905 net.cpp:66] Creating Layer relu8
I0905 17:56:23.977895 31905 net.cpp:329] relu8 <- fc8
I0905 17:56:23.977902 31905 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:56:23.977910 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.977915 31905 net.cpp:125] relu8 needs backward computation.
I0905 17:56:23.977921 31905 net.cpp:66] Creating Layer drop8
I0905 17:56:23.977926 31905 net.cpp:329] drop8 <- fc8
I0905 17:56:23.977932 31905 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:56:23.977938 31905 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:23.977944 31905 net.cpp:125] drop8 needs backward computation.
I0905 17:56:23.977952 31905 net.cpp:66] Creating Layer fc9
I0905 17:56:23.977958 31905 net.cpp:329] fc9 <- fc8
I0905 17:56:23.977965 31905 net.cpp:290] fc9 -> fc9
I0905 17:56:23.978327 31905 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:56:23.978338 31905 net.cpp:125] fc9 needs backward computation.
I0905 17:56:23.978346 31905 net.cpp:66] Creating Layer fc10
I0905 17:56:23.978351 31905 net.cpp:329] fc10 <- fc9
I0905 17:56:23.978359 31905 net.cpp:290] fc10 -> fc10
I0905 17:56:23.978370 31905 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:56:23.978379 31905 net.cpp:125] fc10 needs backward computation.
I0905 17:56:23.978385 31905 net.cpp:66] Creating Layer prob
I0905 17:56:23.978390 31905 net.cpp:329] prob <- fc10
I0905 17:56:23.978397 31905 net.cpp:290] prob -> prob
I0905 17:56:23.978406 31905 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:56:23.978412 31905 net.cpp:125] prob needs backward computation.
I0905 17:56:23.978417 31905 net.cpp:156] This network produces output prob
I0905 17:56:23.978430 31905 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:56:23.978437 31905 net.cpp:167] Network initialization done.
I0905 17:56:23.978442 31905 net.cpp:168] Memory required for data: 6183480
Classifying 50 inputs.
Done in 31.65 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:56:57.461055 31908 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:56:57.461194 31908 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:56:57.461205 31908 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:56:57.461351 31908 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:56:57.461416 31908 net.cpp:292] Input 0 -> data
I0905 17:56:57.461442 31908 net.cpp:66] Creating Layer conv1
I0905 17:56:57.461449 31908 net.cpp:329] conv1 <- data
I0905 17:56:57.461457 31908 net.cpp:290] conv1 -> conv1
I0905 17:56:57.462873 31908 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:56:57.462893 31908 net.cpp:125] conv1 needs backward computation.
I0905 17:56:57.462903 31908 net.cpp:66] Creating Layer relu1
I0905 17:56:57.462908 31908 net.cpp:329] relu1 <- conv1
I0905 17:56:57.462915 31908 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:56:57.462924 31908 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:56:57.462929 31908 net.cpp:125] relu1 needs backward computation.
I0905 17:56:57.462936 31908 net.cpp:66] Creating Layer pool1
I0905 17:56:57.462941 31908 net.cpp:329] pool1 <- conv1
I0905 17:56:57.462949 31908 net.cpp:290] pool1 -> pool1
I0905 17:56:57.462960 31908 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:56:57.462965 31908 net.cpp:125] pool1 needs backward computation.
I0905 17:56:57.462972 31908 net.cpp:66] Creating Layer norm1
I0905 17:56:57.462977 31908 net.cpp:329] norm1 <- pool1
I0905 17:56:57.462985 31908 net.cpp:290] norm1 -> norm1
I0905 17:56:57.462993 31908 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:56:57.463003 31908 net.cpp:125] norm1 needs backward computation.
I0905 17:56:57.463011 31908 net.cpp:66] Creating Layer conv2
I0905 17:56:57.463017 31908 net.cpp:329] conv2 <- norm1
I0905 17:56:57.463024 31908 net.cpp:290] conv2 -> conv2
I0905 17:56:57.472158 31908 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:56:57.472173 31908 net.cpp:125] conv2 needs backward computation.
I0905 17:56:57.472180 31908 net.cpp:66] Creating Layer relu2
I0905 17:56:57.472187 31908 net.cpp:329] relu2 <- conv2
I0905 17:56:57.472193 31908 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:56:57.472199 31908 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:56:57.472205 31908 net.cpp:125] relu2 needs backward computation.
I0905 17:56:57.472214 31908 net.cpp:66] Creating Layer pool2
I0905 17:56:57.472220 31908 net.cpp:329] pool2 <- conv2
I0905 17:56:57.472226 31908 net.cpp:290] pool2 -> pool2
I0905 17:56:57.472234 31908 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:56:57.472240 31908 net.cpp:125] pool2 needs backward computation.
I0905 17:56:57.472247 31908 net.cpp:66] Creating Layer fc7
I0905 17:56:57.472252 31908 net.cpp:329] fc7 <- pool2
I0905 17:56:57.472259 31908 net.cpp:290] fc7 -> fc7
I0905 17:56:58.115474 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.115524 31908 net.cpp:125] fc7 needs backward computation.
I0905 17:56:58.115535 31908 net.cpp:66] Creating Layer relu7
I0905 17:56:58.115543 31908 net.cpp:329] relu7 <- fc7
I0905 17:56:58.115552 31908 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:56:58.115562 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.115568 31908 net.cpp:125] relu7 needs backward computation.
I0905 17:56:58.115576 31908 net.cpp:66] Creating Layer drop7
I0905 17:56:58.115581 31908 net.cpp:329] drop7 <- fc7
I0905 17:56:58.115587 31908 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:56:58.115598 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.115604 31908 net.cpp:125] drop7 needs backward computation.
I0905 17:56:58.115612 31908 net.cpp:66] Creating Layer fc8
I0905 17:56:58.115617 31908 net.cpp:329] fc8 <- fc7
I0905 17:56:58.115628 31908 net.cpp:290] fc8 -> fc8
I0905 17:56:58.123394 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.123406 31908 net.cpp:125] fc8 needs backward computation.
I0905 17:56:58.123414 31908 net.cpp:66] Creating Layer relu8
I0905 17:56:58.123420 31908 net.cpp:329] relu8 <- fc8
I0905 17:56:58.123427 31908 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:56:58.123435 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.123440 31908 net.cpp:125] relu8 needs backward computation.
I0905 17:56:58.123446 31908 net.cpp:66] Creating Layer drop8
I0905 17:56:58.123451 31908 net.cpp:329] drop8 <- fc8
I0905 17:56:58.123458 31908 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:56:58.123464 31908 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:56:58.123471 31908 net.cpp:125] drop8 needs backward computation.
I0905 17:56:58.123478 31908 net.cpp:66] Creating Layer fc9
I0905 17:56:58.123484 31908 net.cpp:329] fc9 <- fc8
I0905 17:56:58.123491 31908 net.cpp:290] fc9 -> fc9
I0905 17:56:58.123867 31908 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:56:58.123878 31908 net.cpp:125] fc9 needs backward computation.
I0905 17:56:58.123886 31908 net.cpp:66] Creating Layer fc10
I0905 17:56:58.123891 31908 net.cpp:329] fc10 <- fc9
I0905 17:56:58.123900 31908 net.cpp:290] fc10 -> fc10
I0905 17:56:58.123911 31908 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:56:58.123919 31908 net.cpp:125] fc10 needs backward computation.
I0905 17:56:58.123925 31908 net.cpp:66] Creating Layer prob
I0905 17:56:58.123931 31908 net.cpp:329] prob <- fc10
I0905 17:56:58.123939 31908 net.cpp:290] prob -> prob
I0905 17:56:58.123949 31908 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:56:58.123953 31908 net.cpp:125] prob needs backward computation.
I0905 17:56:58.123958 31908 net.cpp:156] This network produces output prob
I0905 17:56:58.123971 31908 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:56:58.123989 31908 net.cpp:167] Network initialization done.
I0905 17:56:58.123996 31908 net.cpp:168] Memory required for data: 6183480
Classifying 159 inputs.
Done in 101.50 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 59 is out of bounds for axis 0 with size 59
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:58:43.184507 31914 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:58:43.184643 31914 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:58:43.184653 31914 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:58:43.184797 31914 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:58:43.184857 31914 net.cpp:292] Input 0 -> data
I0905 17:58:43.184883 31914 net.cpp:66] Creating Layer conv1
I0905 17:58:43.184890 31914 net.cpp:329] conv1 <- data
I0905 17:58:43.184898 31914 net.cpp:290] conv1 -> conv1
I0905 17:58:43.186300 31914 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:58:43.186319 31914 net.cpp:125] conv1 needs backward computation.
I0905 17:58:43.186327 31914 net.cpp:66] Creating Layer relu1
I0905 17:58:43.186333 31914 net.cpp:329] relu1 <- conv1
I0905 17:58:43.186341 31914 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:58:43.186348 31914 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:58:43.186353 31914 net.cpp:125] relu1 needs backward computation.
I0905 17:58:43.186360 31914 net.cpp:66] Creating Layer pool1
I0905 17:58:43.186367 31914 net.cpp:329] pool1 <- conv1
I0905 17:58:43.186372 31914 net.cpp:290] pool1 -> pool1
I0905 17:58:43.186383 31914 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:58:43.186388 31914 net.cpp:125] pool1 needs backward computation.
I0905 17:58:43.186395 31914 net.cpp:66] Creating Layer norm1
I0905 17:58:43.186400 31914 net.cpp:329] norm1 <- pool1
I0905 17:58:43.186408 31914 net.cpp:290] norm1 -> norm1
I0905 17:58:43.186416 31914 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:58:43.186422 31914 net.cpp:125] norm1 needs backward computation.
I0905 17:58:43.186429 31914 net.cpp:66] Creating Layer conv2
I0905 17:58:43.186434 31914 net.cpp:329] conv2 <- norm1
I0905 17:58:43.186441 31914 net.cpp:290] conv2 -> conv2
I0905 17:58:43.195703 31914 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:58:43.195718 31914 net.cpp:125] conv2 needs backward computation.
I0905 17:58:43.195724 31914 net.cpp:66] Creating Layer relu2
I0905 17:58:43.195729 31914 net.cpp:329] relu2 <- conv2
I0905 17:58:43.195736 31914 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:58:43.195742 31914 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:58:43.195749 31914 net.cpp:125] relu2 needs backward computation.
I0905 17:58:43.195755 31914 net.cpp:66] Creating Layer pool2
I0905 17:58:43.195760 31914 net.cpp:329] pool2 <- conv2
I0905 17:58:43.195765 31914 net.cpp:290] pool2 -> pool2
I0905 17:58:43.195773 31914 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:58:43.195778 31914 net.cpp:125] pool2 needs backward computation.
I0905 17:58:43.195787 31914 net.cpp:66] Creating Layer fc7
I0905 17:58:43.195793 31914 net.cpp:329] fc7 <- pool2
I0905 17:58:43.195801 31914 net.cpp:290] fc7 -> fc7
I0905 17:58:43.825016 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.825062 31914 net.cpp:125] fc7 needs backward computation.
I0905 17:58:43.825073 31914 net.cpp:66] Creating Layer relu7
I0905 17:58:43.825080 31914 net.cpp:329] relu7 <- fc7
I0905 17:58:43.825090 31914 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:58:43.825100 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.825105 31914 net.cpp:125] relu7 needs backward computation.
I0905 17:58:43.825112 31914 net.cpp:66] Creating Layer drop7
I0905 17:58:43.825119 31914 net.cpp:329] drop7 <- fc7
I0905 17:58:43.825124 31914 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:58:43.825134 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.825140 31914 net.cpp:125] drop7 needs backward computation.
I0905 17:58:43.825148 31914 net.cpp:66] Creating Layer fc8
I0905 17:58:43.825153 31914 net.cpp:329] fc8 <- fc7
I0905 17:58:43.825162 31914 net.cpp:290] fc8 -> fc8
I0905 17:58:43.832733 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.832746 31914 net.cpp:125] fc8 needs backward computation.
I0905 17:58:43.832752 31914 net.cpp:66] Creating Layer relu8
I0905 17:58:43.832758 31914 net.cpp:329] relu8 <- fc8
I0905 17:58:43.832765 31914 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:58:43.832773 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.832778 31914 net.cpp:125] relu8 needs backward computation.
I0905 17:58:43.832784 31914 net.cpp:66] Creating Layer drop8
I0905 17:58:43.832790 31914 net.cpp:329] drop8 <- fc8
I0905 17:58:43.832808 31914 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:58:43.832814 31914 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:58:43.832819 31914 net.cpp:125] drop8 needs backward computation.
I0905 17:58:43.832828 31914 net.cpp:66] Creating Layer fc9
I0905 17:58:43.832834 31914 net.cpp:329] fc9 <- fc8
I0905 17:58:43.832840 31914 net.cpp:290] fc9 -> fc9
I0905 17:58:43.833204 31914 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:58:43.833215 31914 net.cpp:125] fc9 needs backward computation.
I0905 17:58:43.833223 31914 net.cpp:66] Creating Layer fc10
I0905 17:58:43.833230 31914 net.cpp:329] fc10 <- fc9
I0905 17:58:43.833237 31914 net.cpp:290] fc10 -> fc10
I0905 17:58:43.833248 31914 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:58:43.833256 31914 net.cpp:125] fc10 needs backward computation.
I0905 17:58:43.833262 31914 net.cpp:66] Creating Layer prob
I0905 17:58:43.833268 31914 net.cpp:329] prob <- fc10
I0905 17:58:43.833276 31914 net.cpp:290] prob -> prob
I0905 17:58:43.833284 31914 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:58:43.833291 31914 net.cpp:125] prob needs backward computation.
I0905 17:58:43.833295 31914 net.cpp:156] This network produces output prob
I0905 17:58:43.833307 31914 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:58:43.833315 31914 net.cpp:167] Network initialization done.
I0905 17:58:43.833320 31914 net.cpp:168] Memory required for data: 6183480
Classifying 24 inputs.
Done in 14.68 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 17:58:59.745404 31918 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 17:58:59.745540 31918 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 17:58:59.745549 31918 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 17:58:59.745731 31918 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 17:58:59.745795 31918 net.cpp:292] Input 0 -> data
I0905 17:58:59.745820 31918 net.cpp:66] Creating Layer conv1
I0905 17:58:59.745837 31918 net.cpp:329] conv1 <- data
I0905 17:58:59.745846 31918 net.cpp:290] conv1 -> conv1
I0905 17:58:59.747169 31918 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:58:59.747186 31918 net.cpp:125] conv1 needs backward computation.
I0905 17:58:59.747195 31918 net.cpp:66] Creating Layer relu1
I0905 17:58:59.747201 31918 net.cpp:329] relu1 <- conv1
I0905 17:58:59.747207 31918 net.cpp:280] relu1 -> conv1 (in-place)
I0905 17:58:59.747215 31918 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 17:58:59.747221 31918 net.cpp:125] relu1 needs backward computation.
I0905 17:58:59.747228 31918 net.cpp:66] Creating Layer pool1
I0905 17:58:59.747233 31918 net.cpp:329] pool1 <- conv1
I0905 17:58:59.747239 31918 net.cpp:290] pool1 -> pool1
I0905 17:58:59.747251 31918 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:58:59.747256 31918 net.cpp:125] pool1 needs backward computation.
I0905 17:58:59.747262 31918 net.cpp:66] Creating Layer norm1
I0905 17:58:59.747268 31918 net.cpp:329] norm1 <- pool1
I0905 17:58:59.747274 31918 net.cpp:290] norm1 -> norm1
I0905 17:58:59.747284 31918 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 17:58:59.747289 31918 net.cpp:125] norm1 needs backward computation.
I0905 17:58:59.747297 31918 net.cpp:66] Creating Layer conv2
I0905 17:58:59.747303 31918 net.cpp:329] conv2 <- norm1
I0905 17:58:59.747309 31918 net.cpp:290] conv2 -> conv2
I0905 17:58:59.756197 31918 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:58:59.756212 31918 net.cpp:125] conv2 needs backward computation.
I0905 17:58:59.756219 31918 net.cpp:66] Creating Layer relu2
I0905 17:58:59.756225 31918 net.cpp:329] relu2 <- conv2
I0905 17:58:59.756232 31918 net.cpp:280] relu2 -> conv2 (in-place)
I0905 17:58:59.756238 31918 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 17:58:59.756243 31918 net.cpp:125] relu2 needs backward computation.
I0905 17:58:59.756250 31918 net.cpp:66] Creating Layer pool2
I0905 17:58:59.756255 31918 net.cpp:329] pool2 <- conv2
I0905 17:58:59.756261 31918 net.cpp:290] pool2 -> pool2
I0905 17:58:59.756269 31918 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 17:58:59.756274 31918 net.cpp:125] pool2 needs backward computation.
I0905 17:58:59.756283 31918 net.cpp:66] Creating Layer fc7
I0905 17:58:59.756289 31918 net.cpp:329] fc7 <- pool2
I0905 17:58:59.756296 31918 net.cpp:290] fc7 -> fc7
I0905 17:59:00.385164 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.385215 31918 net.cpp:125] fc7 needs backward computation.
I0905 17:59:00.385226 31918 net.cpp:66] Creating Layer relu7
I0905 17:59:00.385233 31918 net.cpp:329] relu7 <- fc7
I0905 17:59:00.385243 31918 net.cpp:280] relu7 -> fc7 (in-place)
I0905 17:59:00.385253 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.385259 31918 net.cpp:125] relu7 needs backward computation.
I0905 17:59:00.385277 31918 net.cpp:66] Creating Layer drop7
I0905 17:59:00.385283 31918 net.cpp:329] drop7 <- fc7
I0905 17:59:00.385289 31918 net.cpp:280] drop7 -> fc7 (in-place)
I0905 17:59:00.385300 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.385306 31918 net.cpp:125] drop7 needs backward computation.
I0905 17:59:00.385314 31918 net.cpp:66] Creating Layer fc8
I0905 17:59:00.385319 31918 net.cpp:329] fc8 <- fc7
I0905 17:59:00.385329 31918 net.cpp:290] fc8 -> fc8
I0905 17:59:00.392890 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.392902 31918 net.cpp:125] fc8 needs backward computation.
I0905 17:59:00.392910 31918 net.cpp:66] Creating Layer relu8
I0905 17:59:00.392915 31918 net.cpp:329] relu8 <- fc8
I0905 17:59:00.392922 31918 net.cpp:280] relu8 -> fc8 (in-place)
I0905 17:59:00.392930 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.392935 31918 net.cpp:125] relu8 needs backward computation.
I0905 17:59:00.392941 31918 net.cpp:66] Creating Layer drop8
I0905 17:59:00.392946 31918 net.cpp:329] drop8 <- fc8
I0905 17:59:00.392952 31918 net.cpp:280] drop8 -> fc8 (in-place)
I0905 17:59:00.392959 31918 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 17:59:00.392964 31918 net.cpp:125] drop8 needs backward computation.
I0905 17:59:00.392973 31918 net.cpp:66] Creating Layer fc9
I0905 17:59:00.392978 31918 net.cpp:329] fc9 <- fc8
I0905 17:59:00.392985 31918 net.cpp:290] fc9 -> fc9
I0905 17:59:00.393348 31918 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 17:59:00.393360 31918 net.cpp:125] fc9 needs backward computation.
I0905 17:59:00.393368 31918 net.cpp:66] Creating Layer fc10
I0905 17:59:00.393374 31918 net.cpp:329] fc10 <- fc9
I0905 17:59:00.393383 31918 net.cpp:290] fc10 -> fc10
I0905 17:59:00.393394 31918 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:59:00.393401 31918 net.cpp:125] fc10 needs backward computation.
I0905 17:59:00.393409 31918 net.cpp:66] Creating Layer prob
I0905 17:59:00.393414 31918 net.cpp:329] prob <- fc10
I0905 17:59:00.393421 31918 net.cpp:290] prob -> prob
I0905 17:59:00.393430 31918 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 17:59:00.393436 31918 net.cpp:125] prob needs backward computation.
I0905 17:59:00.393440 31918 net.cpp:156] This network produces output prob
I0905 17:59:00.393453 31918 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 17:59:00.393461 31918 net.cpp:167] Network initialization done.
I0905 17:59:00.393466 31918 net.cpp:168] Memory required for data: 6183480
Classifying 101 inputs.
Done in 62.67 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 1 is out of bounds for axis 0 with size 1
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:00:07.192270 31922 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:00:07.192405 31922 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:00:07.192414 31922 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:00:07.192559 31922 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:00:07.192620 31922 net.cpp:292] Input 0 -> data
I0905 18:00:07.192646 31922 net.cpp:66] Creating Layer conv1
I0905 18:00:07.192652 31922 net.cpp:329] conv1 <- data
I0905 18:00:07.192661 31922 net.cpp:290] conv1 -> conv1
I0905 18:00:07.194023 31922 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:00:07.194042 31922 net.cpp:125] conv1 needs backward computation.
I0905 18:00:07.194051 31922 net.cpp:66] Creating Layer relu1
I0905 18:00:07.194057 31922 net.cpp:329] relu1 <- conv1
I0905 18:00:07.194063 31922 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:00:07.194072 31922 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:00:07.194077 31922 net.cpp:125] relu1 needs backward computation.
I0905 18:00:07.194084 31922 net.cpp:66] Creating Layer pool1
I0905 18:00:07.194089 31922 net.cpp:329] pool1 <- conv1
I0905 18:00:07.194097 31922 net.cpp:290] pool1 -> pool1
I0905 18:00:07.194106 31922 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:00:07.194113 31922 net.cpp:125] pool1 needs backward computation.
I0905 18:00:07.194119 31922 net.cpp:66] Creating Layer norm1
I0905 18:00:07.194124 31922 net.cpp:329] norm1 <- pool1
I0905 18:00:07.194130 31922 net.cpp:290] norm1 -> norm1
I0905 18:00:07.194140 31922 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:00:07.194145 31922 net.cpp:125] norm1 needs backward computation.
I0905 18:00:07.194152 31922 net.cpp:66] Creating Layer conv2
I0905 18:00:07.194159 31922 net.cpp:329] conv2 <- norm1
I0905 18:00:07.194165 31922 net.cpp:290] conv2 -> conv2
I0905 18:00:07.203044 31922 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:00:07.203058 31922 net.cpp:125] conv2 needs backward computation.
I0905 18:00:07.203070 31922 net.cpp:66] Creating Layer relu2
I0905 18:00:07.203076 31922 net.cpp:329] relu2 <- conv2
I0905 18:00:07.203083 31922 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:00:07.203089 31922 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:00:07.203095 31922 net.cpp:125] relu2 needs backward computation.
I0905 18:00:07.203101 31922 net.cpp:66] Creating Layer pool2
I0905 18:00:07.203106 31922 net.cpp:329] pool2 <- conv2
I0905 18:00:07.203112 31922 net.cpp:290] pool2 -> pool2
I0905 18:00:07.203120 31922 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:00:07.203125 31922 net.cpp:125] pool2 needs backward computation.
I0905 18:00:07.203135 31922 net.cpp:66] Creating Layer fc7
I0905 18:00:07.203140 31922 net.cpp:329] fc7 <- pool2
I0905 18:00:07.203146 31922 net.cpp:290] fc7 -> fc7
I0905 18:00:07.849349 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.849387 31922 net.cpp:125] fc7 needs backward computation.
I0905 18:00:07.849400 31922 net.cpp:66] Creating Layer relu7
I0905 18:00:07.849407 31922 net.cpp:329] relu7 <- fc7
I0905 18:00:07.849416 31922 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:00:07.849426 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.849432 31922 net.cpp:125] relu7 needs backward computation.
I0905 18:00:07.849439 31922 net.cpp:66] Creating Layer drop7
I0905 18:00:07.849444 31922 net.cpp:329] drop7 <- fc7
I0905 18:00:07.849452 31922 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:00:07.849462 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.849467 31922 net.cpp:125] drop7 needs backward computation.
I0905 18:00:07.849475 31922 net.cpp:66] Creating Layer fc8
I0905 18:00:07.849480 31922 net.cpp:329] fc8 <- fc7
I0905 18:00:07.849489 31922 net.cpp:290] fc8 -> fc8
I0905 18:00:07.857290 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.857302 31922 net.cpp:125] fc8 needs backward computation.
I0905 18:00:07.857308 31922 net.cpp:66] Creating Layer relu8
I0905 18:00:07.857314 31922 net.cpp:329] relu8 <- fc8
I0905 18:00:07.857321 31922 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:00:07.857328 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.857334 31922 net.cpp:125] relu8 needs backward computation.
I0905 18:00:07.857341 31922 net.cpp:66] Creating Layer drop8
I0905 18:00:07.857345 31922 net.cpp:329] drop8 <- fc8
I0905 18:00:07.857352 31922 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:00:07.857358 31922 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:00:07.857363 31922 net.cpp:125] drop8 needs backward computation.
I0905 18:00:07.857372 31922 net.cpp:66] Creating Layer fc9
I0905 18:00:07.857378 31922 net.cpp:329] fc9 <- fc8
I0905 18:00:07.857384 31922 net.cpp:290] fc9 -> fc9
I0905 18:00:07.857756 31922 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:00:07.857769 31922 net.cpp:125] fc9 needs backward computation.
I0905 18:00:07.857777 31922 net.cpp:66] Creating Layer fc10
I0905 18:00:07.857782 31922 net.cpp:329] fc10 <- fc9
I0905 18:00:07.857790 31922 net.cpp:290] fc10 -> fc10
I0905 18:00:07.857802 31922 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:00:07.857810 31922 net.cpp:125] fc10 needs backward computation.
I0905 18:00:07.857816 31922 net.cpp:66] Creating Layer prob
I0905 18:00:07.857821 31922 net.cpp:329] prob <- fc10
I0905 18:00:07.857828 31922 net.cpp:290] prob -> prob
I0905 18:00:07.857838 31922 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:00:07.857843 31922 net.cpp:125] prob needs backward computation.
I0905 18:00:07.857848 31922 net.cpp:156] This network produces output prob
I0905 18:00:07.857861 31922 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:00:07.857868 31922 net.cpp:167] Network initialization done.
I0905 18:00:07.857873 31922 net.cpp:168] Memory required for data: 6183480
Classifying 218 inputs.
Done in 138.90 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 18 is out of bounds for axis 0 with size 18
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:02:31.455780 31929 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:02:31.455916 31929 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:02:31.455924 31929 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:02:31.456065 31929 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:02:31.456115 31929 net.cpp:292] Input 0 -> data
I0905 18:02:31.456140 31929 net.cpp:66] Creating Layer conv1
I0905 18:02:31.456146 31929 net.cpp:329] conv1 <- data
I0905 18:02:31.456154 31929 net.cpp:290] conv1 -> conv1
I0905 18:02:31.457473 31929 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:02:31.457501 31929 net.cpp:125] conv1 needs backward computation.
I0905 18:02:31.457510 31929 net.cpp:66] Creating Layer relu1
I0905 18:02:31.457516 31929 net.cpp:329] relu1 <- conv1
I0905 18:02:31.457522 31929 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:02:31.457530 31929 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:02:31.457536 31929 net.cpp:125] relu1 needs backward computation.
I0905 18:02:31.457542 31929 net.cpp:66] Creating Layer pool1
I0905 18:02:31.457548 31929 net.cpp:329] pool1 <- conv1
I0905 18:02:31.457554 31929 net.cpp:290] pool1 -> pool1
I0905 18:02:31.457566 31929 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:02:31.457571 31929 net.cpp:125] pool1 needs backward computation.
I0905 18:02:31.457602 31929 net.cpp:66] Creating Layer norm1
I0905 18:02:31.457610 31929 net.cpp:329] norm1 <- pool1
I0905 18:02:31.457624 31929 net.cpp:290] norm1 -> norm1
I0905 18:02:31.457634 31929 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:02:31.457640 31929 net.cpp:125] norm1 needs backward computation.
I0905 18:02:31.457648 31929 net.cpp:66] Creating Layer conv2
I0905 18:02:31.457656 31929 net.cpp:329] conv2 <- norm1
I0905 18:02:31.457664 31929 net.cpp:290] conv2 -> conv2
I0905 18:02:31.466511 31929 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:02:31.466526 31929 net.cpp:125] conv2 needs backward computation.
I0905 18:02:31.466532 31929 net.cpp:66] Creating Layer relu2
I0905 18:02:31.466537 31929 net.cpp:329] relu2 <- conv2
I0905 18:02:31.466543 31929 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:02:31.466550 31929 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:02:31.466555 31929 net.cpp:125] relu2 needs backward computation.
I0905 18:02:31.466562 31929 net.cpp:66] Creating Layer pool2
I0905 18:02:31.466567 31929 net.cpp:329] pool2 <- conv2
I0905 18:02:31.466572 31929 net.cpp:290] pool2 -> pool2
I0905 18:02:31.466580 31929 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:02:31.466585 31929 net.cpp:125] pool2 needs backward computation.
I0905 18:02:31.466594 31929 net.cpp:66] Creating Layer fc7
I0905 18:02:31.466599 31929 net.cpp:329] fc7 <- pool2
I0905 18:02:31.466606 31929 net.cpp:290] fc7 -> fc7
I0905 18:02:32.096451 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.096498 31929 net.cpp:125] fc7 needs backward computation.
I0905 18:02:32.096509 31929 net.cpp:66] Creating Layer relu7
I0905 18:02:32.096516 31929 net.cpp:329] relu7 <- fc7
I0905 18:02:32.096525 31929 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:02:32.096535 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.096540 31929 net.cpp:125] relu7 needs backward computation.
I0905 18:02:32.096547 31929 net.cpp:66] Creating Layer drop7
I0905 18:02:32.096552 31929 net.cpp:329] drop7 <- fc7
I0905 18:02:32.096559 31929 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:02:32.096570 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.096575 31929 net.cpp:125] drop7 needs backward computation.
I0905 18:02:32.096583 31929 net.cpp:66] Creating Layer fc8
I0905 18:02:32.096588 31929 net.cpp:329] fc8 <- fc7
I0905 18:02:32.096597 31929 net.cpp:290] fc8 -> fc8
I0905 18:02:32.104153 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.104166 31929 net.cpp:125] fc8 needs backward computation.
I0905 18:02:32.104172 31929 net.cpp:66] Creating Layer relu8
I0905 18:02:32.104177 31929 net.cpp:329] relu8 <- fc8
I0905 18:02:32.104184 31929 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:02:32.104192 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.104197 31929 net.cpp:125] relu8 needs backward computation.
I0905 18:02:32.104203 31929 net.cpp:66] Creating Layer drop8
I0905 18:02:32.104208 31929 net.cpp:329] drop8 <- fc8
I0905 18:02:32.104214 31929 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:02:32.104220 31929 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:02:32.104225 31929 net.cpp:125] drop8 needs backward computation.
I0905 18:02:32.104234 31929 net.cpp:66] Creating Layer fc9
I0905 18:02:32.104239 31929 net.cpp:329] fc9 <- fc8
I0905 18:02:32.104246 31929 net.cpp:290] fc9 -> fc9
I0905 18:02:32.104624 31929 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:02:32.104635 31929 net.cpp:125] fc9 needs backward computation.
I0905 18:02:32.104643 31929 net.cpp:66] Creating Layer fc10
I0905 18:02:32.104648 31929 net.cpp:329] fc10 <- fc9
I0905 18:02:32.104657 31929 net.cpp:290] fc10 -> fc10
I0905 18:02:32.104668 31929 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:02:32.104676 31929 net.cpp:125] fc10 needs backward computation.
I0905 18:02:32.104682 31929 net.cpp:66] Creating Layer prob
I0905 18:02:32.104687 31929 net.cpp:329] prob <- fc10
I0905 18:02:32.104694 31929 net.cpp:290] prob -> prob
I0905 18:02:32.104704 31929 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:02:32.104709 31929 net.cpp:125] prob needs backward computation.
I0905 18:02:32.104714 31929 net.cpp:156] This network produces output prob
I0905 18:02:32.104727 31929 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:02:32.104735 31929 net.cpp:167] Network initialization done.
I0905 18:02:32.104740 31929 net.cpp:168] Memory required for data: 6183480
Classifying 152 inputs.
Done in 94.55 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 52 is out of bounds for axis 0 with size 52
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:04:11.949223 31933 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:04:11.949362 31933 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:04:11.949370 31933 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:04:11.949517 31933 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:04:11.949597 31933 net.cpp:292] Input 0 -> data
I0905 18:04:11.949626 31933 net.cpp:66] Creating Layer conv1
I0905 18:04:11.949637 31933 net.cpp:329] conv1 <- data
I0905 18:04:11.949645 31933 net.cpp:290] conv1 -> conv1
I0905 18:04:11.951004 31933 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:04:11.951022 31933 net.cpp:125] conv1 needs backward computation.
I0905 18:04:11.951031 31933 net.cpp:66] Creating Layer relu1
I0905 18:04:11.951037 31933 net.cpp:329] relu1 <- conv1
I0905 18:04:11.951045 31933 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:04:11.951053 31933 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:04:11.951059 31933 net.cpp:125] relu1 needs backward computation.
I0905 18:04:11.951066 31933 net.cpp:66] Creating Layer pool1
I0905 18:04:11.951072 31933 net.cpp:329] pool1 <- conv1
I0905 18:04:11.951079 31933 net.cpp:290] pool1 -> pool1
I0905 18:04:11.951091 31933 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:04:11.951097 31933 net.cpp:125] pool1 needs backward computation.
I0905 18:04:11.951103 31933 net.cpp:66] Creating Layer norm1
I0905 18:04:11.951109 31933 net.cpp:329] norm1 <- pool1
I0905 18:04:11.951117 31933 net.cpp:290] norm1 -> norm1
I0905 18:04:11.951125 31933 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:04:11.951131 31933 net.cpp:125] norm1 needs backward computation.
I0905 18:04:11.951139 31933 net.cpp:66] Creating Layer conv2
I0905 18:04:11.951145 31933 net.cpp:329] conv2 <- norm1
I0905 18:04:11.951153 31933 net.cpp:290] conv2 -> conv2
I0905 18:04:11.960196 31933 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:04:11.960218 31933 net.cpp:125] conv2 needs backward computation.
I0905 18:04:11.960227 31933 net.cpp:66] Creating Layer relu2
I0905 18:04:11.960232 31933 net.cpp:329] relu2 <- conv2
I0905 18:04:11.960238 31933 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:04:11.960247 31933 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:04:11.960253 31933 net.cpp:125] relu2 needs backward computation.
I0905 18:04:11.960258 31933 net.cpp:66] Creating Layer pool2
I0905 18:04:11.960264 31933 net.cpp:329] pool2 <- conv2
I0905 18:04:11.960278 31933 net.cpp:290] pool2 -> pool2
I0905 18:04:11.960285 31933 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:04:11.960291 31933 net.cpp:125] pool2 needs backward computation.
I0905 18:04:11.960299 31933 net.cpp:66] Creating Layer fc7
I0905 18:04:11.960305 31933 net.cpp:329] fc7 <- pool2
I0905 18:04:11.960314 31933 net.cpp:290] fc7 -> fc7
I0905 18:04:12.587015 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.587060 31933 net.cpp:125] fc7 needs backward computation.
I0905 18:04:12.587072 31933 net.cpp:66] Creating Layer relu7
I0905 18:04:12.587080 31933 net.cpp:329] relu7 <- fc7
I0905 18:04:12.587090 31933 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:04:12.587100 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.587105 31933 net.cpp:125] relu7 needs backward computation.
I0905 18:04:12.587112 31933 net.cpp:66] Creating Layer drop7
I0905 18:04:12.587117 31933 net.cpp:329] drop7 <- fc7
I0905 18:04:12.587124 31933 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:04:12.587146 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.587152 31933 net.cpp:125] drop7 needs backward computation.
I0905 18:04:12.587160 31933 net.cpp:66] Creating Layer fc8
I0905 18:04:12.587167 31933 net.cpp:329] fc8 <- fc7
I0905 18:04:12.587175 31933 net.cpp:290] fc8 -> fc8
I0905 18:04:12.594748 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.594761 31933 net.cpp:125] fc8 needs backward computation.
I0905 18:04:12.594769 31933 net.cpp:66] Creating Layer relu8
I0905 18:04:12.594774 31933 net.cpp:329] relu8 <- fc8
I0905 18:04:12.594781 31933 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:04:12.594789 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.594794 31933 net.cpp:125] relu8 needs backward computation.
I0905 18:04:12.594800 31933 net.cpp:66] Creating Layer drop8
I0905 18:04:12.594806 31933 net.cpp:329] drop8 <- fc8
I0905 18:04:12.594812 31933 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:04:12.594820 31933 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:12.594825 31933 net.cpp:125] drop8 needs backward computation.
I0905 18:04:12.594833 31933 net.cpp:66] Creating Layer fc9
I0905 18:04:12.594840 31933 net.cpp:329] fc9 <- fc8
I0905 18:04:12.594846 31933 net.cpp:290] fc9 -> fc9
I0905 18:04:12.595208 31933 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:04:12.595221 31933 net.cpp:125] fc9 needs backward computation.
I0905 18:04:12.595228 31933 net.cpp:66] Creating Layer fc10
I0905 18:04:12.595234 31933 net.cpp:329] fc10 <- fc9
I0905 18:04:12.595242 31933 net.cpp:290] fc10 -> fc10
I0905 18:04:12.595253 31933 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:04:12.595262 31933 net.cpp:125] fc10 needs backward computation.
I0905 18:04:12.595268 31933 net.cpp:66] Creating Layer prob
I0905 18:04:12.595273 31933 net.cpp:329] prob <- fc10
I0905 18:04:12.595281 31933 net.cpp:290] prob -> prob
I0905 18:04:12.595290 31933 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:04:12.595296 31933 net.cpp:125] prob needs backward computation.
I0905 18:04:12.595301 31933 net.cpp:156] This network produces output prob
I0905 18:04:12.595314 31933 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:04:12.595322 31933 net.cpp:167] Network initialization done.
I0905 18:04:12.595327 31933 net.cpp:168] Memory required for data: 6183480
Classifying 61 inputs.
Done in 36.77 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:04:50.988399 31937 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:04:50.988535 31937 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:04:50.988543 31937 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:04:50.988685 31937 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:04:50.988746 31937 net.cpp:292] Input 0 -> data
I0905 18:04:50.988780 31937 net.cpp:66] Creating Layer conv1
I0905 18:04:50.988787 31937 net.cpp:329] conv1 <- data
I0905 18:04:50.988796 31937 net.cpp:290] conv1 -> conv1
I0905 18:04:50.990165 31937 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:04:50.990183 31937 net.cpp:125] conv1 needs backward computation.
I0905 18:04:50.990192 31937 net.cpp:66] Creating Layer relu1
I0905 18:04:50.990200 31937 net.cpp:329] relu1 <- conv1
I0905 18:04:50.990206 31937 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:04:50.990214 31937 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:04:50.990221 31937 net.cpp:125] relu1 needs backward computation.
I0905 18:04:50.990227 31937 net.cpp:66] Creating Layer pool1
I0905 18:04:50.990233 31937 net.cpp:329] pool1 <- conv1
I0905 18:04:50.990241 31937 net.cpp:290] pool1 -> pool1
I0905 18:04:50.990252 31937 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:04:50.990257 31937 net.cpp:125] pool1 needs backward computation.
I0905 18:04:50.990264 31937 net.cpp:66] Creating Layer norm1
I0905 18:04:50.990270 31937 net.cpp:329] norm1 <- pool1
I0905 18:04:50.990278 31937 net.cpp:290] norm1 -> norm1
I0905 18:04:50.990286 31937 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:04:50.990293 31937 net.cpp:125] norm1 needs backward computation.
I0905 18:04:50.990300 31937 net.cpp:66] Creating Layer conv2
I0905 18:04:50.990306 31937 net.cpp:329] conv2 <- norm1
I0905 18:04:50.990314 31937 net.cpp:290] conv2 -> conv2
I0905 18:04:50.999189 31937 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:04:50.999204 31937 net.cpp:125] conv2 needs backward computation.
I0905 18:04:50.999212 31937 net.cpp:66] Creating Layer relu2
I0905 18:04:50.999217 31937 net.cpp:329] relu2 <- conv2
I0905 18:04:50.999224 31937 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:04:50.999232 31937 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:04:50.999236 31937 net.cpp:125] relu2 needs backward computation.
I0905 18:04:50.999243 31937 net.cpp:66] Creating Layer pool2
I0905 18:04:50.999253 31937 net.cpp:329] pool2 <- conv2
I0905 18:04:50.999260 31937 net.cpp:290] pool2 -> pool2
I0905 18:04:50.999269 31937 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:04:50.999274 31937 net.cpp:125] pool2 needs backward computation.
I0905 18:04:50.999284 31937 net.cpp:66] Creating Layer fc7
I0905 18:04:50.999289 31937 net.cpp:329] fc7 <- pool2
I0905 18:04:50.999297 31937 net.cpp:290] fc7 -> fc7
I0905 18:04:51.631717 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.631765 31937 net.cpp:125] fc7 needs backward computation.
I0905 18:04:51.631778 31937 net.cpp:66] Creating Layer relu7
I0905 18:04:51.631784 31937 net.cpp:329] relu7 <- fc7
I0905 18:04:51.631794 31937 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:04:51.631805 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.631811 31937 net.cpp:125] relu7 needs backward computation.
I0905 18:04:51.631819 31937 net.cpp:66] Creating Layer drop7
I0905 18:04:51.631824 31937 net.cpp:329] drop7 <- fc7
I0905 18:04:51.631830 31937 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:04:51.631841 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.631847 31937 net.cpp:125] drop7 needs backward computation.
I0905 18:04:51.631856 31937 net.cpp:66] Creating Layer fc8
I0905 18:04:51.631861 31937 net.cpp:329] fc8 <- fc7
I0905 18:04:51.631870 31937 net.cpp:290] fc8 -> fc8
I0905 18:04:51.639427 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.639439 31937 net.cpp:125] fc8 needs backward computation.
I0905 18:04:51.639446 31937 net.cpp:66] Creating Layer relu8
I0905 18:04:51.639452 31937 net.cpp:329] relu8 <- fc8
I0905 18:04:51.639461 31937 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:04:51.639467 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.639473 31937 net.cpp:125] relu8 needs backward computation.
I0905 18:04:51.639480 31937 net.cpp:66] Creating Layer drop8
I0905 18:04:51.639487 31937 net.cpp:329] drop8 <- fc8
I0905 18:04:51.639492 31937 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:04:51.639499 31937 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:04:51.639505 31937 net.cpp:125] drop8 needs backward computation.
I0905 18:04:51.639514 31937 net.cpp:66] Creating Layer fc9
I0905 18:04:51.639519 31937 net.cpp:329] fc9 <- fc8
I0905 18:04:51.639526 31937 net.cpp:290] fc9 -> fc9
I0905 18:04:51.639889 31937 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:04:51.639900 31937 net.cpp:125] fc9 needs backward computation.
I0905 18:04:51.639909 31937 net.cpp:66] Creating Layer fc10
I0905 18:04:51.639915 31937 net.cpp:329] fc10 <- fc9
I0905 18:04:51.639924 31937 net.cpp:290] fc10 -> fc10
I0905 18:04:51.639935 31937 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:04:51.639942 31937 net.cpp:125] fc10 needs backward computation.
I0905 18:04:51.639950 31937 net.cpp:66] Creating Layer prob
I0905 18:04:51.639955 31937 net.cpp:329] prob <- fc10
I0905 18:04:51.639962 31937 net.cpp:290] prob -> prob
I0905 18:04:51.639972 31937 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:04:51.639978 31937 net.cpp:125] prob needs backward computation.
I0905 18:04:51.639983 31937 net.cpp:156] This network produces output prob
I0905 18:04:51.639997 31937 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:04:51.640004 31937 net.cpp:167] Network initialization done.
I0905 18:04:51.640010 31937 net.cpp:168] Memory required for data: 6183480
Classifying 134 inputs.
Done in 83.73 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 34 is out of bounds for axis 0 with size 34
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:06:18.893903 31942 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:06:18.894037 31942 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:06:18.894057 31942 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:06:18.894199 31942 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:06:18.894249 31942 net.cpp:292] Input 0 -> data
I0905 18:06:18.894274 31942 net.cpp:66] Creating Layer conv1
I0905 18:06:18.894281 31942 net.cpp:329] conv1 <- data
I0905 18:06:18.894289 31942 net.cpp:290] conv1 -> conv1
I0905 18:06:18.895625 31942 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:06:18.895643 31942 net.cpp:125] conv1 needs backward computation.
I0905 18:06:18.895651 31942 net.cpp:66] Creating Layer relu1
I0905 18:06:18.895658 31942 net.cpp:329] relu1 <- conv1
I0905 18:06:18.895664 31942 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:06:18.895673 31942 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:06:18.895678 31942 net.cpp:125] relu1 needs backward computation.
I0905 18:06:18.895684 31942 net.cpp:66] Creating Layer pool1
I0905 18:06:18.895702 31942 net.cpp:329] pool1 <- conv1
I0905 18:06:18.895709 31942 net.cpp:290] pool1 -> pool1
I0905 18:06:18.895721 31942 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:06:18.895727 31942 net.cpp:125] pool1 needs backward computation.
I0905 18:06:18.895733 31942 net.cpp:66] Creating Layer norm1
I0905 18:06:18.895740 31942 net.cpp:329] norm1 <- pool1
I0905 18:06:18.895746 31942 net.cpp:290] norm1 -> norm1
I0905 18:06:18.895756 31942 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:06:18.895761 31942 net.cpp:125] norm1 needs backward computation.
I0905 18:06:18.895769 31942 net.cpp:66] Creating Layer conv2
I0905 18:06:18.895774 31942 net.cpp:329] conv2 <- norm1
I0905 18:06:18.895781 31942 net.cpp:290] conv2 -> conv2
I0905 18:06:18.904665 31942 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:06:18.904680 31942 net.cpp:125] conv2 needs backward computation.
I0905 18:06:18.904687 31942 net.cpp:66] Creating Layer relu2
I0905 18:06:18.904693 31942 net.cpp:329] relu2 <- conv2
I0905 18:06:18.904700 31942 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:06:18.904707 31942 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:06:18.904712 31942 net.cpp:125] relu2 needs backward computation.
I0905 18:06:18.904718 31942 net.cpp:66] Creating Layer pool2
I0905 18:06:18.904723 31942 net.cpp:329] pool2 <- conv2
I0905 18:06:18.904731 31942 net.cpp:290] pool2 -> pool2
I0905 18:06:18.904737 31942 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:06:18.904743 31942 net.cpp:125] pool2 needs backward computation.
I0905 18:06:18.904752 31942 net.cpp:66] Creating Layer fc7
I0905 18:06:18.904757 31942 net.cpp:329] fc7 <- pool2
I0905 18:06:18.904765 31942 net.cpp:290] fc7 -> fc7
I0905 18:06:19.533264 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.533311 31942 net.cpp:125] fc7 needs backward computation.
I0905 18:06:19.533324 31942 net.cpp:66] Creating Layer relu7
I0905 18:06:19.533331 31942 net.cpp:329] relu7 <- fc7
I0905 18:06:19.533340 31942 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:06:19.533350 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.533356 31942 net.cpp:125] relu7 needs backward computation.
I0905 18:06:19.533363 31942 net.cpp:66] Creating Layer drop7
I0905 18:06:19.533368 31942 net.cpp:329] drop7 <- fc7
I0905 18:06:19.533375 31942 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:06:19.533385 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.533391 31942 net.cpp:125] drop7 needs backward computation.
I0905 18:06:19.533399 31942 net.cpp:66] Creating Layer fc8
I0905 18:06:19.533404 31942 net.cpp:329] fc8 <- fc7
I0905 18:06:19.533413 31942 net.cpp:290] fc8 -> fc8
I0905 18:06:19.540951 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.540962 31942 net.cpp:125] fc8 needs backward computation.
I0905 18:06:19.540969 31942 net.cpp:66] Creating Layer relu8
I0905 18:06:19.540974 31942 net.cpp:329] relu8 <- fc8
I0905 18:06:19.540982 31942 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:06:19.540989 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.540995 31942 net.cpp:125] relu8 needs backward computation.
I0905 18:06:19.541002 31942 net.cpp:66] Creating Layer drop8
I0905 18:06:19.541007 31942 net.cpp:329] drop8 <- fc8
I0905 18:06:19.541013 31942 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:06:19.541019 31942 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:06:19.541024 31942 net.cpp:125] drop8 needs backward computation.
I0905 18:06:19.541033 31942 net.cpp:66] Creating Layer fc9
I0905 18:06:19.541038 31942 net.cpp:329] fc9 <- fc8
I0905 18:06:19.541045 31942 net.cpp:290] fc9 -> fc9
I0905 18:06:19.541416 31942 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:06:19.541429 31942 net.cpp:125] fc9 needs backward computation.
I0905 18:06:19.541436 31942 net.cpp:66] Creating Layer fc10
I0905 18:06:19.541441 31942 net.cpp:329] fc10 <- fc9
I0905 18:06:19.541457 31942 net.cpp:290] fc10 -> fc10
I0905 18:06:19.541468 31942 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:06:19.541476 31942 net.cpp:125] fc10 needs backward computation.
I0905 18:06:19.541493 31942 net.cpp:66] Creating Layer prob
I0905 18:06:19.541499 31942 net.cpp:329] prob <- fc10
I0905 18:06:19.541507 31942 net.cpp:290] prob -> prob
I0905 18:06:19.541517 31942 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:06:19.541522 31942 net.cpp:125] prob needs backward computation.
I0905 18:06:19.541527 31942 net.cpp:156] This network produces output prob
I0905 18:06:19.541539 31942 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:06:19.541548 31942 net.cpp:167] Network initialization done.
I0905 18:06:19.541553 31942 net.cpp:168] Memory required for data: 6183480
Classifying 268 inputs.
Done in 163.70 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 68 is out of bounds for axis 0 with size 68
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:09:12.700687 31961 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:09:12.700824 31961 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:09:12.700831 31961 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:09:12.700975 31961 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:09:12.704638 31961 net.cpp:292] Input 0 -> data
I0905 18:09:12.704669 31961 net.cpp:66] Creating Layer conv1
I0905 18:09:12.704676 31961 net.cpp:329] conv1 <- data
I0905 18:09:12.704684 31961 net.cpp:290] conv1 -> conv1
I0905 18:09:12.706038 31961 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:09:12.706058 31961 net.cpp:125] conv1 needs backward computation.
I0905 18:09:12.706066 31961 net.cpp:66] Creating Layer relu1
I0905 18:09:12.706073 31961 net.cpp:329] relu1 <- conv1
I0905 18:09:12.706079 31961 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:09:12.706089 31961 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:09:12.706094 31961 net.cpp:125] relu1 needs backward computation.
I0905 18:09:12.706100 31961 net.cpp:66] Creating Layer pool1
I0905 18:09:12.706106 31961 net.cpp:329] pool1 <- conv1
I0905 18:09:12.706114 31961 net.cpp:290] pool1 -> pool1
I0905 18:09:12.706125 31961 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:09:12.706130 31961 net.cpp:125] pool1 needs backward computation.
I0905 18:09:12.706136 31961 net.cpp:66] Creating Layer norm1
I0905 18:09:12.706142 31961 net.cpp:329] norm1 <- pool1
I0905 18:09:12.706150 31961 net.cpp:290] norm1 -> norm1
I0905 18:09:12.706159 31961 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:09:12.706164 31961 net.cpp:125] norm1 needs backward computation.
I0905 18:09:12.706172 31961 net.cpp:66] Creating Layer conv2
I0905 18:09:12.706177 31961 net.cpp:329] conv2 <- norm1
I0905 18:09:12.706185 31961 net.cpp:290] conv2 -> conv2
I0905 18:09:12.715046 31961 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:09:12.715061 31961 net.cpp:125] conv2 needs backward computation.
I0905 18:09:12.715067 31961 net.cpp:66] Creating Layer relu2
I0905 18:09:12.715073 31961 net.cpp:329] relu2 <- conv2
I0905 18:09:12.715080 31961 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:09:12.715087 31961 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:09:12.715092 31961 net.cpp:125] relu2 needs backward computation.
I0905 18:09:12.715100 31961 net.cpp:66] Creating Layer pool2
I0905 18:09:12.715104 31961 net.cpp:329] pool2 <- conv2
I0905 18:09:12.715111 31961 net.cpp:290] pool2 -> pool2
I0905 18:09:12.715118 31961 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:09:12.715124 31961 net.cpp:125] pool2 needs backward computation.
I0905 18:09:12.715133 31961 net.cpp:66] Creating Layer fc7
I0905 18:09:12.715139 31961 net.cpp:329] fc7 <- pool2
I0905 18:09:12.715147 31961 net.cpp:290] fc7 -> fc7
I0905 18:09:13.352210 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.352257 31961 net.cpp:125] fc7 needs backward computation.
I0905 18:09:13.352270 31961 net.cpp:66] Creating Layer relu7
I0905 18:09:13.352277 31961 net.cpp:329] relu7 <- fc7
I0905 18:09:13.352288 31961 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:09:13.352298 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.352304 31961 net.cpp:125] relu7 needs backward computation.
I0905 18:09:13.352311 31961 net.cpp:66] Creating Layer drop7
I0905 18:09:13.352318 31961 net.cpp:329] drop7 <- fc7
I0905 18:09:13.352324 31961 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:09:13.352335 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.352341 31961 net.cpp:125] drop7 needs backward computation.
I0905 18:09:13.352349 31961 net.cpp:66] Creating Layer fc8
I0905 18:09:13.352355 31961 net.cpp:329] fc8 <- fc7
I0905 18:09:13.352365 31961 net.cpp:290] fc8 -> fc8
I0905 18:09:13.360127 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.360151 31961 net.cpp:125] fc8 needs backward computation.
I0905 18:09:13.360157 31961 net.cpp:66] Creating Layer relu8
I0905 18:09:13.360163 31961 net.cpp:329] relu8 <- fc8
I0905 18:09:13.360172 31961 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:09:13.360179 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.360185 31961 net.cpp:125] relu8 needs backward computation.
I0905 18:09:13.360193 31961 net.cpp:66] Creating Layer drop8
I0905 18:09:13.360198 31961 net.cpp:329] drop8 <- fc8
I0905 18:09:13.360204 31961 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:09:13.360211 31961 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:13.360216 31961 net.cpp:125] drop8 needs backward computation.
I0905 18:09:13.360225 31961 net.cpp:66] Creating Layer fc9
I0905 18:09:13.360231 31961 net.cpp:329] fc9 <- fc8
I0905 18:09:13.360239 31961 net.cpp:290] fc9 -> fc9
I0905 18:09:13.360612 31961 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:09:13.360625 31961 net.cpp:125] fc9 needs backward computation.
I0905 18:09:13.360632 31961 net.cpp:66] Creating Layer fc10
I0905 18:09:13.360638 31961 net.cpp:329] fc10 <- fc9
I0905 18:09:13.360646 31961 net.cpp:290] fc10 -> fc10
I0905 18:09:13.360658 31961 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:09:13.360667 31961 net.cpp:125] fc10 needs backward computation.
I0905 18:09:13.360673 31961 net.cpp:66] Creating Layer prob
I0905 18:09:13.360679 31961 net.cpp:329] prob <- fc10
I0905 18:09:13.360687 31961 net.cpp:290] prob -> prob
I0905 18:09:13.360697 31961 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:09:13.360703 31961 net.cpp:125] prob needs backward computation.
I0905 18:09:13.360708 31961 net.cpp:156] This network produces output prob
I0905 18:09:13.360723 31961 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:09:13.360731 31961 net.cpp:167] Network initialization done.
I0905 18:09:13.360736 31961 net.cpp:168] Memory required for data: 6183480
Classifying 2 inputs.
Done in 1.29 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:09:15.394201 31964 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:09:15.394338 31964 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:09:15.394346 31964 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:09:15.394490 31964 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:09:15.394552 31964 net.cpp:292] Input 0 -> data
I0905 18:09:15.394577 31964 net.cpp:66] Creating Layer conv1
I0905 18:09:15.394584 31964 net.cpp:329] conv1 <- data
I0905 18:09:15.394593 31964 net.cpp:290] conv1 -> conv1
I0905 18:09:15.395913 31964 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:09:15.395931 31964 net.cpp:125] conv1 needs backward computation.
I0905 18:09:15.395939 31964 net.cpp:66] Creating Layer relu1
I0905 18:09:15.395946 31964 net.cpp:329] relu1 <- conv1
I0905 18:09:15.395951 31964 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:09:15.395961 31964 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:09:15.395967 31964 net.cpp:125] relu1 needs backward computation.
I0905 18:09:15.395972 31964 net.cpp:66] Creating Layer pool1
I0905 18:09:15.395977 31964 net.cpp:329] pool1 <- conv1
I0905 18:09:15.395984 31964 net.cpp:290] pool1 -> pool1
I0905 18:09:15.395995 31964 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:09:15.396002 31964 net.cpp:125] pool1 needs backward computation.
I0905 18:09:15.396008 31964 net.cpp:66] Creating Layer norm1
I0905 18:09:15.396013 31964 net.cpp:329] norm1 <- pool1
I0905 18:09:15.396019 31964 net.cpp:290] norm1 -> norm1
I0905 18:09:15.396029 31964 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:09:15.396034 31964 net.cpp:125] norm1 needs backward computation.
I0905 18:09:15.396041 31964 net.cpp:66] Creating Layer conv2
I0905 18:09:15.396047 31964 net.cpp:329] conv2 <- norm1
I0905 18:09:15.396054 31964 net.cpp:290] conv2 -> conv2
I0905 18:09:15.404953 31964 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:09:15.404968 31964 net.cpp:125] conv2 needs backward computation.
I0905 18:09:15.404974 31964 net.cpp:66] Creating Layer relu2
I0905 18:09:15.404980 31964 net.cpp:329] relu2 <- conv2
I0905 18:09:15.404988 31964 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:09:15.404994 31964 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:09:15.404999 31964 net.cpp:125] relu2 needs backward computation.
I0905 18:09:15.405007 31964 net.cpp:66] Creating Layer pool2
I0905 18:09:15.405014 31964 net.cpp:329] pool2 <- conv2
I0905 18:09:15.405020 31964 net.cpp:290] pool2 -> pool2
I0905 18:09:15.405027 31964 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:09:15.405033 31964 net.cpp:125] pool2 needs backward computation.
I0905 18:09:15.405040 31964 net.cpp:66] Creating Layer fc7
I0905 18:09:15.405045 31964 net.cpp:329] fc7 <- pool2
I0905 18:09:15.405052 31964 net.cpp:290] fc7 -> fc7
I0905 18:09:16.033694 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.033740 31964 net.cpp:125] fc7 needs backward computation.
I0905 18:09:16.033752 31964 net.cpp:66] Creating Layer relu7
I0905 18:09:16.033759 31964 net.cpp:329] relu7 <- fc7
I0905 18:09:16.033768 31964 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:09:16.033778 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.033784 31964 net.cpp:125] relu7 needs backward computation.
I0905 18:09:16.033792 31964 net.cpp:66] Creating Layer drop7
I0905 18:09:16.033797 31964 net.cpp:329] drop7 <- fc7
I0905 18:09:16.033803 31964 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:09:16.033814 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.033820 31964 net.cpp:125] drop7 needs backward computation.
I0905 18:09:16.033828 31964 net.cpp:66] Creating Layer fc8
I0905 18:09:16.033834 31964 net.cpp:329] fc8 <- fc7
I0905 18:09:16.033843 31964 net.cpp:290] fc8 -> fc8
I0905 18:09:16.041419 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.041430 31964 net.cpp:125] fc8 needs backward computation.
I0905 18:09:16.041437 31964 net.cpp:66] Creating Layer relu8
I0905 18:09:16.041443 31964 net.cpp:329] relu8 <- fc8
I0905 18:09:16.041451 31964 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:09:16.041458 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.041463 31964 net.cpp:125] relu8 needs backward computation.
I0905 18:09:16.041470 31964 net.cpp:66] Creating Layer drop8
I0905 18:09:16.041476 31964 net.cpp:329] drop8 <- fc8
I0905 18:09:16.041481 31964 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:09:16.041488 31964 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:09:16.041493 31964 net.cpp:125] drop8 needs backward computation.
I0905 18:09:16.041502 31964 net.cpp:66] Creating Layer fc9
I0905 18:09:16.041508 31964 net.cpp:329] fc9 <- fc8
I0905 18:09:16.041515 31964 net.cpp:290] fc9 -> fc9
I0905 18:09:16.041882 31964 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:09:16.041894 31964 net.cpp:125] fc9 needs backward computation.
I0905 18:09:16.041903 31964 net.cpp:66] Creating Layer fc10
I0905 18:09:16.041908 31964 net.cpp:329] fc10 <- fc9
I0905 18:09:16.041916 31964 net.cpp:290] fc10 -> fc10
I0905 18:09:16.041928 31964 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:09:16.041935 31964 net.cpp:125] fc10 needs backward computation.
I0905 18:09:16.041941 31964 net.cpp:66] Creating Layer prob
I0905 18:09:16.041947 31964 net.cpp:329] prob <- fc10
I0905 18:09:16.041955 31964 net.cpp:290] prob -> prob
I0905 18:09:16.041965 31964 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:09:16.041970 31964 net.cpp:125] prob needs backward computation.
I0905 18:09:16.041975 31964 net.cpp:156] This network produces output prob
I0905 18:09:16.041987 31964 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:09:16.041996 31964 net.cpp:167] Network initialization done.
I0905 18:09:16.042001 31964 net.cpp:168] Memory required for data: 6183480
Classifying 88 inputs.
Done in 54.00 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:10:12.748252 31968 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:10:12.748389 31968 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:10:12.748397 31968 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:10:12.748548 31968 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:10:12.748617 31968 net.cpp:292] Input 0 -> data
I0905 18:10:12.748643 31968 net.cpp:66] Creating Layer conv1
I0905 18:10:12.748649 31968 net.cpp:329] conv1 <- data
I0905 18:10:12.748657 31968 net.cpp:290] conv1 -> conv1
I0905 18:10:12.750033 31968 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:10:12.750052 31968 net.cpp:125] conv1 needs backward computation.
I0905 18:10:12.750061 31968 net.cpp:66] Creating Layer relu1
I0905 18:10:12.750066 31968 net.cpp:329] relu1 <- conv1
I0905 18:10:12.750073 31968 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:10:12.750082 31968 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:10:12.750087 31968 net.cpp:125] relu1 needs backward computation.
I0905 18:10:12.750093 31968 net.cpp:66] Creating Layer pool1
I0905 18:10:12.750099 31968 net.cpp:329] pool1 <- conv1
I0905 18:10:12.750105 31968 net.cpp:290] pool1 -> pool1
I0905 18:10:12.750116 31968 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:10:12.750123 31968 net.cpp:125] pool1 needs backward computation.
I0905 18:10:12.750128 31968 net.cpp:66] Creating Layer norm1
I0905 18:10:12.750133 31968 net.cpp:329] norm1 <- pool1
I0905 18:10:12.750140 31968 net.cpp:290] norm1 -> norm1
I0905 18:10:12.750149 31968 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:10:12.750155 31968 net.cpp:125] norm1 needs backward computation.
I0905 18:10:12.750162 31968 net.cpp:66] Creating Layer conv2
I0905 18:10:12.750171 31968 net.cpp:329] conv2 <- norm1
I0905 18:10:12.750180 31968 net.cpp:290] conv2 -> conv2
I0905 18:10:12.759129 31968 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:10:12.759142 31968 net.cpp:125] conv2 needs backward computation.
I0905 18:10:12.759150 31968 net.cpp:66] Creating Layer relu2
I0905 18:10:12.759155 31968 net.cpp:329] relu2 <- conv2
I0905 18:10:12.759160 31968 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:10:12.759167 31968 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:10:12.759173 31968 net.cpp:125] relu2 needs backward computation.
I0905 18:10:12.759181 31968 net.cpp:66] Creating Layer pool2
I0905 18:10:12.759187 31968 net.cpp:329] pool2 <- conv2
I0905 18:10:12.759193 31968 net.cpp:290] pool2 -> pool2
I0905 18:10:12.759202 31968 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:10:12.759207 31968 net.cpp:125] pool2 needs backward computation.
I0905 18:10:12.759213 31968 net.cpp:66] Creating Layer fc7
I0905 18:10:12.759218 31968 net.cpp:329] fc7 <- pool2
I0905 18:10:12.759225 31968 net.cpp:290] fc7 -> fc7
I0905 18:10:13.387992 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.388036 31968 net.cpp:125] fc7 needs backward computation.
I0905 18:10:13.388049 31968 net.cpp:66] Creating Layer relu7
I0905 18:10:13.388056 31968 net.cpp:329] relu7 <- fc7
I0905 18:10:13.388066 31968 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:10:13.388075 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.388080 31968 net.cpp:125] relu7 needs backward computation.
I0905 18:10:13.388088 31968 net.cpp:66] Creating Layer drop7
I0905 18:10:13.388093 31968 net.cpp:329] drop7 <- fc7
I0905 18:10:13.388099 31968 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:10:13.388110 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.388115 31968 net.cpp:125] drop7 needs backward computation.
I0905 18:10:13.388124 31968 net.cpp:66] Creating Layer fc8
I0905 18:10:13.388129 31968 net.cpp:329] fc8 <- fc7
I0905 18:10:13.388139 31968 net.cpp:290] fc8 -> fc8
I0905 18:10:13.395678 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.395689 31968 net.cpp:125] fc8 needs backward computation.
I0905 18:10:13.395695 31968 net.cpp:66] Creating Layer relu8
I0905 18:10:13.395701 31968 net.cpp:329] relu8 <- fc8
I0905 18:10:13.395709 31968 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:10:13.395715 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.395721 31968 net.cpp:125] relu8 needs backward computation.
I0905 18:10:13.395727 31968 net.cpp:66] Creating Layer drop8
I0905 18:10:13.395732 31968 net.cpp:329] drop8 <- fc8
I0905 18:10:13.395738 31968 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:10:13.395745 31968 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:10:13.395750 31968 net.cpp:125] drop8 needs backward computation.
I0905 18:10:13.395758 31968 net.cpp:66] Creating Layer fc9
I0905 18:10:13.395764 31968 net.cpp:329] fc9 <- fc8
I0905 18:10:13.395771 31968 net.cpp:290] fc9 -> fc9
I0905 18:10:13.396136 31968 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:10:13.396147 31968 net.cpp:125] fc9 needs backward computation.
I0905 18:10:13.396154 31968 net.cpp:66] Creating Layer fc10
I0905 18:10:13.396159 31968 net.cpp:329] fc10 <- fc9
I0905 18:10:13.396168 31968 net.cpp:290] fc10 -> fc10
I0905 18:10:13.396179 31968 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:10:13.396186 31968 net.cpp:125] fc10 needs backward computation.
I0905 18:10:13.396193 31968 net.cpp:66] Creating Layer prob
I0905 18:10:13.396198 31968 net.cpp:329] prob <- fc10
I0905 18:10:13.396205 31968 net.cpp:290] prob -> prob
I0905 18:10:13.396214 31968 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:10:13.396220 31968 net.cpp:125] prob needs backward computation.
I0905 18:10:13.396225 31968 net.cpp:156] This network produces output prob
I0905 18:10:13.396237 31968 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:10:13.396245 31968 net.cpp:167] Network initialization done.
I0905 18:10:13.396250 31968 net.cpp:168] Memory required for data: 6183480
Classifying 103 inputs.
Done in 64.40 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 3 is out of bounds for axis 0 with size 3
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:11:20.729217 31973 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:11:20.729351 31973 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:11:20.729359 31973 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:11:20.729502 31973 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:11:20.729552 31973 net.cpp:292] Input 0 -> data
I0905 18:11:20.729578 31973 net.cpp:66] Creating Layer conv1
I0905 18:11:20.729629 31973 net.cpp:329] conv1 <- data
I0905 18:11:20.729639 31973 net.cpp:290] conv1 -> conv1
I0905 18:11:20.730970 31973 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:11:20.730988 31973 net.cpp:125] conv1 needs backward computation.
I0905 18:11:20.730996 31973 net.cpp:66] Creating Layer relu1
I0905 18:11:20.731003 31973 net.cpp:329] relu1 <- conv1
I0905 18:11:20.731009 31973 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:11:20.731017 31973 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:11:20.731022 31973 net.cpp:125] relu1 needs backward computation.
I0905 18:11:20.731029 31973 net.cpp:66] Creating Layer pool1
I0905 18:11:20.731034 31973 net.cpp:329] pool1 <- conv1
I0905 18:11:20.731040 31973 net.cpp:290] pool1 -> pool1
I0905 18:11:20.731051 31973 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:11:20.731057 31973 net.cpp:125] pool1 needs backward computation.
I0905 18:11:20.731063 31973 net.cpp:66] Creating Layer norm1
I0905 18:11:20.731068 31973 net.cpp:329] norm1 <- pool1
I0905 18:11:20.731076 31973 net.cpp:290] norm1 -> norm1
I0905 18:11:20.731084 31973 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:11:20.731089 31973 net.cpp:125] norm1 needs backward computation.
I0905 18:11:20.731096 31973 net.cpp:66] Creating Layer conv2
I0905 18:11:20.731102 31973 net.cpp:329] conv2 <- norm1
I0905 18:11:20.731108 31973 net.cpp:290] conv2 -> conv2
I0905 18:11:20.740025 31973 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:11:20.740039 31973 net.cpp:125] conv2 needs backward computation.
I0905 18:11:20.740046 31973 net.cpp:66] Creating Layer relu2
I0905 18:11:20.740052 31973 net.cpp:329] relu2 <- conv2
I0905 18:11:20.740058 31973 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:11:20.740066 31973 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:11:20.740070 31973 net.cpp:125] relu2 needs backward computation.
I0905 18:11:20.740077 31973 net.cpp:66] Creating Layer pool2
I0905 18:11:20.740082 31973 net.cpp:329] pool2 <- conv2
I0905 18:11:20.740088 31973 net.cpp:290] pool2 -> pool2
I0905 18:11:20.740095 31973 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:11:20.740100 31973 net.cpp:125] pool2 needs backward computation.
I0905 18:11:20.740109 31973 net.cpp:66] Creating Layer fc7
I0905 18:11:20.740115 31973 net.cpp:329] fc7 <- pool2
I0905 18:11:20.740123 31973 net.cpp:290] fc7 -> fc7
I0905 18:11:21.375401 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.375447 31973 net.cpp:125] fc7 needs backward computation.
I0905 18:11:21.375459 31973 net.cpp:66] Creating Layer relu7
I0905 18:11:21.375466 31973 net.cpp:329] relu7 <- fc7
I0905 18:11:21.375475 31973 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:11:21.375485 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.375490 31973 net.cpp:125] relu7 needs backward computation.
I0905 18:11:21.375497 31973 net.cpp:66] Creating Layer drop7
I0905 18:11:21.375502 31973 net.cpp:329] drop7 <- fc7
I0905 18:11:21.375509 31973 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:11:21.375520 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.375526 31973 net.cpp:125] drop7 needs backward computation.
I0905 18:11:21.375535 31973 net.cpp:66] Creating Layer fc8
I0905 18:11:21.375540 31973 net.cpp:329] fc8 <- fc7
I0905 18:11:21.375548 31973 net.cpp:290] fc8 -> fc8
I0905 18:11:21.383316 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.383328 31973 net.cpp:125] fc8 needs backward computation.
I0905 18:11:21.383334 31973 net.cpp:66] Creating Layer relu8
I0905 18:11:21.383340 31973 net.cpp:329] relu8 <- fc8
I0905 18:11:21.383348 31973 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:11:21.383355 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.383362 31973 net.cpp:125] relu8 needs backward computation.
I0905 18:11:21.383368 31973 net.cpp:66] Creating Layer drop8
I0905 18:11:21.383373 31973 net.cpp:329] drop8 <- fc8
I0905 18:11:21.383378 31973 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:11:21.383385 31973 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:21.383390 31973 net.cpp:125] drop8 needs backward computation.
I0905 18:11:21.383409 31973 net.cpp:66] Creating Layer fc9
I0905 18:11:21.383415 31973 net.cpp:329] fc9 <- fc8
I0905 18:11:21.383422 31973 net.cpp:290] fc9 -> fc9
I0905 18:11:21.383797 31973 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:11:21.383810 31973 net.cpp:125] fc9 needs backward computation.
I0905 18:11:21.383817 31973 net.cpp:66] Creating Layer fc10
I0905 18:11:21.383822 31973 net.cpp:329] fc10 <- fc9
I0905 18:11:21.383831 31973 net.cpp:290] fc10 -> fc10
I0905 18:11:21.383842 31973 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:11:21.383851 31973 net.cpp:125] fc10 needs backward computation.
I0905 18:11:21.383857 31973 net.cpp:66] Creating Layer prob
I0905 18:11:21.383862 31973 net.cpp:329] prob <- fc10
I0905 18:11:21.383870 31973 net.cpp:290] prob -> prob
I0905 18:11:21.383879 31973 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:11:21.383885 31973 net.cpp:125] prob needs backward computation.
I0905 18:11:21.383890 31973 net.cpp:156] This network produces output prob
I0905 18:11:21.383903 31973 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:11:21.383910 31973 net.cpp:167] Network initialization done.
I0905 18:11:21.383915 31973 net.cpp:168] Memory required for data: 6183480
Classifying 6 inputs.
Done in 3.87 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:11:26.022202 31976 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:11:26.022336 31976 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:11:26.022344 31976 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:11:26.022485 31976 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:11:26.022547 31976 net.cpp:292] Input 0 -> data
I0905 18:11:26.022572 31976 net.cpp:66] Creating Layer conv1
I0905 18:11:26.022579 31976 net.cpp:329] conv1 <- data
I0905 18:11:26.022588 31976 net.cpp:290] conv1 -> conv1
I0905 18:11:26.023906 31976 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:11:26.023923 31976 net.cpp:125] conv1 needs backward computation.
I0905 18:11:26.023932 31976 net.cpp:66] Creating Layer relu1
I0905 18:11:26.023938 31976 net.cpp:329] relu1 <- conv1
I0905 18:11:26.023946 31976 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:11:26.023953 31976 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:11:26.023959 31976 net.cpp:125] relu1 needs backward computation.
I0905 18:11:26.023967 31976 net.cpp:66] Creating Layer pool1
I0905 18:11:26.023972 31976 net.cpp:329] pool1 <- conv1
I0905 18:11:26.023978 31976 net.cpp:290] pool1 -> pool1
I0905 18:11:26.023989 31976 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:11:26.023995 31976 net.cpp:125] pool1 needs backward computation.
I0905 18:11:26.024001 31976 net.cpp:66] Creating Layer norm1
I0905 18:11:26.024008 31976 net.cpp:329] norm1 <- pool1
I0905 18:11:26.024013 31976 net.cpp:290] norm1 -> norm1
I0905 18:11:26.024024 31976 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:11:26.024029 31976 net.cpp:125] norm1 needs backward computation.
I0905 18:11:26.024036 31976 net.cpp:66] Creating Layer conv2
I0905 18:11:26.024041 31976 net.cpp:329] conv2 <- norm1
I0905 18:11:26.024049 31976 net.cpp:290] conv2 -> conv2
I0905 18:11:26.032924 31976 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:11:26.032939 31976 net.cpp:125] conv2 needs backward computation.
I0905 18:11:26.032946 31976 net.cpp:66] Creating Layer relu2
I0905 18:11:26.032953 31976 net.cpp:329] relu2 <- conv2
I0905 18:11:26.032958 31976 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:11:26.032965 31976 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:11:26.032971 31976 net.cpp:125] relu2 needs backward computation.
I0905 18:11:26.032977 31976 net.cpp:66] Creating Layer pool2
I0905 18:11:26.032982 31976 net.cpp:329] pool2 <- conv2
I0905 18:11:26.032989 31976 net.cpp:290] pool2 -> pool2
I0905 18:11:26.032996 31976 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:11:26.033002 31976 net.cpp:125] pool2 needs backward computation.
I0905 18:11:26.033011 31976 net.cpp:66] Creating Layer fc7
I0905 18:11:26.033017 31976 net.cpp:329] fc7 <- pool2
I0905 18:11:26.033025 31976 net.cpp:290] fc7 -> fc7
I0905 18:11:26.663880 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.663928 31976 net.cpp:125] fc7 needs backward computation.
I0905 18:11:26.663939 31976 net.cpp:66] Creating Layer relu7
I0905 18:11:26.663947 31976 net.cpp:329] relu7 <- fc7
I0905 18:11:26.663956 31976 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:11:26.663966 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.663972 31976 net.cpp:125] relu7 needs backward computation.
I0905 18:11:26.663980 31976 net.cpp:66] Creating Layer drop7
I0905 18:11:26.663985 31976 net.cpp:329] drop7 <- fc7
I0905 18:11:26.663990 31976 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:11:26.664012 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.664018 31976 net.cpp:125] drop7 needs backward computation.
I0905 18:11:26.664027 31976 net.cpp:66] Creating Layer fc8
I0905 18:11:26.664032 31976 net.cpp:329] fc8 <- fc7
I0905 18:11:26.664041 31976 net.cpp:290] fc8 -> fc8
I0905 18:11:26.671582 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.671594 31976 net.cpp:125] fc8 needs backward computation.
I0905 18:11:26.671602 31976 net.cpp:66] Creating Layer relu8
I0905 18:11:26.671607 31976 net.cpp:329] relu8 <- fc8
I0905 18:11:26.671615 31976 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:11:26.671622 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.671627 31976 net.cpp:125] relu8 needs backward computation.
I0905 18:11:26.671634 31976 net.cpp:66] Creating Layer drop8
I0905 18:11:26.671639 31976 net.cpp:329] drop8 <- fc8
I0905 18:11:26.671645 31976 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:11:26.671653 31976 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:11:26.671658 31976 net.cpp:125] drop8 needs backward computation.
I0905 18:11:26.671666 31976 net.cpp:66] Creating Layer fc9
I0905 18:11:26.671671 31976 net.cpp:329] fc9 <- fc8
I0905 18:11:26.671679 31976 net.cpp:290] fc9 -> fc9
I0905 18:11:26.672055 31976 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:11:26.672067 31976 net.cpp:125] fc9 needs backward computation.
I0905 18:11:26.672075 31976 net.cpp:66] Creating Layer fc10
I0905 18:11:26.672080 31976 net.cpp:329] fc10 <- fc9
I0905 18:11:26.672088 31976 net.cpp:290] fc10 -> fc10
I0905 18:11:26.672101 31976 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:11:26.672107 31976 net.cpp:125] fc10 needs backward computation.
I0905 18:11:26.672114 31976 net.cpp:66] Creating Layer prob
I0905 18:11:26.672119 31976 net.cpp:329] prob <- fc10
I0905 18:11:26.672127 31976 net.cpp:290] prob -> prob
I0905 18:11:26.672137 31976 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:11:26.672142 31976 net.cpp:125] prob needs backward computation.
I0905 18:11:26.672147 31976 net.cpp:156] This network produces output prob
I0905 18:11:26.672160 31976 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:11:26.672168 31976 net.cpp:167] Network initialization done.
I0905 18:11:26.672173 31976 net.cpp:168] Memory required for data: 6183480
Classifying 51 inputs.
Done in 31.54 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:12:00.134845 31986 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:12:00.134984 31986 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:12:00.134992 31986 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:12:00.135138 31986 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:12:00.135201 31986 net.cpp:292] Input 0 -> data
I0905 18:12:00.135227 31986 net.cpp:66] Creating Layer conv1
I0905 18:12:00.135234 31986 net.cpp:329] conv1 <- data
I0905 18:12:00.135242 31986 net.cpp:290] conv1 -> conv1
I0905 18:12:00.136626 31986 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:12:00.136644 31986 net.cpp:125] conv1 needs backward computation.
I0905 18:12:00.136653 31986 net.cpp:66] Creating Layer relu1
I0905 18:12:00.136659 31986 net.cpp:329] relu1 <- conv1
I0905 18:12:00.136665 31986 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:12:00.136674 31986 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:12:00.136679 31986 net.cpp:125] relu1 needs backward computation.
I0905 18:12:00.136687 31986 net.cpp:66] Creating Layer pool1
I0905 18:12:00.136693 31986 net.cpp:329] pool1 <- conv1
I0905 18:12:00.136698 31986 net.cpp:290] pool1 -> pool1
I0905 18:12:00.136709 31986 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:12:00.136715 31986 net.cpp:125] pool1 needs backward computation.
I0905 18:12:00.136723 31986 net.cpp:66] Creating Layer norm1
I0905 18:12:00.136728 31986 net.cpp:329] norm1 <- pool1
I0905 18:12:00.136734 31986 net.cpp:290] norm1 -> norm1
I0905 18:12:00.136744 31986 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:12:00.136749 31986 net.cpp:125] norm1 needs backward computation.
I0905 18:12:00.136757 31986 net.cpp:66] Creating Layer conv2
I0905 18:12:00.136762 31986 net.cpp:329] conv2 <- norm1
I0905 18:12:00.136770 31986 net.cpp:290] conv2 -> conv2
I0905 18:12:00.145958 31986 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:12:00.145973 31986 net.cpp:125] conv2 needs backward computation.
I0905 18:12:00.145980 31986 net.cpp:66] Creating Layer relu2
I0905 18:12:00.145987 31986 net.cpp:329] relu2 <- conv2
I0905 18:12:00.145993 31986 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:12:00.146000 31986 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:12:00.146005 31986 net.cpp:125] relu2 needs backward computation.
I0905 18:12:00.146013 31986 net.cpp:66] Creating Layer pool2
I0905 18:12:00.146018 31986 net.cpp:329] pool2 <- conv2
I0905 18:12:00.146030 31986 net.cpp:290] pool2 -> pool2
I0905 18:12:00.146039 31986 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:12:00.146044 31986 net.cpp:125] pool2 needs backward computation.
I0905 18:12:00.146061 31986 net.cpp:66] Creating Layer fc7
I0905 18:12:00.146066 31986 net.cpp:329] fc7 <- pool2
I0905 18:12:00.146073 31986 net.cpp:290] fc7 -> fc7
I0905 18:12:00.796151 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.796195 31986 net.cpp:125] fc7 needs backward computation.
I0905 18:12:00.796207 31986 net.cpp:66] Creating Layer relu7
I0905 18:12:00.796214 31986 net.cpp:329] relu7 <- fc7
I0905 18:12:00.796224 31986 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:12:00.796234 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.796241 31986 net.cpp:125] relu7 needs backward computation.
I0905 18:12:00.796247 31986 net.cpp:66] Creating Layer drop7
I0905 18:12:00.796252 31986 net.cpp:329] drop7 <- fc7
I0905 18:12:00.796259 31986 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:12:00.796270 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.796277 31986 net.cpp:125] drop7 needs backward computation.
I0905 18:12:00.796284 31986 net.cpp:66] Creating Layer fc8
I0905 18:12:00.796290 31986 net.cpp:329] fc8 <- fc7
I0905 18:12:00.796298 31986 net.cpp:290] fc8 -> fc8
I0905 18:12:00.804306 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.804318 31986 net.cpp:125] fc8 needs backward computation.
I0905 18:12:00.804327 31986 net.cpp:66] Creating Layer relu8
I0905 18:12:00.804332 31986 net.cpp:329] relu8 <- fc8
I0905 18:12:00.804339 31986 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:12:00.804347 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.804352 31986 net.cpp:125] relu8 needs backward computation.
I0905 18:12:00.804359 31986 net.cpp:66] Creating Layer drop8
I0905 18:12:00.804364 31986 net.cpp:329] drop8 <- fc8
I0905 18:12:00.804370 31986 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:12:00.804378 31986 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:00.804383 31986 net.cpp:125] drop8 needs backward computation.
I0905 18:12:00.804393 31986 net.cpp:66] Creating Layer fc9
I0905 18:12:00.804399 31986 net.cpp:329] fc9 <- fc8
I0905 18:12:00.804405 31986 net.cpp:290] fc9 -> fc9
I0905 18:12:00.804790 31986 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:12:00.804801 31986 net.cpp:125] fc9 needs backward computation.
I0905 18:12:00.804810 31986 net.cpp:66] Creating Layer fc10
I0905 18:12:00.804816 31986 net.cpp:329] fc10 <- fc9
I0905 18:12:00.804824 31986 net.cpp:290] fc10 -> fc10
I0905 18:12:00.804836 31986 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:12:00.804844 31986 net.cpp:125] fc10 needs backward computation.
I0905 18:12:00.804852 31986 net.cpp:66] Creating Layer prob
I0905 18:12:00.804857 31986 net.cpp:329] prob <- fc10
I0905 18:12:00.804864 31986 net.cpp:290] prob -> prob
I0905 18:12:00.804874 31986 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:12:00.804880 31986 net.cpp:125] prob needs backward computation.
I0905 18:12:00.804885 31986 net.cpp:156] This network produces output prob
I0905 18:12:00.804898 31986 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:12:00.804906 31986 net.cpp:167] Network initialization done.
I0905 18:12:00.804911 31986 net.cpp:168] Memory required for data: 6183480
Classifying 25 inputs.
Done in 16.04 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:12:18.031049 31991 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:12:18.031186 31991 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:12:18.031195 31991 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:12:18.031340 31991 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:12:18.031404 31991 net.cpp:292] Input 0 -> data
I0905 18:12:18.031429 31991 net.cpp:66] Creating Layer conv1
I0905 18:12:18.031435 31991 net.cpp:329] conv1 <- data
I0905 18:12:18.031445 31991 net.cpp:290] conv1 -> conv1
I0905 18:12:18.032825 31991 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:12:18.032843 31991 net.cpp:125] conv1 needs backward computation.
I0905 18:12:18.032852 31991 net.cpp:66] Creating Layer relu1
I0905 18:12:18.032858 31991 net.cpp:329] relu1 <- conv1
I0905 18:12:18.032866 31991 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:12:18.032873 31991 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:12:18.032879 31991 net.cpp:125] relu1 needs backward computation.
I0905 18:12:18.032886 31991 net.cpp:66] Creating Layer pool1
I0905 18:12:18.032891 31991 net.cpp:329] pool1 <- conv1
I0905 18:12:18.032898 31991 net.cpp:290] pool1 -> pool1
I0905 18:12:18.032909 31991 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:12:18.032915 31991 net.cpp:125] pool1 needs backward computation.
I0905 18:12:18.032922 31991 net.cpp:66] Creating Layer norm1
I0905 18:12:18.032932 31991 net.cpp:329] norm1 <- pool1
I0905 18:12:18.032938 31991 net.cpp:290] norm1 -> norm1
I0905 18:12:18.032948 31991 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:12:18.032954 31991 net.cpp:125] norm1 needs backward computation.
I0905 18:12:18.032961 31991 net.cpp:66] Creating Layer conv2
I0905 18:12:18.032968 31991 net.cpp:329] conv2 <- norm1
I0905 18:12:18.032974 31991 net.cpp:290] conv2 -> conv2
I0905 18:12:18.042137 31991 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:12:18.042153 31991 net.cpp:125] conv2 needs backward computation.
I0905 18:12:18.042167 31991 net.cpp:66] Creating Layer relu2
I0905 18:12:18.042173 31991 net.cpp:329] relu2 <- conv2
I0905 18:12:18.042181 31991 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:12:18.042187 31991 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:12:18.042193 31991 net.cpp:125] relu2 needs backward computation.
I0905 18:12:18.042199 31991 net.cpp:66] Creating Layer pool2
I0905 18:12:18.042206 31991 net.cpp:329] pool2 <- conv2
I0905 18:12:18.042212 31991 net.cpp:290] pool2 -> pool2
I0905 18:12:18.042219 31991 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:12:18.042225 31991 net.cpp:125] pool2 needs backward computation.
I0905 18:12:18.042234 31991 net.cpp:66] Creating Layer fc7
I0905 18:12:18.042240 31991 net.cpp:329] fc7 <- pool2
I0905 18:12:18.042248 31991 net.cpp:290] fc7 -> fc7
I0905 18:12:18.694829 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.694872 31991 net.cpp:125] fc7 needs backward computation.
I0905 18:12:18.694885 31991 net.cpp:66] Creating Layer relu7
I0905 18:12:18.694892 31991 net.cpp:329] relu7 <- fc7
I0905 18:12:18.694901 31991 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:12:18.694911 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.694917 31991 net.cpp:125] relu7 needs backward computation.
I0905 18:12:18.694924 31991 net.cpp:66] Creating Layer drop7
I0905 18:12:18.694929 31991 net.cpp:329] drop7 <- fc7
I0905 18:12:18.694936 31991 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:12:18.694947 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.694952 31991 net.cpp:125] drop7 needs backward computation.
I0905 18:12:18.694960 31991 net.cpp:66] Creating Layer fc8
I0905 18:12:18.694965 31991 net.cpp:329] fc8 <- fc7
I0905 18:12:18.694974 31991 net.cpp:290] fc8 -> fc8
I0905 18:12:18.702865 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.702878 31991 net.cpp:125] fc8 needs backward computation.
I0905 18:12:18.702886 31991 net.cpp:66] Creating Layer relu8
I0905 18:12:18.702891 31991 net.cpp:329] relu8 <- fc8
I0905 18:12:18.702900 31991 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:12:18.702908 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.702913 31991 net.cpp:125] relu8 needs backward computation.
I0905 18:12:18.702920 31991 net.cpp:66] Creating Layer drop8
I0905 18:12:18.702925 31991 net.cpp:329] drop8 <- fc8
I0905 18:12:18.702932 31991 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:12:18.702939 31991 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:12:18.702944 31991 net.cpp:125] drop8 needs backward computation.
I0905 18:12:18.702954 31991 net.cpp:66] Creating Layer fc9
I0905 18:12:18.702960 31991 net.cpp:329] fc9 <- fc8
I0905 18:12:18.702966 31991 net.cpp:290] fc9 -> fc9
I0905 18:12:18.703351 31991 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:12:18.703363 31991 net.cpp:125] fc9 needs backward computation.
I0905 18:12:18.703372 31991 net.cpp:66] Creating Layer fc10
I0905 18:12:18.703377 31991 net.cpp:329] fc10 <- fc9
I0905 18:12:18.703385 31991 net.cpp:290] fc10 -> fc10
I0905 18:12:18.703397 31991 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:12:18.703407 31991 net.cpp:125] fc10 needs backward computation.
I0905 18:12:18.703413 31991 net.cpp:66] Creating Layer prob
I0905 18:12:18.703418 31991 net.cpp:329] prob <- fc10
I0905 18:12:18.703426 31991 net.cpp:290] prob -> prob
I0905 18:12:18.703435 31991 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:12:18.703441 31991 net.cpp:125] prob needs backward computation.
I0905 18:12:18.703460 31991 net.cpp:156] This network produces output prob
I0905 18:12:18.703474 31991 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:12:18.703482 31991 net.cpp:167] Network initialization done.
I0905 18:12:18.703487 31991 net.cpp:168] Memory required for data: 6183480
Classifying 163 inputs.
Done in 101.60 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 63 is out of bounds for axis 0 with size 63
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:14:03.851269 31995 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:14:03.851409 31995 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:14:03.851418 31995 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:14:03.851569 31995 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:14:03.851634 31995 net.cpp:292] Input 0 -> data
I0905 18:14:03.851661 31995 net.cpp:66] Creating Layer conv1
I0905 18:14:03.851668 31995 net.cpp:329] conv1 <- data
I0905 18:14:03.851676 31995 net.cpp:290] conv1 -> conv1
I0905 18:14:03.853078 31995 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:14:03.853097 31995 net.cpp:125] conv1 needs backward computation.
I0905 18:14:03.853106 31995 net.cpp:66] Creating Layer relu1
I0905 18:14:03.853112 31995 net.cpp:329] relu1 <- conv1
I0905 18:14:03.853119 31995 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:14:03.853128 31995 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:14:03.853134 31995 net.cpp:125] relu1 needs backward computation.
I0905 18:14:03.853142 31995 net.cpp:66] Creating Layer pool1
I0905 18:14:03.853147 31995 net.cpp:329] pool1 <- conv1
I0905 18:14:03.853153 31995 net.cpp:290] pool1 -> pool1
I0905 18:14:03.853165 31995 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:14:03.853171 31995 net.cpp:125] pool1 needs backward computation.
I0905 18:14:03.853178 31995 net.cpp:66] Creating Layer norm1
I0905 18:14:03.853184 31995 net.cpp:329] norm1 <- pool1
I0905 18:14:03.853191 31995 net.cpp:290] norm1 -> norm1
I0905 18:14:03.853201 31995 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:14:03.853206 31995 net.cpp:125] norm1 needs backward computation.
I0905 18:14:03.853214 31995 net.cpp:66] Creating Layer conv2
I0905 18:14:03.853220 31995 net.cpp:329] conv2 <- norm1
I0905 18:14:03.853227 31995 net.cpp:290] conv2 -> conv2
I0905 18:14:03.862633 31995 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:14:03.862648 31995 net.cpp:125] conv2 needs backward computation.
I0905 18:14:03.862655 31995 net.cpp:66] Creating Layer relu2
I0905 18:14:03.862661 31995 net.cpp:329] relu2 <- conv2
I0905 18:14:03.862668 31995 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:14:03.862676 31995 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:14:03.862681 31995 net.cpp:125] relu2 needs backward computation.
I0905 18:14:03.862689 31995 net.cpp:66] Creating Layer pool2
I0905 18:14:03.862694 31995 net.cpp:329] pool2 <- conv2
I0905 18:14:03.862700 31995 net.cpp:290] pool2 -> pool2
I0905 18:14:03.862709 31995 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:14:03.862714 31995 net.cpp:125] pool2 needs backward computation.
I0905 18:14:03.862723 31995 net.cpp:66] Creating Layer fc7
I0905 18:14:03.862730 31995 net.cpp:329] fc7 <- pool2
I0905 18:14:03.862737 31995 net.cpp:290] fc7 -> fc7
I0905 18:14:04.509654 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.509698 31995 net.cpp:125] fc7 needs backward computation.
I0905 18:14:04.509711 31995 net.cpp:66] Creating Layer relu7
I0905 18:14:04.509718 31995 net.cpp:329] relu7 <- fc7
I0905 18:14:04.509728 31995 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:14:04.509738 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.509744 31995 net.cpp:125] relu7 needs backward computation.
I0905 18:14:04.509752 31995 net.cpp:66] Creating Layer drop7
I0905 18:14:04.509757 31995 net.cpp:329] drop7 <- fc7
I0905 18:14:04.509764 31995 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:14:04.509775 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.509781 31995 net.cpp:125] drop7 needs backward computation.
I0905 18:14:04.509790 31995 net.cpp:66] Creating Layer fc8
I0905 18:14:04.509795 31995 net.cpp:329] fc8 <- fc7
I0905 18:14:04.509804 31995 net.cpp:290] fc8 -> fc8
I0905 18:14:04.517802 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.517813 31995 net.cpp:125] fc8 needs backward computation.
I0905 18:14:04.517820 31995 net.cpp:66] Creating Layer relu8
I0905 18:14:04.517825 31995 net.cpp:329] relu8 <- fc8
I0905 18:14:04.517833 31995 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:14:04.517840 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.517858 31995 net.cpp:125] relu8 needs backward computation.
I0905 18:14:04.517865 31995 net.cpp:66] Creating Layer drop8
I0905 18:14:04.517870 31995 net.cpp:329] drop8 <- fc8
I0905 18:14:04.517876 31995 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:14:04.517884 31995 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:14:04.517889 31995 net.cpp:125] drop8 needs backward computation.
I0905 18:14:04.517897 31995 net.cpp:66] Creating Layer fc9
I0905 18:14:04.517904 31995 net.cpp:329] fc9 <- fc8
I0905 18:14:04.517910 31995 net.cpp:290] fc9 -> fc9
I0905 18:14:04.518282 31995 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:14:04.518295 31995 net.cpp:125] fc9 needs backward computation.
I0905 18:14:04.518302 31995 net.cpp:66] Creating Layer fc10
I0905 18:14:04.518308 31995 net.cpp:329] fc10 <- fc9
I0905 18:14:04.518316 31995 net.cpp:290] fc10 -> fc10
I0905 18:14:04.518328 31995 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:14:04.518337 31995 net.cpp:125] fc10 needs backward computation.
I0905 18:14:04.518342 31995 net.cpp:66] Creating Layer prob
I0905 18:14:04.518348 31995 net.cpp:329] prob <- fc10
I0905 18:14:04.518355 31995 net.cpp:290] prob -> prob
I0905 18:14:04.518365 31995 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:14:04.518371 31995 net.cpp:125] prob needs backward computation.
I0905 18:14:04.518375 31995 net.cpp:156] This network produces output prob
I0905 18:14:04.518388 31995 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:14:04.518396 31995 net.cpp:167] Network initialization done.
I0905 18:14:04.518401 31995 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:16:41.378669 32003 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:16:41.378875 32003 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:16:41.378885 32003 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:16:41.379046 32003 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:16:41.379108 32003 net.cpp:292] Input 0 -> data
I0905 18:16:41.379150 32003 net.cpp:66] Creating Layer conv1
I0905 18:16:41.379159 32003 net.cpp:329] conv1 <- data
I0905 18:16:41.379168 32003 net.cpp:290] conv1 -> conv1
I0905 18:16:41.411433 32003 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:16:41.411458 32003 net.cpp:125] conv1 needs backward computation.
I0905 18:16:41.411468 32003 net.cpp:66] Creating Layer relu1
I0905 18:16:41.411473 32003 net.cpp:329] relu1 <- conv1
I0905 18:16:41.411481 32003 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:16:41.411490 32003 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:16:41.411496 32003 net.cpp:125] relu1 needs backward computation.
I0905 18:16:41.411504 32003 net.cpp:66] Creating Layer pool1
I0905 18:16:41.411509 32003 net.cpp:329] pool1 <- conv1
I0905 18:16:41.411515 32003 net.cpp:290] pool1 -> pool1
I0905 18:16:41.411526 32003 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:16:41.411532 32003 net.cpp:125] pool1 needs backward computation.
I0905 18:16:41.411540 32003 net.cpp:66] Creating Layer norm1
I0905 18:16:41.411545 32003 net.cpp:329] norm1 <- pool1
I0905 18:16:41.411551 32003 net.cpp:290] norm1 -> norm1
I0905 18:16:41.411561 32003 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:16:41.411567 32003 net.cpp:125] norm1 needs backward computation.
I0905 18:16:41.411574 32003 net.cpp:66] Creating Layer conv2
I0905 18:16:41.411581 32003 net.cpp:329] conv2 <- norm1
I0905 18:16:41.411587 32003 net.cpp:290] conv2 -> conv2
I0905 18:16:41.420718 32003 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:16:41.420734 32003 net.cpp:125] conv2 needs backward computation.
I0905 18:16:41.420742 32003 net.cpp:66] Creating Layer relu2
I0905 18:16:41.420747 32003 net.cpp:329] relu2 <- conv2
I0905 18:16:41.420754 32003 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:16:41.420761 32003 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:16:41.420768 32003 net.cpp:125] relu2 needs backward computation.
I0905 18:16:41.420778 32003 net.cpp:66] Creating Layer pool2
I0905 18:16:41.420783 32003 net.cpp:329] pool2 <- conv2
I0905 18:16:41.420790 32003 net.cpp:290] pool2 -> pool2
I0905 18:16:41.420799 32003 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:16:41.420804 32003 net.cpp:125] pool2 needs backward computation.
I0905 18:16:41.420811 32003 net.cpp:66] Creating Layer fc7
I0905 18:16:41.420817 32003 net.cpp:329] fc7 <- pool2
I0905 18:16:41.420825 32003 net.cpp:290] fc7 -> fc7
I0905 18:16:42.072582 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.072628 32003 net.cpp:125] fc7 needs backward computation.
I0905 18:16:42.072640 32003 net.cpp:66] Creating Layer relu7
I0905 18:16:42.072648 32003 net.cpp:329] relu7 <- fc7
I0905 18:16:42.072656 32003 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:16:42.072679 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.072685 32003 net.cpp:125] relu7 needs backward computation.
I0905 18:16:42.072691 32003 net.cpp:66] Creating Layer drop7
I0905 18:16:42.072697 32003 net.cpp:329] drop7 <- fc7
I0905 18:16:42.072703 32003 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:16:42.072715 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.072721 32003 net.cpp:125] drop7 needs backward computation.
I0905 18:16:42.072729 32003 net.cpp:66] Creating Layer fc8
I0905 18:16:42.072734 32003 net.cpp:329] fc8 <- fc7
I0905 18:16:42.072743 32003 net.cpp:290] fc8 -> fc8
I0905 18:16:42.080538 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.080551 32003 net.cpp:125] fc8 needs backward computation.
I0905 18:16:42.080559 32003 net.cpp:66] Creating Layer relu8
I0905 18:16:42.080564 32003 net.cpp:329] relu8 <- fc8
I0905 18:16:42.080571 32003 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:16:42.080579 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.080585 32003 net.cpp:125] relu8 needs backward computation.
I0905 18:16:42.080591 32003 net.cpp:66] Creating Layer drop8
I0905 18:16:42.080597 32003 net.cpp:329] drop8 <- fc8
I0905 18:16:42.080603 32003 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:16:42.080610 32003 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:16:42.080615 32003 net.cpp:125] drop8 needs backward computation.
I0905 18:16:42.080624 32003 net.cpp:66] Creating Layer fc9
I0905 18:16:42.080631 32003 net.cpp:329] fc9 <- fc8
I0905 18:16:42.080637 32003 net.cpp:290] fc9 -> fc9
I0905 18:16:42.081010 32003 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:16:42.081022 32003 net.cpp:125] fc9 needs backward computation.
I0905 18:16:42.081032 32003 net.cpp:66] Creating Layer fc10
I0905 18:16:42.081037 32003 net.cpp:329] fc10 <- fc9
I0905 18:16:42.081045 32003 net.cpp:290] fc10 -> fc10
I0905 18:16:42.081058 32003 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:16:42.081065 32003 net.cpp:125] fc10 needs backward computation.
I0905 18:16:42.081071 32003 net.cpp:66] Creating Layer prob
I0905 18:16:42.081078 32003 net.cpp:329] prob <- fc10
I0905 18:16:42.081085 32003 net.cpp:290] prob -> prob
I0905 18:16:42.081094 32003 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:16:42.081100 32003 net.cpp:125] prob needs backward computation.
I0905 18:16:42.081105 32003 net.cpp:156] This network produces output prob
I0905 18:16:42.081118 32003 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:16:42.081127 32003 net.cpp:167] Network initialization done.
I0905 18:16:42.081132 32003 net.cpp:168] Memory required for data: 6183480
Classifying 244 inputs.
Done in 156.58 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 44 is out of bounds for axis 0 with size 44
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:19:26.306298 32014 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:19:26.306435 32014 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:19:26.306445 32014 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:19:26.306593 32014 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:19:26.306658 32014 net.cpp:292] Input 0 -> data
I0905 18:19:26.306684 32014 net.cpp:66] Creating Layer conv1
I0905 18:19:26.306691 32014 net.cpp:329] conv1 <- data
I0905 18:19:26.306699 32014 net.cpp:290] conv1 -> conv1
I0905 18:19:26.308063 32014 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:19:26.308081 32014 net.cpp:125] conv1 needs backward computation.
I0905 18:19:26.308090 32014 net.cpp:66] Creating Layer relu1
I0905 18:19:26.308096 32014 net.cpp:329] relu1 <- conv1
I0905 18:19:26.308104 32014 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:19:26.308112 32014 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:19:26.308118 32014 net.cpp:125] relu1 needs backward computation.
I0905 18:19:26.308125 32014 net.cpp:66] Creating Layer pool1
I0905 18:19:26.308131 32014 net.cpp:329] pool1 <- conv1
I0905 18:19:26.308138 32014 net.cpp:290] pool1 -> pool1
I0905 18:19:26.308150 32014 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:19:26.308156 32014 net.cpp:125] pool1 needs backward computation.
I0905 18:19:26.308162 32014 net.cpp:66] Creating Layer norm1
I0905 18:19:26.308168 32014 net.cpp:329] norm1 <- pool1
I0905 18:19:26.308174 32014 net.cpp:290] norm1 -> norm1
I0905 18:19:26.308184 32014 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:19:26.308190 32014 net.cpp:125] norm1 needs backward computation.
I0905 18:19:26.308198 32014 net.cpp:66] Creating Layer conv2
I0905 18:19:26.308204 32014 net.cpp:329] conv2 <- norm1
I0905 18:19:26.308212 32014 net.cpp:290] conv2 -> conv2
I0905 18:19:26.317384 32014 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:19:26.317400 32014 net.cpp:125] conv2 needs backward computation.
I0905 18:19:26.317409 32014 net.cpp:66] Creating Layer relu2
I0905 18:19:26.317414 32014 net.cpp:329] relu2 <- conv2
I0905 18:19:26.317421 32014 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:19:26.317428 32014 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:19:26.317435 32014 net.cpp:125] relu2 needs backward computation.
I0905 18:19:26.317441 32014 net.cpp:66] Creating Layer pool2
I0905 18:19:26.317446 32014 net.cpp:329] pool2 <- conv2
I0905 18:19:26.317453 32014 net.cpp:290] pool2 -> pool2
I0905 18:19:26.317461 32014 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:19:26.317467 32014 net.cpp:125] pool2 needs backward computation.
I0905 18:19:26.317477 32014 net.cpp:66] Creating Layer fc7
I0905 18:19:26.317483 32014 net.cpp:329] fc7 <- pool2
I0905 18:19:26.317491 32014 net.cpp:290] fc7 -> fc7
I0905 18:19:26.966301 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.966347 32014 net.cpp:125] fc7 needs backward computation.
I0905 18:19:26.966361 32014 net.cpp:66] Creating Layer relu7
I0905 18:19:26.966368 32014 net.cpp:329] relu7 <- fc7
I0905 18:19:26.966378 32014 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:19:26.966389 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.966395 32014 net.cpp:125] relu7 needs backward computation.
I0905 18:19:26.966403 32014 net.cpp:66] Creating Layer drop7
I0905 18:19:26.966408 32014 net.cpp:329] drop7 <- fc7
I0905 18:19:26.966415 32014 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:19:26.966426 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.966433 32014 net.cpp:125] drop7 needs backward computation.
I0905 18:19:26.966442 32014 net.cpp:66] Creating Layer fc8
I0905 18:19:26.966449 32014 net.cpp:329] fc8 <- fc7
I0905 18:19:26.966457 32014 net.cpp:290] fc8 -> fc8
I0905 18:19:26.974473 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.974488 32014 net.cpp:125] fc8 needs backward computation.
I0905 18:19:26.974494 32014 net.cpp:66] Creating Layer relu8
I0905 18:19:26.974500 32014 net.cpp:329] relu8 <- fc8
I0905 18:19:26.974509 32014 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:19:26.974517 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.974524 32014 net.cpp:125] relu8 needs backward computation.
I0905 18:19:26.974530 32014 net.cpp:66] Creating Layer drop8
I0905 18:19:26.974535 32014 net.cpp:329] drop8 <- fc8
I0905 18:19:26.974542 32014 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:19:26.974550 32014 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:19:26.974555 32014 net.cpp:125] drop8 needs backward computation.
I0905 18:19:26.974565 32014 net.cpp:66] Creating Layer fc9
I0905 18:19:26.974570 32014 net.cpp:329] fc9 <- fc8
I0905 18:19:26.974577 32014 net.cpp:290] fc9 -> fc9
I0905 18:19:26.974964 32014 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:19:26.974977 32014 net.cpp:125] fc9 needs backward computation.
I0905 18:19:26.974985 32014 net.cpp:66] Creating Layer fc10
I0905 18:19:26.974992 32014 net.cpp:329] fc10 <- fc9
I0905 18:19:26.975000 32014 net.cpp:290] fc10 -> fc10
I0905 18:19:26.975013 32014 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:19:26.975020 32014 net.cpp:125] fc10 needs backward computation.
I0905 18:19:26.975028 32014 net.cpp:66] Creating Layer prob
I0905 18:19:26.975033 32014 net.cpp:329] prob <- fc10
I0905 18:19:26.975041 32014 net.cpp:290] prob -> prob
I0905 18:19:26.975051 32014 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:19:26.975057 32014 net.cpp:125] prob needs backward computation.
I0905 18:19:26.975064 32014 net.cpp:156] This network produces output prob
I0905 18:19:26.975076 32014 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:19:26.975085 32014 net.cpp:167] Network initialization done.
I0905 18:19:26.975090 32014 net.cpp:168] Memory required for data: 6183480
Classifying 229 inputs.
Done in 138.28 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 29 is out of bounds for axis 0 with size 29
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:21:51.421947 32020 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:21:51.422086 32020 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:21:51.422096 32020 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:21:51.422240 32020 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:21:51.422294 32020 net.cpp:292] Input 0 -> data
I0905 18:21:51.422320 32020 net.cpp:66] Creating Layer conv1
I0905 18:21:51.422327 32020 net.cpp:329] conv1 <- data
I0905 18:21:51.422345 32020 net.cpp:290] conv1 -> conv1
I0905 18:21:51.423707 32020 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:21:51.423724 32020 net.cpp:125] conv1 needs backward computation.
I0905 18:21:51.423734 32020 net.cpp:66] Creating Layer relu1
I0905 18:21:51.423739 32020 net.cpp:329] relu1 <- conv1
I0905 18:21:51.423746 32020 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:21:51.423755 32020 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:21:51.423761 32020 net.cpp:125] relu1 needs backward computation.
I0905 18:21:51.423768 32020 net.cpp:66] Creating Layer pool1
I0905 18:21:51.423774 32020 net.cpp:329] pool1 <- conv1
I0905 18:21:51.423780 32020 net.cpp:290] pool1 -> pool1
I0905 18:21:51.423791 32020 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:21:51.423797 32020 net.cpp:125] pool1 needs backward computation.
I0905 18:21:51.423804 32020 net.cpp:66] Creating Layer norm1
I0905 18:21:51.423810 32020 net.cpp:329] norm1 <- pool1
I0905 18:21:51.423816 32020 net.cpp:290] norm1 -> norm1
I0905 18:21:51.423826 32020 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:21:51.423832 32020 net.cpp:125] norm1 needs backward computation.
I0905 18:21:51.423840 32020 net.cpp:66] Creating Layer conv2
I0905 18:21:51.423846 32020 net.cpp:329] conv2 <- norm1
I0905 18:21:51.423852 32020 net.cpp:290] conv2 -> conv2
I0905 18:21:51.433003 32020 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:21:51.433018 32020 net.cpp:125] conv2 needs backward computation.
I0905 18:21:51.433027 32020 net.cpp:66] Creating Layer relu2
I0905 18:21:51.433032 32020 net.cpp:329] relu2 <- conv2
I0905 18:21:51.433038 32020 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:21:51.433045 32020 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:21:51.433051 32020 net.cpp:125] relu2 needs backward computation.
I0905 18:21:51.433059 32020 net.cpp:66] Creating Layer pool2
I0905 18:21:51.433065 32020 net.cpp:329] pool2 <- conv2
I0905 18:21:51.433073 32020 net.cpp:290] pool2 -> pool2
I0905 18:21:51.433080 32020 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:21:51.433086 32020 net.cpp:125] pool2 needs backward computation.
I0905 18:21:51.433094 32020 net.cpp:66] Creating Layer fc7
I0905 18:21:51.433099 32020 net.cpp:329] fc7 <- pool2
I0905 18:21:51.433105 32020 net.cpp:290] fc7 -> fc7
I0905 18:21:52.085201 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.085247 32020 net.cpp:125] fc7 needs backward computation.
I0905 18:21:52.085259 32020 net.cpp:66] Creating Layer relu7
I0905 18:21:52.085266 32020 net.cpp:329] relu7 <- fc7
I0905 18:21:52.085276 32020 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:21:52.085286 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.085291 32020 net.cpp:125] relu7 needs backward computation.
I0905 18:21:52.085299 32020 net.cpp:66] Creating Layer drop7
I0905 18:21:52.085304 32020 net.cpp:329] drop7 <- fc7
I0905 18:21:52.085311 32020 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:21:52.085322 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.085328 32020 net.cpp:125] drop7 needs backward computation.
I0905 18:21:52.085336 32020 net.cpp:66] Creating Layer fc8
I0905 18:21:52.085342 32020 net.cpp:329] fc8 <- fc7
I0905 18:21:52.085351 32020 net.cpp:290] fc8 -> fc8
I0905 18:21:52.093116 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.093129 32020 net.cpp:125] fc8 needs backward computation.
I0905 18:21:52.093135 32020 net.cpp:66] Creating Layer relu8
I0905 18:21:52.093142 32020 net.cpp:329] relu8 <- fc8
I0905 18:21:52.093149 32020 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:21:52.093157 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.093163 32020 net.cpp:125] relu8 needs backward computation.
I0905 18:21:52.093168 32020 net.cpp:66] Creating Layer drop8
I0905 18:21:52.093174 32020 net.cpp:329] drop8 <- fc8
I0905 18:21:52.093180 32020 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:21:52.093188 32020 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:21:52.093193 32020 net.cpp:125] drop8 needs backward computation.
I0905 18:21:52.093201 32020 net.cpp:66] Creating Layer fc9
I0905 18:21:52.093219 32020 net.cpp:329] fc9 <- fc8
I0905 18:21:52.093225 32020 net.cpp:290] fc9 -> fc9
I0905 18:21:52.093605 32020 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:21:52.093617 32020 net.cpp:125] fc9 needs backward computation.
I0905 18:21:52.093626 32020 net.cpp:66] Creating Layer fc10
I0905 18:21:52.093631 32020 net.cpp:329] fc10 <- fc9
I0905 18:21:52.093639 32020 net.cpp:290] fc10 -> fc10
I0905 18:21:52.093652 32020 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:21:52.093659 32020 net.cpp:125] fc10 needs backward computation.
I0905 18:21:52.093665 32020 net.cpp:66] Creating Layer prob
I0905 18:21:52.093672 32020 net.cpp:329] prob <- fc10
I0905 18:21:52.093679 32020 net.cpp:290] prob -> prob
I0905 18:21:52.093688 32020 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:21:52.093694 32020 net.cpp:125] prob needs backward computation.
I0905 18:21:52.093699 32020 net.cpp:156] This network produces output prob
I0905 18:21:52.093713 32020 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:21:52.093720 32020 net.cpp:167] Network initialization done.
I0905 18:21:52.093725 32020 net.cpp:168] Memory required for data: 6183480
Classifying 49 inputs.
Done in 31.57 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:22:26.847738 32023 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:22:26.847888 32023 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:22:26.847898 32023 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:22:26.848057 32023 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:22:26.848120 32023 net.cpp:292] Input 0 -> data
I0905 18:22:26.848146 32023 net.cpp:66] Creating Layer conv1
I0905 18:22:26.848153 32023 net.cpp:329] conv1 <- data
I0905 18:22:26.848161 32023 net.cpp:290] conv1 -> conv1
I0905 18:22:26.849524 32023 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:22:26.849541 32023 net.cpp:125] conv1 needs backward computation.
I0905 18:22:26.849550 32023 net.cpp:66] Creating Layer relu1
I0905 18:22:26.849556 32023 net.cpp:329] relu1 <- conv1
I0905 18:22:26.849563 32023 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:22:26.849571 32023 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:22:26.849591 32023 net.cpp:125] relu1 needs backward computation.
I0905 18:22:26.849601 32023 net.cpp:66] Creating Layer pool1
I0905 18:22:26.849607 32023 net.cpp:329] pool1 <- conv1
I0905 18:22:26.849614 32023 net.cpp:290] pool1 -> pool1
I0905 18:22:26.849625 32023 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:22:26.849632 32023 net.cpp:125] pool1 needs backward computation.
I0905 18:22:26.849638 32023 net.cpp:66] Creating Layer norm1
I0905 18:22:26.849644 32023 net.cpp:329] norm1 <- pool1
I0905 18:22:26.849650 32023 net.cpp:290] norm1 -> norm1
I0905 18:22:26.849660 32023 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:22:26.849666 32023 net.cpp:125] norm1 needs backward computation.
I0905 18:22:26.849673 32023 net.cpp:66] Creating Layer conv2
I0905 18:22:26.849679 32023 net.cpp:329] conv2 <- norm1
I0905 18:22:26.849686 32023 net.cpp:290] conv2 -> conv2
I0905 18:22:26.858822 32023 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:22:26.858836 32023 net.cpp:125] conv2 needs backward computation.
I0905 18:22:26.858844 32023 net.cpp:66] Creating Layer relu2
I0905 18:22:26.858850 32023 net.cpp:329] relu2 <- conv2
I0905 18:22:26.858856 32023 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:22:26.858863 32023 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:22:26.858868 32023 net.cpp:125] relu2 needs backward computation.
I0905 18:22:26.858875 32023 net.cpp:66] Creating Layer pool2
I0905 18:22:26.858880 32023 net.cpp:329] pool2 <- conv2
I0905 18:22:26.858886 32023 net.cpp:290] pool2 -> pool2
I0905 18:22:26.858894 32023 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:22:26.858901 32023 net.cpp:125] pool2 needs backward computation.
I0905 18:22:26.858908 32023 net.cpp:66] Creating Layer fc7
I0905 18:22:26.858914 32023 net.cpp:329] fc7 <- pool2
I0905 18:22:26.858922 32023 net.cpp:290] fc7 -> fc7
I0905 18:22:27.509799 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.509846 32023 net.cpp:125] fc7 needs backward computation.
I0905 18:22:27.509860 32023 net.cpp:66] Creating Layer relu7
I0905 18:22:27.509866 32023 net.cpp:329] relu7 <- fc7
I0905 18:22:27.509876 32023 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:22:27.509886 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.509893 32023 net.cpp:125] relu7 needs backward computation.
I0905 18:22:27.509901 32023 net.cpp:66] Creating Layer drop7
I0905 18:22:27.509907 32023 net.cpp:329] drop7 <- fc7
I0905 18:22:27.509912 32023 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:22:27.509923 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.509929 32023 net.cpp:125] drop7 needs backward computation.
I0905 18:22:27.509948 32023 net.cpp:66] Creating Layer fc8
I0905 18:22:27.509954 32023 net.cpp:329] fc8 <- fc7
I0905 18:22:27.509964 32023 net.cpp:290] fc8 -> fc8
I0905 18:22:27.517981 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.517993 32023 net.cpp:125] fc8 needs backward computation.
I0905 18:22:27.518000 32023 net.cpp:66] Creating Layer relu8
I0905 18:22:27.518007 32023 net.cpp:329] relu8 <- fc8
I0905 18:22:27.518014 32023 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:22:27.518021 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.518028 32023 net.cpp:125] relu8 needs backward computation.
I0905 18:22:27.518034 32023 net.cpp:66] Creating Layer drop8
I0905 18:22:27.518039 32023 net.cpp:329] drop8 <- fc8
I0905 18:22:27.518045 32023 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:22:27.518053 32023 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:22:27.518059 32023 net.cpp:125] drop8 needs backward computation.
I0905 18:22:27.518067 32023 net.cpp:66] Creating Layer fc9
I0905 18:22:27.518074 32023 net.cpp:329] fc9 <- fc8
I0905 18:22:27.518080 32023 net.cpp:290] fc9 -> fc9
I0905 18:22:27.518465 32023 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:22:27.518477 32023 net.cpp:125] fc9 needs backward computation.
I0905 18:22:27.518486 32023 net.cpp:66] Creating Layer fc10
I0905 18:22:27.518491 32023 net.cpp:329] fc10 <- fc9
I0905 18:22:27.518499 32023 net.cpp:290] fc10 -> fc10
I0905 18:22:27.518512 32023 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:22:27.518519 32023 net.cpp:125] fc10 needs backward computation.
I0905 18:22:27.518527 32023 net.cpp:66] Creating Layer prob
I0905 18:22:27.518532 32023 net.cpp:329] prob <- fc10
I0905 18:22:27.518539 32023 net.cpp:290] prob -> prob
I0905 18:22:27.518549 32023 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:22:27.518554 32023 net.cpp:125] prob needs backward computation.
I0905 18:22:27.518559 32023 net.cpp:156] This network produces output prob
I0905 18:22:27.518573 32023 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:22:27.518581 32023 net.cpp:167] Network initialization done.
I0905 18:22:27.518586 32023 net.cpp:168] Memory required for data: 6183480
Classifying 310 inputs.
Done in 206.46 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 10 is out of bounds for axis 0 with size 10
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:26:00.860894 32031 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:26:00.861034 32031 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:26:00.861044 32031 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:26:00.861191 32031 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:26:00.861254 32031 net.cpp:292] Input 0 -> data
I0905 18:26:00.861280 32031 net.cpp:66] Creating Layer conv1
I0905 18:26:00.861287 32031 net.cpp:329] conv1 <- data
I0905 18:26:00.861296 32031 net.cpp:290] conv1 -> conv1
I0905 18:26:00.862690 32031 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:26:00.862710 32031 net.cpp:125] conv1 needs backward computation.
I0905 18:26:00.862720 32031 net.cpp:66] Creating Layer relu1
I0905 18:26:00.862725 32031 net.cpp:329] relu1 <- conv1
I0905 18:26:00.862732 32031 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:26:00.862741 32031 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:26:00.862747 32031 net.cpp:125] relu1 needs backward computation.
I0905 18:26:00.862754 32031 net.cpp:66] Creating Layer pool1
I0905 18:26:00.862759 32031 net.cpp:329] pool1 <- conv1
I0905 18:26:00.862766 32031 net.cpp:290] pool1 -> pool1
I0905 18:26:00.862777 32031 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:26:00.862783 32031 net.cpp:125] pool1 needs backward computation.
I0905 18:26:00.862790 32031 net.cpp:66] Creating Layer norm1
I0905 18:26:00.862797 32031 net.cpp:329] norm1 <- pool1
I0905 18:26:00.862802 32031 net.cpp:290] norm1 -> norm1
I0905 18:26:00.862812 32031 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:26:00.862818 32031 net.cpp:125] norm1 needs backward computation.
I0905 18:26:00.862825 32031 net.cpp:66] Creating Layer conv2
I0905 18:26:00.862831 32031 net.cpp:329] conv2 <- norm1
I0905 18:26:00.862838 32031 net.cpp:290] conv2 -> conv2
I0905 18:26:00.871950 32031 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:26:00.871965 32031 net.cpp:125] conv2 needs backward computation.
I0905 18:26:00.871973 32031 net.cpp:66] Creating Layer relu2
I0905 18:26:00.871979 32031 net.cpp:329] relu2 <- conv2
I0905 18:26:00.871985 32031 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:26:00.871994 32031 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:26:00.872004 32031 net.cpp:125] relu2 needs backward computation.
I0905 18:26:00.872011 32031 net.cpp:66] Creating Layer pool2
I0905 18:26:00.872016 32031 net.cpp:329] pool2 <- conv2
I0905 18:26:00.872023 32031 net.cpp:290] pool2 -> pool2
I0905 18:26:00.872031 32031 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:26:00.872037 32031 net.cpp:125] pool2 needs backward computation.
I0905 18:26:00.872046 32031 net.cpp:66] Creating Layer fc7
I0905 18:26:00.872052 32031 net.cpp:329] fc7 <- pool2
I0905 18:26:00.872061 32031 net.cpp:290] fc7 -> fc7
I0905 18:26:01.519932 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.519978 32031 net.cpp:125] fc7 needs backward computation.
I0905 18:26:01.519991 32031 net.cpp:66] Creating Layer relu7
I0905 18:26:01.519999 32031 net.cpp:329] relu7 <- fc7
I0905 18:26:01.520009 32031 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:26:01.520019 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.520025 32031 net.cpp:125] relu7 needs backward computation.
I0905 18:26:01.520032 32031 net.cpp:66] Creating Layer drop7
I0905 18:26:01.520037 32031 net.cpp:329] drop7 <- fc7
I0905 18:26:01.520045 32031 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:26:01.520054 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.520061 32031 net.cpp:125] drop7 needs backward computation.
I0905 18:26:01.520069 32031 net.cpp:66] Creating Layer fc8
I0905 18:26:01.520074 32031 net.cpp:329] fc8 <- fc7
I0905 18:26:01.520083 32031 net.cpp:290] fc8 -> fc8
I0905 18:26:01.527860 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.527873 32031 net.cpp:125] fc8 needs backward computation.
I0905 18:26:01.527880 32031 net.cpp:66] Creating Layer relu8
I0905 18:26:01.527886 32031 net.cpp:329] relu8 <- fc8
I0905 18:26:01.527894 32031 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:26:01.527902 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.527907 32031 net.cpp:125] relu8 needs backward computation.
I0905 18:26:01.527915 32031 net.cpp:66] Creating Layer drop8
I0905 18:26:01.527920 32031 net.cpp:329] drop8 <- fc8
I0905 18:26:01.527926 32031 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:26:01.527932 32031 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:01.527938 32031 net.cpp:125] drop8 needs backward computation.
I0905 18:26:01.527947 32031 net.cpp:66] Creating Layer fc9
I0905 18:26:01.527953 32031 net.cpp:329] fc9 <- fc8
I0905 18:26:01.527961 32031 net.cpp:290] fc9 -> fc9
I0905 18:26:01.528333 32031 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:26:01.528347 32031 net.cpp:125] fc9 needs backward computation.
I0905 18:26:01.528354 32031 net.cpp:66] Creating Layer fc10
I0905 18:26:01.528360 32031 net.cpp:329] fc10 <- fc9
I0905 18:26:01.528369 32031 net.cpp:290] fc10 -> fc10
I0905 18:26:01.528381 32031 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:26:01.528389 32031 net.cpp:125] fc10 needs backward computation.
I0905 18:26:01.528396 32031 net.cpp:66] Creating Layer prob
I0905 18:26:01.528401 32031 net.cpp:329] prob <- fc10
I0905 18:26:01.528409 32031 net.cpp:290] prob -> prob
I0905 18:26:01.528419 32031 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:26:01.528425 32031 net.cpp:125] prob needs backward computation.
I0905 18:26:01.528430 32031 net.cpp:156] This network produces output prob
I0905 18:26:01.528444 32031 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:26:01.528452 32031 net.cpp:167] Network initialization done.
I0905 18:26:01.528457 32031 net.cpp:168] Memory required for data: 6183480
Classifying 78 inputs.
Done in 49.08 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:26:53.290256 32035 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:26:53.290392 32035 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:26:53.290402 32035 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:26:53.290560 32035 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:26:53.290611 32035 net.cpp:292] Input 0 -> data
I0905 18:26:53.290637 32035 net.cpp:66] Creating Layer conv1
I0905 18:26:53.290643 32035 net.cpp:329] conv1 <- data
I0905 18:26:53.290652 32035 net.cpp:290] conv1 -> conv1
I0905 18:26:53.292029 32035 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:26:53.292047 32035 net.cpp:125] conv1 needs backward computation.
I0905 18:26:53.292057 32035 net.cpp:66] Creating Layer relu1
I0905 18:26:53.292062 32035 net.cpp:329] relu1 <- conv1
I0905 18:26:53.292069 32035 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:26:53.292078 32035 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:26:53.292083 32035 net.cpp:125] relu1 needs backward computation.
I0905 18:26:53.292090 32035 net.cpp:66] Creating Layer pool1
I0905 18:26:53.292096 32035 net.cpp:329] pool1 <- conv1
I0905 18:26:53.292104 32035 net.cpp:290] pool1 -> pool1
I0905 18:26:53.292119 32035 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:26:53.292125 32035 net.cpp:125] pool1 needs backward computation.
I0905 18:26:53.292132 32035 net.cpp:66] Creating Layer norm1
I0905 18:26:53.292137 32035 net.cpp:329] norm1 <- pool1
I0905 18:26:53.292145 32035 net.cpp:290] norm1 -> norm1
I0905 18:26:53.292153 32035 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:26:53.292160 32035 net.cpp:125] norm1 needs backward computation.
I0905 18:26:53.292166 32035 net.cpp:66] Creating Layer conv2
I0905 18:26:53.292172 32035 net.cpp:329] conv2 <- norm1
I0905 18:26:53.292179 32035 net.cpp:290] conv2 -> conv2
I0905 18:26:53.301316 32035 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:26:53.301331 32035 net.cpp:125] conv2 needs backward computation.
I0905 18:26:53.301338 32035 net.cpp:66] Creating Layer relu2
I0905 18:26:53.301344 32035 net.cpp:329] relu2 <- conv2
I0905 18:26:53.301352 32035 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:26:53.301358 32035 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:26:53.301364 32035 net.cpp:125] relu2 needs backward computation.
I0905 18:26:53.301370 32035 net.cpp:66] Creating Layer pool2
I0905 18:26:53.301375 32035 net.cpp:329] pool2 <- conv2
I0905 18:26:53.301383 32035 net.cpp:290] pool2 -> pool2
I0905 18:26:53.301390 32035 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:26:53.301395 32035 net.cpp:125] pool2 needs backward computation.
I0905 18:26:53.301405 32035 net.cpp:66] Creating Layer fc7
I0905 18:26:53.301411 32035 net.cpp:329] fc7 <- pool2
I0905 18:26:53.301419 32035 net.cpp:290] fc7 -> fc7
I0905 18:26:53.948449 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.948494 32035 net.cpp:125] fc7 needs backward computation.
I0905 18:26:53.948508 32035 net.cpp:66] Creating Layer relu7
I0905 18:26:53.948514 32035 net.cpp:329] relu7 <- fc7
I0905 18:26:53.948524 32035 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:26:53.948534 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.948539 32035 net.cpp:125] relu7 needs backward computation.
I0905 18:26:53.948547 32035 net.cpp:66] Creating Layer drop7
I0905 18:26:53.948554 32035 net.cpp:329] drop7 <- fc7
I0905 18:26:53.948559 32035 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:26:53.948570 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.948576 32035 net.cpp:125] drop7 needs backward computation.
I0905 18:26:53.948585 32035 net.cpp:66] Creating Layer fc8
I0905 18:26:53.948590 32035 net.cpp:329] fc8 <- fc7
I0905 18:26:53.948600 32035 net.cpp:290] fc8 -> fc8
I0905 18:26:53.956403 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.956416 32035 net.cpp:125] fc8 needs backward computation.
I0905 18:26:53.956423 32035 net.cpp:66] Creating Layer relu8
I0905 18:26:53.956429 32035 net.cpp:329] relu8 <- fc8
I0905 18:26:53.956436 32035 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:26:53.956444 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.956449 32035 net.cpp:125] relu8 needs backward computation.
I0905 18:26:53.956456 32035 net.cpp:66] Creating Layer drop8
I0905 18:26:53.956461 32035 net.cpp:329] drop8 <- fc8
I0905 18:26:53.956468 32035 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:26:53.956475 32035 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:26:53.956480 32035 net.cpp:125] drop8 needs backward computation.
I0905 18:26:53.956490 32035 net.cpp:66] Creating Layer fc9
I0905 18:26:53.956495 32035 net.cpp:329] fc9 <- fc8
I0905 18:26:53.956501 32035 net.cpp:290] fc9 -> fc9
I0905 18:26:53.956877 32035 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:26:53.956888 32035 net.cpp:125] fc9 needs backward computation.
I0905 18:26:53.956897 32035 net.cpp:66] Creating Layer fc10
I0905 18:26:53.956902 32035 net.cpp:329] fc10 <- fc9
I0905 18:26:53.956912 32035 net.cpp:290] fc10 -> fc10
I0905 18:26:53.956923 32035 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:26:53.956930 32035 net.cpp:125] fc10 needs backward computation.
I0905 18:26:53.956936 32035 net.cpp:66] Creating Layer prob
I0905 18:26:53.956943 32035 net.cpp:329] prob <- fc10
I0905 18:26:53.956959 32035 net.cpp:290] prob -> prob
I0905 18:26:53.956969 32035 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:26:53.956975 32035 net.cpp:125] prob needs backward computation.
I0905 18:26:53.956980 32035 net.cpp:156] This network produces output prob
I0905 18:26:53.956993 32035 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:26:53.957001 32035 net.cpp:167] Network initialization done.
I0905 18:26:53.957007 32035 net.cpp:168] Memory required for data: 6183480
Classifying 13 inputs.
Done in 8.48 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:27:04.023638 32038 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:27:04.023778 32038 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:27:04.023787 32038 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:27:04.023936 32038 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:27:04.023998 32038 net.cpp:292] Input 0 -> data
I0905 18:27:04.024025 32038 net.cpp:66] Creating Layer conv1
I0905 18:27:04.024032 32038 net.cpp:329] conv1 <- data
I0905 18:27:04.024040 32038 net.cpp:290] conv1 -> conv1
I0905 18:27:04.025424 32038 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:27:04.025441 32038 net.cpp:125] conv1 needs backward computation.
I0905 18:27:04.025450 32038 net.cpp:66] Creating Layer relu1
I0905 18:27:04.025457 32038 net.cpp:329] relu1 <- conv1
I0905 18:27:04.025463 32038 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:27:04.025472 32038 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:27:04.025478 32038 net.cpp:125] relu1 needs backward computation.
I0905 18:27:04.025485 32038 net.cpp:66] Creating Layer pool1
I0905 18:27:04.025490 32038 net.cpp:329] pool1 <- conv1
I0905 18:27:04.025497 32038 net.cpp:290] pool1 -> pool1
I0905 18:27:04.025507 32038 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:27:04.025513 32038 net.cpp:125] pool1 needs backward computation.
I0905 18:27:04.025521 32038 net.cpp:66] Creating Layer norm1
I0905 18:27:04.025527 32038 net.cpp:329] norm1 <- pool1
I0905 18:27:04.025533 32038 net.cpp:290] norm1 -> norm1
I0905 18:27:04.025542 32038 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:27:04.025548 32038 net.cpp:125] norm1 needs backward computation.
I0905 18:27:04.025555 32038 net.cpp:66] Creating Layer conv2
I0905 18:27:04.025562 32038 net.cpp:329] conv2 <- norm1
I0905 18:27:04.025568 32038 net.cpp:290] conv2 -> conv2
I0905 18:27:04.034735 32038 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:27:04.034751 32038 net.cpp:125] conv2 needs backward computation.
I0905 18:27:04.034759 32038 net.cpp:66] Creating Layer relu2
I0905 18:27:04.034765 32038 net.cpp:329] relu2 <- conv2
I0905 18:27:04.034770 32038 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:27:04.034778 32038 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:27:04.034783 32038 net.cpp:125] relu2 needs backward computation.
I0905 18:27:04.034790 32038 net.cpp:66] Creating Layer pool2
I0905 18:27:04.034795 32038 net.cpp:329] pool2 <- conv2
I0905 18:27:04.034802 32038 net.cpp:290] pool2 -> pool2
I0905 18:27:04.034809 32038 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:27:04.034816 32038 net.cpp:125] pool2 needs backward computation.
I0905 18:27:04.034826 32038 net.cpp:66] Creating Layer fc7
I0905 18:27:04.034832 32038 net.cpp:329] fc7 <- pool2
I0905 18:27:04.034839 32038 net.cpp:290] fc7 -> fc7
I0905 18:27:04.684265 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.684309 32038 net.cpp:125] fc7 needs backward computation.
I0905 18:27:04.684322 32038 net.cpp:66] Creating Layer relu7
I0905 18:27:04.684329 32038 net.cpp:329] relu7 <- fc7
I0905 18:27:04.684340 32038 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:27:04.684350 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.684355 32038 net.cpp:125] relu7 needs backward computation.
I0905 18:27:04.684362 32038 net.cpp:66] Creating Layer drop7
I0905 18:27:04.684367 32038 net.cpp:329] drop7 <- fc7
I0905 18:27:04.684375 32038 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:27:04.684386 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.684391 32038 net.cpp:125] drop7 needs backward computation.
I0905 18:27:04.684399 32038 net.cpp:66] Creating Layer fc8
I0905 18:27:04.684406 32038 net.cpp:329] fc8 <- fc7
I0905 18:27:04.684414 32038 net.cpp:290] fc8 -> fc8
I0905 18:27:04.692224 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.692237 32038 net.cpp:125] fc8 needs backward computation.
I0905 18:27:04.692245 32038 net.cpp:66] Creating Layer relu8
I0905 18:27:04.692250 32038 net.cpp:329] relu8 <- fc8
I0905 18:27:04.692258 32038 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:27:04.692265 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.692271 32038 net.cpp:125] relu8 needs backward computation.
I0905 18:27:04.692287 32038 net.cpp:66] Creating Layer drop8
I0905 18:27:04.692293 32038 net.cpp:329] drop8 <- fc8
I0905 18:27:04.692301 32038 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:27:04.692307 32038 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:27:04.692312 32038 net.cpp:125] drop8 needs backward computation.
I0905 18:27:04.692322 32038 net.cpp:66] Creating Layer fc9
I0905 18:27:04.692327 32038 net.cpp:329] fc9 <- fc8
I0905 18:27:04.692334 32038 net.cpp:290] fc9 -> fc9
I0905 18:27:04.692708 32038 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:27:04.692719 32038 net.cpp:125] fc9 needs backward computation.
I0905 18:27:04.692728 32038 net.cpp:66] Creating Layer fc10
I0905 18:27:04.692734 32038 net.cpp:329] fc10 <- fc9
I0905 18:27:04.692741 32038 net.cpp:290] fc10 -> fc10
I0905 18:27:04.692754 32038 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:27:04.692761 32038 net.cpp:125] fc10 needs backward computation.
I0905 18:27:04.692769 32038 net.cpp:66] Creating Layer prob
I0905 18:27:04.692773 32038 net.cpp:329] prob <- fc10
I0905 18:27:04.692781 32038 net.cpp:290] prob -> prob
I0905 18:27:04.692790 32038 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:27:04.692796 32038 net.cpp:125] prob needs backward computation.
I0905 18:27:04.692801 32038 net.cpp:156] This network produces output prob
I0905 18:27:04.692814 32038 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:27:04.692823 32038 net.cpp:167] Network initialization done.
I0905 18:27:04.692828 32038 net.cpp:168] Memory required for data: 6183480
Classifying 82 inputs.
Done in 53.43 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:28:01.705018 32044 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:28:01.705157 32044 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:28:01.705167 32044 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:28:01.705313 32044 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:28:01.705377 32044 net.cpp:292] Input 0 -> data
I0905 18:28:01.705404 32044 net.cpp:66] Creating Layer conv1
I0905 18:28:01.705410 32044 net.cpp:329] conv1 <- data
I0905 18:28:01.705417 32044 net.cpp:290] conv1 -> conv1
I0905 18:28:01.706801 32044 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:28:01.706820 32044 net.cpp:125] conv1 needs backward computation.
I0905 18:28:01.706830 32044 net.cpp:66] Creating Layer relu1
I0905 18:28:01.706835 32044 net.cpp:329] relu1 <- conv1
I0905 18:28:01.706842 32044 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:28:01.706851 32044 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:28:01.706857 32044 net.cpp:125] relu1 needs backward computation.
I0905 18:28:01.706864 32044 net.cpp:66] Creating Layer pool1
I0905 18:28:01.706869 32044 net.cpp:329] pool1 <- conv1
I0905 18:28:01.706876 32044 net.cpp:290] pool1 -> pool1
I0905 18:28:01.706887 32044 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:28:01.706893 32044 net.cpp:125] pool1 needs backward computation.
I0905 18:28:01.706899 32044 net.cpp:66] Creating Layer norm1
I0905 18:28:01.706905 32044 net.cpp:329] norm1 <- pool1
I0905 18:28:01.706912 32044 net.cpp:290] norm1 -> norm1
I0905 18:28:01.706922 32044 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:28:01.706928 32044 net.cpp:125] norm1 needs backward computation.
I0905 18:28:01.706934 32044 net.cpp:66] Creating Layer conv2
I0905 18:28:01.706940 32044 net.cpp:329] conv2 <- norm1
I0905 18:28:01.706948 32044 net.cpp:290] conv2 -> conv2
I0905 18:28:01.716081 32044 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:28:01.716097 32044 net.cpp:125] conv2 needs backward computation.
I0905 18:28:01.716104 32044 net.cpp:66] Creating Layer relu2
I0905 18:28:01.716110 32044 net.cpp:329] relu2 <- conv2
I0905 18:28:01.716116 32044 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:28:01.716123 32044 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:28:01.716130 32044 net.cpp:125] relu2 needs backward computation.
I0905 18:28:01.716138 32044 net.cpp:66] Creating Layer pool2
I0905 18:28:01.716145 32044 net.cpp:329] pool2 <- conv2
I0905 18:28:01.716150 32044 net.cpp:290] pool2 -> pool2
I0905 18:28:01.716158 32044 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:28:01.716164 32044 net.cpp:125] pool2 needs backward computation.
I0905 18:28:01.716171 32044 net.cpp:66] Creating Layer fc7
I0905 18:28:01.716176 32044 net.cpp:329] fc7 <- pool2
I0905 18:28:01.716183 32044 net.cpp:290] fc7 -> fc7
I0905 18:28:02.365727 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.365773 32044 net.cpp:125] fc7 needs backward computation.
I0905 18:28:02.365787 32044 net.cpp:66] Creating Layer relu7
I0905 18:28:02.365793 32044 net.cpp:329] relu7 <- fc7
I0905 18:28:02.365803 32044 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:28:02.365813 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.365830 32044 net.cpp:125] relu7 needs backward computation.
I0905 18:28:02.365839 32044 net.cpp:66] Creating Layer drop7
I0905 18:28:02.365844 32044 net.cpp:329] drop7 <- fc7
I0905 18:28:02.365850 32044 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:28:02.365861 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.365867 32044 net.cpp:125] drop7 needs backward computation.
I0905 18:28:02.365875 32044 net.cpp:66] Creating Layer fc8
I0905 18:28:02.365881 32044 net.cpp:329] fc8 <- fc7
I0905 18:28:02.365890 32044 net.cpp:290] fc8 -> fc8
I0905 18:28:02.373699 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.373713 32044 net.cpp:125] fc8 needs backward computation.
I0905 18:28:02.373720 32044 net.cpp:66] Creating Layer relu8
I0905 18:28:02.373725 32044 net.cpp:329] relu8 <- fc8
I0905 18:28:02.373733 32044 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:28:02.373741 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.373746 32044 net.cpp:125] relu8 needs backward computation.
I0905 18:28:02.373754 32044 net.cpp:66] Creating Layer drop8
I0905 18:28:02.373759 32044 net.cpp:329] drop8 <- fc8
I0905 18:28:02.373764 32044 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:28:02.373771 32044 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:02.373777 32044 net.cpp:125] drop8 needs backward computation.
I0905 18:28:02.373785 32044 net.cpp:66] Creating Layer fc9
I0905 18:28:02.373791 32044 net.cpp:329] fc9 <- fc8
I0905 18:28:02.373798 32044 net.cpp:290] fc9 -> fc9
I0905 18:28:02.374171 32044 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:28:02.374183 32044 net.cpp:125] fc9 needs backward computation.
I0905 18:28:02.374191 32044 net.cpp:66] Creating Layer fc10
I0905 18:28:02.374197 32044 net.cpp:329] fc10 <- fc9
I0905 18:28:02.374205 32044 net.cpp:290] fc10 -> fc10
I0905 18:28:02.374217 32044 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:28:02.374224 32044 net.cpp:125] fc10 needs backward computation.
I0905 18:28:02.374232 32044 net.cpp:66] Creating Layer prob
I0905 18:28:02.374238 32044 net.cpp:329] prob <- fc10
I0905 18:28:02.374244 32044 net.cpp:290] prob -> prob
I0905 18:28:02.374253 32044 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:28:02.374259 32044 net.cpp:125] prob needs backward computation.
I0905 18:28:02.374264 32044 net.cpp:156] This network produces output prob
I0905 18:28:02.374277 32044 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:28:02.374285 32044 net.cpp:167] Network initialization done.
I0905 18:28:02.374290 32044 net.cpp:168] Memory required for data: 6183480
Classifying 63 inputs.
Done in 39.53 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:28:44.756975 32048 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:28:44.757114 32048 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:28:44.757123 32048 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:28:44.757271 32048 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:28:44.757333 32048 net.cpp:292] Input 0 -> data
I0905 18:28:44.757359 32048 net.cpp:66] Creating Layer conv1
I0905 18:28:44.757366 32048 net.cpp:329] conv1 <- data
I0905 18:28:44.757375 32048 net.cpp:290] conv1 -> conv1
I0905 18:28:44.758746 32048 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:28:44.758765 32048 net.cpp:125] conv1 needs backward computation.
I0905 18:28:44.758774 32048 net.cpp:66] Creating Layer relu1
I0905 18:28:44.758780 32048 net.cpp:329] relu1 <- conv1
I0905 18:28:44.758787 32048 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:28:44.758796 32048 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:28:44.758802 32048 net.cpp:125] relu1 needs backward computation.
I0905 18:28:44.758810 32048 net.cpp:66] Creating Layer pool1
I0905 18:28:44.758815 32048 net.cpp:329] pool1 <- conv1
I0905 18:28:44.758821 32048 net.cpp:290] pool1 -> pool1
I0905 18:28:44.758832 32048 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:28:44.758838 32048 net.cpp:125] pool1 needs backward computation.
I0905 18:28:44.758846 32048 net.cpp:66] Creating Layer norm1
I0905 18:28:44.758852 32048 net.cpp:329] norm1 <- pool1
I0905 18:28:44.758858 32048 net.cpp:290] norm1 -> norm1
I0905 18:28:44.758867 32048 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:28:44.758873 32048 net.cpp:125] norm1 needs backward computation.
I0905 18:28:44.758880 32048 net.cpp:66] Creating Layer conv2
I0905 18:28:44.758887 32048 net.cpp:329] conv2 <- norm1
I0905 18:28:44.758893 32048 net.cpp:290] conv2 -> conv2
I0905 18:28:44.768015 32048 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:28:44.768030 32048 net.cpp:125] conv2 needs backward computation.
I0905 18:28:44.768038 32048 net.cpp:66] Creating Layer relu2
I0905 18:28:44.768043 32048 net.cpp:329] relu2 <- conv2
I0905 18:28:44.768050 32048 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:28:44.768062 32048 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:28:44.768069 32048 net.cpp:125] relu2 needs backward computation.
I0905 18:28:44.768075 32048 net.cpp:66] Creating Layer pool2
I0905 18:28:44.768080 32048 net.cpp:329] pool2 <- conv2
I0905 18:28:44.768087 32048 net.cpp:290] pool2 -> pool2
I0905 18:28:44.768095 32048 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:28:44.768101 32048 net.cpp:125] pool2 needs backward computation.
I0905 18:28:44.768110 32048 net.cpp:66] Creating Layer fc7
I0905 18:28:44.768116 32048 net.cpp:329] fc7 <- pool2
I0905 18:28:44.768123 32048 net.cpp:290] fc7 -> fc7
I0905 18:28:45.414332 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.414376 32048 net.cpp:125] fc7 needs backward computation.
I0905 18:28:45.414388 32048 net.cpp:66] Creating Layer relu7
I0905 18:28:45.414396 32048 net.cpp:329] relu7 <- fc7
I0905 18:28:45.414404 32048 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:28:45.414415 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.414422 32048 net.cpp:125] relu7 needs backward computation.
I0905 18:28:45.414429 32048 net.cpp:66] Creating Layer drop7
I0905 18:28:45.414434 32048 net.cpp:329] drop7 <- fc7
I0905 18:28:45.414440 32048 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:28:45.414451 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.414458 32048 net.cpp:125] drop7 needs backward computation.
I0905 18:28:45.414466 32048 net.cpp:66] Creating Layer fc8
I0905 18:28:45.414471 32048 net.cpp:329] fc8 <- fc7
I0905 18:28:45.414480 32048 net.cpp:290] fc8 -> fc8
I0905 18:28:45.422245 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.422256 32048 net.cpp:125] fc8 needs backward computation.
I0905 18:28:45.422265 32048 net.cpp:66] Creating Layer relu8
I0905 18:28:45.422269 32048 net.cpp:329] relu8 <- fc8
I0905 18:28:45.422279 32048 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:28:45.422286 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.422292 32048 net.cpp:125] relu8 needs backward computation.
I0905 18:28:45.422299 32048 net.cpp:66] Creating Layer drop8
I0905 18:28:45.422304 32048 net.cpp:329] drop8 <- fc8
I0905 18:28:45.422310 32048 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:28:45.422317 32048 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:28:45.422323 32048 net.cpp:125] drop8 needs backward computation.
I0905 18:28:45.422333 32048 net.cpp:66] Creating Layer fc9
I0905 18:28:45.422339 32048 net.cpp:329] fc9 <- fc8
I0905 18:28:45.422346 32048 net.cpp:290] fc9 -> fc9
I0905 18:28:45.422719 32048 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:28:45.422730 32048 net.cpp:125] fc9 needs backward computation.
I0905 18:28:45.422739 32048 net.cpp:66] Creating Layer fc10
I0905 18:28:45.422744 32048 net.cpp:329] fc10 <- fc9
I0905 18:28:45.422754 32048 net.cpp:290] fc10 -> fc10
I0905 18:28:45.422765 32048 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:28:45.422772 32048 net.cpp:125] fc10 needs backward computation.
I0905 18:28:45.422780 32048 net.cpp:66] Creating Layer prob
I0905 18:28:45.422785 32048 net.cpp:329] prob <- fc10
I0905 18:28:45.422792 32048 net.cpp:290] prob -> prob
I0905 18:28:45.422801 32048 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:28:45.422807 32048 net.cpp:125] prob needs backward computation.
I0905 18:28:45.422812 32048 net.cpp:156] This network produces output prob
I0905 18:28:45.422826 32048 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:28:45.422833 32048 net.cpp:167] Network initialization done.
I0905 18:28:45.422838 32048 net.cpp:168] Memory required for data: 6183480
Classifying 111 inputs.
Done in 68.74 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 11 is out of bounds for axis 0 with size 11
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:29:58.330132 32053 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:29:58.330281 32053 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:29:58.330291 32053 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:29:58.330436 32053 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:29:58.330487 32053 net.cpp:292] Input 0 -> data
I0905 18:29:58.330513 32053 net.cpp:66] Creating Layer conv1
I0905 18:29:58.330520 32053 net.cpp:329] conv1 <- data
I0905 18:29:58.330528 32053 net.cpp:290] conv1 -> conv1
I0905 18:29:58.331907 32053 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:29:58.331925 32053 net.cpp:125] conv1 needs backward computation.
I0905 18:29:58.331934 32053 net.cpp:66] Creating Layer relu1
I0905 18:29:58.331940 32053 net.cpp:329] relu1 <- conv1
I0905 18:29:58.331951 32053 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:29:58.331960 32053 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:29:58.331966 32053 net.cpp:125] relu1 needs backward computation.
I0905 18:29:58.331974 32053 net.cpp:66] Creating Layer pool1
I0905 18:29:58.331979 32053 net.cpp:329] pool1 <- conv1
I0905 18:29:58.331985 32053 net.cpp:290] pool1 -> pool1
I0905 18:29:58.331995 32053 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:29:58.332001 32053 net.cpp:125] pool1 needs backward computation.
I0905 18:29:58.332008 32053 net.cpp:66] Creating Layer norm1
I0905 18:29:58.332013 32053 net.cpp:329] norm1 <- pool1
I0905 18:29:58.332020 32053 net.cpp:290] norm1 -> norm1
I0905 18:29:58.332031 32053 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:29:58.332036 32053 net.cpp:125] norm1 needs backward computation.
I0905 18:29:58.332042 32053 net.cpp:66] Creating Layer conv2
I0905 18:29:58.332048 32053 net.cpp:329] conv2 <- norm1
I0905 18:29:58.332056 32053 net.cpp:290] conv2 -> conv2
I0905 18:29:58.341421 32053 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:29:58.341439 32053 net.cpp:125] conv2 needs backward computation.
I0905 18:29:58.341445 32053 net.cpp:66] Creating Layer relu2
I0905 18:29:58.341451 32053 net.cpp:329] relu2 <- conv2
I0905 18:29:58.341459 32053 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:29:58.341465 32053 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:29:58.341470 32053 net.cpp:125] relu2 needs backward computation.
I0905 18:29:58.341480 32053 net.cpp:66] Creating Layer pool2
I0905 18:29:58.341485 32053 net.cpp:329] pool2 <- conv2
I0905 18:29:58.341492 32053 net.cpp:290] pool2 -> pool2
I0905 18:29:58.341500 32053 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:29:58.341506 32053 net.cpp:125] pool2 needs backward computation.
I0905 18:29:58.341513 32053 net.cpp:66] Creating Layer fc7
I0905 18:29:58.341519 32053 net.cpp:329] fc7 <- pool2
I0905 18:29:58.341526 32053 net.cpp:290] fc7 -> fc7
I0905 18:29:58.990474 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.990520 32053 net.cpp:125] fc7 needs backward computation.
I0905 18:29:58.990532 32053 net.cpp:66] Creating Layer relu7
I0905 18:29:58.990540 32053 net.cpp:329] relu7 <- fc7
I0905 18:29:58.990550 32053 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:29:58.990561 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.990566 32053 net.cpp:125] relu7 needs backward computation.
I0905 18:29:58.990574 32053 net.cpp:66] Creating Layer drop7
I0905 18:29:58.990579 32053 net.cpp:329] drop7 <- fc7
I0905 18:29:58.990586 32053 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:29:58.990597 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.990602 32053 net.cpp:125] drop7 needs backward computation.
I0905 18:29:58.990612 32053 net.cpp:66] Creating Layer fc8
I0905 18:29:58.990617 32053 net.cpp:329] fc8 <- fc7
I0905 18:29:58.990625 32053 net.cpp:290] fc8 -> fc8
I0905 18:29:58.998632 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.998646 32053 net.cpp:125] fc8 needs backward computation.
I0905 18:29:58.998652 32053 net.cpp:66] Creating Layer relu8
I0905 18:29:58.998658 32053 net.cpp:329] relu8 <- fc8
I0905 18:29:58.998667 32053 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:29:58.998674 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.998679 32053 net.cpp:125] relu8 needs backward computation.
I0905 18:29:58.998687 32053 net.cpp:66] Creating Layer drop8
I0905 18:29:58.998692 32053 net.cpp:329] drop8 <- fc8
I0905 18:29:58.998698 32053 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:29:58.998705 32053 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:29:58.998710 32053 net.cpp:125] drop8 needs backward computation.
I0905 18:29:58.998719 32053 net.cpp:66] Creating Layer fc9
I0905 18:29:58.998725 32053 net.cpp:329] fc9 <- fc8
I0905 18:29:58.998733 32053 net.cpp:290] fc9 -> fc9
I0905 18:29:58.999116 32053 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:29:58.999128 32053 net.cpp:125] fc9 needs backward computation.
I0905 18:29:58.999137 32053 net.cpp:66] Creating Layer fc10
I0905 18:29:58.999152 32053 net.cpp:329] fc10 <- fc9
I0905 18:29:58.999161 32053 net.cpp:290] fc10 -> fc10
I0905 18:29:58.999173 32053 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:29:58.999182 32053 net.cpp:125] fc10 needs backward computation.
I0905 18:29:58.999189 32053 net.cpp:66] Creating Layer prob
I0905 18:29:58.999194 32053 net.cpp:329] prob <- fc10
I0905 18:29:58.999202 32053 net.cpp:290] prob -> prob
I0905 18:29:58.999212 32053 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:29:58.999218 32053 net.cpp:125] prob needs backward computation.
I0905 18:29:58.999223 32053 net.cpp:156] This network produces output prob
I0905 18:29:58.999236 32053 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:29:58.999245 32053 net.cpp:167] Network initialization done.
I0905 18:29:58.999250 32053 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 277 inputs.
Done in 173.88 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 77 is out of bounds for axis 0 with size 77
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 18:33:04.016294 32059 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 18:33:04.016438 32059 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 18:33:04.016448 32059 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 18:33:04.016599 32059 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 18:33:04.016666 32059 net.cpp:292] Input 0 -> data
I0905 18:33:04.016693 32059 net.cpp:66] Creating Layer conv1
I0905 18:33:04.016700 32059 net.cpp:329] conv1 <- data
I0905 18:33:04.016710 32059 net.cpp:290] conv1 -> conv1
I0905 18:33:04.018131 32059 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:33:04.018151 32059 net.cpp:125] conv1 needs backward computation.
I0905 18:33:04.018159 32059 net.cpp:66] Creating Layer relu1
I0905 18:33:04.018167 32059 net.cpp:329] relu1 <- conv1
I0905 18:33:04.018173 32059 net.cpp:280] relu1 -> conv1 (in-place)
I0905 18:33:04.018182 32059 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 18:33:04.018188 32059 net.cpp:125] relu1 needs backward computation.
I0905 18:33:04.018195 32059 net.cpp:66] Creating Layer pool1
I0905 18:33:04.018201 32059 net.cpp:329] pool1 <- conv1
I0905 18:33:04.018208 32059 net.cpp:290] pool1 -> pool1
I0905 18:33:04.018219 32059 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:33:04.018225 32059 net.cpp:125] pool1 needs backward computation.
I0905 18:33:04.018234 32059 net.cpp:66] Creating Layer norm1
I0905 18:33:04.018239 32059 net.cpp:329] norm1 <- pool1
I0905 18:33:04.018245 32059 net.cpp:290] norm1 -> norm1
I0905 18:33:04.018255 32059 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 18:33:04.018261 32059 net.cpp:125] norm1 needs backward computation.
I0905 18:33:04.018270 32059 net.cpp:66] Creating Layer conv2
I0905 18:33:04.018275 32059 net.cpp:329] conv2 <- norm1
I0905 18:33:04.018283 32059 net.cpp:290] conv2 -> conv2
I0905 18:33:04.027676 32059 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:33:04.027693 32059 net.cpp:125] conv2 needs backward computation.
I0905 18:33:04.027700 32059 net.cpp:66] Creating Layer relu2
I0905 18:33:04.027706 32059 net.cpp:329] relu2 <- conv2
I0905 18:33:04.027714 32059 net.cpp:280] relu2 -> conv2 (in-place)
I0905 18:33:04.027721 32059 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 18:33:04.027726 32059 net.cpp:125] relu2 needs backward computation.
I0905 18:33:04.027735 32059 net.cpp:66] Creating Layer pool2
I0905 18:33:04.027741 32059 net.cpp:329] pool2 <- conv2
I0905 18:33:04.027748 32059 net.cpp:290] pool2 -> pool2
I0905 18:33:04.027757 32059 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 18:33:04.027763 32059 net.cpp:125] pool2 needs backward computation.
I0905 18:33:04.027770 32059 net.cpp:66] Creating Layer fc7
I0905 18:33:04.027776 32059 net.cpp:329] fc7 <- pool2
I0905 18:33:04.027783 32059 net.cpp:290] fc7 -> fc7
I0905 18:33:04.679241 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.679288 32059 net.cpp:125] fc7 needs backward computation.
I0905 18:33:04.679301 32059 net.cpp:66] Creating Layer relu7
I0905 18:33:04.679308 32059 net.cpp:329] relu7 <- fc7
I0905 18:33:04.679318 32059 net.cpp:280] relu7 -> fc7 (in-place)
I0905 18:33:04.679329 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.679335 32059 net.cpp:125] relu7 needs backward computation.
I0905 18:33:04.679343 32059 net.cpp:66] Creating Layer drop7
I0905 18:33:04.679348 32059 net.cpp:329] drop7 <- fc7
I0905 18:33:04.679355 32059 net.cpp:280] drop7 -> fc7 (in-place)
I0905 18:33:04.679378 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.679384 32059 net.cpp:125] drop7 needs backward computation.
I0905 18:33:04.679394 32059 net.cpp:66] Creating Layer fc8
I0905 18:33:04.679399 32059 net.cpp:329] fc8 <- fc7
I0905 18:33:04.679409 32059 net.cpp:290] fc8 -> fc8
I0905 18:33:04.687438 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.687453 32059 net.cpp:125] fc8 needs backward computation.
I0905 18:33:04.687459 32059 net.cpp:66] Creating Layer relu8
I0905 18:33:04.687465 32059 net.cpp:329] relu8 <- fc8
I0905 18:33:04.687474 32059 net.cpp:280] relu8 -> fc8 (in-place)
I0905 18:33:04.687481 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.687487 32059 net.cpp:125] relu8 needs backward computation.
I0905 18:33:04.687494 32059 net.cpp:66] Creating Layer drop8
I0905 18:33:04.687500 32059 net.cpp:329] drop8 <- fc8
I0905 18:33:04.687506 32059 net.cpp:280] drop8 -> fc8 (in-place)
I0905 18:33:04.687513 32059 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 18:33:04.687520 32059 net.cpp:125] drop8 needs backward computation.
I0905 18:33:04.687528 32059 net.cpp:66] Creating Layer fc9
I0905 18:33:04.687533 32059 net.cpp:329] fc9 <- fc8
I0905 18:33:04.687541 32059 net.cpp:290] fc9 -> fc9
I0905 18:33:04.687928 32059 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 18:33:04.687940 32059 net.cpp:125] fc9 needs backward computation.
I0905 18:33:04.687949 32059 net.cpp:66] Creating Layer fc10
I0905 18:33:04.687955 32059 net.cpp:329] fc10 <- fc9
I0905 18:33:04.687964 32059 net.cpp:290] fc10 -> fc10
I0905 18:33:04.687976 32059 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:33:04.687984 32059 net.cpp:125] fc10 needs backward computation.
I0905 18:33:04.687991 32059 net.cpp:66] Creating Layer prob
I0905 18:33:04.687997 32059 net.cpp:329] prob <- fc10
I0905 18:33:04.688005 32059 net.cpp:290] prob -> prob
I0905 18:33:04.688015 32059 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 18:33:04.688021 32059 net.cpp:125] prob needs backward computation.
I0905 18:33:04.688026 32059 net.cpp:156] This network produces output prob
I0905 18:33:04.688040 32059 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 18:33:04.688048 32059 net.cpp:167] Network initialization done.
I0905 18:33:04.688053 32059 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:28:29.337474 32124 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:28:29.337641 32124 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:28:29.337652 32124 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:28:29.337815 32124 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:28:29.337882 32124 net.cpp:292] Input 0 -> data
I0905 19:28:29.338297 32124 net.cpp:66] Creating Layer conv1
I0905 19:28:29.338315 32124 net.cpp:329] conv1 <- data
I0905 19:28:29.338327 32124 net.cpp:290] conv1 -> conv1
I0905 19:28:29.355799 32124 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:28:29.355824 32124 net.cpp:125] conv1 needs backward computation.
I0905 19:28:29.355834 32124 net.cpp:66] Creating Layer relu1
I0905 19:28:29.355840 32124 net.cpp:329] relu1 <- conv1
I0905 19:28:29.355847 32124 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:28:29.355855 32124 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:28:29.355861 32124 net.cpp:125] relu1 needs backward computation.
I0905 19:28:29.355869 32124 net.cpp:66] Creating Layer pool1
I0905 19:28:29.355875 32124 net.cpp:329] pool1 <- conv1
I0905 19:28:29.355881 32124 net.cpp:290] pool1 -> pool1
I0905 19:28:29.355892 32124 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:28:29.355898 32124 net.cpp:125] pool1 needs backward computation.
I0905 19:28:29.355904 32124 net.cpp:66] Creating Layer norm1
I0905 19:28:29.355911 32124 net.cpp:329] norm1 <- pool1
I0905 19:28:29.355917 32124 net.cpp:290] norm1 -> norm1
I0905 19:28:29.355934 32124 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:28:29.355940 32124 net.cpp:125] norm1 needs backward computation.
I0905 19:28:29.355948 32124 net.cpp:66] Creating Layer conv2
I0905 19:28:29.355954 32124 net.cpp:329] conv2 <- norm1
I0905 19:28:29.355962 32124 net.cpp:290] conv2 -> conv2
I0905 19:28:29.365329 32124 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:28:29.365347 32124 net.cpp:125] conv2 needs backward computation.
I0905 19:28:29.365355 32124 net.cpp:66] Creating Layer relu2
I0905 19:28:29.365361 32124 net.cpp:329] relu2 <- conv2
I0905 19:28:29.365368 32124 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:28:29.365376 32124 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:28:29.365382 32124 net.cpp:125] relu2 needs backward computation.
I0905 19:28:29.365388 32124 net.cpp:66] Creating Layer pool2
I0905 19:28:29.365394 32124 net.cpp:329] pool2 <- conv2
I0905 19:28:29.365407 32124 net.cpp:290] pool2 -> pool2
I0905 19:28:29.365417 32124 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:28:29.365422 32124 net.cpp:125] pool2 needs backward computation.
I0905 19:28:29.365432 32124 net.cpp:66] Creating Layer fc7
I0905 19:28:29.365438 32124 net.cpp:329] fc7 <- pool2
I0905 19:28:29.365445 32124 net.cpp:290] fc7 -> fc7
I0905 19:28:30.023326 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.023368 32124 net.cpp:125] fc7 needs backward computation.
I0905 19:28:30.023381 32124 net.cpp:66] Creating Layer relu7
I0905 19:28:30.023388 32124 net.cpp:329] relu7 <- fc7
I0905 19:28:30.023397 32124 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:28:30.023407 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.023413 32124 net.cpp:125] relu7 needs backward computation.
I0905 19:28:30.023421 32124 net.cpp:66] Creating Layer drop7
I0905 19:28:30.023425 32124 net.cpp:329] drop7 <- fc7
I0905 19:28:30.023432 32124 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:28:30.023442 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.023447 32124 net.cpp:125] drop7 needs backward computation.
I0905 19:28:30.023457 32124 net.cpp:66] Creating Layer fc8
I0905 19:28:30.023461 32124 net.cpp:329] fc8 <- fc7
I0905 19:28:30.023470 32124 net.cpp:290] fc8 -> fc8
I0905 19:28:30.031262 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.031275 32124 net.cpp:125] fc8 needs backward computation.
I0905 19:28:30.031281 32124 net.cpp:66] Creating Layer relu8
I0905 19:28:30.031287 32124 net.cpp:329] relu8 <- fc8
I0905 19:28:30.031296 32124 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:28:30.031302 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.031308 32124 net.cpp:125] relu8 needs backward computation.
I0905 19:28:30.031314 32124 net.cpp:66] Creating Layer drop8
I0905 19:28:30.031321 32124 net.cpp:329] drop8 <- fc8
I0905 19:28:30.031327 32124 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:28:30.031333 32124 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:28:30.031338 32124 net.cpp:125] drop8 needs backward computation.
I0905 19:28:30.031347 32124 net.cpp:66] Creating Layer fc9
I0905 19:28:30.031353 32124 net.cpp:329] fc9 <- fc8
I0905 19:28:30.031360 32124 net.cpp:290] fc9 -> fc9
I0905 19:28:30.031733 32124 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:28:30.031745 32124 net.cpp:125] fc9 needs backward computation.
I0905 19:28:30.031754 32124 net.cpp:66] Creating Layer fc10
I0905 19:28:30.031759 32124 net.cpp:329] fc10 <- fc9
I0905 19:28:30.031767 32124 net.cpp:290] fc10 -> fc10
I0905 19:28:30.031780 32124 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:28:30.031787 32124 net.cpp:125] fc10 needs backward computation.
I0905 19:28:30.031793 32124 net.cpp:66] Creating Layer prob
I0905 19:28:30.031800 32124 net.cpp:329] prob <- fc10
I0905 19:28:30.031807 32124 net.cpp:290] prob -> prob
I0905 19:28:30.031816 32124 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:28:30.031822 32124 net.cpp:125] prob needs backward computation.
I0905 19:28:30.031827 32124 net.cpp:156] This network produces output prob
I0905 19:28:30.031839 32124 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:28:30.031848 32124 net.cpp:167] Network initialization done.
I0905 19:28:30.031853 32124 net.cpp:168] Memory required for data: 6183480
Classifying 87 inputs.
Done in 56.10 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:29:33.808257 32150 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:29:33.808398 32150 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:29:33.808406 32150 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:29:33.808552 32150 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:29:33.808615 32150 net.cpp:292] Input 0 -> data
I0905 19:29:33.808642 32150 net.cpp:66] Creating Layer conv1
I0905 19:29:33.808648 32150 net.cpp:329] conv1 <- data
I0905 19:29:33.808657 32150 net.cpp:290] conv1 -> conv1
I0905 19:29:33.810032 32150 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:29:33.810051 32150 net.cpp:125] conv1 needs backward computation.
I0905 19:29:33.810060 32150 net.cpp:66] Creating Layer relu1
I0905 19:29:33.810066 32150 net.cpp:329] relu1 <- conv1
I0905 19:29:33.810073 32150 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:29:33.810082 32150 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:29:33.810088 32150 net.cpp:125] relu1 needs backward computation.
I0905 19:29:33.810094 32150 net.cpp:66] Creating Layer pool1
I0905 19:29:33.810101 32150 net.cpp:329] pool1 <- conv1
I0905 19:29:33.810107 32150 net.cpp:290] pool1 -> pool1
I0905 19:29:33.810118 32150 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:29:33.810124 32150 net.cpp:125] pool1 needs backward computation.
I0905 19:29:33.810132 32150 net.cpp:66] Creating Layer norm1
I0905 19:29:33.810142 32150 net.cpp:329] norm1 <- pool1
I0905 19:29:33.810149 32150 net.cpp:290] norm1 -> norm1
I0905 19:29:33.810159 32150 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:29:33.810165 32150 net.cpp:125] norm1 needs backward computation.
I0905 19:29:33.810173 32150 net.cpp:66] Creating Layer conv2
I0905 19:29:33.810178 32150 net.cpp:329] conv2 <- norm1
I0905 19:29:33.810185 32150 net.cpp:290] conv2 -> conv2
I0905 19:29:33.819468 32150 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:29:33.819483 32150 net.cpp:125] conv2 needs backward computation.
I0905 19:29:33.819490 32150 net.cpp:66] Creating Layer relu2
I0905 19:29:33.819496 32150 net.cpp:329] relu2 <- conv2
I0905 19:29:33.819504 32150 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:29:33.819511 32150 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:29:33.819517 32150 net.cpp:125] relu2 needs backward computation.
I0905 19:29:33.819524 32150 net.cpp:66] Creating Layer pool2
I0905 19:29:33.819528 32150 net.cpp:329] pool2 <- conv2
I0905 19:29:33.819535 32150 net.cpp:290] pool2 -> pool2
I0905 19:29:33.819543 32150 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:29:33.819550 32150 net.cpp:125] pool2 needs backward computation.
I0905 19:29:33.819560 32150 net.cpp:66] Creating Layer fc7
I0905 19:29:33.819566 32150 net.cpp:329] fc7 <- pool2
I0905 19:29:33.819572 32150 net.cpp:290] fc7 -> fc7
I0905 19:29:34.466976 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.467020 32150 net.cpp:125] fc7 needs backward computation.
I0905 19:29:34.467033 32150 net.cpp:66] Creating Layer relu7
I0905 19:29:34.467041 32150 net.cpp:329] relu7 <- fc7
I0905 19:29:34.467049 32150 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:29:34.467059 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.467066 32150 net.cpp:125] relu7 needs backward computation.
I0905 19:29:34.467072 32150 net.cpp:66] Creating Layer drop7
I0905 19:29:34.467078 32150 net.cpp:329] drop7 <- fc7
I0905 19:29:34.467084 32150 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:29:34.467095 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.467102 32150 net.cpp:125] drop7 needs backward computation.
I0905 19:29:34.467109 32150 net.cpp:66] Creating Layer fc8
I0905 19:29:34.467115 32150 net.cpp:329] fc8 <- fc7
I0905 19:29:34.467125 32150 net.cpp:290] fc8 -> fc8
I0905 19:29:34.474908 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.474920 32150 net.cpp:125] fc8 needs backward computation.
I0905 19:29:34.474927 32150 net.cpp:66] Creating Layer relu8
I0905 19:29:34.474933 32150 net.cpp:329] relu8 <- fc8
I0905 19:29:34.474941 32150 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:29:34.474948 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.474954 32150 net.cpp:125] relu8 needs backward computation.
I0905 19:29:34.474961 32150 net.cpp:66] Creating Layer drop8
I0905 19:29:34.474967 32150 net.cpp:329] drop8 <- fc8
I0905 19:29:34.474972 32150 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:29:34.474979 32150 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:29:34.474984 32150 net.cpp:125] drop8 needs backward computation.
I0905 19:29:34.474993 32150 net.cpp:66] Creating Layer fc9
I0905 19:29:34.474999 32150 net.cpp:329] fc9 <- fc8
I0905 19:29:34.475006 32150 net.cpp:290] fc9 -> fc9
I0905 19:29:34.475380 32150 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:29:34.475391 32150 net.cpp:125] fc9 needs backward computation.
I0905 19:29:34.475400 32150 net.cpp:66] Creating Layer fc10
I0905 19:29:34.475405 32150 net.cpp:329] fc10 <- fc9
I0905 19:29:34.475414 32150 net.cpp:290] fc10 -> fc10
I0905 19:29:34.475426 32150 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:29:34.475435 32150 net.cpp:125] fc10 needs backward computation.
I0905 19:29:34.475440 32150 net.cpp:66] Creating Layer prob
I0905 19:29:34.475446 32150 net.cpp:329] prob <- fc10
I0905 19:29:34.475455 32150 net.cpp:290] prob -> prob
I0905 19:29:34.475463 32150 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:29:34.475469 32150 net.cpp:125] prob needs backward computation.
I0905 19:29:34.475484 32150 net.cpp:156] This network produces output prob
I0905 19:29:34.475497 32150 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:29:34.475505 32150 net.cpp:167] Network initialization done.
I0905 19:29:34.475512 32150 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:32:58.953320 32157 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:32:58.954380 32157 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:32:58.954444 32157 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:32:58.955963 32157 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:32:58.956336 32157 net.cpp:292] Input 0 -> data
I0905 19:32:58.956571 32157 net.cpp:66] Creating Layer conv1
I0905 19:32:58.956627 32157 net.cpp:329] conv1 <- data
I0905 19:32:58.956696 32157 net.cpp:290] conv1 -> conv1
I0905 19:32:58.983686 32157 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:32:58.983714 32157 net.cpp:125] conv1 needs backward computation.
I0905 19:32:58.983724 32157 net.cpp:66] Creating Layer relu1
I0905 19:32:58.983731 32157 net.cpp:329] relu1 <- conv1
I0905 19:32:58.983738 32157 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:32:58.983747 32157 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:32:58.983753 32157 net.cpp:125] relu1 needs backward computation.
I0905 19:32:58.983760 32157 net.cpp:66] Creating Layer pool1
I0905 19:32:58.983767 32157 net.cpp:329] pool1 <- conv1
I0905 19:32:58.983773 32157 net.cpp:290] pool1 -> pool1
I0905 19:32:58.983784 32157 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:32:58.983790 32157 net.cpp:125] pool1 needs backward computation.
I0905 19:32:58.983798 32157 net.cpp:66] Creating Layer norm1
I0905 19:32:58.983803 32157 net.cpp:329] norm1 <- pool1
I0905 19:32:58.983810 32157 net.cpp:290] norm1 -> norm1
I0905 19:32:58.983820 32157 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:32:58.983826 32157 net.cpp:125] norm1 needs backward computation.
I0905 19:32:58.983834 32157 net.cpp:66] Creating Layer conv2
I0905 19:32:58.983839 32157 net.cpp:329] conv2 <- norm1
I0905 19:32:58.983847 32157 net.cpp:290] conv2 -> conv2
I0905 19:32:58.994335 32157 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:32:58.994354 32157 net.cpp:125] conv2 needs backward computation.
I0905 19:32:58.994364 32157 net.cpp:66] Creating Layer relu2
I0905 19:32:58.994370 32157 net.cpp:329] relu2 <- conv2
I0905 19:32:58.994379 32157 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:32:58.994386 32157 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:32:58.994393 32157 net.cpp:125] relu2 needs backward computation.
I0905 19:32:58.994400 32157 net.cpp:66] Creating Layer pool2
I0905 19:32:58.994407 32157 net.cpp:329] pool2 <- conv2
I0905 19:32:58.994415 32157 net.cpp:290] pool2 -> pool2
I0905 19:32:58.994424 32157 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:32:58.994431 32157 net.cpp:125] pool2 needs backward computation.
I0905 19:32:58.994442 32157 net.cpp:66] Creating Layer fc7
I0905 19:32:58.994449 32157 net.cpp:329] fc7 <- pool2
I0905 19:32:58.994457 32157 net.cpp:290] fc7 -> fc7
I0905 19:32:59.656105 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.656152 32157 net.cpp:125] fc7 needs backward computation.
I0905 19:32:59.656164 32157 net.cpp:66] Creating Layer relu7
I0905 19:32:59.656172 32157 net.cpp:329] relu7 <- fc7
I0905 19:32:59.656182 32157 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:32:59.656191 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.656198 32157 net.cpp:125] relu7 needs backward computation.
I0905 19:32:59.656204 32157 net.cpp:66] Creating Layer drop7
I0905 19:32:59.656209 32157 net.cpp:329] drop7 <- fc7
I0905 19:32:59.656216 32157 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:32:59.656226 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.656232 32157 net.cpp:125] drop7 needs backward computation.
I0905 19:32:59.656240 32157 net.cpp:66] Creating Layer fc8
I0905 19:32:59.656246 32157 net.cpp:329] fc8 <- fc7
I0905 19:32:59.656255 32157 net.cpp:290] fc8 -> fc8
I0905 19:32:59.664216 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.664229 32157 net.cpp:125] fc8 needs backward computation.
I0905 19:32:59.664237 32157 net.cpp:66] Creating Layer relu8
I0905 19:32:59.664242 32157 net.cpp:329] relu8 <- fc8
I0905 19:32:59.664250 32157 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:32:59.664258 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.664263 32157 net.cpp:125] relu8 needs backward computation.
I0905 19:32:59.664269 32157 net.cpp:66] Creating Layer drop8
I0905 19:32:59.664274 32157 net.cpp:329] drop8 <- fc8
I0905 19:32:59.664280 32157 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:32:59.664288 32157 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:32:59.664304 32157 net.cpp:125] drop8 needs backward computation.
I0905 19:32:59.664314 32157 net.cpp:66] Creating Layer fc9
I0905 19:32:59.664320 32157 net.cpp:329] fc9 <- fc8
I0905 19:32:59.664326 32157 net.cpp:290] fc9 -> fc9
I0905 19:32:59.664708 32157 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:32:59.664721 32157 net.cpp:125] fc9 needs backward computation.
I0905 19:32:59.664728 32157 net.cpp:66] Creating Layer fc10
I0905 19:32:59.664733 32157 net.cpp:329] fc10 <- fc9
I0905 19:32:59.664742 32157 net.cpp:290] fc10 -> fc10
I0905 19:32:59.664753 32157 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:32:59.664762 32157 net.cpp:125] fc10 needs backward computation.
I0905 19:32:59.664767 32157 net.cpp:66] Creating Layer prob
I0905 19:32:59.664773 32157 net.cpp:329] prob <- fc10
I0905 19:32:59.664782 32157 net.cpp:290] prob -> prob
I0905 19:32:59.664790 32157 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:32:59.664796 32157 net.cpp:125] prob needs backward computation.
I0905 19:32:59.664801 32157 net.cpp:156] This network produces output prob
I0905 19:32:59.664813 32157 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:32:59.664823 32157 net.cpp:167] Network initialization done.
I0905 19:32:59.664827 32157 net.cpp:168] Memory required for data: 6183480
Classifying 240 inputs.
Done in 152.58 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 40 is out of bounds for axis 0 with size 40
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:35:44.230702 32163 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:35:44.230844 32163 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:35:44.230852 32163 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:35:44.230999 32163 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:35:44.231063 32163 net.cpp:292] Input 0 -> data
I0905 19:35:44.231089 32163 net.cpp:66] Creating Layer conv1
I0905 19:35:44.231096 32163 net.cpp:329] conv1 <- data
I0905 19:35:44.231104 32163 net.cpp:290] conv1 -> conv1
I0905 19:35:44.232463 32163 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:35:44.232481 32163 net.cpp:125] conv1 needs backward computation.
I0905 19:35:44.232491 32163 net.cpp:66] Creating Layer relu1
I0905 19:35:44.232496 32163 net.cpp:329] relu1 <- conv1
I0905 19:35:44.232503 32163 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:35:44.232511 32163 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:35:44.232517 32163 net.cpp:125] relu1 needs backward computation.
I0905 19:35:44.232524 32163 net.cpp:66] Creating Layer pool1
I0905 19:35:44.232529 32163 net.cpp:329] pool1 <- conv1
I0905 19:35:44.232537 32163 net.cpp:290] pool1 -> pool1
I0905 19:35:44.232547 32163 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:35:44.232553 32163 net.cpp:125] pool1 needs backward computation.
I0905 19:35:44.232559 32163 net.cpp:66] Creating Layer norm1
I0905 19:35:44.232565 32163 net.cpp:329] norm1 <- pool1
I0905 19:35:44.232571 32163 net.cpp:290] norm1 -> norm1
I0905 19:35:44.232581 32163 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:35:44.232588 32163 net.cpp:125] norm1 needs backward computation.
I0905 19:35:44.232594 32163 net.cpp:66] Creating Layer conv2
I0905 19:35:44.232599 32163 net.cpp:329] conv2 <- norm1
I0905 19:35:44.232606 32163 net.cpp:290] conv2 -> conv2
I0905 19:35:44.241775 32163 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:35:44.241789 32163 net.cpp:125] conv2 needs backward computation.
I0905 19:35:44.241797 32163 net.cpp:66] Creating Layer relu2
I0905 19:35:44.241802 32163 net.cpp:329] relu2 <- conv2
I0905 19:35:44.241809 32163 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:35:44.241816 32163 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:35:44.241822 32163 net.cpp:125] relu2 needs backward computation.
I0905 19:35:44.241832 32163 net.cpp:66] Creating Layer pool2
I0905 19:35:44.241837 32163 net.cpp:329] pool2 <- conv2
I0905 19:35:44.241843 32163 net.cpp:290] pool2 -> pool2
I0905 19:35:44.241852 32163 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:35:44.241857 32163 net.cpp:125] pool2 needs backward computation.
I0905 19:35:44.241863 32163 net.cpp:66] Creating Layer fc7
I0905 19:35:44.241868 32163 net.cpp:329] fc7 <- pool2
I0905 19:35:44.241875 32163 net.cpp:290] fc7 -> fc7
I0905 19:35:44.890015 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.890063 32163 net.cpp:125] fc7 needs backward computation.
I0905 19:35:44.890074 32163 net.cpp:66] Creating Layer relu7
I0905 19:35:44.890081 32163 net.cpp:329] relu7 <- fc7
I0905 19:35:44.890091 32163 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:35:44.890112 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.890120 32163 net.cpp:125] relu7 needs backward computation.
I0905 19:35:44.890126 32163 net.cpp:66] Creating Layer drop7
I0905 19:35:44.890132 32163 net.cpp:329] drop7 <- fc7
I0905 19:35:44.890138 32163 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:35:44.890149 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.890156 32163 net.cpp:125] drop7 needs backward computation.
I0905 19:35:44.890164 32163 net.cpp:66] Creating Layer fc8
I0905 19:35:44.890169 32163 net.cpp:329] fc8 <- fc7
I0905 19:35:44.890178 32163 net.cpp:290] fc8 -> fc8
I0905 19:35:44.897970 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.897984 32163 net.cpp:125] fc8 needs backward computation.
I0905 19:35:44.897990 32163 net.cpp:66] Creating Layer relu8
I0905 19:35:44.897996 32163 net.cpp:329] relu8 <- fc8
I0905 19:35:44.898005 32163 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:35:44.898011 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.898017 32163 net.cpp:125] relu8 needs backward computation.
I0905 19:35:44.898023 32163 net.cpp:66] Creating Layer drop8
I0905 19:35:44.898030 32163 net.cpp:329] drop8 <- fc8
I0905 19:35:44.898036 32163 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:35:44.898042 32163 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:44.898047 32163 net.cpp:125] drop8 needs backward computation.
I0905 19:35:44.898056 32163 net.cpp:66] Creating Layer fc9
I0905 19:35:44.898062 32163 net.cpp:329] fc9 <- fc8
I0905 19:35:44.898069 32163 net.cpp:290] fc9 -> fc9
I0905 19:35:44.898442 32163 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:35:44.898454 32163 net.cpp:125] fc9 needs backward computation.
I0905 19:35:44.898463 32163 net.cpp:66] Creating Layer fc10
I0905 19:35:44.898468 32163 net.cpp:329] fc10 <- fc9
I0905 19:35:44.898478 32163 net.cpp:290] fc10 -> fc10
I0905 19:35:44.898488 32163 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:35:44.898496 32163 net.cpp:125] fc10 needs backward computation.
I0905 19:35:44.898502 32163 net.cpp:66] Creating Layer prob
I0905 19:35:44.898509 32163 net.cpp:329] prob <- fc10
I0905 19:35:44.898515 32163 net.cpp:290] prob -> prob
I0905 19:35:44.898525 32163 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:35:44.898530 32163 net.cpp:125] prob needs backward computation.
I0905 19:35:44.898535 32163 net.cpp:156] This network produces output prob
I0905 19:35:44.898548 32163 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:35:44.898557 32163 net.cpp:167] Network initialization done.
I0905 19:35:44.898562 32163 net.cpp:168] Memory required for data: 6183480
Classifying 13 inputs.
Done in 8.02 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:35:54.251045 32166 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:35:54.251185 32166 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:35:54.251194 32166 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:35:54.251343 32166 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:35:54.251406 32166 net.cpp:292] Input 0 -> data
I0905 19:35:54.251432 32166 net.cpp:66] Creating Layer conv1
I0905 19:35:54.251440 32166 net.cpp:329] conv1 <- data
I0905 19:35:54.251447 32166 net.cpp:290] conv1 -> conv1
I0905 19:35:54.252805 32166 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:35:54.252825 32166 net.cpp:125] conv1 needs backward computation.
I0905 19:35:54.252833 32166 net.cpp:66] Creating Layer relu1
I0905 19:35:54.252840 32166 net.cpp:329] relu1 <- conv1
I0905 19:35:54.252846 32166 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:35:54.252854 32166 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:35:54.252861 32166 net.cpp:125] relu1 needs backward computation.
I0905 19:35:54.252866 32166 net.cpp:66] Creating Layer pool1
I0905 19:35:54.252872 32166 net.cpp:329] pool1 <- conv1
I0905 19:35:54.252878 32166 net.cpp:290] pool1 -> pool1
I0905 19:35:54.252890 32166 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:35:54.252895 32166 net.cpp:125] pool1 needs backward computation.
I0905 19:35:54.252902 32166 net.cpp:66] Creating Layer norm1
I0905 19:35:54.252907 32166 net.cpp:329] norm1 <- pool1
I0905 19:35:54.252914 32166 net.cpp:290] norm1 -> norm1
I0905 19:35:54.252924 32166 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:35:54.252930 32166 net.cpp:125] norm1 needs backward computation.
I0905 19:35:54.252938 32166 net.cpp:66] Creating Layer conv2
I0905 19:35:54.252943 32166 net.cpp:329] conv2 <- norm1
I0905 19:35:54.252949 32166 net.cpp:290] conv2 -> conv2
I0905 19:35:54.262122 32166 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:35:54.262138 32166 net.cpp:125] conv2 needs backward computation.
I0905 19:35:54.262145 32166 net.cpp:66] Creating Layer relu2
I0905 19:35:54.262151 32166 net.cpp:329] relu2 <- conv2
I0905 19:35:54.262162 32166 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:35:54.262171 32166 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:35:54.262176 32166 net.cpp:125] relu2 needs backward computation.
I0905 19:35:54.262184 32166 net.cpp:66] Creating Layer pool2
I0905 19:35:54.262190 32166 net.cpp:329] pool2 <- conv2
I0905 19:35:54.262197 32166 net.cpp:290] pool2 -> pool2
I0905 19:35:54.262204 32166 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:35:54.262210 32166 net.cpp:125] pool2 needs backward computation.
I0905 19:35:54.262217 32166 net.cpp:66] Creating Layer fc7
I0905 19:35:54.262223 32166 net.cpp:329] fc7 <- pool2
I0905 19:35:54.262229 32166 net.cpp:290] fc7 -> fc7
I0905 19:35:54.913836 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.913883 32166 net.cpp:125] fc7 needs backward computation.
I0905 19:35:54.913895 32166 net.cpp:66] Creating Layer relu7
I0905 19:35:54.913903 32166 net.cpp:329] relu7 <- fc7
I0905 19:35:54.913913 32166 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:35:54.913923 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.913928 32166 net.cpp:125] relu7 needs backward computation.
I0905 19:35:54.913935 32166 net.cpp:66] Creating Layer drop7
I0905 19:35:54.913941 32166 net.cpp:329] drop7 <- fc7
I0905 19:35:54.913947 32166 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:35:54.913959 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.913964 32166 net.cpp:125] drop7 needs backward computation.
I0905 19:35:54.913974 32166 net.cpp:66] Creating Layer fc8
I0905 19:35:54.913979 32166 net.cpp:329] fc8 <- fc7
I0905 19:35:54.913987 32166 net.cpp:290] fc8 -> fc8
I0905 19:35:54.921797 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.921809 32166 net.cpp:125] fc8 needs backward computation.
I0905 19:35:54.921816 32166 net.cpp:66] Creating Layer relu8
I0905 19:35:54.921823 32166 net.cpp:329] relu8 <- fc8
I0905 19:35:54.921829 32166 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:35:54.921836 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.921843 32166 net.cpp:125] relu8 needs backward computation.
I0905 19:35:54.921849 32166 net.cpp:66] Creating Layer drop8
I0905 19:35:54.921854 32166 net.cpp:329] drop8 <- fc8
I0905 19:35:54.921860 32166 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:35:54.921867 32166 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:35:54.921872 32166 net.cpp:125] drop8 needs backward computation.
I0905 19:35:54.921881 32166 net.cpp:66] Creating Layer fc9
I0905 19:35:54.921887 32166 net.cpp:329] fc9 <- fc8
I0905 19:35:54.921895 32166 net.cpp:290] fc9 -> fc9
I0905 19:35:54.922268 32166 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:35:54.922281 32166 net.cpp:125] fc9 needs backward computation.
I0905 19:35:54.922288 32166 net.cpp:66] Creating Layer fc10
I0905 19:35:54.922293 32166 net.cpp:329] fc10 <- fc9
I0905 19:35:54.922302 32166 net.cpp:290] fc10 -> fc10
I0905 19:35:54.922313 32166 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:35:54.922322 32166 net.cpp:125] fc10 needs backward computation.
I0905 19:35:54.922327 32166 net.cpp:66] Creating Layer prob
I0905 19:35:54.922333 32166 net.cpp:329] prob <- fc10
I0905 19:35:54.922340 32166 net.cpp:290] prob -> prob
I0905 19:35:54.922349 32166 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:35:54.922355 32166 net.cpp:125] prob needs backward computation.
I0905 19:35:54.922360 32166 net.cpp:156] This network produces output prob
I0905 19:35:54.922372 32166 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:35:54.922381 32166 net.cpp:167] Network initialization done.
I0905 19:35:54.922386 32166 net.cpp:168] Memory required for data: 6183480
Classifying 58 inputs.
Done in 39.92 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:36:37.059237 32171 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:36:37.059376 32171 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:36:37.059397 32171 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:36:37.059545 32171 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:36:37.059598 32171 net.cpp:292] Input 0 -> data
I0905 19:36:37.059624 32171 net.cpp:66] Creating Layer conv1
I0905 19:36:37.059631 32171 net.cpp:329] conv1 <- data
I0905 19:36:37.059639 32171 net.cpp:290] conv1 -> conv1
I0905 19:36:37.061022 32171 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:36:37.061039 32171 net.cpp:125] conv1 needs backward computation.
I0905 19:36:37.061048 32171 net.cpp:66] Creating Layer relu1
I0905 19:36:37.061054 32171 net.cpp:329] relu1 <- conv1
I0905 19:36:37.061061 32171 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:36:37.061070 32171 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:36:37.061076 32171 net.cpp:125] relu1 needs backward computation.
I0905 19:36:37.061084 32171 net.cpp:66] Creating Layer pool1
I0905 19:36:37.061095 32171 net.cpp:329] pool1 <- conv1
I0905 19:36:37.061102 32171 net.cpp:290] pool1 -> pool1
I0905 19:36:37.061113 32171 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:36:37.061120 32171 net.cpp:125] pool1 needs backward computation.
I0905 19:36:37.061126 32171 net.cpp:66] Creating Layer norm1
I0905 19:36:37.061132 32171 net.cpp:329] norm1 <- pool1
I0905 19:36:37.061139 32171 net.cpp:290] norm1 -> norm1
I0905 19:36:37.061148 32171 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:36:37.061154 32171 net.cpp:125] norm1 needs backward computation.
I0905 19:36:37.061162 32171 net.cpp:66] Creating Layer conv2
I0905 19:36:37.061167 32171 net.cpp:329] conv2 <- norm1
I0905 19:36:37.061182 32171 net.cpp:290] conv2 -> conv2
I0905 19:36:37.070345 32171 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:36:37.070361 32171 net.cpp:125] conv2 needs backward computation.
I0905 19:36:37.070369 32171 net.cpp:66] Creating Layer relu2
I0905 19:36:37.070375 32171 net.cpp:329] relu2 <- conv2
I0905 19:36:37.070381 32171 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:36:37.070389 32171 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:36:37.070394 32171 net.cpp:125] relu2 needs backward computation.
I0905 19:36:37.070401 32171 net.cpp:66] Creating Layer pool2
I0905 19:36:37.070407 32171 net.cpp:329] pool2 <- conv2
I0905 19:36:37.070413 32171 net.cpp:290] pool2 -> pool2
I0905 19:36:37.070421 32171 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:36:37.070427 32171 net.cpp:125] pool2 needs backward computation.
I0905 19:36:37.070437 32171 net.cpp:66] Creating Layer fc7
I0905 19:36:37.070443 32171 net.cpp:329] fc7 <- pool2
I0905 19:36:37.070451 32171 net.cpp:290] fc7 -> fc7
I0905 19:36:37.718057 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.718102 32171 net.cpp:125] fc7 needs backward computation.
I0905 19:36:37.718116 32171 net.cpp:66] Creating Layer relu7
I0905 19:36:37.718122 32171 net.cpp:329] relu7 <- fc7
I0905 19:36:37.718132 32171 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:36:37.718142 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.718148 32171 net.cpp:125] relu7 needs backward computation.
I0905 19:36:37.718155 32171 net.cpp:66] Creating Layer drop7
I0905 19:36:37.718161 32171 net.cpp:329] drop7 <- fc7
I0905 19:36:37.718168 32171 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:36:37.718178 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.718184 32171 net.cpp:125] drop7 needs backward computation.
I0905 19:36:37.718194 32171 net.cpp:66] Creating Layer fc8
I0905 19:36:37.718199 32171 net.cpp:329] fc8 <- fc7
I0905 19:36:37.718209 32171 net.cpp:290] fc8 -> fc8
I0905 19:36:37.725991 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.726002 32171 net.cpp:125] fc8 needs backward computation.
I0905 19:36:37.726011 32171 net.cpp:66] Creating Layer relu8
I0905 19:36:37.726016 32171 net.cpp:329] relu8 <- fc8
I0905 19:36:37.726023 32171 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:36:37.726032 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.726037 32171 net.cpp:125] relu8 needs backward computation.
I0905 19:36:37.726043 32171 net.cpp:66] Creating Layer drop8
I0905 19:36:37.726048 32171 net.cpp:329] drop8 <- fc8
I0905 19:36:37.726055 32171 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:36:37.726063 32171 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:36:37.726068 32171 net.cpp:125] drop8 needs backward computation.
I0905 19:36:37.726076 32171 net.cpp:66] Creating Layer fc9
I0905 19:36:37.726083 32171 net.cpp:329] fc9 <- fc8
I0905 19:36:37.726089 32171 net.cpp:290] fc9 -> fc9
I0905 19:36:37.726462 32171 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:36:37.726475 32171 net.cpp:125] fc9 needs backward computation.
I0905 19:36:37.726482 32171 net.cpp:66] Creating Layer fc10
I0905 19:36:37.726488 32171 net.cpp:329] fc10 <- fc9
I0905 19:36:37.726496 32171 net.cpp:290] fc10 -> fc10
I0905 19:36:37.726508 32171 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:36:37.726516 32171 net.cpp:125] fc10 needs backward computation.
I0905 19:36:37.726533 32171 net.cpp:66] Creating Layer prob
I0905 19:36:37.726539 32171 net.cpp:329] prob <- fc10
I0905 19:36:37.726547 32171 net.cpp:290] prob -> prob
I0905 19:36:37.726557 32171 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:36:37.726563 32171 net.cpp:125] prob needs backward computation.
I0905 19:36:37.726568 32171 net.cpp:156] This network produces output prob
I0905 19:36:37.726582 32171 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:36:37.726589 32171 net.cpp:167] Network initialization done.
I0905 19:36:37.726595 32171 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:40:32.578399 32187 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:40:32.578606 32187 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:40:32.578618 32187 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:40:32.578785 32187 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:40:32.578846 32187 net.cpp:292] Input 0 -> data
I0905 19:40:32.578891 32187 net.cpp:66] Creating Layer conv1
I0905 19:40:32.578899 32187 net.cpp:329] conv1 <- data
I0905 19:40:32.578907 32187 net.cpp:290] conv1 -> conv1
I0905 19:40:32.611079 32187 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:40:32.611104 32187 net.cpp:125] conv1 needs backward computation.
I0905 19:40:32.611114 32187 net.cpp:66] Creating Layer relu1
I0905 19:40:32.611121 32187 net.cpp:329] relu1 <- conv1
I0905 19:40:32.611129 32187 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:40:32.611137 32187 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:40:32.611143 32187 net.cpp:125] relu1 needs backward computation.
I0905 19:40:32.611151 32187 net.cpp:66] Creating Layer pool1
I0905 19:40:32.611156 32187 net.cpp:329] pool1 <- conv1
I0905 19:40:32.611165 32187 net.cpp:290] pool1 -> pool1
I0905 19:40:32.611174 32187 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:40:32.611181 32187 net.cpp:125] pool1 needs backward computation.
I0905 19:40:32.611188 32187 net.cpp:66] Creating Layer norm1
I0905 19:40:32.611194 32187 net.cpp:329] norm1 <- pool1
I0905 19:40:32.611202 32187 net.cpp:290] norm1 -> norm1
I0905 19:40:32.611210 32187 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:40:32.611217 32187 net.cpp:125] norm1 needs backward computation.
I0905 19:40:32.611224 32187 net.cpp:66] Creating Layer conv2
I0905 19:40:32.611230 32187 net.cpp:329] conv2 <- norm1
I0905 19:40:32.611238 32187 net.cpp:290] conv2 -> conv2
I0905 19:40:32.620600 32187 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:40:32.620616 32187 net.cpp:125] conv2 needs backward computation.
I0905 19:40:32.620625 32187 net.cpp:66] Creating Layer relu2
I0905 19:40:32.620630 32187 net.cpp:329] relu2 <- conv2
I0905 19:40:32.620637 32187 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:40:32.620645 32187 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:40:32.620651 32187 net.cpp:125] relu2 needs backward computation.
I0905 19:40:32.620657 32187 net.cpp:66] Creating Layer pool2
I0905 19:40:32.620663 32187 net.cpp:329] pool2 <- conv2
I0905 19:40:32.620671 32187 net.cpp:290] pool2 -> pool2
I0905 19:40:32.620679 32187 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:40:32.620684 32187 net.cpp:125] pool2 needs backward computation.
I0905 19:40:32.620694 32187 net.cpp:66] Creating Layer fc7
I0905 19:40:32.620702 32187 net.cpp:329] fc7 <- pool2
I0905 19:40:32.620708 32187 net.cpp:290] fc7 -> fc7
I0905 19:40:33.285334 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.285379 32187 net.cpp:125] fc7 needs backward computation.
I0905 19:40:33.285392 32187 net.cpp:66] Creating Layer relu7
I0905 19:40:33.285399 32187 net.cpp:329] relu7 <- fc7
I0905 19:40:33.285409 32187 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:40:33.285419 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.285425 32187 net.cpp:125] relu7 needs backward computation.
I0905 19:40:33.285434 32187 net.cpp:66] Creating Layer drop7
I0905 19:40:33.285439 32187 net.cpp:329] drop7 <- fc7
I0905 19:40:33.285444 32187 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:40:33.285455 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.285461 32187 net.cpp:125] drop7 needs backward computation.
I0905 19:40:33.285470 32187 net.cpp:66] Creating Layer fc8
I0905 19:40:33.285475 32187 net.cpp:329] fc8 <- fc7
I0905 19:40:33.285486 32187 net.cpp:290] fc8 -> fc8
I0905 19:40:33.293465 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.293478 32187 net.cpp:125] fc8 needs backward computation.
I0905 19:40:33.293485 32187 net.cpp:66] Creating Layer relu8
I0905 19:40:33.293490 32187 net.cpp:329] relu8 <- fc8
I0905 19:40:33.293499 32187 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:40:33.293506 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.293524 32187 net.cpp:125] relu8 needs backward computation.
I0905 19:40:33.293530 32187 net.cpp:66] Creating Layer drop8
I0905 19:40:33.293535 32187 net.cpp:329] drop8 <- fc8
I0905 19:40:33.293542 32187 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:40:33.293550 32187 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:40:33.293555 32187 net.cpp:125] drop8 needs backward computation.
I0905 19:40:33.293563 32187 net.cpp:66] Creating Layer fc9
I0905 19:40:33.293570 32187 net.cpp:329] fc9 <- fc8
I0905 19:40:33.293576 32187 net.cpp:290] fc9 -> fc9
I0905 19:40:33.293988 32187 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:40:33.294000 32187 net.cpp:125] fc9 needs backward computation.
I0905 19:40:33.294009 32187 net.cpp:66] Creating Layer fc10
I0905 19:40:33.294015 32187 net.cpp:329] fc10 <- fc9
I0905 19:40:33.294024 32187 net.cpp:290] fc10 -> fc10
I0905 19:40:33.294036 32187 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:40:33.294044 32187 net.cpp:125] fc10 needs backward computation.
I0905 19:40:33.294051 32187 net.cpp:66] Creating Layer prob
I0905 19:40:33.294057 32187 net.cpp:329] prob <- fc10
I0905 19:40:33.294065 32187 net.cpp:290] prob -> prob
I0905 19:40:33.294075 32187 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:40:33.294081 32187 net.cpp:125] prob needs backward computation.
I0905 19:40:33.294086 32187 net.cpp:156] This network produces output prob
I0905 19:40:33.294100 32187 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:40:33.294108 32187 net.cpp:167] Network initialization done.
I0905 19:40:33.294113 32187 net.cpp:168] Memory required for data: 6183480
Classifying 94 inputs.
Done in 1002.36 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:58:29.860929 32226 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:58:29.861078 32226 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:58:29.861086 32226 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:58:29.861233 32226 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:58:29.861294 32226 net.cpp:292] Input 0 -> data
I0905 19:58:29.861320 32226 net.cpp:66] Creating Layer conv1
I0905 19:58:29.861327 32226 net.cpp:329] conv1 <- data
I0905 19:58:29.861335 32226 net.cpp:290] conv1 -> conv1
I0905 19:58:29.869681 32226 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:58:29.869705 32226 net.cpp:125] conv1 needs backward computation.
I0905 19:58:29.869714 32226 net.cpp:66] Creating Layer relu1
I0905 19:58:29.869721 32226 net.cpp:329] relu1 <- conv1
I0905 19:58:29.869729 32226 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:58:29.869737 32226 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:58:29.869742 32226 net.cpp:125] relu1 needs backward computation.
I0905 19:58:29.869750 32226 net.cpp:66] Creating Layer pool1
I0905 19:58:29.869755 32226 net.cpp:329] pool1 <- conv1
I0905 19:58:29.869762 32226 net.cpp:290] pool1 -> pool1
I0905 19:58:29.869773 32226 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:58:29.869779 32226 net.cpp:125] pool1 needs backward computation.
I0905 19:58:29.869786 32226 net.cpp:66] Creating Layer norm1
I0905 19:58:29.869791 32226 net.cpp:329] norm1 <- pool1
I0905 19:58:29.869798 32226 net.cpp:290] norm1 -> norm1
I0905 19:58:29.869807 32226 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:58:29.869813 32226 net.cpp:125] norm1 needs backward computation.
I0905 19:58:29.869820 32226 net.cpp:66] Creating Layer conv2
I0905 19:58:29.869827 32226 net.cpp:329] conv2 <- norm1
I0905 19:58:29.869833 32226 net.cpp:290] conv2 -> conv2
I0905 19:58:29.878955 32226 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:58:29.878970 32226 net.cpp:125] conv2 needs backward computation.
I0905 19:58:29.878978 32226 net.cpp:66] Creating Layer relu2
I0905 19:58:29.878983 32226 net.cpp:329] relu2 <- conv2
I0905 19:58:29.878990 32226 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:58:29.878998 32226 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:58:29.879004 32226 net.cpp:125] relu2 needs backward computation.
I0905 19:58:29.879011 32226 net.cpp:66] Creating Layer pool2
I0905 19:58:29.879017 32226 net.cpp:329] pool2 <- conv2
I0905 19:58:29.879024 32226 net.cpp:290] pool2 -> pool2
I0905 19:58:29.879032 32226 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:58:29.879037 32226 net.cpp:125] pool2 needs backward computation.
I0905 19:58:29.879045 32226 net.cpp:66] Creating Layer fc7
I0905 19:58:29.879050 32226 net.cpp:329] fc7 <- pool2
I0905 19:58:29.879057 32226 net.cpp:290] fc7 -> fc7
I0905 19:58:30.531465 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.531512 32226 net.cpp:125] fc7 needs backward computation.
I0905 19:58:30.531524 32226 net.cpp:66] Creating Layer relu7
I0905 19:58:30.531532 32226 net.cpp:329] relu7 <- fc7
I0905 19:58:30.531553 32226 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:58:30.531564 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.531570 32226 net.cpp:125] relu7 needs backward computation.
I0905 19:58:30.531579 32226 net.cpp:66] Creating Layer drop7
I0905 19:58:30.531584 32226 net.cpp:329] drop7 <- fc7
I0905 19:58:30.531590 32226 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:58:30.531600 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.531606 32226 net.cpp:125] drop7 needs backward computation.
I0905 19:58:30.531615 32226 net.cpp:66] Creating Layer fc8
I0905 19:58:30.531620 32226 net.cpp:329] fc8 <- fc7
I0905 19:58:30.531630 32226 net.cpp:290] fc8 -> fc8
I0905 19:58:30.539634 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.539647 32226 net.cpp:125] fc8 needs backward computation.
I0905 19:58:30.539654 32226 net.cpp:66] Creating Layer relu8
I0905 19:58:30.539660 32226 net.cpp:329] relu8 <- fc8
I0905 19:58:30.539669 32226 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:58:30.539675 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.539681 32226 net.cpp:125] relu8 needs backward computation.
I0905 19:58:30.539688 32226 net.cpp:66] Creating Layer drop8
I0905 19:58:30.539693 32226 net.cpp:329] drop8 <- fc8
I0905 19:58:30.539700 32226 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:58:30.539706 32226 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:30.539712 32226 net.cpp:125] drop8 needs backward computation.
I0905 19:58:30.539721 32226 net.cpp:66] Creating Layer fc9
I0905 19:58:30.539726 32226 net.cpp:329] fc9 <- fc8
I0905 19:58:30.539733 32226 net.cpp:290] fc9 -> fc9
I0905 19:58:30.540118 32226 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:58:30.540130 32226 net.cpp:125] fc9 needs backward computation.
I0905 19:58:30.540139 32226 net.cpp:66] Creating Layer fc10
I0905 19:58:30.540145 32226 net.cpp:329] fc10 <- fc9
I0905 19:58:30.540153 32226 net.cpp:290] fc10 -> fc10
I0905 19:58:30.540165 32226 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:58:30.540174 32226 net.cpp:125] fc10 needs backward computation.
I0905 19:58:30.540180 32226 net.cpp:66] Creating Layer prob
I0905 19:58:30.540186 32226 net.cpp:329] prob <- fc10
I0905 19:58:30.540194 32226 net.cpp:290] prob -> prob
I0905 19:58:30.555856 32226 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:58:30.555877 32226 net.cpp:125] prob needs backward computation.
I0905 19:58:30.555886 32226 net.cpp:156] This network produces output prob
I0905 19:58:30.555907 32226 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:58:30.555918 32226 net.cpp:167] Network initialization done.
I0905 19:58:30.555924 32226 net.cpp:168] Memory required for data: 6183480
Classifying 26 inputs.
Done in 16.81 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:58:50.247989 32231 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:58:50.248128 32231 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:58:50.248137 32231 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:58:50.248285 32231 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:58:50.248347 32231 net.cpp:292] Input 0 -> data
I0905 19:58:50.248373 32231 net.cpp:66] Creating Layer conv1
I0905 19:58:50.248380 32231 net.cpp:329] conv1 <- data
I0905 19:58:50.248389 32231 net.cpp:290] conv1 -> conv1
I0905 19:58:50.249788 32231 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:58:50.249807 32231 net.cpp:125] conv1 needs backward computation.
I0905 19:58:50.249816 32231 net.cpp:66] Creating Layer relu1
I0905 19:58:50.249822 32231 net.cpp:329] relu1 <- conv1
I0905 19:58:50.249830 32231 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:58:50.249838 32231 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:58:50.249845 32231 net.cpp:125] relu1 needs backward computation.
I0905 19:58:50.249851 32231 net.cpp:66] Creating Layer pool1
I0905 19:58:50.249856 32231 net.cpp:329] pool1 <- conv1
I0905 19:58:50.249863 32231 net.cpp:290] pool1 -> pool1
I0905 19:58:50.249874 32231 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:58:50.249881 32231 net.cpp:125] pool1 needs backward computation.
I0905 19:58:50.249886 32231 net.cpp:66] Creating Layer norm1
I0905 19:58:50.249892 32231 net.cpp:329] norm1 <- pool1
I0905 19:58:50.249898 32231 net.cpp:290] norm1 -> norm1
I0905 19:58:50.249908 32231 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:58:50.249914 32231 net.cpp:125] norm1 needs backward computation.
I0905 19:58:50.249922 32231 net.cpp:66] Creating Layer conv2
I0905 19:58:50.249927 32231 net.cpp:329] conv2 <- norm1
I0905 19:58:50.249933 32231 net.cpp:290] conv2 -> conv2
I0905 19:58:50.259062 32231 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:58:50.259076 32231 net.cpp:125] conv2 needs backward computation.
I0905 19:58:50.259083 32231 net.cpp:66] Creating Layer relu2
I0905 19:58:50.259094 32231 net.cpp:329] relu2 <- conv2
I0905 19:58:50.259101 32231 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:58:50.259109 32231 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:58:50.259114 32231 net.cpp:125] relu2 needs backward computation.
I0905 19:58:50.259120 32231 net.cpp:66] Creating Layer pool2
I0905 19:58:50.259125 32231 net.cpp:329] pool2 <- conv2
I0905 19:58:50.259131 32231 net.cpp:290] pool2 -> pool2
I0905 19:58:50.259140 32231 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:58:50.259145 32231 net.cpp:125] pool2 needs backward computation.
I0905 19:58:50.259155 32231 net.cpp:66] Creating Layer fc7
I0905 19:58:50.259160 32231 net.cpp:329] fc7 <- pool2
I0905 19:58:50.259167 32231 net.cpp:290] fc7 -> fc7
I0905 19:58:50.909064 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.909106 32231 net.cpp:125] fc7 needs backward computation.
I0905 19:58:50.909119 32231 net.cpp:66] Creating Layer relu7
I0905 19:58:50.909126 32231 net.cpp:329] relu7 <- fc7
I0905 19:58:50.909134 32231 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:58:50.909145 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.909152 32231 net.cpp:125] relu7 needs backward computation.
I0905 19:58:50.909158 32231 net.cpp:66] Creating Layer drop7
I0905 19:58:50.909163 32231 net.cpp:329] drop7 <- fc7
I0905 19:58:50.909169 32231 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:58:50.909180 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.909186 32231 net.cpp:125] drop7 needs backward computation.
I0905 19:58:50.909194 32231 net.cpp:66] Creating Layer fc8
I0905 19:58:50.909200 32231 net.cpp:329] fc8 <- fc7
I0905 19:58:50.909209 32231 net.cpp:290] fc8 -> fc8
I0905 19:58:50.916985 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.916997 32231 net.cpp:125] fc8 needs backward computation.
I0905 19:58:50.917004 32231 net.cpp:66] Creating Layer relu8
I0905 19:58:50.917011 32231 net.cpp:329] relu8 <- fc8
I0905 19:58:50.917018 32231 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:58:50.917026 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.917032 32231 net.cpp:125] relu8 needs backward computation.
I0905 19:58:50.917037 32231 net.cpp:66] Creating Layer drop8
I0905 19:58:50.917043 32231 net.cpp:329] drop8 <- fc8
I0905 19:58:50.917049 32231 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:58:50.917057 32231 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:58:50.917062 32231 net.cpp:125] drop8 needs backward computation.
I0905 19:58:50.917070 32231 net.cpp:66] Creating Layer fc9
I0905 19:58:50.917076 32231 net.cpp:329] fc9 <- fc8
I0905 19:58:50.917083 32231 net.cpp:290] fc9 -> fc9
I0905 19:58:50.917455 32231 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:58:50.917467 32231 net.cpp:125] fc9 needs backward computation.
I0905 19:58:50.917475 32231 net.cpp:66] Creating Layer fc10
I0905 19:58:50.917481 32231 net.cpp:329] fc10 <- fc9
I0905 19:58:50.917490 32231 net.cpp:290] fc10 -> fc10
I0905 19:58:50.917500 32231 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:58:50.917508 32231 net.cpp:125] fc10 needs backward computation.
I0905 19:58:50.917515 32231 net.cpp:66] Creating Layer prob
I0905 19:58:50.917520 32231 net.cpp:329] prob <- fc10
I0905 19:58:50.917528 32231 net.cpp:290] prob -> prob
I0905 19:58:50.917537 32231 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:58:50.917543 32231 net.cpp:125] prob needs backward computation.
I0905 19:58:50.917548 32231 net.cpp:156] This network produces output prob
I0905 19:58:50.917562 32231 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:58:50.917569 32231 net.cpp:167] Network initialization done.
I0905 19:58:50.917574 32231 net.cpp:168] Memory required for data: 6183480
Classifying 400 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:59:00.085974 32235 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:59:00.086112 32235 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:59:00.086120 32235 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:59:00.086267 32235 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:59:00.086319 32235 net.cpp:292] Input 0 -> data
I0905 19:59:00.086344 32235 net.cpp:66] Creating Layer conv1
I0905 19:59:00.086351 32235 net.cpp:329] conv1 <- data
I0905 19:59:00.086360 32235 net.cpp:290] conv1 -> conv1
I0905 19:59:00.087821 32235 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:59:00.087839 32235 net.cpp:125] conv1 needs backward computation.
I0905 19:59:00.087848 32235 net.cpp:66] Creating Layer relu1
I0905 19:59:00.087856 32235 net.cpp:329] relu1 <- conv1
I0905 19:59:00.087862 32235 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:59:00.087872 32235 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:59:00.087877 32235 net.cpp:125] relu1 needs backward computation.
I0905 19:59:00.087884 32235 net.cpp:66] Creating Layer pool1
I0905 19:59:00.087890 32235 net.cpp:329] pool1 <- conv1
I0905 19:59:00.087898 32235 net.cpp:290] pool1 -> pool1
I0905 19:59:00.087909 32235 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:59:00.087915 32235 net.cpp:125] pool1 needs backward computation.
I0905 19:59:00.087923 32235 net.cpp:66] Creating Layer norm1
I0905 19:59:00.087929 32235 net.cpp:329] norm1 <- pool1
I0905 19:59:00.087935 32235 net.cpp:290] norm1 -> norm1
I0905 19:59:00.087945 32235 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:59:00.087951 32235 net.cpp:125] norm1 needs backward computation.
I0905 19:59:00.087960 32235 net.cpp:66] Creating Layer conv2
I0905 19:59:00.087965 32235 net.cpp:329] conv2 <- norm1
I0905 19:59:00.087972 32235 net.cpp:290] conv2 -> conv2
I0905 19:59:00.097311 32235 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:59:00.097327 32235 net.cpp:125] conv2 needs backward computation.
I0905 19:59:00.097336 32235 net.cpp:66] Creating Layer relu2
I0905 19:59:00.097342 32235 net.cpp:329] relu2 <- conv2
I0905 19:59:00.097349 32235 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:59:00.097357 32235 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:59:00.097362 32235 net.cpp:125] relu2 needs backward computation.
I0905 19:59:00.097369 32235 net.cpp:66] Creating Layer pool2
I0905 19:59:00.097375 32235 net.cpp:329] pool2 <- conv2
I0905 19:59:00.097383 32235 net.cpp:290] pool2 -> pool2
I0905 19:59:00.097390 32235 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:59:00.097396 32235 net.cpp:125] pool2 needs backward computation.
I0905 19:59:00.097405 32235 net.cpp:66] Creating Layer fc7
I0905 19:59:00.097411 32235 net.cpp:329] fc7 <- pool2
I0905 19:59:00.097419 32235 net.cpp:290] fc7 -> fc7
I0905 19:59:00.752884 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.752931 32235 net.cpp:125] fc7 needs backward computation.
I0905 19:59:00.752945 32235 net.cpp:66] Creating Layer relu7
I0905 19:59:00.752954 32235 net.cpp:329] relu7 <- fc7
I0905 19:59:00.752964 32235 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:59:00.752974 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.752979 32235 net.cpp:125] relu7 needs backward computation.
I0905 19:59:00.752987 32235 net.cpp:66] Creating Layer drop7
I0905 19:59:00.752993 32235 net.cpp:329] drop7 <- fc7
I0905 19:59:00.753000 32235 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:59:00.753010 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.753017 32235 net.cpp:125] drop7 needs backward computation.
I0905 19:59:00.753026 32235 net.cpp:66] Creating Layer fc8
I0905 19:59:00.753031 32235 net.cpp:329] fc8 <- fc7
I0905 19:59:00.753041 32235 net.cpp:290] fc8 -> fc8
I0905 19:59:00.760821 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.760834 32235 net.cpp:125] fc8 needs backward computation.
I0905 19:59:00.760841 32235 net.cpp:66] Creating Layer relu8
I0905 19:59:00.760848 32235 net.cpp:329] relu8 <- fc8
I0905 19:59:00.760855 32235 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:59:00.760862 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.760869 32235 net.cpp:125] relu8 needs backward computation.
I0905 19:59:00.760875 32235 net.cpp:66] Creating Layer drop8
I0905 19:59:00.760880 32235 net.cpp:329] drop8 <- fc8
I0905 19:59:00.760887 32235 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:59:00.760895 32235 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:00.760900 32235 net.cpp:125] drop8 needs backward computation.
I0905 19:59:00.760908 32235 net.cpp:66] Creating Layer fc9
I0905 19:59:00.760915 32235 net.cpp:329] fc9 <- fc8
I0905 19:59:00.760931 32235 net.cpp:290] fc9 -> fc9
I0905 19:59:00.761304 32235 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:59:00.761317 32235 net.cpp:125] fc9 needs backward computation.
I0905 19:59:00.761325 32235 net.cpp:66] Creating Layer fc10
I0905 19:59:00.761332 32235 net.cpp:329] fc10 <- fc9
I0905 19:59:00.761340 32235 net.cpp:290] fc10 -> fc10
I0905 19:59:00.761353 32235 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:59:00.761360 32235 net.cpp:125] fc10 needs backward computation.
I0905 19:59:00.761368 32235 net.cpp:66] Creating Layer prob
I0905 19:59:00.761373 32235 net.cpp:329] prob <- fc10
I0905 19:59:00.761380 32235 net.cpp:290] prob -> prob
I0905 19:59:00.761390 32235 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:59:00.761396 32235 net.cpp:125] prob needs backward computation.
I0905 19:59:00.761401 32235 net.cpp:156] This network produces output prob
I0905 19:59:00.761415 32235 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:59:00.761422 32235 net.cpp:167] Network initialization done.
I0905 19:59:00.761428 32235 net.cpp:168] Memory required for data: 6183480
Classifying 27 inputs.
Done in 17.37 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 19:59:19.492086 32240 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 19:59:19.492225 32240 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 19:59:19.492234 32240 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 19:59:19.492383 32240 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 19:59:19.492444 32240 net.cpp:292] Input 0 -> data
I0905 19:59:19.492470 32240 net.cpp:66] Creating Layer conv1
I0905 19:59:19.492477 32240 net.cpp:329] conv1 <- data
I0905 19:59:19.492486 32240 net.cpp:290] conv1 -> conv1
I0905 19:59:19.493896 32240 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:59:19.493916 32240 net.cpp:125] conv1 needs backward computation.
I0905 19:59:19.493924 32240 net.cpp:66] Creating Layer relu1
I0905 19:59:19.493931 32240 net.cpp:329] relu1 <- conv1
I0905 19:59:19.493937 32240 net.cpp:280] relu1 -> conv1 (in-place)
I0905 19:59:19.493945 32240 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 19:59:19.493952 32240 net.cpp:125] relu1 needs backward computation.
I0905 19:59:19.493958 32240 net.cpp:66] Creating Layer pool1
I0905 19:59:19.493964 32240 net.cpp:329] pool1 <- conv1
I0905 19:59:19.493971 32240 net.cpp:290] pool1 -> pool1
I0905 19:59:19.493983 32240 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:59:19.493988 32240 net.cpp:125] pool1 needs backward computation.
I0905 19:59:19.493994 32240 net.cpp:66] Creating Layer norm1
I0905 19:59:19.494000 32240 net.cpp:329] norm1 <- pool1
I0905 19:59:19.494006 32240 net.cpp:290] norm1 -> norm1
I0905 19:59:19.494016 32240 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 19:59:19.494022 32240 net.cpp:125] norm1 needs backward computation.
I0905 19:59:19.494029 32240 net.cpp:66] Creating Layer conv2
I0905 19:59:19.494035 32240 net.cpp:329] conv2 <- norm1
I0905 19:59:19.494042 32240 net.cpp:290] conv2 -> conv2
I0905 19:59:19.503324 32240 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:59:19.503340 32240 net.cpp:125] conv2 needs backward computation.
I0905 19:59:19.503346 32240 net.cpp:66] Creating Layer relu2
I0905 19:59:19.503352 32240 net.cpp:329] relu2 <- conv2
I0905 19:59:19.503360 32240 net.cpp:280] relu2 -> conv2 (in-place)
I0905 19:59:19.503366 32240 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 19:59:19.503371 32240 net.cpp:125] relu2 needs backward computation.
I0905 19:59:19.503378 32240 net.cpp:66] Creating Layer pool2
I0905 19:59:19.503383 32240 net.cpp:329] pool2 <- conv2
I0905 19:59:19.503391 32240 net.cpp:290] pool2 -> pool2
I0905 19:59:19.503398 32240 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 19:59:19.503403 32240 net.cpp:125] pool2 needs backward computation.
I0905 19:59:19.503413 32240 net.cpp:66] Creating Layer fc7
I0905 19:59:19.503419 32240 net.cpp:329] fc7 <- pool2
I0905 19:59:19.503427 32240 net.cpp:290] fc7 -> fc7
I0905 19:59:20.156229 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.156275 32240 net.cpp:125] fc7 needs backward computation.
I0905 19:59:20.156287 32240 net.cpp:66] Creating Layer relu7
I0905 19:59:20.156294 32240 net.cpp:329] relu7 <- fc7
I0905 19:59:20.156304 32240 net.cpp:280] relu7 -> fc7 (in-place)
I0905 19:59:20.156314 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.156321 32240 net.cpp:125] relu7 needs backward computation.
I0905 19:59:20.156328 32240 net.cpp:66] Creating Layer drop7
I0905 19:59:20.156333 32240 net.cpp:329] drop7 <- fc7
I0905 19:59:20.156340 32240 net.cpp:280] drop7 -> fc7 (in-place)
I0905 19:59:20.156352 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.156358 32240 net.cpp:125] drop7 needs backward computation.
I0905 19:59:20.156378 32240 net.cpp:66] Creating Layer fc8
I0905 19:59:20.156383 32240 net.cpp:329] fc8 <- fc7
I0905 19:59:20.156393 32240 net.cpp:290] fc8 -> fc8
I0905 19:59:20.164397 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.164409 32240 net.cpp:125] fc8 needs backward computation.
I0905 19:59:20.164417 32240 net.cpp:66] Creating Layer relu8
I0905 19:59:20.164422 32240 net.cpp:329] relu8 <- fc8
I0905 19:59:20.164432 32240 net.cpp:280] relu8 -> fc8 (in-place)
I0905 19:59:20.164438 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.164444 32240 net.cpp:125] relu8 needs backward computation.
I0905 19:59:20.164451 32240 net.cpp:66] Creating Layer drop8
I0905 19:59:20.164456 32240 net.cpp:329] drop8 <- fc8
I0905 19:59:20.164463 32240 net.cpp:280] drop8 -> fc8 (in-place)
I0905 19:59:20.164470 32240 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 19:59:20.164476 32240 net.cpp:125] drop8 needs backward computation.
I0905 19:59:20.164484 32240 net.cpp:66] Creating Layer fc9
I0905 19:59:20.164490 32240 net.cpp:329] fc9 <- fc8
I0905 19:59:20.164499 32240 net.cpp:290] fc9 -> fc9
I0905 19:59:20.164883 32240 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 19:59:20.164897 32240 net.cpp:125] fc9 needs backward computation.
I0905 19:59:20.164906 32240 net.cpp:66] Creating Layer fc10
I0905 19:59:20.164911 32240 net.cpp:329] fc10 <- fc9
I0905 19:59:20.164921 32240 net.cpp:290] fc10 -> fc10
I0905 19:59:20.164932 32240 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:59:20.164940 32240 net.cpp:125] fc10 needs backward computation.
I0905 19:59:20.164947 32240 net.cpp:66] Creating Layer prob
I0905 19:59:20.164953 32240 net.cpp:329] prob <- fc10
I0905 19:59:20.164960 32240 net.cpp:290] prob -> prob
I0905 19:59:20.164970 32240 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 19:59:20.164976 32240 net.cpp:125] prob needs backward computation.
I0905 19:59:20.164981 32240 net.cpp:156] This network produces output prob
I0905 19:59:20.164994 32240 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 19:59:20.165004 32240 net.cpp:167] Network initialization done.
I0905 19:59:20.165009 32240 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:03:29.318964 32248 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:03:29.319167 32248 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:03:29.319177 32248 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:03:29.319351 32248 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:03:29.319419 32248 net.cpp:292] Input 0 -> data
I0905 20:03:29.319463 32248 net.cpp:66] Creating Layer conv1
I0905 20:03:29.319471 32248 net.cpp:329] conv1 <- data
I0905 20:03:29.319479 32248 net.cpp:290] conv1 -> conv1
I0905 20:03:29.351677 32248 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:03:29.351702 32248 net.cpp:125] conv1 needs backward computation.
I0905 20:03:29.351719 32248 net.cpp:66] Creating Layer relu1
I0905 20:03:29.351726 32248 net.cpp:329] relu1 <- conv1
I0905 20:03:29.351732 32248 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:03:29.351742 32248 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:03:29.351747 32248 net.cpp:125] relu1 needs backward computation.
I0905 20:03:29.351754 32248 net.cpp:66] Creating Layer pool1
I0905 20:03:29.351759 32248 net.cpp:329] pool1 <- conv1
I0905 20:03:29.351766 32248 net.cpp:290] pool1 -> pool1
I0905 20:03:29.351778 32248 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:03:29.351783 32248 net.cpp:125] pool1 needs backward computation.
I0905 20:03:29.351790 32248 net.cpp:66] Creating Layer norm1
I0905 20:03:29.351796 32248 net.cpp:329] norm1 <- pool1
I0905 20:03:29.351802 32248 net.cpp:290] norm1 -> norm1
I0905 20:03:29.351812 32248 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:03:29.351819 32248 net.cpp:125] norm1 needs backward computation.
I0905 20:03:29.351825 32248 net.cpp:66] Creating Layer conv2
I0905 20:03:29.351831 32248 net.cpp:329] conv2 <- norm1
I0905 20:03:29.351838 32248 net.cpp:290] conv2 -> conv2
I0905 20:03:29.361038 32248 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:03:29.361054 32248 net.cpp:125] conv2 needs backward computation.
I0905 20:03:29.361063 32248 net.cpp:66] Creating Layer relu2
I0905 20:03:29.361068 32248 net.cpp:329] relu2 <- conv2
I0905 20:03:29.361075 32248 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:03:29.361083 32248 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:03:29.361088 32248 net.cpp:125] relu2 needs backward computation.
I0905 20:03:29.361098 32248 net.cpp:66] Creating Layer pool2
I0905 20:03:29.361104 32248 net.cpp:329] pool2 <- conv2
I0905 20:03:29.361110 32248 net.cpp:290] pool2 -> pool2
I0905 20:03:29.361119 32248 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:03:29.361129 32248 net.cpp:125] pool2 needs backward computation.
I0905 20:03:29.361137 32248 net.cpp:66] Creating Layer fc7
I0905 20:03:29.361143 32248 net.cpp:329] fc7 <- pool2
I0905 20:03:29.361151 32248 net.cpp:290] fc7 -> fc7
I0905 20:03:30.008189 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.008234 32248 net.cpp:125] fc7 needs backward computation.
I0905 20:03:30.008247 32248 net.cpp:66] Creating Layer relu7
I0905 20:03:30.008255 32248 net.cpp:329] relu7 <- fc7
I0905 20:03:30.008265 32248 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:03:30.008275 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.008280 32248 net.cpp:125] relu7 needs backward computation.
I0905 20:03:30.008287 32248 net.cpp:66] Creating Layer drop7
I0905 20:03:30.008292 32248 net.cpp:329] drop7 <- fc7
I0905 20:03:30.008299 32248 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:03:30.008309 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.008316 32248 net.cpp:125] drop7 needs backward computation.
I0905 20:03:30.008324 32248 net.cpp:66] Creating Layer fc8
I0905 20:03:30.008329 32248 net.cpp:329] fc8 <- fc7
I0905 20:03:30.008338 32248 net.cpp:290] fc8 -> fc8
I0905 20:03:30.016111 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.016124 32248 net.cpp:125] fc8 needs backward computation.
I0905 20:03:30.016131 32248 net.cpp:66] Creating Layer relu8
I0905 20:03:30.016136 32248 net.cpp:329] relu8 <- fc8
I0905 20:03:30.016144 32248 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:03:30.016151 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.016157 32248 net.cpp:125] relu8 needs backward computation.
I0905 20:03:30.016163 32248 net.cpp:66] Creating Layer drop8
I0905 20:03:30.016170 32248 net.cpp:329] drop8 <- fc8
I0905 20:03:30.016175 32248 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:03:30.016182 32248 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:30.016187 32248 net.cpp:125] drop8 needs backward computation.
I0905 20:03:30.016196 32248 net.cpp:66] Creating Layer fc9
I0905 20:03:30.016201 32248 net.cpp:329] fc9 <- fc8
I0905 20:03:30.016208 32248 net.cpp:290] fc9 -> fc9
I0905 20:03:30.016583 32248 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:03:30.016594 32248 net.cpp:125] fc9 needs backward computation.
I0905 20:03:30.016603 32248 net.cpp:66] Creating Layer fc10
I0905 20:03:30.016608 32248 net.cpp:329] fc10 <- fc9
I0905 20:03:30.016618 32248 net.cpp:290] fc10 -> fc10
I0905 20:03:30.016628 32248 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:03:30.016636 32248 net.cpp:125] fc10 needs backward computation.
I0905 20:03:30.016644 32248 net.cpp:66] Creating Layer prob
I0905 20:03:30.016649 32248 net.cpp:329] prob <- fc10
I0905 20:03:30.016656 32248 net.cpp:290] prob -> prob
I0905 20:03:30.016666 32248 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:03:30.016672 32248 net.cpp:125] prob needs backward computation.
I0905 20:03:30.016677 32248 net.cpp:156] This network produces output prob
I0905 20:03:30.016690 32248 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:03:30.016698 32248 net.cpp:167] Network initialization done.
I0905 20:03:30.016703 32248 net.cpp:168] Memory required for data: 6183480
Classifying 28 inputs.
Done in 17.50 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:03:50.512115 32253 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:03:50.512253 32253 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:03:50.512262 32253 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:03:50.512410 32253 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:03:50.512473 32253 net.cpp:292] Input 0 -> data
I0905 20:03:50.512500 32253 net.cpp:66] Creating Layer conv1
I0905 20:03:50.512506 32253 net.cpp:329] conv1 <- data
I0905 20:03:50.512514 32253 net.cpp:290] conv1 -> conv1
I0905 20:03:50.513890 32253 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:03:50.513908 32253 net.cpp:125] conv1 needs backward computation.
I0905 20:03:50.513917 32253 net.cpp:66] Creating Layer relu1
I0905 20:03:50.513923 32253 net.cpp:329] relu1 <- conv1
I0905 20:03:50.513931 32253 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:03:50.513939 32253 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:03:50.513945 32253 net.cpp:125] relu1 needs backward computation.
I0905 20:03:50.513952 32253 net.cpp:66] Creating Layer pool1
I0905 20:03:50.513957 32253 net.cpp:329] pool1 <- conv1
I0905 20:03:50.513964 32253 net.cpp:290] pool1 -> pool1
I0905 20:03:50.513975 32253 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:03:50.513981 32253 net.cpp:125] pool1 needs backward computation.
I0905 20:03:50.513988 32253 net.cpp:66] Creating Layer norm1
I0905 20:03:50.513993 32253 net.cpp:329] norm1 <- pool1
I0905 20:03:50.514000 32253 net.cpp:290] norm1 -> norm1
I0905 20:03:50.514015 32253 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:03:50.514021 32253 net.cpp:125] norm1 needs backward computation.
I0905 20:03:50.514029 32253 net.cpp:66] Creating Layer conv2
I0905 20:03:50.514034 32253 net.cpp:329] conv2 <- norm1
I0905 20:03:50.514041 32253 net.cpp:290] conv2 -> conv2
I0905 20:03:50.523164 32253 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:03:50.523180 32253 net.cpp:125] conv2 needs backward computation.
I0905 20:03:50.523186 32253 net.cpp:66] Creating Layer relu2
I0905 20:03:50.523192 32253 net.cpp:329] relu2 <- conv2
I0905 20:03:50.523200 32253 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:03:50.523206 32253 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:03:50.523211 32253 net.cpp:125] relu2 needs backward computation.
I0905 20:03:50.523218 32253 net.cpp:66] Creating Layer pool2
I0905 20:03:50.523223 32253 net.cpp:329] pool2 <- conv2
I0905 20:03:50.523229 32253 net.cpp:290] pool2 -> pool2
I0905 20:03:50.523237 32253 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:03:50.523243 32253 net.cpp:125] pool2 needs backward computation.
I0905 20:03:50.523252 32253 net.cpp:66] Creating Layer fc7
I0905 20:03:50.523258 32253 net.cpp:329] fc7 <- pool2
I0905 20:03:50.523265 32253 net.cpp:290] fc7 -> fc7
I0905 20:03:51.172067 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.172113 32253 net.cpp:125] fc7 needs backward computation.
I0905 20:03:51.172126 32253 net.cpp:66] Creating Layer relu7
I0905 20:03:51.172132 32253 net.cpp:329] relu7 <- fc7
I0905 20:03:51.172142 32253 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:03:51.172152 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.172158 32253 net.cpp:125] relu7 needs backward computation.
I0905 20:03:51.172165 32253 net.cpp:66] Creating Layer drop7
I0905 20:03:51.172170 32253 net.cpp:329] drop7 <- fc7
I0905 20:03:51.172176 32253 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:03:51.172188 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.172194 32253 net.cpp:125] drop7 needs backward computation.
I0905 20:03:51.172201 32253 net.cpp:66] Creating Layer fc8
I0905 20:03:51.172207 32253 net.cpp:329] fc8 <- fc7
I0905 20:03:51.172216 32253 net.cpp:290] fc8 -> fc8
I0905 20:03:51.179998 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.180011 32253 net.cpp:125] fc8 needs backward computation.
I0905 20:03:51.180018 32253 net.cpp:66] Creating Layer relu8
I0905 20:03:51.180024 32253 net.cpp:329] relu8 <- fc8
I0905 20:03:51.180032 32253 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:03:51.180039 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.180045 32253 net.cpp:125] relu8 needs backward computation.
I0905 20:03:51.180052 32253 net.cpp:66] Creating Layer drop8
I0905 20:03:51.180058 32253 net.cpp:329] drop8 <- fc8
I0905 20:03:51.180063 32253 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:03:51.180070 32253 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:03:51.180076 32253 net.cpp:125] drop8 needs backward computation.
I0905 20:03:51.180084 32253 net.cpp:66] Creating Layer fc9
I0905 20:03:51.180090 32253 net.cpp:329] fc9 <- fc8
I0905 20:03:51.180097 32253 net.cpp:290] fc9 -> fc9
I0905 20:03:51.180469 32253 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:03:51.180481 32253 net.cpp:125] fc9 needs backward computation.
I0905 20:03:51.180490 32253 net.cpp:66] Creating Layer fc10
I0905 20:03:51.180495 32253 net.cpp:329] fc10 <- fc9
I0905 20:03:51.180503 32253 net.cpp:290] fc10 -> fc10
I0905 20:03:51.180516 32253 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:03:51.180523 32253 net.cpp:125] fc10 needs backward computation.
I0905 20:03:51.180531 32253 net.cpp:66] Creating Layer prob
I0905 20:03:51.180536 32253 net.cpp:329] prob <- fc10
I0905 20:03:51.180543 32253 net.cpp:290] prob -> prob
I0905 20:03:51.180553 32253 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:03:51.180559 32253 net.cpp:125] prob needs backward computation.
I0905 20:03:51.180564 32253 net.cpp:156] This network produces output prob
I0905 20:03:51.180577 32253 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:03:51.180594 32253 net.cpp:167] Network initialization done.
I0905 20:03:51.180600 32253 net.cpp:168] Memory required for data: 6183480
Classifying 200 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:04:01.169884 32257 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:04:01.170035 32257 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:04:01.170045 32257 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:04:01.170199 32257 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:04:01.170263 32257 net.cpp:292] Input 0 -> data
I0905 20:04:01.170289 32257 net.cpp:66] Creating Layer conv1
I0905 20:04:01.170295 32257 net.cpp:329] conv1 <- data
I0905 20:04:01.170303 32257 net.cpp:290] conv1 -> conv1
I0905 20:04:01.171663 32257 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:04:01.171681 32257 net.cpp:125] conv1 needs backward computation.
I0905 20:04:01.171690 32257 net.cpp:66] Creating Layer relu1
I0905 20:04:01.171696 32257 net.cpp:329] relu1 <- conv1
I0905 20:04:01.171704 32257 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:04:01.171712 32257 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:04:01.171718 32257 net.cpp:125] relu1 needs backward computation.
I0905 20:04:01.171725 32257 net.cpp:66] Creating Layer pool1
I0905 20:04:01.171730 32257 net.cpp:329] pool1 <- conv1
I0905 20:04:01.171737 32257 net.cpp:290] pool1 -> pool1
I0905 20:04:01.171748 32257 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:04:01.171754 32257 net.cpp:125] pool1 needs backward computation.
I0905 20:04:01.171761 32257 net.cpp:66] Creating Layer norm1
I0905 20:04:01.171767 32257 net.cpp:329] norm1 <- pool1
I0905 20:04:01.171774 32257 net.cpp:290] norm1 -> norm1
I0905 20:04:01.171784 32257 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:04:01.171790 32257 net.cpp:125] norm1 needs backward computation.
I0905 20:04:01.171797 32257 net.cpp:66] Creating Layer conv2
I0905 20:04:01.171803 32257 net.cpp:329] conv2 <- norm1
I0905 20:04:01.171810 32257 net.cpp:290] conv2 -> conv2
I0905 20:04:01.180938 32257 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:04:01.180953 32257 net.cpp:125] conv2 needs backward computation.
I0905 20:04:01.180961 32257 net.cpp:66] Creating Layer relu2
I0905 20:04:01.180966 32257 net.cpp:329] relu2 <- conv2
I0905 20:04:01.180974 32257 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:04:01.180980 32257 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:04:01.180986 32257 net.cpp:125] relu2 needs backward computation.
I0905 20:04:01.180994 32257 net.cpp:66] Creating Layer pool2
I0905 20:04:01.180999 32257 net.cpp:329] pool2 <- conv2
I0905 20:04:01.181005 32257 net.cpp:290] pool2 -> pool2
I0905 20:04:01.181013 32257 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:04:01.181020 32257 net.cpp:125] pool2 needs backward computation.
I0905 20:04:01.181027 32257 net.cpp:66] Creating Layer fc7
I0905 20:04:01.181033 32257 net.cpp:329] fc7 <- pool2
I0905 20:04:01.181041 32257 net.cpp:290] fc7 -> fc7
I0905 20:04:01.831064 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.831110 32257 net.cpp:125] fc7 needs backward computation.
I0905 20:04:01.831123 32257 net.cpp:66] Creating Layer relu7
I0905 20:04:01.831130 32257 net.cpp:329] relu7 <- fc7
I0905 20:04:01.831140 32257 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:04:01.831151 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.831156 32257 net.cpp:125] relu7 needs backward computation.
I0905 20:04:01.831163 32257 net.cpp:66] Creating Layer drop7
I0905 20:04:01.831169 32257 net.cpp:329] drop7 <- fc7
I0905 20:04:01.831176 32257 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:04:01.831187 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.831193 32257 net.cpp:125] drop7 needs backward computation.
I0905 20:04:01.831202 32257 net.cpp:66] Creating Layer fc8
I0905 20:04:01.831207 32257 net.cpp:329] fc8 <- fc7
I0905 20:04:01.831217 32257 net.cpp:290] fc8 -> fc8
I0905 20:04:01.839223 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.839236 32257 net.cpp:125] fc8 needs backward computation.
I0905 20:04:01.839243 32257 net.cpp:66] Creating Layer relu8
I0905 20:04:01.839249 32257 net.cpp:329] relu8 <- fc8
I0905 20:04:01.839257 32257 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:04:01.839264 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.839282 32257 net.cpp:125] relu8 needs backward computation.
I0905 20:04:01.839288 32257 net.cpp:66] Creating Layer drop8
I0905 20:04:01.839294 32257 net.cpp:329] drop8 <- fc8
I0905 20:04:01.839300 32257 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:04:01.839308 32257 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:01.839313 32257 net.cpp:125] drop8 needs backward computation.
I0905 20:04:01.839323 32257 net.cpp:66] Creating Layer fc9
I0905 20:04:01.839329 32257 net.cpp:329] fc9 <- fc8
I0905 20:04:01.839335 32257 net.cpp:290] fc9 -> fc9
I0905 20:04:01.839720 32257 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:04:01.839731 32257 net.cpp:125] fc9 needs backward computation.
I0905 20:04:01.839740 32257 net.cpp:66] Creating Layer fc10
I0905 20:04:01.839746 32257 net.cpp:329] fc10 <- fc9
I0905 20:04:01.839754 32257 net.cpp:290] fc10 -> fc10
I0905 20:04:01.839767 32257 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:04:01.839776 32257 net.cpp:125] fc10 needs backward computation.
I0905 20:04:01.839782 32257 net.cpp:66] Creating Layer prob
I0905 20:04:01.839787 32257 net.cpp:329] prob <- fc10
I0905 20:04:01.839795 32257 net.cpp:290] prob -> prob
I0905 20:04:01.839805 32257 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:04:01.839812 32257 net.cpp:125] prob needs backward computation.
I0905 20:04:01.839817 32257 net.cpp:156] This network produces output prob
I0905 20:04:01.839829 32257 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:04:01.839838 32257 net.cpp:167] Network initialization done.
I0905 20:04:01.839843 32257 net.cpp:168] Memory required for data: 6183480
Classifying 33 inputs.
Done in 22.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:04:27.075306 32262 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:04:27.075444 32262 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:04:27.075453 32262 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:04:27.075599 32262 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:04:27.075664 32262 net.cpp:292] Input 0 -> data
I0905 20:04:27.075688 32262 net.cpp:66] Creating Layer conv1
I0905 20:04:27.075695 32262 net.cpp:329] conv1 <- data
I0905 20:04:27.075703 32262 net.cpp:290] conv1 -> conv1
I0905 20:04:27.077080 32262 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:04:27.077098 32262 net.cpp:125] conv1 needs backward computation.
I0905 20:04:27.077107 32262 net.cpp:66] Creating Layer relu1
I0905 20:04:27.077113 32262 net.cpp:329] relu1 <- conv1
I0905 20:04:27.077119 32262 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:04:27.077128 32262 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:04:27.077134 32262 net.cpp:125] relu1 needs backward computation.
I0905 20:04:27.077142 32262 net.cpp:66] Creating Layer pool1
I0905 20:04:27.077147 32262 net.cpp:329] pool1 <- conv1
I0905 20:04:27.077153 32262 net.cpp:290] pool1 -> pool1
I0905 20:04:27.077164 32262 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:04:27.077169 32262 net.cpp:125] pool1 needs backward computation.
I0905 20:04:27.077177 32262 net.cpp:66] Creating Layer norm1
I0905 20:04:27.077183 32262 net.cpp:329] norm1 <- pool1
I0905 20:04:27.077188 32262 net.cpp:290] norm1 -> norm1
I0905 20:04:27.077198 32262 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:04:27.077204 32262 net.cpp:125] norm1 needs backward computation.
I0905 20:04:27.077211 32262 net.cpp:66] Creating Layer conv2
I0905 20:04:27.077217 32262 net.cpp:329] conv2 <- norm1
I0905 20:04:27.077224 32262 net.cpp:290] conv2 -> conv2
I0905 20:04:27.086359 32262 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:04:27.086374 32262 net.cpp:125] conv2 needs backward computation.
I0905 20:04:27.086380 32262 net.cpp:66] Creating Layer relu2
I0905 20:04:27.086386 32262 net.cpp:329] relu2 <- conv2
I0905 20:04:27.086392 32262 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:04:27.086400 32262 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:04:27.086405 32262 net.cpp:125] relu2 needs backward computation.
I0905 20:04:27.086413 32262 net.cpp:66] Creating Layer pool2
I0905 20:04:27.086419 32262 net.cpp:329] pool2 <- conv2
I0905 20:04:27.086426 32262 net.cpp:290] pool2 -> pool2
I0905 20:04:27.086434 32262 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:04:27.086441 32262 net.cpp:125] pool2 needs backward computation.
I0905 20:04:27.086447 32262 net.cpp:66] Creating Layer fc7
I0905 20:04:27.086452 32262 net.cpp:329] fc7 <- pool2
I0905 20:04:27.086459 32262 net.cpp:290] fc7 -> fc7
I0905 20:04:27.735885 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.735931 32262 net.cpp:125] fc7 needs backward computation.
I0905 20:04:27.735944 32262 net.cpp:66] Creating Layer relu7
I0905 20:04:27.735950 32262 net.cpp:329] relu7 <- fc7
I0905 20:04:27.735960 32262 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:04:27.735981 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.735987 32262 net.cpp:125] relu7 needs backward computation.
I0905 20:04:27.735995 32262 net.cpp:66] Creating Layer drop7
I0905 20:04:27.736001 32262 net.cpp:329] drop7 <- fc7
I0905 20:04:27.736006 32262 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:04:27.736017 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.736023 32262 net.cpp:125] drop7 needs backward computation.
I0905 20:04:27.736032 32262 net.cpp:66] Creating Layer fc8
I0905 20:04:27.736037 32262 net.cpp:329] fc8 <- fc7
I0905 20:04:27.736045 32262 net.cpp:290] fc8 -> fc8
I0905 20:04:27.743852 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.743865 32262 net.cpp:125] fc8 needs backward computation.
I0905 20:04:27.743872 32262 net.cpp:66] Creating Layer relu8
I0905 20:04:27.743877 32262 net.cpp:329] relu8 <- fc8
I0905 20:04:27.743885 32262 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:04:27.743893 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.743898 32262 net.cpp:125] relu8 needs backward computation.
I0905 20:04:27.743904 32262 net.cpp:66] Creating Layer drop8
I0905 20:04:27.743911 32262 net.cpp:329] drop8 <- fc8
I0905 20:04:27.743916 32262 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:04:27.743923 32262 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:04:27.743929 32262 net.cpp:125] drop8 needs backward computation.
I0905 20:04:27.743937 32262 net.cpp:66] Creating Layer fc9
I0905 20:04:27.743943 32262 net.cpp:329] fc9 <- fc8
I0905 20:04:27.743950 32262 net.cpp:290] fc9 -> fc9
I0905 20:04:27.744324 32262 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:04:27.744336 32262 net.cpp:125] fc9 needs backward computation.
I0905 20:04:27.744344 32262 net.cpp:66] Creating Layer fc10
I0905 20:04:27.744350 32262 net.cpp:329] fc10 <- fc9
I0905 20:04:27.744359 32262 net.cpp:290] fc10 -> fc10
I0905 20:04:27.744370 32262 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:04:27.744377 32262 net.cpp:125] fc10 needs backward computation.
I0905 20:04:27.744385 32262 net.cpp:66] Creating Layer prob
I0905 20:04:27.744390 32262 net.cpp:329] prob <- fc10
I0905 20:04:27.744398 32262 net.cpp:290] prob -> prob
I0905 20:04:27.744407 32262 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:04:27.744413 32262 net.cpp:125] prob needs backward computation.
I0905 20:04:27.744418 32262 net.cpp:156] This network produces output prob
I0905 20:04:27.744431 32262 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:04:27.744439 32262 net.cpp:167] Network initialization done.
I0905 20:04:27.744444 32262 net.cpp:168] Memory required for data: 6183480
Classifying 143 inputs.
Done in 93.22 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 43 is out of bounds for axis 0 with size 43
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:06:04.800662 32268 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:06:04.800803 32268 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:06:04.800812 32268 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:06:04.800963 32268 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:06:04.801026 32268 net.cpp:292] Input 0 -> data
I0905 20:06:04.801053 32268 net.cpp:66] Creating Layer conv1
I0905 20:06:04.801060 32268 net.cpp:329] conv1 <- data
I0905 20:06:04.801069 32268 net.cpp:290] conv1 -> conv1
I0905 20:06:04.802481 32268 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:04.802501 32268 net.cpp:125] conv1 needs backward computation.
I0905 20:06:04.802510 32268 net.cpp:66] Creating Layer relu1
I0905 20:06:04.802516 32268 net.cpp:329] relu1 <- conv1
I0905 20:06:04.802523 32268 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:06:04.802532 32268 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:04.802538 32268 net.cpp:125] relu1 needs backward computation.
I0905 20:06:04.802546 32268 net.cpp:66] Creating Layer pool1
I0905 20:06:04.802551 32268 net.cpp:329] pool1 <- conv1
I0905 20:06:04.802558 32268 net.cpp:290] pool1 -> pool1
I0905 20:06:04.802569 32268 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:04.802577 32268 net.cpp:125] pool1 needs backward computation.
I0905 20:06:04.802583 32268 net.cpp:66] Creating Layer norm1
I0905 20:06:04.802588 32268 net.cpp:329] norm1 <- pool1
I0905 20:06:04.802595 32268 net.cpp:290] norm1 -> norm1
I0905 20:06:04.802605 32268 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:04.802611 32268 net.cpp:125] norm1 needs backward computation.
I0905 20:06:04.802619 32268 net.cpp:66] Creating Layer conv2
I0905 20:06:04.802625 32268 net.cpp:329] conv2 <- norm1
I0905 20:06:04.802639 32268 net.cpp:290] conv2 -> conv2
I0905 20:06:04.812031 32268 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:04.812047 32268 net.cpp:125] conv2 needs backward computation.
I0905 20:06:04.812054 32268 net.cpp:66] Creating Layer relu2
I0905 20:06:04.812062 32268 net.cpp:329] relu2 <- conv2
I0905 20:06:04.812068 32268 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:06:04.812075 32268 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:04.812082 32268 net.cpp:125] relu2 needs backward computation.
I0905 20:06:04.812088 32268 net.cpp:66] Creating Layer pool2
I0905 20:06:04.812093 32268 net.cpp:329] pool2 <- conv2
I0905 20:06:04.812100 32268 net.cpp:290] pool2 -> pool2
I0905 20:06:04.812108 32268 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:06:04.812114 32268 net.cpp:125] pool2 needs backward computation.
I0905 20:06:04.812124 32268 net.cpp:66] Creating Layer fc7
I0905 20:06:04.812130 32268 net.cpp:329] fc7 <- pool2
I0905 20:06:04.812137 32268 net.cpp:290] fc7 -> fc7
I0905 20:06:05.460922 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.460968 32268 net.cpp:125] fc7 needs backward computation.
I0905 20:06:05.460981 32268 net.cpp:66] Creating Layer relu7
I0905 20:06:05.460988 32268 net.cpp:329] relu7 <- fc7
I0905 20:06:05.460997 32268 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:06:05.461007 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.461014 32268 net.cpp:125] relu7 needs backward computation.
I0905 20:06:05.461021 32268 net.cpp:66] Creating Layer drop7
I0905 20:06:05.461026 32268 net.cpp:329] drop7 <- fc7
I0905 20:06:05.461032 32268 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:06:05.461043 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.461050 32268 net.cpp:125] drop7 needs backward computation.
I0905 20:06:05.461058 32268 net.cpp:66] Creating Layer fc8
I0905 20:06:05.461063 32268 net.cpp:329] fc8 <- fc7
I0905 20:06:05.461074 32268 net.cpp:290] fc8 -> fc8
I0905 20:06:05.468951 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.468966 32268 net.cpp:125] fc8 needs backward computation.
I0905 20:06:05.468972 32268 net.cpp:66] Creating Layer relu8
I0905 20:06:05.468978 32268 net.cpp:329] relu8 <- fc8
I0905 20:06:05.468986 32268 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:06:05.468994 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.468999 32268 net.cpp:125] relu8 needs backward computation.
I0905 20:06:05.469007 32268 net.cpp:66] Creating Layer drop8
I0905 20:06:05.469012 32268 net.cpp:329] drop8 <- fc8
I0905 20:06:05.469018 32268 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:06:05.469025 32268 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:05.469032 32268 net.cpp:125] drop8 needs backward computation.
I0905 20:06:05.469040 32268 net.cpp:66] Creating Layer fc9
I0905 20:06:05.469046 32268 net.cpp:329] fc9 <- fc8
I0905 20:06:05.469053 32268 net.cpp:290] fc9 -> fc9
I0905 20:06:05.469439 32268 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:06:05.469450 32268 net.cpp:125] fc9 needs backward computation.
I0905 20:06:05.469458 32268 net.cpp:66] Creating Layer fc10
I0905 20:06:05.469465 32268 net.cpp:329] fc10 <- fc9
I0905 20:06:05.469473 32268 net.cpp:290] fc10 -> fc10
I0905 20:06:05.469485 32268 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:05.469494 32268 net.cpp:125] fc10 needs backward computation.
I0905 20:06:05.469501 32268 net.cpp:66] Creating Layer prob
I0905 20:06:05.469506 32268 net.cpp:329] prob <- fc10
I0905 20:06:05.469514 32268 net.cpp:290] prob -> prob
I0905 20:06:05.469524 32268 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:05.469532 32268 net.cpp:125] prob needs backward computation.
I0905 20:06:05.469537 32268 net.cpp:156] This network produces output prob
I0905 20:06:05.469549 32268 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:06:05.469558 32268 net.cpp:167] Network initialization done.
I0905 20:06:05.469563 32268 net.cpp:168] Memory required for data: 6183480
Classifying 15 inputs.
Done in 9.28 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:06:16.480658 32273 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:06:16.480809 32273 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:06:16.480819 32273 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:06:16.480968 32273 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:06:16.481021 32273 net.cpp:292] Input 0 -> data
I0905 20:06:16.481047 32273 net.cpp:66] Creating Layer conv1
I0905 20:06:16.481055 32273 net.cpp:329] conv1 <- data
I0905 20:06:16.481062 32273 net.cpp:290] conv1 -> conv1
I0905 20:06:16.482481 32273 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:16.482501 32273 net.cpp:125] conv1 needs backward computation.
I0905 20:06:16.482511 32273 net.cpp:66] Creating Layer relu1
I0905 20:06:16.482522 32273 net.cpp:329] relu1 <- conv1
I0905 20:06:16.482528 32273 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:06:16.482537 32273 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:16.482543 32273 net.cpp:125] relu1 needs backward computation.
I0905 20:06:16.482550 32273 net.cpp:66] Creating Layer pool1
I0905 20:06:16.482556 32273 net.cpp:329] pool1 <- conv1
I0905 20:06:16.482563 32273 net.cpp:290] pool1 -> pool1
I0905 20:06:16.482574 32273 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:16.482580 32273 net.cpp:125] pool1 needs backward computation.
I0905 20:06:16.482588 32273 net.cpp:66] Creating Layer norm1
I0905 20:06:16.482594 32273 net.cpp:329] norm1 <- pool1
I0905 20:06:16.482600 32273 net.cpp:290] norm1 -> norm1
I0905 20:06:16.482610 32273 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:16.482616 32273 net.cpp:125] norm1 needs backward computation.
I0905 20:06:16.482624 32273 net.cpp:66] Creating Layer conv2
I0905 20:06:16.482630 32273 net.cpp:329] conv2 <- norm1
I0905 20:06:16.482637 32273 net.cpp:290] conv2 -> conv2
I0905 20:06:16.491747 32273 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:16.491762 32273 net.cpp:125] conv2 needs backward computation.
I0905 20:06:16.491770 32273 net.cpp:66] Creating Layer relu2
I0905 20:06:16.491775 32273 net.cpp:329] relu2 <- conv2
I0905 20:06:16.491781 32273 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:06:16.491788 32273 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:16.491794 32273 net.cpp:125] relu2 needs backward computation.
I0905 20:06:16.491801 32273 net.cpp:66] Creating Layer pool2
I0905 20:06:16.491806 32273 net.cpp:329] pool2 <- conv2
I0905 20:06:16.491812 32273 net.cpp:290] pool2 -> pool2
I0905 20:06:16.491821 32273 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:06:16.491825 32273 net.cpp:125] pool2 needs backward computation.
I0905 20:06:16.491834 32273 net.cpp:66] Creating Layer fc7
I0905 20:06:16.491840 32273 net.cpp:329] fc7 <- pool2
I0905 20:06:16.491848 32273 net.cpp:290] fc7 -> fc7
I0905 20:06:17.138675 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.138720 32273 net.cpp:125] fc7 needs backward computation.
I0905 20:06:17.138732 32273 net.cpp:66] Creating Layer relu7
I0905 20:06:17.138739 32273 net.cpp:329] relu7 <- fc7
I0905 20:06:17.138748 32273 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:06:17.138757 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.138763 32273 net.cpp:125] relu7 needs backward computation.
I0905 20:06:17.138770 32273 net.cpp:66] Creating Layer drop7
I0905 20:06:17.138777 32273 net.cpp:329] drop7 <- fc7
I0905 20:06:17.138782 32273 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:06:17.138793 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.138799 32273 net.cpp:125] drop7 needs backward computation.
I0905 20:06:17.138808 32273 net.cpp:66] Creating Layer fc8
I0905 20:06:17.138813 32273 net.cpp:329] fc8 <- fc7
I0905 20:06:17.138823 32273 net.cpp:290] fc8 -> fc8
I0905 20:06:17.146610 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.146621 32273 net.cpp:125] fc8 needs backward computation.
I0905 20:06:17.146628 32273 net.cpp:66] Creating Layer relu8
I0905 20:06:17.146634 32273 net.cpp:329] relu8 <- fc8
I0905 20:06:17.146641 32273 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:06:17.146649 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.146654 32273 net.cpp:125] relu8 needs backward computation.
I0905 20:06:17.146661 32273 net.cpp:66] Creating Layer drop8
I0905 20:06:17.146667 32273 net.cpp:329] drop8 <- fc8
I0905 20:06:17.146672 32273 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:06:17.146679 32273 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:17.146684 32273 net.cpp:125] drop8 needs backward computation.
I0905 20:06:17.146693 32273 net.cpp:66] Creating Layer fc9
I0905 20:06:17.146699 32273 net.cpp:329] fc9 <- fc8
I0905 20:06:17.146706 32273 net.cpp:290] fc9 -> fc9
I0905 20:06:17.147079 32273 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:06:17.147090 32273 net.cpp:125] fc9 needs backward computation.
I0905 20:06:17.147109 32273 net.cpp:66] Creating Layer fc10
I0905 20:06:17.147114 32273 net.cpp:329] fc10 <- fc9
I0905 20:06:17.147124 32273 net.cpp:290] fc10 -> fc10
I0905 20:06:17.147135 32273 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:17.147143 32273 net.cpp:125] fc10 needs backward computation.
I0905 20:06:17.147150 32273 net.cpp:66] Creating Layer prob
I0905 20:06:17.147156 32273 net.cpp:329] prob <- fc10
I0905 20:06:17.147162 32273 net.cpp:290] prob -> prob
I0905 20:06:17.147172 32273 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:17.147178 32273 net.cpp:125] prob needs backward computation.
I0905 20:06:17.147183 32273 net.cpp:156] This network produces output prob
I0905 20:06:17.147196 32273 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:06:17.147204 32273 net.cpp:167] Network initialization done.
I0905 20:06:17.147209 32273 net.cpp:168] Memory required for data: 6183480
Classifying 36 inputs.
Done in 22.86 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:06:41.611884 32277 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:06:41.612022 32277 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:06:41.612031 32277 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:06:41.612179 32277 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:06:41.612242 32277 net.cpp:292] Input 0 -> data
I0905 20:06:41.612277 32277 net.cpp:66] Creating Layer conv1
I0905 20:06:41.612283 32277 net.cpp:329] conv1 <- data
I0905 20:06:41.612292 32277 net.cpp:290] conv1 -> conv1
I0905 20:06:41.613677 32277 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:41.613698 32277 net.cpp:125] conv1 needs backward computation.
I0905 20:06:41.613706 32277 net.cpp:66] Creating Layer relu1
I0905 20:06:41.613713 32277 net.cpp:329] relu1 <- conv1
I0905 20:06:41.613718 32277 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:06:41.613728 32277 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:41.613734 32277 net.cpp:125] relu1 needs backward computation.
I0905 20:06:41.613740 32277 net.cpp:66] Creating Layer pool1
I0905 20:06:41.613745 32277 net.cpp:329] pool1 <- conv1
I0905 20:06:41.613752 32277 net.cpp:290] pool1 -> pool1
I0905 20:06:41.613764 32277 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:41.613770 32277 net.cpp:125] pool1 needs backward computation.
I0905 20:06:41.613776 32277 net.cpp:66] Creating Layer norm1
I0905 20:06:41.613781 32277 net.cpp:329] norm1 <- pool1
I0905 20:06:41.613788 32277 net.cpp:290] norm1 -> norm1
I0905 20:06:41.613798 32277 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:41.613804 32277 net.cpp:125] norm1 needs backward computation.
I0905 20:06:41.613811 32277 net.cpp:66] Creating Layer conv2
I0905 20:06:41.613816 32277 net.cpp:329] conv2 <- norm1
I0905 20:06:41.613823 32277 net.cpp:290] conv2 -> conv2
I0905 20:06:41.622953 32277 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:41.622968 32277 net.cpp:125] conv2 needs backward computation.
I0905 20:06:41.622975 32277 net.cpp:66] Creating Layer relu2
I0905 20:06:41.622982 32277 net.cpp:329] relu2 <- conv2
I0905 20:06:41.622987 32277 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:06:41.622994 32277 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:41.623000 32277 net.cpp:125] relu2 needs backward computation.
I0905 20:06:41.623008 32277 net.cpp:66] Creating Layer pool2
I0905 20:06:41.623014 32277 net.cpp:329] pool2 <- conv2
I0905 20:06:41.623021 32277 net.cpp:290] pool2 -> pool2
I0905 20:06:41.623029 32277 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:06:41.623034 32277 net.cpp:125] pool2 needs backward computation.
I0905 20:06:41.623041 32277 net.cpp:66] Creating Layer fc7
I0905 20:06:41.623046 32277 net.cpp:329] fc7 <- pool2
I0905 20:06:41.623054 32277 net.cpp:290] fc7 -> fc7
I0905 20:06:42.271731 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.271777 32277 net.cpp:125] fc7 needs backward computation.
I0905 20:06:42.271790 32277 net.cpp:66] Creating Layer relu7
I0905 20:06:42.271796 32277 net.cpp:329] relu7 <- fc7
I0905 20:06:42.271806 32277 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:06:42.271816 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.271822 32277 net.cpp:125] relu7 needs backward computation.
I0905 20:06:42.271829 32277 net.cpp:66] Creating Layer drop7
I0905 20:06:42.271834 32277 net.cpp:329] drop7 <- fc7
I0905 20:06:42.271842 32277 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:06:42.271852 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.271857 32277 net.cpp:125] drop7 needs backward computation.
I0905 20:06:42.271867 32277 net.cpp:66] Creating Layer fc8
I0905 20:06:42.271872 32277 net.cpp:329] fc8 <- fc7
I0905 20:06:42.271880 32277 net.cpp:290] fc8 -> fc8
I0905 20:06:42.279665 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.279687 32277 net.cpp:125] fc8 needs backward computation.
I0905 20:06:42.279695 32277 net.cpp:66] Creating Layer relu8
I0905 20:06:42.279700 32277 net.cpp:329] relu8 <- fc8
I0905 20:06:42.279708 32277 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:06:42.279716 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.279721 32277 net.cpp:125] relu8 needs backward computation.
I0905 20:06:42.279728 32277 net.cpp:66] Creating Layer drop8
I0905 20:06:42.279733 32277 net.cpp:329] drop8 <- fc8
I0905 20:06:42.279739 32277 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:06:42.279747 32277 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:42.279752 32277 net.cpp:125] drop8 needs backward computation.
I0905 20:06:42.279760 32277 net.cpp:66] Creating Layer fc9
I0905 20:06:42.279765 32277 net.cpp:329] fc9 <- fc8
I0905 20:06:42.279772 32277 net.cpp:290] fc9 -> fc9
I0905 20:06:42.280145 32277 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:06:42.280158 32277 net.cpp:125] fc9 needs backward computation.
I0905 20:06:42.280165 32277 net.cpp:66] Creating Layer fc10
I0905 20:06:42.280171 32277 net.cpp:329] fc10 <- fc9
I0905 20:06:42.280179 32277 net.cpp:290] fc10 -> fc10
I0905 20:06:42.280191 32277 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:42.280199 32277 net.cpp:125] fc10 needs backward computation.
I0905 20:06:42.280205 32277 net.cpp:66] Creating Layer prob
I0905 20:06:42.280210 32277 net.cpp:329] prob <- fc10
I0905 20:06:42.280218 32277 net.cpp:290] prob -> prob
I0905 20:06:42.280227 32277 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:42.280233 32277 net.cpp:125] prob needs backward computation.
I0905 20:06:42.280238 32277 net.cpp:156] This network produces output prob
I0905 20:06:42.280251 32277 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:06:42.280258 32277 net.cpp:167] Network initialization done.
I0905 20:06:42.280263 32277 net.cpp:168] Memory required for data: 6183480
Classifying 5 inputs.
Done in 3.30 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:06:46.686028 32281 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:06:46.686167 32281 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:06:46.686175 32281 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:06:46.686323 32281 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:06:46.686388 32281 net.cpp:292] Input 0 -> data
I0905 20:06:46.686414 32281 net.cpp:66] Creating Layer conv1
I0905 20:06:46.686421 32281 net.cpp:329] conv1 <- data
I0905 20:06:46.686429 32281 net.cpp:290] conv1 -> conv1
I0905 20:06:46.687791 32281 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:46.687809 32281 net.cpp:125] conv1 needs backward computation.
I0905 20:06:46.687819 32281 net.cpp:66] Creating Layer relu1
I0905 20:06:46.687824 32281 net.cpp:329] relu1 <- conv1
I0905 20:06:46.687831 32281 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:06:46.687840 32281 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:06:46.687846 32281 net.cpp:125] relu1 needs backward computation.
I0905 20:06:46.687854 32281 net.cpp:66] Creating Layer pool1
I0905 20:06:46.687860 32281 net.cpp:329] pool1 <- conv1
I0905 20:06:46.687865 32281 net.cpp:290] pool1 -> pool1
I0905 20:06:46.687876 32281 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:46.687883 32281 net.cpp:125] pool1 needs backward computation.
I0905 20:06:46.687891 32281 net.cpp:66] Creating Layer norm1
I0905 20:06:46.687896 32281 net.cpp:329] norm1 <- pool1
I0905 20:06:46.687902 32281 net.cpp:290] norm1 -> norm1
I0905 20:06:46.687912 32281 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:06:46.687918 32281 net.cpp:125] norm1 needs backward computation.
I0905 20:06:46.687927 32281 net.cpp:66] Creating Layer conv2
I0905 20:06:46.687932 32281 net.cpp:329] conv2 <- norm1
I0905 20:06:46.687939 32281 net.cpp:290] conv2 -> conv2
I0905 20:06:46.697286 32281 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:46.697304 32281 net.cpp:125] conv2 needs backward computation.
I0905 20:06:46.697311 32281 net.cpp:66] Creating Layer relu2
I0905 20:06:46.697317 32281 net.cpp:329] relu2 <- conv2
I0905 20:06:46.697325 32281 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:06:46.697332 32281 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:06:46.697338 32281 net.cpp:125] relu2 needs backward computation.
I0905 20:06:46.697347 32281 net.cpp:66] Creating Layer pool2
I0905 20:06:46.697355 32281 net.cpp:329] pool2 <- conv2
I0905 20:06:46.697361 32281 net.cpp:290] pool2 -> pool2
I0905 20:06:46.697370 32281 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:06:46.697376 32281 net.cpp:125] pool2 needs backward computation.
I0905 20:06:46.697383 32281 net.cpp:66] Creating Layer fc7
I0905 20:06:46.697389 32281 net.cpp:329] fc7 <- pool2
I0905 20:06:46.697402 32281 net.cpp:290] fc7 -> fc7
I0905 20:06:47.373947 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.373992 32281 net.cpp:125] fc7 needs backward computation.
I0905 20:06:47.374006 32281 net.cpp:66] Creating Layer relu7
I0905 20:06:47.374012 32281 net.cpp:329] relu7 <- fc7
I0905 20:06:47.374022 32281 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:06:47.374032 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.374038 32281 net.cpp:125] relu7 needs backward computation.
I0905 20:06:47.374047 32281 net.cpp:66] Creating Layer drop7
I0905 20:06:47.374053 32281 net.cpp:329] drop7 <- fc7
I0905 20:06:47.374058 32281 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:06:47.374069 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.374076 32281 net.cpp:125] drop7 needs backward computation.
I0905 20:06:47.374084 32281 net.cpp:66] Creating Layer fc8
I0905 20:06:47.374090 32281 net.cpp:329] fc8 <- fc7
I0905 20:06:47.374099 32281 net.cpp:290] fc8 -> fc8
I0905 20:06:47.382239 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.382252 32281 net.cpp:125] fc8 needs backward computation.
I0905 20:06:47.382261 32281 net.cpp:66] Creating Layer relu8
I0905 20:06:47.382266 32281 net.cpp:329] relu8 <- fc8
I0905 20:06:47.382274 32281 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:06:47.382282 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.382287 32281 net.cpp:125] relu8 needs backward computation.
I0905 20:06:47.382294 32281 net.cpp:66] Creating Layer drop8
I0905 20:06:47.382300 32281 net.cpp:329] drop8 <- fc8
I0905 20:06:47.382307 32281 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:06:47.382314 32281 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:06:47.382320 32281 net.cpp:125] drop8 needs backward computation.
I0905 20:06:47.382329 32281 net.cpp:66] Creating Layer fc9
I0905 20:06:47.382335 32281 net.cpp:329] fc9 <- fc8
I0905 20:06:47.382343 32281 net.cpp:290] fc9 -> fc9
I0905 20:06:47.382745 32281 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:06:47.382756 32281 net.cpp:125] fc9 needs backward computation.
I0905 20:06:47.382766 32281 net.cpp:66] Creating Layer fc10
I0905 20:06:47.382771 32281 net.cpp:329] fc10 <- fc9
I0905 20:06:47.382781 32281 net.cpp:290] fc10 -> fc10
I0905 20:06:47.382792 32281 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:47.382800 32281 net.cpp:125] fc10 needs backward computation.
I0905 20:06:47.382808 32281 net.cpp:66] Creating Layer prob
I0905 20:06:47.382813 32281 net.cpp:329] prob <- fc10
I0905 20:06:47.382822 32281 net.cpp:290] prob -> prob
I0905 20:06:47.382833 32281 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:06:47.382838 32281 net.cpp:125] prob needs backward computation.
I0905 20:06:47.382843 32281 net.cpp:156] This network produces output prob
I0905 20:06:47.382858 32281 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:06:47.382865 32281 net.cpp:167] Network initialization done.
I0905 20:06:47.382871 32281 net.cpp:168] Memory required for data: 6183480
Classifying 40 inputs.
Done in 25.52 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:07:14.145292 32285 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:07:14.145431 32285 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:07:14.145439 32285 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:07:14.145598 32285 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:07:14.145661 32285 net.cpp:292] Input 0 -> data
I0905 20:07:14.145687 32285 net.cpp:66] Creating Layer conv1
I0905 20:07:14.145694 32285 net.cpp:329] conv1 <- data
I0905 20:07:14.145702 32285 net.cpp:290] conv1 -> conv1
I0905 20:07:14.147068 32285 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:07:14.147084 32285 net.cpp:125] conv1 needs backward computation.
I0905 20:07:14.147094 32285 net.cpp:66] Creating Layer relu1
I0905 20:07:14.147099 32285 net.cpp:329] relu1 <- conv1
I0905 20:07:14.147106 32285 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:07:14.147114 32285 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:07:14.147120 32285 net.cpp:125] relu1 needs backward computation.
I0905 20:07:14.147127 32285 net.cpp:66] Creating Layer pool1
I0905 20:07:14.147133 32285 net.cpp:329] pool1 <- conv1
I0905 20:07:14.147140 32285 net.cpp:290] pool1 -> pool1
I0905 20:07:14.147150 32285 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:07:14.147156 32285 net.cpp:125] pool1 needs backward computation.
I0905 20:07:14.147163 32285 net.cpp:66] Creating Layer norm1
I0905 20:07:14.147168 32285 net.cpp:329] norm1 <- pool1
I0905 20:07:14.147176 32285 net.cpp:290] norm1 -> norm1
I0905 20:07:14.147184 32285 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:07:14.147191 32285 net.cpp:125] norm1 needs backward computation.
I0905 20:07:14.147197 32285 net.cpp:66] Creating Layer conv2
I0905 20:07:14.147208 32285 net.cpp:329] conv2 <- norm1
I0905 20:07:14.147215 32285 net.cpp:290] conv2 -> conv2
I0905 20:07:14.156364 32285 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:07:14.156379 32285 net.cpp:125] conv2 needs backward computation.
I0905 20:07:14.156388 32285 net.cpp:66] Creating Layer relu2
I0905 20:07:14.156393 32285 net.cpp:329] relu2 <- conv2
I0905 20:07:14.156399 32285 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:07:14.156406 32285 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:07:14.156411 32285 net.cpp:125] relu2 needs backward computation.
I0905 20:07:14.156419 32285 net.cpp:66] Creating Layer pool2
I0905 20:07:14.156424 32285 net.cpp:329] pool2 <- conv2
I0905 20:07:14.156430 32285 net.cpp:290] pool2 -> pool2
I0905 20:07:14.156437 32285 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:07:14.156443 32285 net.cpp:125] pool2 needs backward computation.
I0905 20:07:14.156452 32285 net.cpp:66] Creating Layer fc7
I0905 20:07:14.156458 32285 net.cpp:329] fc7 <- pool2
I0905 20:07:14.156466 32285 net.cpp:290] fc7 -> fc7
I0905 20:07:14.804957 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.805003 32285 net.cpp:125] fc7 needs backward computation.
I0905 20:07:14.805016 32285 net.cpp:66] Creating Layer relu7
I0905 20:07:14.805023 32285 net.cpp:329] relu7 <- fc7
I0905 20:07:14.805032 32285 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:07:14.805042 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.805048 32285 net.cpp:125] relu7 needs backward computation.
I0905 20:07:14.805057 32285 net.cpp:66] Creating Layer drop7
I0905 20:07:14.805061 32285 net.cpp:329] drop7 <- fc7
I0905 20:07:14.805068 32285 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:07:14.805078 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.805084 32285 net.cpp:125] drop7 needs backward computation.
I0905 20:07:14.805094 32285 net.cpp:66] Creating Layer fc8
I0905 20:07:14.805099 32285 net.cpp:329] fc8 <- fc7
I0905 20:07:14.805107 32285 net.cpp:290] fc8 -> fc8
I0905 20:07:14.813104 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.813117 32285 net.cpp:125] fc8 needs backward computation.
I0905 20:07:14.813124 32285 net.cpp:66] Creating Layer relu8
I0905 20:07:14.813130 32285 net.cpp:329] relu8 <- fc8
I0905 20:07:14.813138 32285 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:07:14.813146 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.813153 32285 net.cpp:125] relu8 needs backward computation.
I0905 20:07:14.813158 32285 net.cpp:66] Creating Layer drop8
I0905 20:07:14.813164 32285 net.cpp:329] drop8 <- fc8
I0905 20:07:14.813170 32285 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:07:14.813177 32285 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:07:14.813184 32285 net.cpp:125] drop8 needs backward computation.
I0905 20:07:14.813192 32285 net.cpp:66] Creating Layer fc9
I0905 20:07:14.813199 32285 net.cpp:329] fc9 <- fc8
I0905 20:07:14.813205 32285 net.cpp:290] fc9 -> fc9
I0905 20:07:14.813597 32285 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:07:14.813611 32285 net.cpp:125] fc9 needs backward computation.
I0905 20:07:14.813619 32285 net.cpp:66] Creating Layer fc10
I0905 20:07:14.813626 32285 net.cpp:329] fc10 <- fc9
I0905 20:07:14.813634 32285 net.cpp:290] fc10 -> fc10
I0905 20:07:14.813647 32285 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:07:14.813654 32285 net.cpp:125] fc10 needs backward computation.
I0905 20:07:14.813662 32285 net.cpp:66] Creating Layer prob
I0905 20:07:14.813668 32285 net.cpp:329] prob <- fc10
I0905 20:07:14.813674 32285 net.cpp:290] prob -> prob
I0905 20:07:14.813684 32285 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:07:14.813690 32285 net.cpp:125] prob needs backward computation.
I0905 20:07:14.813695 32285 net.cpp:156] This network produces output prob
I0905 20:07:14.813709 32285 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:07:14.813717 32285 net.cpp:167] Network initialization done.
I0905 20:07:14.813722 32285 net.cpp:168] Memory required for data: 6183480
Classifying 339 inputs.
Done in 213.24 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 39 is out of bounds for axis 0 with size 39
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:10:55.698210 32307 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:10:55.698349 32307 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:10:55.698359 32307 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:10:55.698508 32307 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:10:55.698561 32307 net.cpp:292] Input 0 -> data
I0905 20:10:55.698598 32307 net.cpp:66] Creating Layer conv1
I0905 20:10:55.698606 32307 net.cpp:329] conv1 <- data
I0905 20:10:55.698614 32307 net.cpp:290] conv1 -> conv1
I0905 20:10:55.700016 32307 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:10:55.700034 32307 net.cpp:125] conv1 needs backward computation.
I0905 20:10:55.700043 32307 net.cpp:66] Creating Layer relu1
I0905 20:10:55.700050 32307 net.cpp:329] relu1 <- conv1
I0905 20:10:55.700057 32307 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:10:55.700067 32307 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:10:55.700073 32307 net.cpp:125] relu1 needs backward computation.
I0905 20:10:55.700079 32307 net.cpp:66] Creating Layer pool1
I0905 20:10:55.700085 32307 net.cpp:329] pool1 <- conv1
I0905 20:10:55.700093 32307 net.cpp:290] pool1 -> pool1
I0905 20:10:55.700103 32307 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:10:55.700109 32307 net.cpp:125] pool1 needs backward computation.
I0905 20:10:55.700116 32307 net.cpp:66] Creating Layer norm1
I0905 20:10:55.700122 32307 net.cpp:329] norm1 <- pool1
I0905 20:10:55.700129 32307 net.cpp:290] norm1 -> norm1
I0905 20:10:55.700139 32307 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:10:55.700145 32307 net.cpp:125] norm1 needs backward computation.
I0905 20:10:55.700153 32307 net.cpp:66] Creating Layer conv2
I0905 20:10:55.700158 32307 net.cpp:329] conv2 <- norm1
I0905 20:10:55.700166 32307 net.cpp:290] conv2 -> conv2
I0905 20:10:55.709542 32307 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:10:55.709558 32307 net.cpp:125] conv2 needs backward computation.
I0905 20:10:55.709564 32307 net.cpp:66] Creating Layer relu2
I0905 20:10:55.709570 32307 net.cpp:329] relu2 <- conv2
I0905 20:10:55.709586 32307 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:10:55.709596 32307 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:10:55.709602 32307 net.cpp:125] relu2 needs backward computation.
I0905 20:10:55.709609 32307 net.cpp:66] Creating Layer pool2
I0905 20:10:55.709614 32307 net.cpp:329] pool2 <- conv2
I0905 20:10:55.709621 32307 net.cpp:290] pool2 -> pool2
I0905 20:10:55.709630 32307 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:10:55.709635 32307 net.cpp:125] pool2 needs backward computation.
I0905 20:10:55.709645 32307 net.cpp:66] Creating Layer fc7
I0905 20:10:55.709651 32307 net.cpp:329] fc7 <- pool2
I0905 20:10:55.709660 32307 net.cpp:290] fc7 -> fc7
I0905 20:10:56.360167 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.360209 32307 net.cpp:125] fc7 needs backward computation.
I0905 20:10:56.360223 32307 net.cpp:66] Creating Layer relu7
I0905 20:10:56.360230 32307 net.cpp:329] relu7 <- fc7
I0905 20:10:56.360240 32307 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:10:56.360251 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.360257 32307 net.cpp:125] relu7 needs backward computation.
I0905 20:10:56.360265 32307 net.cpp:66] Creating Layer drop7
I0905 20:10:56.360270 32307 net.cpp:329] drop7 <- fc7
I0905 20:10:56.360276 32307 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:10:56.360287 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.360293 32307 net.cpp:125] drop7 needs backward computation.
I0905 20:10:56.360302 32307 net.cpp:66] Creating Layer fc8
I0905 20:10:56.360307 32307 net.cpp:329] fc8 <- fc7
I0905 20:10:56.360316 32307 net.cpp:290] fc8 -> fc8
I0905 20:10:56.368444 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.368458 32307 net.cpp:125] fc8 needs backward computation.
I0905 20:10:56.368465 32307 net.cpp:66] Creating Layer relu8
I0905 20:10:56.368471 32307 net.cpp:329] relu8 <- fc8
I0905 20:10:56.368479 32307 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:10:56.368487 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.368494 32307 net.cpp:125] relu8 needs backward computation.
I0905 20:10:56.368500 32307 net.cpp:66] Creating Layer drop8
I0905 20:10:56.368505 32307 net.cpp:329] drop8 <- fc8
I0905 20:10:56.368512 32307 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:10:56.368520 32307 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:10:56.368535 32307 net.cpp:125] drop8 needs backward computation.
I0905 20:10:56.368545 32307 net.cpp:66] Creating Layer fc9
I0905 20:10:56.368551 32307 net.cpp:329] fc9 <- fc8
I0905 20:10:56.368558 32307 net.cpp:290] fc9 -> fc9
I0905 20:10:56.368943 32307 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:10:56.368957 32307 net.cpp:125] fc9 needs backward computation.
I0905 20:10:56.368964 32307 net.cpp:66] Creating Layer fc10
I0905 20:10:56.368970 32307 net.cpp:329] fc10 <- fc9
I0905 20:10:56.368978 32307 net.cpp:290] fc10 -> fc10
I0905 20:10:56.368991 32307 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:10:56.368999 32307 net.cpp:125] fc10 needs backward computation.
I0905 20:10:56.369006 32307 net.cpp:66] Creating Layer prob
I0905 20:10:56.369011 32307 net.cpp:329] prob <- fc10
I0905 20:10:56.369019 32307 net.cpp:290] prob -> prob
I0905 20:10:56.369029 32307 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:10:56.369035 32307 net.cpp:125] prob needs backward computation.
I0905 20:10:56.369040 32307 net.cpp:156] This network produces output prob
I0905 20:10:56.369055 32307 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:10:56.369065 32307 net.cpp:167] Network initialization done.
I0905 20:10:56.369070 32307 net.cpp:168] Memory required for data: 6183480
Classifying 290 inputs.
Done in 181.60 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 90 is out of bounds for axis 0 with size 90
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:14:03.434902 32315 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:14:03.435044 32315 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:14:03.435052 32315 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:14:03.435202 32315 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:14:03.435266 32315 net.cpp:292] Input 0 -> data
I0905 20:14:03.435292 32315 net.cpp:66] Creating Layer conv1
I0905 20:14:03.435299 32315 net.cpp:329] conv1 <- data
I0905 20:14:03.435308 32315 net.cpp:290] conv1 -> conv1
I0905 20:14:03.436710 32315 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:14:03.436728 32315 net.cpp:125] conv1 needs backward computation.
I0905 20:14:03.436738 32315 net.cpp:66] Creating Layer relu1
I0905 20:14:03.436744 32315 net.cpp:329] relu1 <- conv1
I0905 20:14:03.436751 32315 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:14:03.436759 32315 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:14:03.436766 32315 net.cpp:125] relu1 needs backward computation.
I0905 20:14:03.436774 32315 net.cpp:66] Creating Layer pool1
I0905 20:14:03.436779 32315 net.cpp:329] pool1 <- conv1
I0905 20:14:03.436785 32315 net.cpp:290] pool1 -> pool1
I0905 20:14:03.436796 32315 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:14:03.436802 32315 net.cpp:125] pool1 needs backward computation.
I0905 20:14:03.436810 32315 net.cpp:66] Creating Layer norm1
I0905 20:14:03.436815 32315 net.cpp:329] norm1 <- pool1
I0905 20:14:03.436822 32315 net.cpp:290] norm1 -> norm1
I0905 20:14:03.436832 32315 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:14:03.436838 32315 net.cpp:125] norm1 needs backward computation.
I0905 20:14:03.436846 32315 net.cpp:66] Creating Layer conv2
I0905 20:14:03.436851 32315 net.cpp:329] conv2 <- norm1
I0905 20:14:03.436858 32315 net.cpp:290] conv2 -> conv2
I0905 20:14:03.446161 32315 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:14:03.446177 32315 net.cpp:125] conv2 needs backward computation.
I0905 20:14:03.446184 32315 net.cpp:66] Creating Layer relu2
I0905 20:14:03.446190 32315 net.cpp:329] relu2 <- conv2
I0905 20:14:03.446197 32315 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:14:03.446204 32315 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:14:03.446210 32315 net.cpp:125] relu2 needs backward computation.
I0905 20:14:03.446216 32315 net.cpp:66] Creating Layer pool2
I0905 20:14:03.446223 32315 net.cpp:329] pool2 <- conv2
I0905 20:14:03.446228 32315 net.cpp:290] pool2 -> pool2
I0905 20:14:03.446236 32315 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:14:03.446243 32315 net.cpp:125] pool2 needs backward computation.
I0905 20:14:03.446252 32315 net.cpp:66] Creating Layer fc7
I0905 20:14:03.446259 32315 net.cpp:329] fc7 <- pool2
I0905 20:14:03.446265 32315 net.cpp:290] fc7 -> fc7
I0905 20:14:04.120010 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.120057 32315 net.cpp:125] fc7 needs backward computation.
I0905 20:14:04.120070 32315 net.cpp:66] Creating Layer relu7
I0905 20:14:04.120079 32315 net.cpp:329] relu7 <- fc7
I0905 20:14:04.120089 32315 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:14:04.120110 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.120116 32315 net.cpp:125] relu7 needs backward computation.
I0905 20:14:04.120123 32315 net.cpp:66] Creating Layer drop7
I0905 20:14:04.120129 32315 net.cpp:329] drop7 <- fc7
I0905 20:14:04.120136 32315 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:14:04.120146 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.120152 32315 net.cpp:125] drop7 needs backward computation.
I0905 20:14:04.120162 32315 net.cpp:66] Creating Layer fc8
I0905 20:14:04.120167 32315 net.cpp:329] fc8 <- fc7
I0905 20:14:04.120175 32315 net.cpp:290] fc8 -> fc8
I0905 20:14:04.128465 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.128479 32315 net.cpp:125] fc8 needs backward computation.
I0905 20:14:04.128486 32315 net.cpp:66] Creating Layer relu8
I0905 20:14:04.128491 32315 net.cpp:329] relu8 <- fc8
I0905 20:14:04.128499 32315 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:14:04.128507 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.128514 32315 net.cpp:125] relu8 needs backward computation.
I0905 20:14:04.128520 32315 net.cpp:66] Creating Layer drop8
I0905 20:14:04.128525 32315 net.cpp:329] drop8 <- fc8
I0905 20:14:04.128531 32315 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:14:04.128538 32315 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:04.128545 32315 net.cpp:125] drop8 needs backward computation.
I0905 20:14:04.128553 32315 net.cpp:66] Creating Layer fc9
I0905 20:14:04.128558 32315 net.cpp:329] fc9 <- fc8
I0905 20:14:04.128566 32315 net.cpp:290] fc9 -> fc9
I0905 20:14:04.128962 32315 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:14:04.128974 32315 net.cpp:125] fc9 needs backward computation.
I0905 20:14:04.128983 32315 net.cpp:66] Creating Layer fc10
I0905 20:14:04.128988 32315 net.cpp:329] fc10 <- fc9
I0905 20:14:04.128996 32315 net.cpp:290] fc10 -> fc10
I0905 20:14:04.129009 32315 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:14:04.129016 32315 net.cpp:125] fc10 needs backward computation.
I0905 20:14:04.129024 32315 net.cpp:66] Creating Layer prob
I0905 20:14:04.129029 32315 net.cpp:329] prob <- fc10
I0905 20:14:04.129036 32315 net.cpp:290] prob -> prob
I0905 20:14:04.129046 32315 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:14:04.129052 32315 net.cpp:125] prob needs backward computation.
I0905 20:14:04.129057 32315 net.cpp:156] This network produces output prob
I0905 20:14:04.129070 32315 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:14:04.129078 32315 net.cpp:167] Network initialization done.
I0905 20:14:04.129083 32315 net.cpp:168] Memory required for data: 6183480
Classifying 63 inputs.
Done in 40.36 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:14:47.425550 32320 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:14:47.425699 32320 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:14:47.425709 32320 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:14:47.425855 32320 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:14:47.425918 32320 net.cpp:292] Input 0 -> data
I0905 20:14:47.425945 32320 net.cpp:66] Creating Layer conv1
I0905 20:14:47.425951 32320 net.cpp:329] conv1 <- data
I0905 20:14:47.425961 32320 net.cpp:290] conv1 -> conv1
I0905 20:14:47.427322 32320 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:14:47.427340 32320 net.cpp:125] conv1 needs backward computation.
I0905 20:14:47.427350 32320 net.cpp:66] Creating Layer relu1
I0905 20:14:47.427356 32320 net.cpp:329] relu1 <- conv1
I0905 20:14:47.427363 32320 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:14:47.427372 32320 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:14:47.427378 32320 net.cpp:125] relu1 needs backward computation.
I0905 20:14:47.427386 32320 net.cpp:66] Creating Layer pool1
I0905 20:14:47.427391 32320 net.cpp:329] pool1 <- conv1
I0905 20:14:47.427397 32320 net.cpp:290] pool1 -> pool1
I0905 20:14:47.427409 32320 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:14:47.427415 32320 net.cpp:125] pool1 needs backward computation.
I0905 20:14:47.427423 32320 net.cpp:66] Creating Layer norm1
I0905 20:14:47.427428 32320 net.cpp:329] norm1 <- pool1
I0905 20:14:47.427435 32320 net.cpp:290] norm1 -> norm1
I0905 20:14:47.427445 32320 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:14:47.427451 32320 net.cpp:125] norm1 needs backward computation.
I0905 20:14:47.427459 32320 net.cpp:66] Creating Layer conv2
I0905 20:14:47.427465 32320 net.cpp:329] conv2 <- norm1
I0905 20:14:47.427472 32320 net.cpp:290] conv2 -> conv2
I0905 20:14:47.436612 32320 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:14:47.436630 32320 net.cpp:125] conv2 needs backward computation.
I0905 20:14:47.436636 32320 net.cpp:66] Creating Layer relu2
I0905 20:14:47.436642 32320 net.cpp:329] relu2 <- conv2
I0905 20:14:47.436655 32320 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:14:47.436663 32320 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:14:47.436669 32320 net.cpp:125] relu2 needs backward computation.
I0905 20:14:47.436676 32320 net.cpp:66] Creating Layer pool2
I0905 20:14:47.436681 32320 net.cpp:329] pool2 <- conv2
I0905 20:14:47.436688 32320 net.cpp:290] pool2 -> pool2
I0905 20:14:47.436697 32320 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:14:47.436702 32320 net.cpp:125] pool2 needs backward computation.
I0905 20:14:47.436712 32320 net.cpp:66] Creating Layer fc7
I0905 20:14:47.436717 32320 net.cpp:329] fc7 <- pool2
I0905 20:14:47.436725 32320 net.cpp:290] fc7 -> fc7
I0905 20:14:48.089494 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.089541 32320 net.cpp:125] fc7 needs backward computation.
I0905 20:14:48.089555 32320 net.cpp:66] Creating Layer relu7
I0905 20:14:48.089561 32320 net.cpp:329] relu7 <- fc7
I0905 20:14:48.089571 32320 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:14:48.089589 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.089606 32320 net.cpp:125] relu7 needs backward computation.
I0905 20:14:48.089612 32320 net.cpp:66] Creating Layer drop7
I0905 20:14:48.089618 32320 net.cpp:329] drop7 <- fc7
I0905 20:14:48.089625 32320 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:14:48.089637 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.089643 32320 net.cpp:125] drop7 needs backward computation.
I0905 20:14:48.089653 32320 net.cpp:66] Creating Layer fc8
I0905 20:14:48.089658 32320 net.cpp:329] fc8 <- fc7
I0905 20:14:48.089668 32320 net.cpp:290] fc8 -> fc8
I0905 20:14:48.097679 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.097692 32320 net.cpp:125] fc8 needs backward computation.
I0905 20:14:48.097700 32320 net.cpp:66] Creating Layer relu8
I0905 20:14:48.097707 32320 net.cpp:329] relu8 <- fc8
I0905 20:14:48.097714 32320 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:14:48.097723 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.097728 32320 net.cpp:125] relu8 needs backward computation.
I0905 20:14:48.097735 32320 net.cpp:66] Creating Layer drop8
I0905 20:14:48.097741 32320 net.cpp:329] drop8 <- fc8
I0905 20:14:48.097748 32320 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:14:48.097755 32320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:14:48.097761 32320 net.cpp:125] drop8 needs backward computation.
I0905 20:14:48.097770 32320 net.cpp:66] Creating Layer fc9
I0905 20:14:48.097777 32320 net.cpp:329] fc9 <- fc8
I0905 20:14:48.097784 32320 net.cpp:290] fc9 -> fc9
I0905 20:14:48.098171 32320 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:14:48.098184 32320 net.cpp:125] fc9 needs backward computation.
I0905 20:14:48.098193 32320 net.cpp:66] Creating Layer fc10
I0905 20:14:48.098199 32320 net.cpp:329] fc10 <- fc9
I0905 20:14:48.098208 32320 net.cpp:290] fc10 -> fc10
I0905 20:14:48.098222 32320 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:14:48.098229 32320 net.cpp:125] fc10 needs backward computation.
I0905 20:14:48.098237 32320 net.cpp:66] Creating Layer prob
I0905 20:14:48.098242 32320 net.cpp:329] prob <- fc10
I0905 20:14:48.098250 32320 net.cpp:290] prob -> prob
I0905 20:14:48.098260 32320 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:14:48.098266 32320 net.cpp:125] prob needs backward computation.
I0905 20:14:48.098273 32320 net.cpp:156] This network produces output prob
I0905 20:14:48.098285 32320 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:14:48.098294 32320 net.cpp:167] Network initialization done.
I0905 20:14:48.098300 32320 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 456 inputs.
Done in 298.38 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 56 is out of bounds for axis 0 with size 56
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:19:54.361248 32335 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:19:54.361388 32335 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:19:54.361397 32335 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:19:54.361548 32335 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:19:54.361616 32335 net.cpp:292] Input 0 -> data
I0905 20:19:54.361644 32335 net.cpp:66] Creating Layer conv1
I0905 20:19:54.361651 32335 net.cpp:329] conv1 <- data
I0905 20:19:54.361660 32335 net.cpp:290] conv1 -> conv1
I0905 20:19:54.363142 32335 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:19:54.363169 32335 net.cpp:125] conv1 needs backward computation.
I0905 20:19:54.363179 32335 net.cpp:66] Creating Layer relu1
I0905 20:19:54.363185 32335 net.cpp:329] relu1 <- conv1
I0905 20:19:54.363193 32335 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:19:54.363201 32335 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:19:54.363209 32335 net.cpp:125] relu1 needs backward computation.
I0905 20:19:54.363215 32335 net.cpp:66] Creating Layer pool1
I0905 20:19:54.363220 32335 net.cpp:329] pool1 <- conv1
I0905 20:19:54.363227 32335 net.cpp:290] pool1 -> pool1
I0905 20:19:54.363239 32335 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:19:54.363245 32335 net.cpp:125] pool1 needs backward computation.
I0905 20:19:54.363252 32335 net.cpp:66] Creating Layer norm1
I0905 20:19:54.363257 32335 net.cpp:329] norm1 <- pool1
I0905 20:19:54.363265 32335 net.cpp:290] norm1 -> norm1
I0905 20:19:54.363275 32335 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:19:54.363281 32335 net.cpp:125] norm1 needs backward computation.
I0905 20:19:54.363288 32335 net.cpp:66] Creating Layer conv2
I0905 20:19:54.363294 32335 net.cpp:329] conv2 <- norm1
I0905 20:19:54.363301 32335 net.cpp:290] conv2 -> conv2
I0905 20:19:54.372753 32335 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:19:54.372771 32335 net.cpp:125] conv2 needs backward computation.
I0905 20:19:54.372779 32335 net.cpp:66] Creating Layer relu2
I0905 20:19:54.372784 32335 net.cpp:329] relu2 <- conv2
I0905 20:19:54.372792 32335 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:19:54.372799 32335 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:19:54.372805 32335 net.cpp:125] relu2 needs backward computation.
I0905 20:19:54.372812 32335 net.cpp:66] Creating Layer pool2
I0905 20:19:54.372817 32335 net.cpp:329] pool2 <- conv2
I0905 20:19:54.372823 32335 net.cpp:290] pool2 -> pool2
I0905 20:19:54.372831 32335 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:19:54.372838 32335 net.cpp:125] pool2 needs backward computation.
I0905 20:19:54.372846 32335 net.cpp:66] Creating Layer fc7
I0905 20:19:54.372853 32335 net.cpp:329] fc7 <- pool2
I0905 20:19:54.372860 32335 net.cpp:290] fc7 -> fc7
I0905 20:19:55.022161 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.022207 32335 net.cpp:125] fc7 needs backward computation.
I0905 20:19:55.022219 32335 net.cpp:66] Creating Layer relu7
I0905 20:19:55.022227 32335 net.cpp:329] relu7 <- fc7
I0905 20:19:55.022236 32335 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:19:55.022246 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.022253 32335 net.cpp:125] relu7 needs backward computation.
I0905 20:19:55.022259 32335 net.cpp:66] Creating Layer drop7
I0905 20:19:55.022265 32335 net.cpp:329] drop7 <- fc7
I0905 20:19:55.022271 32335 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:19:55.022282 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.022289 32335 net.cpp:125] drop7 needs backward computation.
I0905 20:19:55.022296 32335 net.cpp:66] Creating Layer fc8
I0905 20:19:55.022302 32335 net.cpp:329] fc8 <- fc7
I0905 20:19:55.022310 32335 net.cpp:290] fc8 -> fc8
I0905 20:19:55.030108 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.030122 32335 net.cpp:125] fc8 needs backward computation.
I0905 20:19:55.030128 32335 net.cpp:66] Creating Layer relu8
I0905 20:19:55.030134 32335 net.cpp:329] relu8 <- fc8
I0905 20:19:55.030143 32335 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:19:55.030149 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.030155 32335 net.cpp:125] relu8 needs backward computation.
I0905 20:19:55.030161 32335 net.cpp:66] Creating Layer drop8
I0905 20:19:55.030167 32335 net.cpp:329] drop8 <- fc8
I0905 20:19:55.030174 32335 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:19:55.030180 32335 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:19:55.030186 32335 net.cpp:125] drop8 needs backward computation.
I0905 20:19:55.030195 32335 net.cpp:66] Creating Layer fc9
I0905 20:19:55.030201 32335 net.cpp:329] fc9 <- fc8
I0905 20:19:55.030208 32335 net.cpp:290] fc9 -> fc9
I0905 20:19:55.030603 32335 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:19:55.030617 32335 net.cpp:125] fc9 needs backward computation.
I0905 20:19:55.030625 32335 net.cpp:66] Creating Layer fc10
I0905 20:19:55.030632 32335 net.cpp:329] fc10 <- fc9
I0905 20:19:55.030640 32335 net.cpp:290] fc10 -> fc10
I0905 20:19:55.030652 32335 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:19:55.030661 32335 net.cpp:125] fc10 needs backward computation.
I0905 20:19:55.030668 32335 net.cpp:66] Creating Layer prob
I0905 20:19:55.030673 32335 net.cpp:329] prob <- fc10
I0905 20:19:55.030681 32335 net.cpp:290] prob -> prob
I0905 20:19:55.030691 32335 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:19:55.030697 32335 net.cpp:125] prob needs backward computation.
I0905 20:19:55.030702 32335 net.cpp:156] This network produces output prob
I0905 20:19:55.030715 32335 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:19:55.030725 32335 net.cpp:167] Network initialization done.
I0905 20:19:55.030730 32335 net.cpp:168] Memory required for data: 6183480
Classifying 160 inputs.
Done in 100.73 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 60 is out of bounds for axis 0 with size 60
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:21:42.423673 32341 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:21:42.423811 32341 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:21:42.423820 32341 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:21:42.423965 32341 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:21:42.424028 32341 net.cpp:292] Input 0 -> data
I0905 20:21:42.424054 32341 net.cpp:66] Creating Layer conv1
I0905 20:21:42.424062 32341 net.cpp:329] conv1 <- data
I0905 20:21:42.424069 32341 net.cpp:290] conv1 -> conv1
I0905 20:21:42.425431 32341 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:21:42.425449 32341 net.cpp:125] conv1 needs backward computation.
I0905 20:21:42.425458 32341 net.cpp:66] Creating Layer relu1
I0905 20:21:42.425463 32341 net.cpp:329] relu1 <- conv1
I0905 20:21:42.425470 32341 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:21:42.425479 32341 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:21:42.425485 32341 net.cpp:125] relu1 needs backward computation.
I0905 20:21:42.425493 32341 net.cpp:66] Creating Layer pool1
I0905 20:21:42.425498 32341 net.cpp:329] pool1 <- conv1
I0905 20:21:42.425504 32341 net.cpp:290] pool1 -> pool1
I0905 20:21:42.425515 32341 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:21:42.425521 32341 net.cpp:125] pool1 needs backward computation.
I0905 20:21:42.425528 32341 net.cpp:66] Creating Layer norm1
I0905 20:21:42.425534 32341 net.cpp:329] norm1 <- pool1
I0905 20:21:42.425539 32341 net.cpp:290] norm1 -> norm1
I0905 20:21:42.425549 32341 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:21:42.425555 32341 net.cpp:125] norm1 needs backward computation.
I0905 20:21:42.425562 32341 net.cpp:66] Creating Layer conv2
I0905 20:21:42.425568 32341 net.cpp:329] conv2 <- norm1
I0905 20:21:42.425575 32341 net.cpp:290] conv2 -> conv2
I0905 20:21:42.434993 32341 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:21:42.435009 32341 net.cpp:125] conv2 needs backward computation.
I0905 20:21:42.435015 32341 net.cpp:66] Creating Layer relu2
I0905 20:21:42.435021 32341 net.cpp:329] relu2 <- conv2
I0905 20:21:42.435029 32341 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:21:42.435035 32341 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:21:42.435041 32341 net.cpp:125] relu2 needs backward computation.
I0905 20:21:42.435050 32341 net.cpp:66] Creating Layer pool2
I0905 20:21:42.435056 32341 net.cpp:329] pool2 <- conv2
I0905 20:21:42.435063 32341 net.cpp:290] pool2 -> pool2
I0905 20:21:42.435071 32341 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:21:42.435077 32341 net.cpp:125] pool2 needs backward computation.
I0905 20:21:42.435084 32341 net.cpp:66] Creating Layer fc7
I0905 20:21:42.435091 32341 net.cpp:329] fc7 <- pool2
I0905 20:21:42.435097 32341 net.cpp:290] fc7 -> fc7
I0905 20:21:43.083534 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.083580 32341 net.cpp:125] fc7 needs backward computation.
I0905 20:21:43.083592 32341 net.cpp:66] Creating Layer relu7
I0905 20:21:43.083600 32341 net.cpp:329] relu7 <- fc7
I0905 20:21:43.083609 32341 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:21:43.083619 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.083626 32341 net.cpp:125] relu7 needs backward computation.
I0905 20:21:43.083633 32341 net.cpp:66] Creating Layer drop7
I0905 20:21:43.083639 32341 net.cpp:329] drop7 <- fc7
I0905 20:21:43.083657 32341 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:21:43.083668 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.083674 32341 net.cpp:125] drop7 needs backward computation.
I0905 20:21:43.083683 32341 net.cpp:66] Creating Layer fc8
I0905 20:21:43.083688 32341 net.cpp:329] fc8 <- fc7
I0905 20:21:43.083698 32341 net.cpp:290] fc8 -> fc8
I0905 20:21:43.091712 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.091725 32341 net.cpp:125] fc8 needs backward computation.
I0905 20:21:43.091732 32341 net.cpp:66] Creating Layer relu8
I0905 20:21:43.091738 32341 net.cpp:329] relu8 <- fc8
I0905 20:21:43.091747 32341 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:21:43.091754 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.091759 32341 net.cpp:125] relu8 needs backward computation.
I0905 20:21:43.091766 32341 net.cpp:66] Creating Layer drop8
I0905 20:21:43.091771 32341 net.cpp:329] drop8 <- fc8
I0905 20:21:43.091778 32341 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:21:43.091785 32341 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:21:43.091791 32341 net.cpp:125] drop8 needs backward computation.
I0905 20:21:43.091800 32341 net.cpp:66] Creating Layer fc9
I0905 20:21:43.091806 32341 net.cpp:329] fc9 <- fc8
I0905 20:21:43.091814 32341 net.cpp:290] fc9 -> fc9
I0905 20:21:43.092198 32341 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:21:43.092211 32341 net.cpp:125] fc9 needs backward computation.
I0905 20:21:43.092219 32341 net.cpp:66] Creating Layer fc10
I0905 20:21:43.092224 32341 net.cpp:329] fc10 <- fc9
I0905 20:21:43.092233 32341 net.cpp:290] fc10 -> fc10
I0905 20:21:43.092245 32341 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:21:43.092253 32341 net.cpp:125] fc10 needs backward computation.
I0905 20:21:43.092260 32341 net.cpp:66] Creating Layer prob
I0905 20:21:43.092267 32341 net.cpp:329] prob <- fc10
I0905 20:21:43.092274 32341 net.cpp:290] prob -> prob
I0905 20:21:43.092284 32341 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:21:43.092290 32341 net.cpp:125] prob needs backward computation.
I0905 20:21:43.092295 32341 net.cpp:156] This network produces output prob
I0905 20:21:43.092308 32341 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:21:43.092316 32341 net.cpp:167] Network initialization done.
I0905 20:21:43.092321 32341 net.cpp:168] Memory required for data: 6183480
Classifying 258 inputs.
Done in 166.45 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 58 is out of bounds for axis 0 with size 58
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:24:35.040632 32349 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:24:35.040770 32349 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:24:35.040778 32349 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:24:35.040925 32349 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:24:35.040989 32349 net.cpp:292] Input 0 -> data
I0905 20:24:35.041014 32349 net.cpp:66] Creating Layer conv1
I0905 20:24:35.041021 32349 net.cpp:329] conv1 <- data
I0905 20:24:35.041030 32349 net.cpp:290] conv1 -> conv1
I0905 20:24:35.042402 32349 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:24:35.042421 32349 net.cpp:125] conv1 needs backward computation.
I0905 20:24:35.042430 32349 net.cpp:66] Creating Layer relu1
I0905 20:24:35.042436 32349 net.cpp:329] relu1 <- conv1
I0905 20:24:35.042443 32349 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:24:35.042453 32349 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:24:35.042459 32349 net.cpp:125] relu1 needs backward computation.
I0905 20:24:35.042465 32349 net.cpp:66] Creating Layer pool1
I0905 20:24:35.042471 32349 net.cpp:329] pool1 <- conv1
I0905 20:24:35.042479 32349 net.cpp:290] pool1 -> pool1
I0905 20:24:35.042490 32349 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:24:35.042495 32349 net.cpp:125] pool1 needs backward computation.
I0905 20:24:35.042502 32349 net.cpp:66] Creating Layer norm1
I0905 20:24:35.042507 32349 net.cpp:329] norm1 <- pool1
I0905 20:24:35.042515 32349 net.cpp:290] norm1 -> norm1
I0905 20:24:35.042525 32349 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:24:35.042531 32349 net.cpp:125] norm1 needs backward computation.
I0905 20:24:35.042537 32349 net.cpp:66] Creating Layer conv2
I0905 20:24:35.042543 32349 net.cpp:329] conv2 <- norm1
I0905 20:24:35.042551 32349 net.cpp:290] conv2 -> conv2
I0905 20:24:35.051713 32349 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:24:35.051729 32349 net.cpp:125] conv2 needs backward computation.
I0905 20:24:35.051736 32349 net.cpp:66] Creating Layer relu2
I0905 20:24:35.051743 32349 net.cpp:329] relu2 <- conv2
I0905 20:24:35.051753 32349 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:24:35.051761 32349 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:24:35.051767 32349 net.cpp:125] relu2 needs backward computation.
I0905 20:24:35.051775 32349 net.cpp:66] Creating Layer pool2
I0905 20:24:35.051781 32349 net.cpp:329] pool2 <- conv2
I0905 20:24:35.051789 32349 net.cpp:290] pool2 -> pool2
I0905 20:24:35.051796 32349 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:24:35.051802 32349 net.cpp:125] pool2 needs backward computation.
I0905 20:24:35.051810 32349 net.cpp:66] Creating Layer fc7
I0905 20:24:35.051815 32349 net.cpp:329] fc7 <- pool2
I0905 20:24:35.051822 32349 net.cpp:290] fc7 -> fc7
I0905 20:24:35.702723 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.702769 32349 net.cpp:125] fc7 needs backward computation.
I0905 20:24:35.702782 32349 net.cpp:66] Creating Layer relu7
I0905 20:24:35.702790 32349 net.cpp:329] relu7 <- fc7
I0905 20:24:35.702800 32349 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:24:35.702810 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.702816 32349 net.cpp:125] relu7 needs backward computation.
I0905 20:24:35.702823 32349 net.cpp:66] Creating Layer drop7
I0905 20:24:35.702828 32349 net.cpp:329] drop7 <- fc7
I0905 20:24:35.702834 32349 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:24:35.702847 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.702852 32349 net.cpp:125] drop7 needs backward computation.
I0905 20:24:35.702860 32349 net.cpp:66] Creating Layer fc8
I0905 20:24:35.702867 32349 net.cpp:329] fc8 <- fc7
I0905 20:24:35.702875 32349 net.cpp:290] fc8 -> fc8
I0905 20:24:35.710813 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.710826 32349 net.cpp:125] fc8 needs backward computation.
I0905 20:24:35.710834 32349 net.cpp:66] Creating Layer relu8
I0905 20:24:35.710839 32349 net.cpp:329] relu8 <- fc8
I0905 20:24:35.710847 32349 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:24:35.710855 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.710861 32349 net.cpp:125] relu8 needs backward computation.
I0905 20:24:35.710868 32349 net.cpp:66] Creating Layer drop8
I0905 20:24:35.710873 32349 net.cpp:329] drop8 <- fc8
I0905 20:24:35.710880 32349 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:24:35.710887 32349 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:35.710893 32349 net.cpp:125] drop8 needs backward computation.
I0905 20:24:35.710902 32349 net.cpp:66] Creating Layer fc9
I0905 20:24:35.710908 32349 net.cpp:329] fc9 <- fc8
I0905 20:24:35.710916 32349 net.cpp:290] fc9 -> fc9
I0905 20:24:35.711302 32349 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:24:35.711313 32349 net.cpp:125] fc9 needs backward computation.
I0905 20:24:35.711323 32349 net.cpp:66] Creating Layer fc10
I0905 20:24:35.711328 32349 net.cpp:329] fc10 <- fc9
I0905 20:24:35.711338 32349 net.cpp:290] fc10 -> fc10
I0905 20:24:35.711351 32349 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:24:35.711359 32349 net.cpp:125] fc10 needs backward computation.
I0905 20:24:35.711366 32349 net.cpp:66] Creating Layer prob
I0905 20:24:35.711372 32349 net.cpp:329] prob <- fc10
I0905 20:24:35.711380 32349 net.cpp:290] prob -> prob
I0905 20:24:35.711390 32349 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:24:35.711396 32349 net.cpp:125] prob needs backward computation.
I0905 20:24:35.711401 32349 net.cpp:156] This network produces output prob
I0905 20:24:35.711416 32349 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:24:35.711423 32349 net.cpp:167] Network initialization done.
I0905 20:24:35.711429 32349 net.cpp:168] Memory required for data: 6183480
Classifying 19 inputs.
Done in 11.95 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:24:48.768755 32353 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:24:48.768894 32353 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:24:48.768915 32353 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:24:48.769062 32353 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:24:48.769115 32353 net.cpp:292] Input 0 -> data
I0905 20:24:48.769140 32353 net.cpp:66] Creating Layer conv1
I0905 20:24:48.769147 32353 net.cpp:329] conv1 <- data
I0905 20:24:48.769155 32353 net.cpp:290] conv1 -> conv1
I0905 20:24:48.770529 32353 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:24:48.770547 32353 net.cpp:125] conv1 needs backward computation.
I0905 20:24:48.770556 32353 net.cpp:66] Creating Layer relu1
I0905 20:24:48.770562 32353 net.cpp:329] relu1 <- conv1
I0905 20:24:48.770570 32353 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:24:48.770578 32353 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:24:48.770584 32353 net.cpp:125] relu1 needs backward computation.
I0905 20:24:48.770596 32353 net.cpp:66] Creating Layer pool1
I0905 20:24:48.770601 32353 net.cpp:329] pool1 <- conv1
I0905 20:24:48.770608 32353 net.cpp:290] pool1 -> pool1
I0905 20:24:48.770619 32353 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:24:48.770625 32353 net.cpp:125] pool1 needs backward computation.
I0905 20:24:48.770632 32353 net.cpp:66] Creating Layer norm1
I0905 20:24:48.770637 32353 net.cpp:329] norm1 <- pool1
I0905 20:24:48.770644 32353 net.cpp:290] norm1 -> norm1
I0905 20:24:48.770654 32353 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:24:48.770659 32353 net.cpp:125] norm1 needs backward computation.
I0905 20:24:48.770668 32353 net.cpp:66] Creating Layer conv2
I0905 20:24:48.770673 32353 net.cpp:329] conv2 <- norm1
I0905 20:24:48.770679 32353 net.cpp:290] conv2 -> conv2
I0905 20:24:48.779810 32353 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:24:48.779826 32353 net.cpp:125] conv2 needs backward computation.
I0905 20:24:48.779834 32353 net.cpp:66] Creating Layer relu2
I0905 20:24:48.779839 32353 net.cpp:329] relu2 <- conv2
I0905 20:24:48.779845 32353 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:24:48.779852 32353 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:24:48.779858 32353 net.cpp:125] relu2 needs backward computation.
I0905 20:24:48.779866 32353 net.cpp:66] Creating Layer pool2
I0905 20:24:48.779872 32353 net.cpp:329] pool2 <- conv2
I0905 20:24:48.779880 32353 net.cpp:290] pool2 -> pool2
I0905 20:24:48.779887 32353 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:24:48.779892 32353 net.cpp:125] pool2 needs backward computation.
I0905 20:24:48.779899 32353 net.cpp:66] Creating Layer fc7
I0905 20:24:48.779906 32353 net.cpp:329] fc7 <- pool2
I0905 20:24:48.779912 32353 net.cpp:290] fc7 -> fc7
I0905 20:24:49.427562 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.427608 32353 net.cpp:125] fc7 needs backward computation.
I0905 20:24:49.427621 32353 net.cpp:66] Creating Layer relu7
I0905 20:24:49.427628 32353 net.cpp:329] relu7 <- fc7
I0905 20:24:49.427638 32353 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:24:49.427647 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.427654 32353 net.cpp:125] relu7 needs backward computation.
I0905 20:24:49.427661 32353 net.cpp:66] Creating Layer drop7
I0905 20:24:49.427666 32353 net.cpp:329] drop7 <- fc7
I0905 20:24:49.427672 32353 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:24:49.427683 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.427690 32353 net.cpp:125] drop7 needs backward computation.
I0905 20:24:49.427697 32353 net.cpp:66] Creating Layer fc8
I0905 20:24:49.427703 32353 net.cpp:329] fc8 <- fc7
I0905 20:24:49.427711 32353 net.cpp:290] fc8 -> fc8
I0905 20:24:49.435510 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.435523 32353 net.cpp:125] fc8 needs backward computation.
I0905 20:24:49.435530 32353 net.cpp:66] Creating Layer relu8
I0905 20:24:49.435535 32353 net.cpp:329] relu8 <- fc8
I0905 20:24:49.435544 32353 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:24:49.435550 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.435556 32353 net.cpp:125] relu8 needs backward computation.
I0905 20:24:49.435562 32353 net.cpp:66] Creating Layer drop8
I0905 20:24:49.435569 32353 net.cpp:329] drop8 <- fc8
I0905 20:24:49.435575 32353 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:24:49.435580 32353 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:24:49.435586 32353 net.cpp:125] drop8 needs backward computation.
I0905 20:24:49.435595 32353 net.cpp:66] Creating Layer fc9
I0905 20:24:49.435600 32353 net.cpp:329] fc9 <- fc8
I0905 20:24:49.435607 32353 net.cpp:290] fc9 -> fc9
I0905 20:24:49.435981 32353 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:24:49.435993 32353 net.cpp:125] fc9 needs backward computation.
I0905 20:24:49.436002 32353 net.cpp:66] Creating Layer fc10
I0905 20:24:49.436007 32353 net.cpp:329] fc10 <- fc9
I0905 20:24:49.436015 32353 net.cpp:290] fc10 -> fc10
I0905 20:24:49.436028 32353 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:24:49.436034 32353 net.cpp:125] fc10 needs backward computation.
I0905 20:24:49.436050 32353 net.cpp:66] Creating Layer prob
I0905 20:24:49.436056 32353 net.cpp:329] prob <- fc10
I0905 20:24:49.436064 32353 net.cpp:290] prob -> prob
I0905 20:24:49.436074 32353 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:24:49.436079 32353 net.cpp:125] prob needs backward computation.
I0905 20:24:49.436084 32353 net.cpp:156] This network produces output prob
I0905 20:24:49.436097 32353 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:24:49.436105 32353 net.cpp:167] Network initialization done.
I0905 20:24:49.436110 32353 net.cpp:168] Memory required for data: 6183480
Classifying 173 inputs.
Done in 111.25 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 73 is out of bounds for axis 0 with size 73
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:26:45.955860 32359 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:26:45.955999 32359 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:26:45.956008 32359 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:26:45.956157 32359 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:26:45.956220 32359 net.cpp:292] Input 0 -> data
I0905 20:26:45.956248 32359 net.cpp:66] Creating Layer conv1
I0905 20:26:45.956254 32359 net.cpp:329] conv1 <- data
I0905 20:26:45.956262 32359 net.cpp:290] conv1 -> conv1
I0905 20:26:45.957650 32359 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:26:45.957670 32359 net.cpp:125] conv1 needs backward computation.
I0905 20:26:45.957679 32359 net.cpp:66] Creating Layer relu1
I0905 20:26:45.957686 32359 net.cpp:329] relu1 <- conv1
I0905 20:26:45.957694 32359 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:26:45.957702 32359 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:26:45.957708 32359 net.cpp:125] relu1 needs backward computation.
I0905 20:26:45.957715 32359 net.cpp:66] Creating Layer pool1
I0905 20:26:45.957721 32359 net.cpp:329] pool1 <- conv1
I0905 20:26:45.957728 32359 net.cpp:290] pool1 -> pool1
I0905 20:26:45.957741 32359 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:26:45.957746 32359 net.cpp:125] pool1 needs backward computation.
I0905 20:26:45.957754 32359 net.cpp:66] Creating Layer norm1
I0905 20:26:45.957759 32359 net.cpp:329] norm1 <- pool1
I0905 20:26:45.957767 32359 net.cpp:290] norm1 -> norm1
I0905 20:26:45.957777 32359 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:26:45.957783 32359 net.cpp:125] norm1 needs backward computation.
I0905 20:26:45.957792 32359 net.cpp:66] Creating Layer conv2
I0905 20:26:45.957798 32359 net.cpp:329] conv2 <- norm1
I0905 20:26:45.957804 32359 net.cpp:290] conv2 -> conv2
I0905 20:26:45.966986 32359 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:26:45.967001 32359 net.cpp:125] conv2 needs backward computation.
I0905 20:26:45.967010 32359 net.cpp:66] Creating Layer relu2
I0905 20:26:45.967015 32359 net.cpp:329] relu2 <- conv2
I0905 20:26:45.967022 32359 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:26:45.967030 32359 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:26:45.967036 32359 net.cpp:125] relu2 needs backward computation.
I0905 20:26:45.967041 32359 net.cpp:66] Creating Layer pool2
I0905 20:26:45.967047 32359 net.cpp:329] pool2 <- conv2
I0905 20:26:45.967054 32359 net.cpp:290] pool2 -> pool2
I0905 20:26:45.967062 32359 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:26:45.967068 32359 net.cpp:125] pool2 needs backward computation.
I0905 20:26:45.967077 32359 net.cpp:66] Creating Layer fc7
I0905 20:26:45.967083 32359 net.cpp:329] fc7 <- pool2
I0905 20:26:45.967092 32359 net.cpp:290] fc7 -> fc7
I0905 20:26:46.616173 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.616219 32359 net.cpp:125] fc7 needs backward computation.
I0905 20:26:46.616231 32359 net.cpp:66] Creating Layer relu7
I0905 20:26:46.616240 32359 net.cpp:329] relu7 <- fc7
I0905 20:26:46.616250 32359 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:26:46.616260 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.616266 32359 net.cpp:125] relu7 needs backward computation.
I0905 20:26:46.616272 32359 net.cpp:66] Creating Layer drop7
I0905 20:26:46.616278 32359 net.cpp:329] drop7 <- fc7
I0905 20:26:46.616286 32359 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:26:46.616297 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.616302 32359 net.cpp:125] drop7 needs backward computation.
I0905 20:26:46.616312 32359 net.cpp:66] Creating Layer fc8
I0905 20:26:46.616317 32359 net.cpp:329] fc8 <- fc7
I0905 20:26:46.616327 32359 net.cpp:290] fc8 -> fc8
I0905 20:26:46.624352 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.624375 32359 net.cpp:125] fc8 needs backward computation.
I0905 20:26:46.624383 32359 net.cpp:66] Creating Layer relu8
I0905 20:26:46.624388 32359 net.cpp:329] relu8 <- fc8
I0905 20:26:46.624397 32359 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:26:46.624405 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.624410 32359 net.cpp:125] relu8 needs backward computation.
I0905 20:26:46.624418 32359 net.cpp:66] Creating Layer drop8
I0905 20:26:46.624423 32359 net.cpp:329] drop8 <- fc8
I0905 20:26:46.624429 32359 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:26:46.624438 32359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:26:46.624443 32359 net.cpp:125] drop8 needs backward computation.
I0905 20:26:46.624451 32359 net.cpp:66] Creating Layer fc9
I0905 20:26:46.624457 32359 net.cpp:329] fc9 <- fc8
I0905 20:26:46.624465 32359 net.cpp:290] fc9 -> fc9
I0905 20:26:46.624851 32359 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:26:46.624863 32359 net.cpp:125] fc9 needs backward computation.
I0905 20:26:46.624871 32359 net.cpp:66] Creating Layer fc10
I0905 20:26:46.624877 32359 net.cpp:329] fc10 <- fc9
I0905 20:26:46.624886 32359 net.cpp:290] fc10 -> fc10
I0905 20:26:46.624898 32359 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:26:46.624907 32359 net.cpp:125] fc10 needs backward computation.
I0905 20:26:46.624913 32359 net.cpp:66] Creating Layer prob
I0905 20:26:46.624919 32359 net.cpp:329] prob <- fc10
I0905 20:26:46.624927 32359 net.cpp:290] prob -> prob
I0905 20:26:46.624938 32359 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:26:46.624943 32359 net.cpp:125] prob needs backward computation.
I0905 20:26:46.624948 32359 net.cpp:156] This network produces output prob
I0905 20:26:46.624961 32359 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:26:46.624970 32359 net.cpp:167] Network initialization done.
I0905 20:26:46.624975 32359 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 111 inputs.
Done in 67.67 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 11 is out of bounds for axis 0 with size 11
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:27:56.365116 32365 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:27:56.365259 32365 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:27:56.365268 32365 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:27:56.365420 32365 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:27:56.365486 32365 net.cpp:292] Input 0 -> data
I0905 20:27:56.365512 32365 net.cpp:66] Creating Layer conv1
I0905 20:27:56.365520 32365 net.cpp:329] conv1 <- data
I0905 20:27:56.365528 32365 net.cpp:290] conv1 -> conv1
I0905 20:27:56.366942 32365 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:27:56.366962 32365 net.cpp:125] conv1 needs backward computation.
I0905 20:27:56.366971 32365 net.cpp:66] Creating Layer relu1
I0905 20:27:56.366977 32365 net.cpp:329] relu1 <- conv1
I0905 20:27:56.366986 32365 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:27:56.366994 32365 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:27:56.367000 32365 net.cpp:125] relu1 needs backward computation.
I0905 20:27:56.367007 32365 net.cpp:66] Creating Layer pool1
I0905 20:27:56.367013 32365 net.cpp:329] pool1 <- conv1
I0905 20:27:56.367020 32365 net.cpp:290] pool1 -> pool1
I0905 20:27:56.367032 32365 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:27:56.367038 32365 net.cpp:125] pool1 needs backward computation.
I0905 20:27:56.367044 32365 net.cpp:66] Creating Layer norm1
I0905 20:27:56.367050 32365 net.cpp:329] norm1 <- pool1
I0905 20:27:56.367058 32365 net.cpp:290] norm1 -> norm1
I0905 20:27:56.367068 32365 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:27:56.367074 32365 net.cpp:125] norm1 needs backward computation.
I0905 20:27:56.367080 32365 net.cpp:66] Creating Layer conv2
I0905 20:27:56.367086 32365 net.cpp:329] conv2 <- norm1
I0905 20:27:56.367094 32365 net.cpp:290] conv2 -> conv2
I0905 20:27:56.376852 32365 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:27:56.376870 32365 net.cpp:125] conv2 needs backward computation.
I0905 20:27:56.376878 32365 net.cpp:66] Creating Layer relu2
I0905 20:27:56.376885 32365 net.cpp:329] relu2 <- conv2
I0905 20:27:56.376893 32365 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:27:56.376900 32365 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:27:56.376906 32365 net.cpp:125] relu2 needs backward computation.
I0905 20:27:56.376921 32365 net.cpp:66] Creating Layer pool2
I0905 20:27:56.376929 32365 net.cpp:329] pool2 <- conv2
I0905 20:27:56.376935 32365 net.cpp:290] pool2 -> pool2
I0905 20:27:56.376945 32365 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:27:56.376950 32365 net.cpp:125] pool2 needs backward computation.
I0905 20:27:56.376957 32365 net.cpp:66] Creating Layer fc7
I0905 20:27:56.376963 32365 net.cpp:329] fc7 <- pool2
I0905 20:27:56.376971 32365 net.cpp:290] fc7 -> fc7
I0905 20:27:57.034688 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.034734 32365 net.cpp:125] fc7 needs backward computation.
I0905 20:27:57.034746 32365 net.cpp:66] Creating Layer relu7
I0905 20:27:57.034754 32365 net.cpp:329] relu7 <- fc7
I0905 20:27:57.034765 32365 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:27:57.034775 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.034780 32365 net.cpp:125] relu7 needs backward computation.
I0905 20:27:57.034787 32365 net.cpp:66] Creating Layer drop7
I0905 20:27:57.034793 32365 net.cpp:329] drop7 <- fc7
I0905 20:27:57.034800 32365 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:27:57.034811 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.034816 32365 net.cpp:125] drop7 needs backward computation.
I0905 20:27:57.034826 32365 net.cpp:66] Creating Layer fc8
I0905 20:27:57.034831 32365 net.cpp:329] fc8 <- fc7
I0905 20:27:57.034839 32365 net.cpp:290] fc8 -> fc8
I0905 20:27:57.042726 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.042740 32365 net.cpp:125] fc8 needs backward computation.
I0905 20:27:57.042747 32365 net.cpp:66] Creating Layer relu8
I0905 20:27:57.042753 32365 net.cpp:329] relu8 <- fc8
I0905 20:27:57.042762 32365 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:27:57.042770 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.042775 32365 net.cpp:125] relu8 needs backward computation.
I0905 20:27:57.042783 32365 net.cpp:66] Creating Layer drop8
I0905 20:27:57.042788 32365 net.cpp:329] drop8 <- fc8
I0905 20:27:57.042794 32365 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:27:57.042801 32365 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:27:57.042807 32365 net.cpp:125] drop8 needs backward computation.
I0905 20:27:57.042816 32365 net.cpp:66] Creating Layer fc9
I0905 20:27:57.042824 32365 net.cpp:329] fc9 <- fc8
I0905 20:27:57.042830 32365 net.cpp:290] fc9 -> fc9
I0905 20:27:57.043215 32365 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:27:57.043228 32365 net.cpp:125] fc9 needs backward computation.
I0905 20:27:57.043237 32365 net.cpp:66] Creating Layer fc10
I0905 20:27:57.043243 32365 net.cpp:329] fc10 <- fc9
I0905 20:27:57.043251 32365 net.cpp:290] fc10 -> fc10
I0905 20:27:57.043264 32365 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:27:57.043272 32365 net.cpp:125] fc10 needs backward computation.
I0905 20:27:57.043279 32365 net.cpp:66] Creating Layer prob
I0905 20:27:57.043285 32365 net.cpp:329] prob <- fc10
I0905 20:27:57.043293 32365 net.cpp:290] prob -> prob
I0905 20:27:57.043303 32365 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:27:57.043309 32365 net.cpp:125] prob needs backward computation.
I0905 20:27:57.043314 32365 net.cpp:156] This network produces output prob
I0905 20:27:57.043328 32365 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:27:57.043336 32365 net.cpp:167] Network initialization done.
I0905 20:27:57.043341 32365 net.cpp:168] Memory required for data: 6183480
Classifying 100 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:28:03.101013 32369 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:28:03.101169 32369 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:28:03.101178 32369 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:28:03.101325 32369 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:28:03.101377 32369 net.cpp:292] Input 0 -> data
I0905 20:28:03.101402 32369 net.cpp:66] Creating Layer conv1
I0905 20:28:03.101409 32369 net.cpp:329] conv1 <- data
I0905 20:28:03.101418 32369 net.cpp:290] conv1 -> conv1
I0905 20:28:03.102792 32369 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:28:03.102810 32369 net.cpp:125] conv1 needs backward computation.
I0905 20:28:03.102819 32369 net.cpp:66] Creating Layer relu1
I0905 20:28:03.102825 32369 net.cpp:329] relu1 <- conv1
I0905 20:28:03.102838 32369 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:28:03.102846 32369 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:28:03.102852 32369 net.cpp:125] relu1 needs backward computation.
I0905 20:28:03.102859 32369 net.cpp:66] Creating Layer pool1
I0905 20:28:03.102864 32369 net.cpp:329] pool1 <- conv1
I0905 20:28:03.102871 32369 net.cpp:290] pool1 -> pool1
I0905 20:28:03.102882 32369 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:28:03.102888 32369 net.cpp:125] pool1 needs backward computation.
I0905 20:28:03.102895 32369 net.cpp:66] Creating Layer norm1
I0905 20:28:03.102900 32369 net.cpp:329] norm1 <- pool1
I0905 20:28:03.102906 32369 net.cpp:290] norm1 -> norm1
I0905 20:28:03.102916 32369 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:28:03.102922 32369 net.cpp:125] norm1 needs backward computation.
I0905 20:28:03.102929 32369 net.cpp:66] Creating Layer conv2
I0905 20:28:03.102936 32369 net.cpp:329] conv2 <- norm1
I0905 20:28:03.102942 32369 net.cpp:290] conv2 -> conv2
I0905 20:28:03.112193 32369 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:28:03.112210 32369 net.cpp:125] conv2 needs backward computation.
I0905 20:28:03.112238 32369 net.cpp:66] Creating Layer relu2
I0905 20:28:03.112252 32369 net.cpp:329] relu2 <- conv2
I0905 20:28:03.112259 32369 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:28:03.112267 32369 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:28:03.112273 32369 net.cpp:125] relu2 needs backward computation.
I0905 20:28:03.112280 32369 net.cpp:66] Creating Layer pool2
I0905 20:28:03.112285 32369 net.cpp:329] pool2 <- conv2
I0905 20:28:03.112293 32369 net.cpp:290] pool2 -> pool2
I0905 20:28:03.112300 32369 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:28:03.112306 32369 net.cpp:125] pool2 needs backward computation.
I0905 20:28:03.112316 32369 net.cpp:66] Creating Layer fc7
I0905 20:28:03.112323 32369 net.cpp:329] fc7 <- pool2
I0905 20:28:03.112329 32369 net.cpp:290] fc7 -> fc7
I0905 20:28:03.770441 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.770486 32369 net.cpp:125] fc7 needs backward computation.
I0905 20:28:03.770499 32369 net.cpp:66] Creating Layer relu7
I0905 20:28:03.770508 32369 net.cpp:329] relu7 <- fc7
I0905 20:28:03.770516 32369 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:28:03.770527 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.770534 32369 net.cpp:125] relu7 needs backward computation.
I0905 20:28:03.770541 32369 net.cpp:66] Creating Layer drop7
I0905 20:28:03.770546 32369 net.cpp:329] drop7 <- fc7
I0905 20:28:03.770553 32369 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:28:03.770565 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.770570 32369 net.cpp:125] drop7 needs backward computation.
I0905 20:28:03.770578 32369 net.cpp:66] Creating Layer fc8
I0905 20:28:03.770583 32369 net.cpp:329] fc8 <- fc7
I0905 20:28:03.770593 32369 net.cpp:290] fc8 -> fc8
I0905 20:28:03.778672 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.778687 32369 net.cpp:125] fc8 needs backward computation.
I0905 20:28:03.778693 32369 net.cpp:66] Creating Layer relu8
I0905 20:28:03.778699 32369 net.cpp:329] relu8 <- fc8
I0905 20:28:03.778707 32369 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:28:03.778715 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.778722 32369 net.cpp:125] relu8 needs backward computation.
I0905 20:28:03.778728 32369 net.cpp:66] Creating Layer drop8
I0905 20:28:03.778733 32369 net.cpp:329] drop8 <- fc8
I0905 20:28:03.778740 32369 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:28:03.778748 32369 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:28:03.778753 32369 net.cpp:125] drop8 needs backward computation.
I0905 20:28:03.778762 32369 net.cpp:66] Creating Layer fc9
I0905 20:28:03.778769 32369 net.cpp:329] fc9 <- fc8
I0905 20:28:03.778775 32369 net.cpp:290] fc9 -> fc9
I0905 20:28:03.779160 32369 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:28:03.779172 32369 net.cpp:125] fc9 needs backward computation.
I0905 20:28:03.779181 32369 net.cpp:66] Creating Layer fc10
I0905 20:28:03.779196 32369 net.cpp:329] fc10 <- fc9
I0905 20:28:03.779206 32369 net.cpp:290] fc10 -> fc10
I0905 20:28:03.779218 32369 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:28:03.779227 32369 net.cpp:125] fc10 needs backward computation.
I0905 20:28:03.779232 32369 net.cpp:66] Creating Layer prob
I0905 20:28:03.779238 32369 net.cpp:329] prob <- fc10
I0905 20:28:03.779247 32369 net.cpp:290] prob -> prob
I0905 20:28:03.779256 32369 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:28:03.779263 32369 net.cpp:125] prob needs backward computation.
I0905 20:28:03.779268 32369 net.cpp:156] This network produces output prob
I0905 20:28:03.779279 32369 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:28:03.779289 32369 net.cpp:167] Network initialization done.
I0905 20:28:03.779294 32369 net.cpp:168] Memory required for data: 6183480
Classifying 176 inputs.
Done in 121.53 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 76 is out of bounds for axis 0 with size 76
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:30:09.690822 32376 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:30:09.690961 32376 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:30:09.690970 32376 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:30:09.691114 32376 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:30:09.691176 32376 net.cpp:292] Input 0 -> data
I0905 20:30:09.691202 32376 net.cpp:66] Creating Layer conv1
I0905 20:30:09.691210 32376 net.cpp:329] conv1 <- data
I0905 20:30:09.691217 32376 net.cpp:290] conv1 -> conv1
I0905 20:30:09.692577 32376 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:30:09.692595 32376 net.cpp:125] conv1 needs backward computation.
I0905 20:30:09.692605 32376 net.cpp:66] Creating Layer relu1
I0905 20:30:09.692610 32376 net.cpp:329] relu1 <- conv1
I0905 20:30:09.692617 32376 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:30:09.692625 32376 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:30:09.692631 32376 net.cpp:125] relu1 needs backward computation.
I0905 20:30:09.692638 32376 net.cpp:66] Creating Layer pool1
I0905 20:30:09.692643 32376 net.cpp:329] pool1 <- conv1
I0905 20:30:09.692651 32376 net.cpp:290] pool1 -> pool1
I0905 20:30:09.692662 32376 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:30:09.692667 32376 net.cpp:125] pool1 needs backward computation.
I0905 20:30:09.692674 32376 net.cpp:66] Creating Layer norm1
I0905 20:30:09.692679 32376 net.cpp:329] norm1 <- pool1
I0905 20:30:09.692687 32376 net.cpp:290] norm1 -> norm1
I0905 20:30:09.692697 32376 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:30:09.692702 32376 net.cpp:125] norm1 needs backward computation.
I0905 20:30:09.692709 32376 net.cpp:66] Creating Layer conv2
I0905 20:30:09.692715 32376 net.cpp:329] conv2 <- norm1
I0905 20:30:09.692723 32376 net.cpp:290] conv2 -> conv2
I0905 20:30:09.702090 32376 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:30:09.702105 32376 net.cpp:125] conv2 needs backward computation.
I0905 20:30:09.702113 32376 net.cpp:66] Creating Layer relu2
I0905 20:30:09.702119 32376 net.cpp:329] relu2 <- conv2
I0905 20:30:09.702126 32376 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:30:09.702133 32376 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:30:09.702139 32376 net.cpp:125] relu2 needs backward computation.
I0905 20:30:09.702147 32376 net.cpp:66] Creating Layer pool2
I0905 20:30:09.702152 32376 net.cpp:329] pool2 <- conv2
I0905 20:30:09.702158 32376 net.cpp:290] pool2 -> pool2
I0905 20:30:09.702167 32376 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:30:09.702172 32376 net.cpp:125] pool2 needs backward computation.
I0905 20:30:09.702183 32376 net.cpp:66] Creating Layer fc7
I0905 20:30:09.702188 32376 net.cpp:329] fc7 <- pool2
I0905 20:30:09.702195 32376 net.cpp:290] fc7 -> fc7
I0905 20:30:10.351600 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.351639 32376 net.cpp:125] fc7 needs backward computation.
I0905 20:30:10.351651 32376 net.cpp:66] Creating Layer relu7
I0905 20:30:10.351658 32376 net.cpp:329] relu7 <- fc7
I0905 20:30:10.351667 32376 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:30:10.351677 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.351683 32376 net.cpp:125] relu7 needs backward computation.
I0905 20:30:10.351691 32376 net.cpp:66] Creating Layer drop7
I0905 20:30:10.351696 32376 net.cpp:329] drop7 <- fc7
I0905 20:30:10.351701 32376 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:30:10.351712 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.351718 32376 net.cpp:125] drop7 needs backward computation.
I0905 20:30:10.351737 32376 net.cpp:66] Creating Layer fc8
I0905 20:30:10.351742 32376 net.cpp:329] fc8 <- fc7
I0905 20:30:10.351753 32376 net.cpp:290] fc8 -> fc8
I0905 20:30:10.359649 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.359663 32376 net.cpp:125] fc8 needs backward computation.
I0905 20:30:10.359669 32376 net.cpp:66] Creating Layer relu8
I0905 20:30:10.359675 32376 net.cpp:329] relu8 <- fc8
I0905 20:30:10.359683 32376 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:30:10.359691 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.359697 32376 net.cpp:125] relu8 needs backward computation.
I0905 20:30:10.359704 32376 net.cpp:66] Creating Layer drop8
I0905 20:30:10.359709 32376 net.cpp:329] drop8 <- fc8
I0905 20:30:10.359715 32376 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:30:10.359722 32376 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:30:10.359729 32376 net.cpp:125] drop8 needs backward computation.
I0905 20:30:10.359736 32376 net.cpp:66] Creating Layer fc9
I0905 20:30:10.359742 32376 net.cpp:329] fc9 <- fc8
I0905 20:30:10.359750 32376 net.cpp:290] fc9 -> fc9
I0905 20:30:10.360136 32376 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:30:10.360147 32376 net.cpp:125] fc9 needs backward computation.
I0905 20:30:10.360155 32376 net.cpp:66] Creating Layer fc10
I0905 20:30:10.360162 32376 net.cpp:329] fc10 <- fc9
I0905 20:30:10.360170 32376 net.cpp:290] fc10 -> fc10
I0905 20:30:10.360183 32376 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:30:10.360190 32376 net.cpp:125] fc10 needs backward computation.
I0905 20:30:10.360198 32376 net.cpp:66] Creating Layer prob
I0905 20:30:10.360203 32376 net.cpp:329] prob <- fc10
I0905 20:30:10.360210 32376 net.cpp:290] prob -> prob
I0905 20:30:10.360220 32376 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:30:10.360226 32376 net.cpp:125] prob needs backward computation.
I0905 20:30:10.360231 32376 net.cpp:156] This network produces output prob
I0905 20:30:10.360244 32376 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:30:10.360252 32376 net.cpp:167] Network initialization done.
I0905 20:30:10.360258 32376 net.cpp:168] Memory required for data: 6183480
Classifying 171 inputs.
Done in 114.05 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 71 is out of bounds for axis 0 with size 71
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:32:08.237625 32383 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:32:08.237763 32383 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:32:08.237773 32383 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:32:08.237931 32383 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:32:08.238003 32383 net.cpp:292] Input 0 -> data
I0905 20:32:08.238029 32383 net.cpp:66] Creating Layer conv1
I0905 20:32:08.238036 32383 net.cpp:329] conv1 <- data
I0905 20:32:08.238044 32383 net.cpp:290] conv1 -> conv1
I0905 20:32:08.239408 32383 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:32:08.239425 32383 net.cpp:125] conv1 needs backward computation.
I0905 20:32:08.239434 32383 net.cpp:66] Creating Layer relu1
I0905 20:32:08.239440 32383 net.cpp:329] relu1 <- conv1
I0905 20:32:08.239447 32383 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:32:08.239455 32383 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:32:08.239461 32383 net.cpp:125] relu1 needs backward computation.
I0905 20:32:08.239469 32383 net.cpp:66] Creating Layer pool1
I0905 20:32:08.239473 32383 net.cpp:329] pool1 <- conv1
I0905 20:32:08.239480 32383 net.cpp:290] pool1 -> pool1
I0905 20:32:08.239491 32383 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:32:08.239497 32383 net.cpp:125] pool1 needs backward computation.
I0905 20:32:08.239505 32383 net.cpp:66] Creating Layer norm1
I0905 20:32:08.239511 32383 net.cpp:329] norm1 <- pool1
I0905 20:32:08.239517 32383 net.cpp:290] norm1 -> norm1
I0905 20:32:08.239527 32383 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:32:08.239533 32383 net.cpp:125] norm1 needs backward computation.
I0905 20:32:08.239541 32383 net.cpp:66] Creating Layer conv2
I0905 20:32:08.239547 32383 net.cpp:329] conv2 <- norm1
I0905 20:32:08.239553 32383 net.cpp:290] conv2 -> conv2
I0905 20:32:08.248724 32383 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:32:08.248740 32383 net.cpp:125] conv2 needs backward computation.
I0905 20:32:08.248747 32383 net.cpp:66] Creating Layer relu2
I0905 20:32:08.248754 32383 net.cpp:329] relu2 <- conv2
I0905 20:32:08.248760 32383 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:32:08.248767 32383 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:32:08.248774 32383 net.cpp:125] relu2 needs backward computation.
I0905 20:32:08.248788 32383 net.cpp:66] Creating Layer pool2
I0905 20:32:08.248795 32383 net.cpp:329] pool2 <- conv2
I0905 20:32:08.248801 32383 net.cpp:290] pool2 -> pool2
I0905 20:32:08.248810 32383 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:32:08.248816 32383 net.cpp:125] pool2 needs backward computation.
I0905 20:32:08.248824 32383 net.cpp:66] Creating Layer fc7
I0905 20:32:08.248831 32383 net.cpp:329] fc7 <- pool2
I0905 20:32:08.248837 32383 net.cpp:290] fc7 -> fc7
I0905 20:32:08.895782 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.895828 32383 net.cpp:125] fc7 needs backward computation.
I0905 20:32:08.895840 32383 net.cpp:66] Creating Layer relu7
I0905 20:32:08.895848 32383 net.cpp:329] relu7 <- fc7
I0905 20:32:08.895858 32383 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:32:08.895867 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.895872 32383 net.cpp:125] relu7 needs backward computation.
I0905 20:32:08.895880 32383 net.cpp:66] Creating Layer drop7
I0905 20:32:08.895885 32383 net.cpp:329] drop7 <- fc7
I0905 20:32:08.895892 32383 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:32:08.895903 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.895908 32383 net.cpp:125] drop7 needs backward computation.
I0905 20:32:08.895917 32383 net.cpp:66] Creating Layer fc8
I0905 20:32:08.895922 32383 net.cpp:329] fc8 <- fc7
I0905 20:32:08.895932 32383 net.cpp:290] fc8 -> fc8
I0905 20:32:08.903723 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.903734 32383 net.cpp:125] fc8 needs backward computation.
I0905 20:32:08.903741 32383 net.cpp:66] Creating Layer relu8
I0905 20:32:08.903748 32383 net.cpp:329] relu8 <- fc8
I0905 20:32:08.903755 32383 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:32:08.903762 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.903769 32383 net.cpp:125] relu8 needs backward computation.
I0905 20:32:08.903775 32383 net.cpp:66] Creating Layer drop8
I0905 20:32:08.903780 32383 net.cpp:329] drop8 <- fc8
I0905 20:32:08.903786 32383 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:32:08.903794 32383 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:32:08.903800 32383 net.cpp:125] drop8 needs backward computation.
I0905 20:32:08.903808 32383 net.cpp:66] Creating Layer fc9
I0905 20:32:08.903813 32383 net.cpp:329] fc9 <- fc8
I0905 20:32:08.903820 32383 net.cpp:290] fc9 -> fc9
I0905 20:32:08.904194 32383 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:32:08.904206 32383 net.cpp:125] fc9 needs backward computation.
I0905 20:32:08.904214 32383 net.cpp:66] Creating Layer fc10
I0905 20:32:08.904220 32383 net.cpp:329] fc10 <- fc9
I0905 20:32:08.904228 32383 net.cpp:290] fc10 -> fc10
I0905 20:32:08.904240 32383 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:32:08.904248 32383 net.cpp:125] fc10 needs backward computation.
I0905 20:32:08.904255 32383 net.cpp:66] Creating Layer prob
I0905 20:32:08.904260 32383 net.cpp:329] prob <- fc10
I0905 20:32:08.904268 32383 net.cpp:290] prob -> prob
I0905 20:32:08.904278 32383 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:32:08.904284 32383 net.cpp:125] prob needs backward computation.
I0905 20:32:08.904289 32383 net.cpp:156] This network produces output prob
I0905 20:32:08.904301 32383 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:32:08.904310 32383 net.cpp:167] Network initialization done.
I0905 20:32:08.904315 32383 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 325 inputs.
Done in 206.19 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 25 is out of bounds for axis 0 with size 25
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:35:39.769171 32391 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:35:39.769326 32391 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:35:39.769336 32391 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:35:39.769487 32391 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:35:39.769541 32391 net.cpp:292] Input 0 -> data
I0905 20:35:39.769568 32391 net.cpp:66] Creating Layer conv1
I0905 20:35:39.769575 32391 net.cpp:329] conv1 <- data
I0905 20:35:39.769601 32391 net.cpp:290] conv1 -> conv1
I0905 20:35:39.771008 32391 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:35:39.771026 32391 net.cpp:125] conv1 needs backward computation.
I0905 20:35:39.771035 32391 net.cpp:66] Creating Layer relu1
I0905 20:35:39.771042 32391 net.cpp:329] relu1 <- conv1
I0905 20:35:39.771054 32391 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:35:39.771064 32391 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:35:39.771070 32391 net.cpp:125] relu1 needs backward computation.
I0905 20:35:39.771077 32391 net.cpp:66] Creating Layer pool1
I0905 20:35:39.771083 32391 net.cpp:329] pool1 <- conv1
I0905 20:35:39.771090 32391 net.cpp:290] pool1 -> pool1
I0905 20:35:39.771102 32391 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:35:39.771108 32391 net.cpp:125] pool1 needs backward computation.
I0905 20:35:39.771116 32391 net.cpp:66] Creating Layer norm1
I0905 20:35:39.771121 32391 net.cpp:329] norm1 <- pool1
I0905 20:35:39.771128 32391 net.cpp:290] norm1 -> norm1
I0905 20:35:39.771138 32391 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:35:39.771144 32391 net.cpp:125] norm1 needs backward computation.
I0905 20:35:39.771152 32391 net.cpp:66] Creating Layer conv2
I0905 20:35:39.771158 32391 net.cpp:329] conv2 <- norm1
I0905 20:35:39.771165 32391 net.cpp:290] conv2 -> conv2
I0905 20:35:39.780560 32391 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:35:39.780575 32391 net.cpp:125] conv2 needs backward computation.
I0905 20:35:39.780583 32391 net.cpp:66] Creating Layer relu2
I0905 20:35:39.780589 32391 net.cpp:329] relu2 <- conv2
I0905 20:35:39.780596 32391 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:35:39.780603 32391 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:35:39.780609 32391 net.cpp:125] relu2 needs backward computation.
I0905 20:35:39.780619 32391 net.cpp:66] Creating Layer pool2
I0905 20:35:39.780625 32391 net.cpp:329] pool2 <- conv2
I0905 20:35:39.780632 32391 net.cpp:290] pool2 -> pool2
I0905 20:35:39.780640 32391 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:35:39.780647 32391 net.cpp:125] pool2 needs backward computation.
I0905 20:35:39.780654 32391 net.cpp:66] Creating Layer fc7
I0905 20:35:39.780659 32391 net.cpp:329] fc7 <- pool2
I0905 20:35:39.780668 32391 net.cpp:290] fc7 -> fc7
I0905 20:35:40.429292 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.429337 32391 net.cpp:125] fc7 needs backward computation.
I0905 20:35:40.429350 32391 net.cpp:66] Creating Layer relu7
I0905 20:35:40.429358 32391 net.cpp:329] relu7 <- fc7
I0905 20:35:40.429366 32391 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:35:40.429378 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.429383 32391 net.cpp:125] relu7 needs backward computation.
I0905 20:35:40.429390 32391 net.cpp:66] Creating Layer drop7
I0905 20:35:40.429396 32391 net.cpp:329] drop7 <- fc7
I0905 20:35:40.429402 32391 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:35:40.429414 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.429420 32391 net.cpp:125] drop7 needs backward computation.
I0905 20:35:40.429429 32391 net.cpp:66] Creating Layer fc8
I0905 20:35:40.429433 32391 net.cpp:329] fc8 <- fc7
I0905 20:35:40.429442 32391 net.cpp:290] fc8 -> fc8
I0905 20:35:40.437268 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.437280 32391 net.cpp:125] fc8 needs backward computation.
I0905 20:35:40.437288 32391 net.cpp:66] Creating Layer relu8
I0905 20:35:40.437294 32391 net.cpp:329] relu8 <- fc8
I0905 20:35:40.437301 32391 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:35:40.437309 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.437314 32391 net.cpp:125] relu8 needs backward computation.
I0905 20:35:40.437321 32391 net.cpp:66] Creating Layer drop8
I0905 20:35:40.437326 32391 net.cpp:329] drop8 <- fc8
I0905 20:35:40.437333 32391 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:35:40.437340 32391 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:35:40.437346 32391 net.cpp:125] drop8 needs backward computation.
I0905 20:35:40.437355 32391 net.cpp:66] Creating Layer fc9
I0905 20:35:40.437361 32391 net.cpp:329] fc9 <- fc8
I0905 20:35:40.437367 32391 net.cpp:290] fc9 -> fc9
I0905 20:35:40.437743 32391 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:35:40.437757 32391 net.cpp:125] fc9 needs backward computation.
I0905 20:35:40.437767 32391 net.cpp:66] Creating Layer fc10
I0905 20:35:40.437782 32391 net.cpp:329] fc10 <- fc9
I0905 20:35:40.437790 32391 net.cpp:290] fc10 -> fc10
I0905 20:35:40.437803 32391 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:35:40.437811 32391 net.cpp:125] fc10 needs backward computation.
I0905 20:35:40.437818 32391 net.cpp:66] Creating Layer prob
I0905 20:35:40.437824 32391 net.cpp:329] prob <- fc10
I0905 20:35:40.437831 32391 net.cpp:290] prob -> prob
I0905 20:35:40.437840 32391 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:35:40.437846 32391 net.cpp:125] prob needs backward computation.
I0905 20:35:40.437851 32391 net.cpp:156] This network produces output prob
I0905 20:35:40.437865 32391 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:35:40.437873 32391 net.cpp:167] Network initialization done.
I0905 20:35:40.437878 32391 net.cpp:168] Memory required for data: 6183480
Classifying 278 inputs.
Done in 170.20 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 78 is out of bounds for axis 0 with size 78
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:38:39.968369 32399 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:38:39.968508 32399 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:38:39.968516 32399 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:38:39.968664 32399 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:38:39.968726 32399 net.cpp:292] Input 0 -> data
I0905 20:38:39.968752 32399 net.cpp:66] Creating Layer conv1
I0905 20:38:39.968758 32399 net.cpp:329] conv1 <- data
I0905 20:38:39.968767 32399 net.cpp:290] conv1 -> conv1
I0905 20:38:39.970159 32399 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:38:39.970177 32399 net.cpp:125] conv1 needs backward computation.
I0905 20:38:39.970186 32399 net.cpp:66] Creating Layer relu1
I0905 20:38:39.970192 32399 net.cpp:329] relu1 <- conv1
I0905 20:38:39.970198 32399 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:38:39.970207 32399 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:38:39.970213 32399 net.cpp:125] relu1 needs backward computation.
I0905 20:38:39.970221 32399 net.cpp:66] Creating Layer pool1
I0905 20:38:39.970226 32399 net.cpp:329] pool1 <- conv1
I0905 20:38:39.970232 32399 net.cpp:290] pool1 -> pool1
I0905 20:38:39.970243 32399 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:38:39.970249 32399 net.cpp:125] pool1 needs backward computation.
I0905 20:38:39.970255 32399 net.cpp:66] Creating Layer norm1
I0905 20:38:39.970262 32399 net.cpp:329] norm1 <- pool1
I0905 20:38:39.970268 32399 net.cpp:290] norm1 -> norm1
I0905 20:38:39.970278 32399 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:38:39.970283 32399 net.cpp:125] norm1 needs backward computation.
I0905 20:38:39.970290 32399 net.cpp:66] Creating Layer conv2
I0905 20:38:39.970296 32399 net.cpp:329] conv2 <- norm1
I0905 20:38:39.970304 32399 net.cpp:290] conv2 -> conv2
I0905 20:38:39.979426 32399 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:38:39.979441 32399 net.cpp:125] conv2 needs backward computation.
I0905 20:38:39.979449 32399 net.cpp:66] Creating Layer relu2
I0905 20:38:39.979454 32399 net.cpp:329] relu2 <- conv2
I0905 20:38:39.979461 32399 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:38:39.979468 32399 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:38:39.979473 32399 net.cpp:125] relu2 needs backward computation.
I0905 20:38:39.979480 32399 net.cpp:66] Creating Layer pool2
I0905 20:38:39.979485 32399 net.cpp:329] pool2 <- conv2
I0905 20:38:39.979491 32399 net.cpp:290] pool2 -> pool2
I0905 20:38:39.979500 32399 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:38:39.979506 32399 net.cpp:125] pool2 needs backward computation.
I0905 20:38:39.979514 32399 net.cpp:66] Creating Layer fc7
I0905 20:38:39.979521 32399 net.cpp:329] fc7 <- pool2
I0905 20:38:39.979527 32399 net.cpp:290] fc7 -> fc7
I0905 20:38:40.628949 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.628995 32399 net.cpp:125] fc7 needs backward computation.
I0905 20:38:40.629009 32399 net.cpp:66] Creating Layer relu7
I0905 20:38:40.629015 32399 net.cpp:329] relu7 <- fc7
I0905 20:38:40.629024 32399 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:38:40.629035 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.629041 32399 net.cpp:125] relu7 needs backward computation.
I0905 20:38:40.629048 32399 net.cpp:66] Creating Layer drop7
I0905 20:38:40.629055 32399 net.cpp:329] drop7 <- fc7
I0905 20:38:40.629060 32399 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:38:40.629071 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.629077 32399 net.cpp:125] drop7 needs backward computation.
I0905 20:38:40.629096 32399 net.cpp:66] Creating Layer fc8
I0905 20:38:40.629102 32399 net.cpp:329] fc8 <- fc7
I0905 20:38:40.629112 32399 net.cpp:290] fc8 -> fc8
I0905 20:38:40.637060 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.637073 32399 net.cpp:125] fc8 needs backward computation.
I0905 20:38:40.637079 32399 net.cpp:66] Creating Layer relu8
I0905 20:38:40.637085 32399 net.cpp:329] relu8 <- fc8
I0905 20:38:40.637094 32399 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:38:40.637100 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.637106 32399 net.cpp:125] relu8 needs backward computation.
I0905 20:38:40.637112 32399 net.cpp:66] Creating Layer drop8
I0905 20:38:40.637117 32399 net.cpp:329] drop8 <- fc8
I0905 20:38:40.637125 32399 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:38:40.637130 32399 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:38:40.637136 32399 net.cpp:125] drop8 needs backward computation.
I0905 20:38:40.637145 32399 net.cpp:66] Creating Layer fc9
I0905 20:38:40.637151 32399 net.cpp:329] fc9 <- fc8
I0905 20:38:40.637157 32399 net.cpp:290] fc9 -> fc9
I0905 20:38:40.637529 32399 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:38:40.637542 32399 net.cpp:125] fc9 needs backward computation.
I0905 20:38:40.637549 32399 net.cpp:66] Creating Layer fc10
I0905 20:38:40.637554 32399 net.cpp:329] fc10 <- fc9
I0905 20:38:40.637563 32399 net.cpp:290] fc10 -> fc10
I0905 20:38:40.637574 32399 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:38:40.637590 32399 net.cpp:125] fc10 needs backward computation.
I0905 20:38:40.637598 32399 net.cpp:66] Creating Layer prob
I0905 20:38:40.637603 32399 net.cpp:329] prob <- fc10
I0905 20:38:40.637610 32399 net.cpp:290] prob -> prob
I0905 20:38:40.637620 32399 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:38:40.637626 32399 net.cpp:125] prob needs backward computation.
I0905 20:38:40.637631 32399 net.cpp:156] This network produces output prob
I0905 20:38:40.637644 32399 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:38:40.637652 32399 net.cpp:167] Network initialization done.
I0905 20:38:40.637656 32399 net.cpp:168] Memory required for data: 6183480
Classifying 719 inputs.
Done in 442.85 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 19 is out of bounds for axis 0 with size 19
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:46:20.581956 32424 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:46:20.582098 32424 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:46:20.582106 32424 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:46:20.582253 32424 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:46:20.582314 32424 net.cpp:292] Input 0 -> data
I0905 20:46:20.582340 32424 net.cpp:66] Creating Layer conv1
I0905 20:46:20.582347 32424 net.cpp:329] conv1 <- data
I0905 20:46:20.582355 32424 net.cpp:290] conv1 -> conv1
I0905 20:46:20.583716 32424 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:46:20.583734 32424 net.cpp:125] conv1 needs backward computation.
I0905 20:46:20.583742 32424 net.cpp:66] Creating Layer relu1
I0905 20:46:20.583748 32424 net.cpp:329] relu1 <- conv1
I0905 20:46:20.583755 32424 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:46:20.583765 32424 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:46:20.583770 32424 net.cpp:125] relu1 needs backward computation.
I0905 20:46:20.583776 32424 net.cpp:66] Creating Layer pool1
I0905 20:46:20.583782 32424 net.cpp:329] pool1 <- conv1
I0905 20:46:20.583789 32424 net.cpp:290] pool1 -> pool1
I0905 20:46:20.583801 32424 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:46:20.583806 32424 net.cpp:125] pool1 needs backward computation.
I0905 20:46:20.583812 32424 net.cpp:66] Creating Layer norm1
I0905 20:46:20.583818 32424 net.cpp:329] norm1 <- pool1
I0905 20:46:20.583824 32424 net.cpp:290] norm1 -> norm1
I0905 20:46:20.583834 32424 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:46:20.583839 32424 net.cpp:125] norm1 needs backward computation.
I0905 20:46:20.583847 32424 net.cpp:66] Creating Layer conv2
I0905 20:46:20.583853 32424 net.cpp:329] conv2 <- norm1
I0905 20:46:20.583859 32424 net.cpp:290] conv2 -> conv2
I0905 20:46:20.592973 32424 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:46:20.592989 32424 net.cpp:125] conv2 needs backward computation.
I0905 20:46:20.592996 32424 net.cpp:66] Creating Layer relu2
I0905 20:46:20.593003 32424 net.cpp:329] relu2 <- conv2
I0905 20:46:20.593008 32424 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:46:20.593015 32424 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:46:20.593026 32424 net.cpp:125] relu2 needs backward computation.
I0905 20:46:20.593034 32424 net.cpp:66] Creating Layer pool2
I0905 20:46:20.593039 32424 net.cpp:329] pool2 <- conv2
I0905 20:46:20.593045 32424 net.cpp:290] pool2 -> pool2
I0905 20:46:20.593053 32424 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:46:20.593058 32424 net.cpp:125] pool2 needs backward computation.
I0905 20:46:20.593067 32424 net.cpp:66] Creating Layer fc7
I0905 20:46:20.593073 32424 net.cpp:329] fc7 <- pool2
I0905 20:46:20.593080 32424 net.cpp:290] fc7 -> fc7
I0905 20:46:21.242496 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.242547 32424 net.cpp:125] fc7 needs backward computation.
I0905 20:46:21.242559 32424 net.cpp:66] Creating Layer relu7
I0905 20:46:21.242566 32424 net.cpp:329] relu7 <- fc7
I0905 20:46:21.242575 32424 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:46:21.242586 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.242593 32424 net.cpp:125] relu7 needs backward computation.
I0905 20:46:21.242599 32424 net.cpp:66] Creating Layer drop7
I0905 20:46:21.242604 32424 net.cpp:329] drop7 <- fc7
I0905 20:46:21.242611 32424 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:46:21.242621 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.242627 32424 net.cpp:125] drop7 needs backward computation.
I0905 20:46:21.242635 32424 net.cpp:66] Creating Layer fc8
I0905 20:46:21.242641 32424 net.cpp:329] fc8 <- fc7
I0905 20:46:21.242650 32424 net.cpp:290] fc8 -> fc8
I0905 20:46:21.250413 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.250427 32424 net.cpp:125] fc8 needs backward computation.
I0905 20:46:21.250433 32424 net.cpp:66] Creating Layer relu8
I0905 20:46:21.250439 32424 net.cpp:329] relu8 <- fc8
I0905 20:46:21.250447 32424 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:46:21.250454 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.250460 32424 net.cpp:125] relu8 needs backward computation.
I0905 20:46:21.250466 32424 net.cpp:66] Creating Layer drop8
I0905 20:46:21.250471 32424 net.cpp:329] drop8 <- fc8
I0905 20:46:21.250478 32424 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:46:21.250484 32424 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:21.250490 32424 net.cpp:125] drop8 needs backward computation.
I0905 20:46:21.250499 32424 net.cpp:66] Creating Layer fc9
I0905 20:46:21.250504 32424 net.cpp:329] fc9 <- fc8
I0905 20:46:21.250511 32424 net.cpp:290] fc9 -> fc9
I0905 20:46:21.250885 32424 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:46:21.250896 32424 net.cpp:125] fc9 needs backward computation.
I0905 20:46:21.250905 32424 net.cpp:66] Creating Layer fc10
I0905 20:46:21.250910 32424 net.cpp:329] fc10 <- fc9
I0905 20:46:21.250919 32424 net.cpp:290] fc10 -> fc10
I0905 20:46:21.250931 32424 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:46:21.250938 32424 net.cpp:125] fc10 needs backward computation.
I0905 20:46:21.250946 32424 net.cpp:66] Creating Layer prob
I0905 20:46:21.250951 32424 net.cpp:329] prob <- fc10
I0905 20:46:21.250958 32424 net.cpp:290] prob -> prob
I0905 20:46:21.250968 32424 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:46:21.250974 32424 net.cpp:125] prob needs backward computation.
I0905 20:46:21.250979 32424 net.cpp:156] This network produces output prob
I0905 20:46:21.250991 32424 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:46:21.251000 32424 net.cpp:167] Network initialization done.
I0905 20:46:21.251005 32424 net.cpp:168] Memory required for data: 6183480
Classifying 36 inputs.
Done in 22.00 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:46:44.688666 32430 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:46:44.688803 32430 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:46:44.688812 32430 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:46:44.688971 32430 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:46:44.689023 32430 net.cpp:292] Input 0 -> data
I0905 20:46:44.689049 32430 net.cpp:66] Creating Layer conv1
I0905 20:46:44.689055 32430 net.cpp:329] conv1 <- data
I0905 20:46:44.689064 32430 net.cpp:290] conv1 -> conv1
I0905 20:46:44.690435 32430 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:46:44.690454 32430 net.cpp:125] conv1 needs backward computation.
I0905 20:46:44.690464 32430 net.cpp:66] Creating Layer relu1
I0905 20:46:44.690469 32430 net.cpp:329] relu1 <- conv1
I0905 20:46:44.690476 32430 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:46:44.690485 32430 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:46:44.690491 32430 net.cpp:125] relu1 needs backward computation.
I0905 20:46:44.690497 32430 net.cpp:66] Creating Layer pool1
I0905 20:46:44.690503 32430 net.cpp:329] pool1 <- conv1
I0905 20:46:44.690510 32430 net.cpp:290] pool1 -> pool1
I0905 20:46:44.690526 32430 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:46:44.690531 32430 net.cpp:125] pool1 needs backward computation.
I0905 20:46:44.690538 32430 net.cpp:66] Creating Layer norm1
I0905 20:46:44.690544 32430 net.cpp:329] norm1 <- pool1
I0905 20:46:44.690551 32430 net.cpp:290] norm1 -> norm1
I0905 20:46:44.690560 32430 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:46:44.690567 32430 net.cpp:125] norm1 needs backward computation.
I0905 20:46:44.690573 32430 net.cpp:66] Creating Layer conv2
I0905 20:46:44.690578 32430 net.cpp:329] conv2 <- norm1
I0905 20:46:44.690585 32430 net.cpp:290] conv2 -> conv2
I0905 20:46:44.699717 32430 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:46:44.699733 32430 net.cpp:125] conv2 needs backward computation.
I0905 20:46:44.699741 32430 net.cpp:66] Creating Layer relu2
I0905 20:46:44.699746 32430 net.cpp:329] relu2 <- conv2
I0905 20:46:44.699753 32430 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:46:44.699760 32430 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:46:44.699766 32430 net.cpp:125] relu2 needs backward computation.
I0905 20:46:44.699775 32430 net.cpp:66] Creating Layer pool2
I0905 20:46:44.699780 32430 net.cpp:329] pool2 <- conv2
I0905 20:46:44.699787 32430 net.cpp:290] pool2 -> pool2
I0905 20:46:44.699795 32430 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:46:44.699801 32430 net.cpp:125] pool2 needs backward computation.
I0905 20:46:44.699808 32430 net.cpp:66] Creating Layer fc7
I0905 20:46:44.699815 32430 net.cpp:329] fc7 <- pool2
I0905 20:46:44.699821 32430 net.cpp:290] fc7 -> fc7
I0905 20:46:45.348680 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.348726 32430 net.cpp:125] fc7 needs backward computation.
I0905 20:46:45.348739 32430 net.cpp:66] Creating Layer relu7
I0905 20:46:45.348747 32430 net.cpp:329] relu7 <- fc7
I0905 20:46:45.348757 32430 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:46:45.348767 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.348773 32430 net.cpp:125] relu7 needs backward computation.
I0905 20:46:45.348779 32430 net.cpp:66] Creating Layer drop7
I0905 20:46:45.348785 32430 net.cpp:329] drop7 <- fc7
I0905 20:46:45.348791 32430 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:46:45.348803 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.348809 32430 net.cpp:125] drop7 needs backward computation.
I0905 20:46:45.348816 32430 net.cpp:66] Creating Layer fc8
I0905 20:46:45.348821 32430 net.cpp:329] fc8 <- fc7
I0905 20:46:45.348830 32430 net.cpp:290] fc8 -> fc8
I0905 20:46:45.356670 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.356683 32430 net.cpp:125] fc8 needs backward computation.
I0905 20:46:45.356690 32430 net.cpp:66] Creating Layer relu8
I0905 20:46:45.356696 32430 net.cpp:329] relu8 <- fc8
I0905 20:46:45.356704 32430 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:46:45.356711 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.356717 32430 net.cpp:125] relu8 needs backward computation.
I0905 20:46:45.356724 32430 net.cpp:66] Creating Layer drop8
I0905 20:46:45.356729 32430 net.cpp:329] drop8 <- fc8
I0905 20:46:45.356735 32430 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:46:45.356741 32430 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:46:45.356747 32430 net.cpp:125] drop8 needs backward computation.
I0905 20:46:45.356758 32430 net.cpp:66] Creating Layer fc9
I0905 20:46:45.356763 32430 net.cpp:329] fc9 <- fc8
I0905 20:46:45.356770 32430 net.cpp:290] fc9 -> fc9
I0905 20:46:45.357146 32430 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:46:45.357158 32430 net.cpp:125] fc9 needs backward computation.
I0905 20:46:45.357167 32430 net.cpp:66] Creating Layer fc10
I0905 20:46:45.357172 32430 net.cpp:329] fc10 <- fc9
I0905 20:46:45.357180 32430 net.cpp:290] fc10 -> fc10
I0905 20:46:45.357192 32430 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:46:45.357200 32430 net.cpp:125] fc10 needs backward computation.
I0905 20:46:45.357206 32430 net.cpp:66] Creating Layer prob
I0905 20:46:45.357213 32430 net.cpp:329] prob <- fc10
I0905 20:46:45.357220 32430 net.cpp:290] prob -> prob
I0905 20:46:45.357240 32430 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:46:45.357246 32430 net.cpp:125] prob needs backward computation.
I0905 20:46:45.357251 32430 net.cpp:156] This network produces output prob
I0905 20:46:45.357264 32430 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:46:45.357272 32430 net.cpp:167] Network initialization done.
I0905 20:46:45.357277 32430 net.cpp:168] Memory required for data: 6183480
Classifying 160 inputs.
Done in 102.06 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 60 is out of bounds for axis 0 with size 60
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:48:31.215685 32436 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:48:31.215824 32436 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:48:31.215833 32436 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:48:31.215980 32436 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:48:31.216042 32436 net.cpp:292] Input 0 -> data
I0905 20:48:31.216068 32436 net.cpp:66] Creating Layer conv1
I0905 20:48:31.216075 32436 net.cpp:329] conv1 <- data
I0905 20:48:31.216084 32436 net.cpp:290] conv1 -> conv1
I0905 20:48:31.217445 32436 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:48:31.217463 32436 net.cpp:125] conv1 needs backward computation.
I0905 20:48:31.217471 32436 net.cpp:66] Creating Layer relu1
I0905 20:48:31.217478 32436 net.cpp:329] relu1 <- conv1
I0905 20:48:31.217484 32436 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:48:31.217494 32436 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:48:31.217499 32436 net.cpp:125] relu1 needs backward computation.
I0905 20:48:31.217506 32436 net.cpp:66] Creating Layer pool1
I0905 20:48:31.217511 32436 net.cpp:329] pool1 <- conv1
I0905 20:48:31.217519 32436 net.cpp:290] pool1 -> pool1
I0905 20:48:31.217530 32436 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:48:31.217535 32436 net.cpp:125] pool1 needs backward computation.
I0905 20:48:31.217541 32436 net.cpp:66] Creating Layer norm1
I0905 20:48:31.217547 32436 net.cpp:329] norm1 <- pool1
I0905 20:48:31.217553 32436 net.cpp:290] norm1 -> norm1
I0905 20:48:31.217563 32436 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:48:31.217569 32436 net.cpp:125] norm1 needs backward computation.
I0905 20:48:31.217577 32436 net.cpp:66] Creating Layer conv2
I0905 20:48:31.217608 32436 net.cpp:329] conv2 <- norm1
I0905 20:48:31.217617 32436 net.cpp:290] conv2 -> conv2
I0905 20:48:31.226758 32436 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:48:31.226773 32436 net.cpp:125] conv2 needs backward computation.
I0905 20:48:31.226780 32436 net.cpp:66] Creating Layer relu2
I0905 20:48:31.226786 32436 net.cpp:329] relu2 <- conv2
I0905 20:48:31.226794 32436 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:48:31.226800 32436 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:48:31.226806 32436 net.cpp:125] relu2 needs backward computation.
I0905 20:48:31.226812 32436 net.cpp:66] Creating Layer pool2
I0905 20:48:31.226817 32436 net.cpp:329] pool2 <- conv2
I0905 20:48:31.226824 32436 net.cpp:290] pool2 -> pool2
I0905 20:48:31.226831 32436 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:48:31.226837 32436 net.cpp:125] pool2 needs backward computation.
I0905 20:48:31.226846 32436 net.cpp:66] Creating Layer fc7
I0905 20:48:31.226852 32436 net.cpp:329] fc7 <- pool2
I0905 20:48:31.226861 32436 net.cpp:290] fc7 -> fc7
I0905 20:48:31.878407 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.878450 32436 net.cpp:125] fc7 needs backward computation.
I0905 20:48:31.878463 32436 net.cpp:66] Creating Layer relu7
I0905 20:48:31.878470 32436 net.cpp:329] relu7 <- fc7
I0905 20:48:31.878481 32436 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:48:31.878491 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.878497 32436 net.cpp:125] relu7 needs backward computation.
I0905 20:48:31.878505 32436 net.cpp:66] Creating Layer drop7
I0905 20:48:31.878510 32436 net.cpp:329] drop7 <- fc7
I0905 20:48:31.878516 32436 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:48:31.878527 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.878533 32436 net.cpp:125] drop7 needs backward computation.
I0905 20:48:31.878541 32436 net.cpp:66] Creating Layer fc8
I0905 20:48:31.878546 32436 net.cpp:329] fc8 <- fc7
I0905 20:48:31.878556 32436 net.cpp:290] fc8 -> fc8
I0905 20:48:31.886340 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.886353 32436 net.cpp:125] fc8 needs backward computation.
I0905 20:48:31.886360 32436 net.cpp:66] Creating Layer relu8
I0905 20:48:31.886375 32436 net.cpp:329] relu8 <- fc8
I0905 20:48:31.886384 32436 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:48:31.886391 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.886396 32436 net.cpp:125] relu8 needs backward computation.
I0905 20:48:31.886404 32436 net.cpp:66] Creating Layer drop8
I0905 20:48:31.886409 32436 net.cpp:329] drop8 <- fc8
I0905 20:48:31.886415 32436 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:48:31.886421 32436 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:48:31.886426 32436 net.cpp:125] drop8 needs backward computation.
I0905 20:48:31.886435 32436 net.cpp:66] Creating Layer fc9
I0905 20:48:31.886441 32436 net.cpp:329] fc9 <- fc8
I0905 20:48:31.886448 32436 net.cpp:290] fc9 -> fc9
I0905 20:48:31.886821 32436 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:48:31.886833 32436 net.cpp:125] fc9 needs backward computation.
I0905 20:48:31.886842 32436 net.cpp:66] Creating Layer fc10
I0905 20:48:31.886847 32436 net.cpp:329] fc10 <- fc9
I0905 20:48:31.886855 32436 net.cpp:290] fc10 -> fc10
I0905 20:48:31.886868 32436 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:48:31.886875 32436 net.cpp:125] fc10 needs backward computation.
I0905 20:48:31.886881 32436 net.cpp:66] Creating Layer prob
I0905 20:48:31.886888 32436 net.cpp:329] prob <- fc10
I0905 20:48:31.886895 32436 net.cpp:290] prob -> prob
I0905 20:48:31.886904 32436 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:48:31.886910 32436 net.cpp:125] prob needs backward computation.
I0905 20:48:31.886915 32436 net.cpp:156] This network produces output prob
I0905 20:48:31.886927 32436 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:48:31.886935 32436 net.cpp:167] Network initialization done.
I0905 20:48:31.886940 32436 net.cpp:168] Memory required for data: 6183480
Classifying 409 inputs.
Done in 265.28 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 9 is out of bounds for axis 0 with size 9
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:53:06.000321 32455 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:53:06.000464 32455 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:53:06.000473 32455 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:53:06.000624 32455 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:53:06.000690 32455 net.cpp:292] Input 0 -> data
I0905 20:53:06.000718 32455 net.cpp:66] Creating Layer conv1
I0905 20:53:06.000725 32455 net.cpp:329] conv1 <- data
I0905 20:53:06.000735 32455 net.cpp:290] conv1 -> conv1
I0905 20:53:06.002149 32455 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:06.002169 32455 net.cpp:125] conv1 needs backward computation.
I0905 20:53:06.002179 32455 net.cpp:66] Creating Layer relu1
I0905 20:53:06.002185 32455 net.cpp:329] relu1 <- conv1
I0905 20:53:06.002192 32455 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:53:06.002202 32455 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:06.002208 32455 net.cpp:125] relu1 needs backward computation.
I0905 20:53:06.002215 32455 net.cpp:66] Creating Layer pool1
I0905 20:53:06.002221 32455 net.cpp:329] pool1 <- conv1
I0905 20:53:06.002228 32455 net.cpp:290] pool1 -> pool1
I0905 20:53:06.002239 32455 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:06.002245 32455 net.cpp:125] pool1 needs backward computation.
I0905 20:53:06.002254 32455 net.cpp:66] Creating Layer norm1
I0905 20:53:06.002259 32455 net.cpp:329] norm1 <- pool1
I0905 20:53:06.002265 32455 net.cpp:290] norm1 -> norm1
I0905 20:53:06.002275 32455 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:06.002281 32455 net.cpp:125] norm1 needs backward computation.
I0905 20:53:06.002290 32455 net.cpp:66] Creating Layer conv2
I0905 20:53:06.002295 32455 net.cpp:329] conv2 <- norm1
I0905 20:53:06.002302 32455 net.cpp:290] conv2 -> conv2
I0905 20:53:06.011590 32455 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:06.011606 32455 net.cpp:125] conv2 needs backward computation.
I0905 20:53:06.011615 32455 net.cpp:66] Creating Layer relu2
I0905 20:53:06.011620 32455 net.cpp:329] relu2 <- conv2
I0905 20:53:06.011627 32455 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:53:06.011634 32455 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:06.011641 32455 net.cpp:125] relu2 needs backward computation.
I0905 20:53:06.011651 32455 net.cpp:66] Creating Layer pool2
I0905 20:53:06.011657 32455 net.cpp:329] pool2 <- conv2
I0905 20:53:06.011664 32455 net.cpp:290] pool2 -> pool2
I0905 20:53:06.011672 32455 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:53:06.011678 32455 net.cpp:125] pool2 needs backward computation.
I0905 20:53:06.011692 32455 net.cpp:66] Creating Layer fc7
I0905 20:53:06.011698 32455 net.cpp:329] fc7 <- pool2
I0905 20:53:06.011705 32455 net.cpp:290] fc7 -> fc7
I0905 20:53:06.661417 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.661461 32455 net.cpp:125] fc7 needs backward computation.
I0905 20:53:06.661475 32455 net.cpp:66] Creating Layer relu7
I0905 20:53:06.661483 32455 net.cpp:329] relu7 <- fc7
I0905 20:53:06.661492 32455 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:53:06.661502 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.661509 32455 net.cpp:125] relu7 needs backward computation.
I0905 20:53:06.661516 32455 net.cpp:66] Creating Layer drop7
I0905 20:53:06.661521 32455 net.cpp:329] drop7 <- fc7
I0905 20:53:06.661528 32455 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:53:06.661538 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.661545 32455 net.cpp:125] drop7 needs backward computation.
I0905 20:53:06.661553 32455 net.cpp:66] Creating Layer fc8
I0905 20:53:06.661559 32455 net.cpp:329] fc8 <- fc7
I0905 20:53:06.661568 32455 net.cpp:290] fc8 -> fc8
I0905 20:53:06.669356 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.669368 32455 net.cpp:125] fc8 needs backward computation.
I0905 20:53:06.669376 32455 net.cpp:66] Creating Layer relu8
I0905 20:53:06.669381 32455 net.cpp:329] relu8 <- fc8
I0905 20:53:06.669389 32455 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:53:06.669397 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.669402 32455 net.cpp:125] relu8 needs backward computation.
I0905 20:53:06.669409 32455 net.cpp:66] Creating Layer drop8
I0905 20:53:06.669414 32455 net.cpp:329] drop8 <- fc8
I0905 20:53:06.669420 32455 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:53:06.669427 32455 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:06.669433 32455 net.cpp:125] drop8 needs backward computation.
I0905 20:53:06.669442 32455 net.cpp:66] Creating Layer fc9
I0905 20:53:06.669448 32455 net.cpp:329] fc9 <- fc8
I0905 20:53:06.669456 32455 net.cpp:290] fc9 -> fc9
I0905 20:53:06.669832 32455 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:53:06.669845 32455 net.cpp:125] fc9 needs backward computation.
I0905 20:53:06.669853 32455 net.cpp:66] Creating Layer fc10
I0905 20:53:06.669859 32455 net.cpp:329] fc10 <- fc9
I0905 20:53:06.669867 32455 net.cpp:290] fc10 -> fc10
I0905 20:53:06.669879 32455 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:06.669888 32455 net.cpp:125] fc10 needs backward computation.
I0905 20:53:06.669894 32455 net.cpp:66] Creating Layer prob
I0905 20:53:06.669899 32455 net.cpp:329] prob <- fc10
I0905 20:53:06.669908 32455 net.cpp:290] prob -> prob
I0905 20:53:06.669917 32455 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:06.669924 32455 net.cpp:125] prob needs backward computation.
I0905 20:53:06.669929 32455 net.cpp:156] This network produces output prob
I0905 20:53:06.669941 32455 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:53:06.669950 32455 net.cpp:167] Network initialization done.
I0905 20:53:06.669955 32455 net.cpp:168] Memory required for data: 6183480
Classifying 17 inputs.
Done in 10.85 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:53:18.538029 32460 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:53:18.538167 32460 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:53:18.538177 32460 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:53:18.538323 32460 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:53:18.538385 32460 net.cpp:292] Input 0 -> data
I0905 20:53:18.538411 32460 net.cpp:66] Creating Layer conv1
I0905 20:53:18.538419 32460 net.cpp:329] conv1 <- data
I0905 20:53:18.538427 32460 net.cpp:290] conv1 -> conv1
I0905 20:53:18.539805 32460 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:18.539824 32460 net.cpp:125] conv1 needs backward computation.
I0905 20:53:18.539832 32460 net.cpp:66] Creating Layer relu1
I0905 20:53:18.539839 32460 net.cpp:329] relu1 <- conv1
I0905 20:53:18.539845 32460 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:53:18.539854 32460 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:18.539860 32460 net.cpp:125] relu1 needs backward computation.
I0905 20:53:18.539866 32460 net.cpp:66] Creating Layer pool1
I0905 20:53:18.539872 32460 net.cpp:329] pool1 <- conv1
I0905 20:53:18.539878 32460 net.cpp:290] pool1 -> pool1
I0905 20:53:18.539891 32460 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:18.539896 32460 net.cpp:125] pool1 needs backward computation.
I0905 20:53:18.539902 32460 net.cpp:66] Creating Layer norm1
I0905 20:53:18.539908 32460 net.cpp:329] norm1 <- pool1
I0905 20:53:18.539916 32460 net.cpp:290] norm1 -> norm1
I0905 20:53:18.539926 32460 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:18.539932 32460 net.cpp:125] norm1 needs backward computation.
I0905 20:53:18.539942 32460 net.cpp:66] Creating Layer conv2
I0905 20:53:18.539948 32460 net.cpp:329] conv2 <- norm1
I0905 20:53:18.539957 32460 net.cpp:290] conv2 -> conv2
I0905 20:53:18.549186 32460 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:18.549202 32460 net.cpp:125] conv2 needs backward computation.
I0905 20:53:18.549209 32460 net.cpp:66] Creating Layer relu2
I0905 20:53:18.549216 32460 net.cpp:329] relu2 <- conv2
I0905 20:53:18.549222 32460 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:53:18.549229 32460 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:18.549235 32460 net.cpp:125] relu2 needs backward computation.
I0905 20:53:18.549244 32460 net.cpp:66] Creating Layer pool2
I0905 20:53:18.549250 32460 net.cpp:329] pool2 <- conv2
I0905 20:53:18.549257 32460 net.cpp:290] pool2 -> pool2
I0905 20:53:18.549265 32460 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:53:18.549271 32460 net.cpp:125] pool2 needs backward computation.
I0905 20:53:18.549278 32460 net.cpp:66] Creating Layer fc7
I0905 20:53:18.549284 32460 net.cpp:329] fc7 <- pool2
I0905 20:53:18.549293 32460 net.cpp:290] fc7 -> fc7
I0905 20:53:19.195690 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.195737 32460 net.cpp:125] fc7 needs backward computation.
I0905 20:53:19.195750 32460 net.cpp:66] Creating Layer relu7
I0905 20:53:19.195757 32460 net.cpp:329] relu7 <- fc7
I0905 20:53:19.195766 32460 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:53:19.195777 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.195783 32460 net.cpp:125] relu7 needs backward computation.
I0905 20:53:19.195791 32460 net.cpp:66] Creating Layer drop7
I0905 20:53:19.195796 32460 net.cpp:329] drop7 <- fc7
I0905 20:53:19.195802 32460 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:53:19.195813 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.195819 32460 net.cpp:125] drop7 needs backward computation.
I0905 20:53:19.195828 32460 net.cpp:66] Creating Layer fc8
I0905 20:53:19.195833 32460 net.cpp:329] fc8 <- fc7
I0905 20:53:19.195842 32460 net.cpp:290] fc8 -> fc8
I0905 20:53:19.203622 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.203635 32460 net.cpp:125] fc8 needs backward computation.
I0905 20:53:19.203642 32460 net.cpp:66] Creating Layer relu8
I0905 20:53:19.203649 32460 net.cpp:329] relu8 <- fc8
I0905 20:53:19.203656 32460 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:53:19.203663 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.203668 32460 net.cpp:125] relu8 needs backward computation.
I0905 20:53:19.203675 32460 net.cpp:66] Creating Layer drop8
I0905 20:53:19.203680 32460 net.cpp:329] drop8 <- fc8
I0905 20:53:19.203687 32460 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:53:19.203693 32460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:19.203699 32460 net.cpp:125] drop8 needs backward computation.
I0905 20:53:19.203707 32460 net.cpp:66] Creating Layer fc9
I0905 20:53:19.203713 32460 net.cpp:329] fc9 <- fc8
I0905 20:53:19.203721 32460 net.cpp:290] fc9 -> fc9
I0905 20:53:19.204095 32460 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:53:19.204107 32460 net.cpp:125] fc9 needs backward computation.
I0905 20:53:19.204115 32460 net.cpp:66] Creating Layer fc10
I0905 20:53:19.204121 32460 net.cpp:329] fc10 <- fc9
I0905 20:53:19.204129 32460 net.cpp:290] fc10 -> fc10
I0905 20:53:19.204141 32460 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:19.204149 32460 net.cpp:125] fc10 needs backward computation.
I0905 20:53:19.204155 32460 net.cpp:66] Creating Layer prob
I0905 20:53:19.204161 32460 net.cpp:329] prob <- fc10
I0905 20:53:19.204169 32460 net.cpp:290] prob -> prob
I0905 20:53:19.204179 32460 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:19.204185 32460 net.cpp:125] prob needs backward computation.
I0905 20:53:19.204190 32460 net.cpp:156] This network produces output prob
I0905 20:53:19.204202 32460 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:53:19.204210 32460 net.cpp:167] Network initialization done.
I0905 20:53:19.204226 32460 net.cpp:168] Memory required for data: 6183480
Classifying 38 inputs.
Done in 24.88 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:53:46.151325 32465 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:53:46.151463 32465 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:53:46.151471 32465 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:53:46.151619 32465 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:53:46.151670 32465 net.cpp:292] Input 0 -> data
I0905 20:53:46.151695 32465 net.cpp:66] Creating Layer conv1
I0905 20:53:46.151703 32465 net.cpp:329] conv1 <- data
I0905 20:53:46.151711 32465 net.cpp:290] conv1 -> conv1
I0905 20:53:46.153074 32465 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:46.153102 32465 net.cpp:125] conv1 needs backward computation.
I0905 20:53:46.153112 32465 net.cpp:66] Creating Layer relu1
I0905 20:53:46.153118 32465 net.cpp:329] relu1 <- conv1
I0905 20:53:46.153126 32465 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:53:46.153133 32465 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:53:46.153139 32465 net.cpp:125] relu1 needs backward computation.
I0905 20:53:46.153146 32465 net.cpp:66] Creating Layer pool1
I0905 20:53:46.153152 32465 net.cpp:329] pool1 <- conv1
I0905 20:53:46.153159 32465 net.cpp:290] pool1 -> pool1
I0905 20:53:46.153170 32465 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:46.153177 32465 net.cpp:125] pool1 needs backward computation.
I0905 20:53:46.153182 32465 net.cpp:66] Creating Layer norm1
I0905 20:53:46.153188 32465 net.cpp:329] norm1 <- pool1
I0905 20:53:46.153195 32465 net.cpp:290] norm1 -> norm1
I0905 20:53:46.153205 32465 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:53:46.153210 32465 net.cpp:125] norm1 needs backward computation.
I0905 20:53:46.153218 32465 net.cpp:66] Creating Layer conv2
I0905 20:53:46.153224 32465 net.cpp:329] conv2 <- norm1
I0905 20:53:46.153231 32465 net.cpp:290] conv2 -> conv2
I0905 20:53:46.162405 32465 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:46.162420 32465 net.cpp:125] conv2 needs backward computation.
I0905 20:53:46.162427 32465 net.cpp:66] Creating Layer relu2
I0905 20:53:46.162433 32465 net.cpp:329] relu2 <- conv2
I0905 20:53:46.162441 32465 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:53:46.162448 32465 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:53:46.162453 32465 net.cpp:125] relu2 needs backward computation.
I0905 20:53:46.162461 32465 net.cpp:66] Creating Layer pool2
I0905 20:53:46.162466 32465 net.cpp:329] pool2 <- conv2
I0905 20:53:46.162472 32465 net.cpp:290] pool2 -> pool2
I0905 20:53:46.162480 32465 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:53:46.162485 32465 net.cpp:125] pool2 needs backward computation.
I0905 20:53:46.162495 32465 net.cpp:66] Creating Layer fc7
I0905 20:53:46.162502 32465 net.cpp:329] fc7 <- pool2
I0905 20:53:46.162508 32465 net.cpp:290] fc7 -> fc7
I0905 20:53:46.836097 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.836143 32465 net.cpp:125] fc7 needs backward computation.
I0905 20:53:46.836155 32465 net.cpp:66] Creating Layer relu7
I0905 20:53:46.836163 32465 net.cpp:329] relu7 <- fc7
I0905 20:53:46.836172 32465 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:53:46.836182 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.836189 32465 net.cpp:125] relu7 needs backward computation.
I0905 20:53:46.836195 32465 net.cpp:66] Creating Layer drop7
I0905 20:53:46.836201 32465 net.cpp:329] drop7 <- fc7
I0905 20:53:46.836207 32465 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:53:46.836218 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.836225 32465 net.cpp:125] drop7 needs backward computation.
I0905 20:53:46.836232 32465 net.cpp:66] Creating Layer fc8
I0905 20:53:46.836238 32465 net.cpp:329] fc8 <- fc7
I0905 20:53:46.836247 32465 net.cpp:290] fc8 -> fc8
I0905 20:53:46.844323 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.844336 32465 net.cpp:125] fc8 needs backward computation.
I0905 20:53:46.844343 32465 net.cpp:66] Creating Layer relu8
I0905 20:53:46.844348 32465 net.cpp:329] relu8 <- fc8
I0905 20:53:46.844357 32465 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:53:46.844364 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.844370 32465 net.cpp:125] relu8 needs backward computation.
I0905 20:53:46.844377 32465 net.cpp:66] Creating Layer drop8
I0905 20:53:46.844383 32465 net.cpp:329] drop8 <- fc8
I0905 20:53:46.844388 32465 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:53:46.844395 32465 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:53:46.844400 32465 net.cpp:125] drop8 needs backward computation.
I0905 20:53:46.844409 32465 net.cpp:66] Creating Layer fc9
I0905 20:53:46.844415 32465 net.cpp:329] fc9 <- fc8
I0905 20:53:46.844432 32465 net.cpp:290] fc9 -> fc9
I0905 20:53:46.844822 32465 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:53:46.844835 32465 net.cpp:125] fc9 needs backward computation.
I0905 20:53:46.844842 32465 net.cpp:66] Creating Layer fc10
I0905 20:53:46.844848 32465 net.cpp:329] fc10 <- fc9
I0905 20:53:46.844857 32465 net.cpp:290] fc10 -> fc10
I0905 20:53:46.844869 32465 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:46.844877 32465 net.cpp:125] fc10 needs backward computation.
I0905 20:53:46.844884 32465 net.cpp:66] Creating Layer prob
I0905 20:53:46.844889 32465 net.cpp:329] prob <- fc10
I0905 20:53:46.844897 32465 net.cpp:290] prob -> prob
I0905 20:53:46.844907 32465 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:53:46.844913 32465 net.cpp:125] prob needs backward computation.
I0905 20:53:46.844918 32465 net.cpp:156] This network produces output prob
I0905 20:53:46.844930 32465 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:53:46.844938 32465 net.cpp:167] Network initialization done.
I0905 20:53:46.844944 32465 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 22 inputs.
Done in 15.33 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:54:03.273702 32469 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:54:03.273838 32469 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:54:03.273847 32469 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:54:03.273993 32469 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:54:03.274055 32469 net.cpp:292] Input 0 -> data
I0905 20:54:03.274080 32469 net.cpp:66] Creating Layer conv1
I0905 20:54:03.274087 32469 net.cpp:329] conv1 <- data
I0905 20:54:03.274096 32469 net.cpp:290] conv1 -> conv1
I0905 20:54:03.275454 32469 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:54:03.275472 32469 net.cpp:125] conv1 needs backward computation.
I0905 20:54:03.275481 32469 net.cpp:66] Creating Layer relu1
I0905 20:54:03.275488 32469 net.cpp:329] relu1 <- conv1
I0905 20:54:03.275496 32469 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:54:03.275503 32469 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:54:03.275509 32469 net.cpp:125] relu1 needs backward computation.
I0905 20:54:03.275516 32469 net.cpp:66] Creating Layer pool1
I0905 20:54:03.275521 32469 net.cpp:329] pool1 <- conv1
I0905 20:54:03.275528 32469 net.cpp:290] pool1 -> pool1
I0905 20:54:03.275539 32469 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:54:03.275545 32469 net.cpp:125] pool1 needs backward computation.
I0905 20:54:03.275552 32469 net.cpp:66] Creating Layer norm1
I0905 20:54:03.275557 32469 net.cpp:329] norm1 <- pool1
I0905 20:54:03.275563 32469 net.cpp:290] norm1 -> norm1
I0905 20:54:03.275573 32469 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:54:03.275579 32469 net.cpp:125] norm1 needs backward computation.
I0905 20:54:03.275586 32469 net.cpp:66] Creating Layer conv2
I0905 20:54:03.275593 32469 net.cpp:329] conv2 <- norm1
I0905 20:54:03.275599 32469 net.cpp:290] conv2 -> conv2
I0905 20:54:03.284742 32469 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:54:03.284759 32469 net.cpp:125] conv2 needs backward computation.
I0905 20:54:03.284766 32469 net.cpp:66] Creating Layer relu2
I0905 20:54:03.284772 32469 net.cpp:329] relu2 <- conv2
I0905 20:54:03.284780 32469 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:54:03.284786 32469 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:54:03.284792 32469 net.cpp:125] relu2 needs backward computation.
I0905 20:54:03.284798 32469 net.cpp:66] Creating Layer pool2
I0905 20:54:03.284803 32469 net.cpp:329] pool2 <- conv2
I0905 20:54:03.284811 32469 net.cpp:290] pool2 -> pool2
I0905 20:54:03.284818 32469 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:54:03.284823 32469 net.cpp:125] pool2 needs backward computation.
I0905 20:54:03.284833 32469 net.cpp:66] Creating Layer fc7
I0905 20:54:03.284839 32469 net.cpp:329] fc7 <- pool2
I0905 20:54:03.284847 32469 net.cpp:290] fc7 -> fc7
I0905 20:54:03.939900 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.939952 32469 net.cpp:125] fc7 needs backward computation.
I0905 20:54:03.939966 32469 net.cpp:66] Creating Layer relu7
I0905 20:54:03.939973 32469 net.cpp:329] relu7 <- fc7
I0905 20:54:03.939983 32469 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:54:03.939995 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.940001 32469 net.cpp:125] relu7 needs backward computation.
I0905 20:54:03.940008 32469 net.cpp:66] Creating Layer drop7
I0905 20:54:03.940013 32469 net.cpp:329] drop7 <- fc7
I0905 20:54:03.940021 32469 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:54:03.940043 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.940049 32469 net.cpp:125] drop7 needs backward computation.
I0905 20:54:03.940058 32469 net.cpp:66] Creating Layer fc8
I0905 20:54:03.940063 32469 net.cpp:329] fc8 <- fc7
I0905 20:54:03.940073 32469 net.cpp:290] fc8 -> fc8
I0905 20:54:03.947834 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.947847 32469 net.cpp:125] fc8 needs backward computation.
I0905 20:54:03.947854 32469 net.cpp:66] Creating Layer relu8
I0905 20:54:03.947860 32469 net.cpp:329] relu8 <- fc8
I0905 20:54:03.947870 32469 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:54:03.947876 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.947882 32469 net.cpp:125] relu8 needs backward computation.
I0905 20:54:03.947890 32469 net.cpp:66] Creating Layer drop8
I0905 20:54:03.947895 32469 net.cpp:329] drop8 <- fc8
I0905 20:54:03.947901 32469 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:54:03.947907 32469 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:03.947913 32469 net.cpp:125] drop8 needs backward computation.
I0905 20:54:03.947921 32469 net.cpp:66] Creating Layer fc9
I0905 20:54:03.947927 32469 net.cpp:329] fc9 <- fc8
I0905 20:54:03.947934 32469 net.cpp:290] fc9 -> fc9
I0905 20:54:03.948309 32469 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:54:03.948321 32469 net.cpp:125] fc9 needs backward computation.
I0905 20:54:03.948329 32469 net.cpp:66] Creating Layer fc10
I0905 20:54:03.948335 32469 net.cpp:329] fc10 <- fc9
I0905 20:54:03.948343 32469 net.cpp:290] fc10 -> fc10
I0905 20:54:03.948355 32469 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:54:03.948364 32469 net.cpp:125] fc10 needs backward computation.
I0905 20:54:03.948370 32469 net.cpp:66] Creating Layer prob
I0905 20:54:03.948376 32469 net.cpp:329] prob <- fc10
I0905 20:54:03.948385 32469 net.cpp:290] prob -> prob
I0905 20:54:03.948393 32469 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:54:03.948400 32469 net.cpp:125] prob needs backward computation.
I0905 20:54:03.948405 32469 net.cpp:156] This network produces output prob
I0905 20:54:03.948418 32469 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:54:03.948427 32469 net.cpp:167] Network initialization done.
I0905 20:54:03.948432 32469 net.cpp:168] Memory required for data: 6183480
Classifying 62 inputs.
Done in 41.99 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:54:49.963461 32474 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:54:49.963603 32474 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:54:49.963613 32474 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:54:49.963764 32474 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:54:49.963829 32474 net.cpp:292] Input 0 -> data
I0905 20:54:49.963855 32474 net.cpp:66] Creating Layer conv1
I0905 20:54:49.963862 32474 net.cpp:329] conv1 <- data
I0905 20:54:49.963871 32474 net.cpp:290] conv1 -> conv1
I0905 20:54:49.965271 32474 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:54:49.965289 32474 net.cpp:125] conv1 needs backward computation.
I0905 20:54:49.965298 32474 net.cpp:66] Creating Layer relu1
I0905 20:54:49.965304 32474 net.cpp:329] relu1 <- conv1
I0905 20:54:49.965312 32474 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:54:49.965320 32474 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:54:49.965327 32474 net.cpp:125] relu1 needs backward computation.
I0905 20:54:49.965333 32474 net.cpp:66] Creating Layer pool1
I0905 20:54:49.965339 32474 net.cpp:329] pool1 <- conv1
I0905 20:54:49.965347 32474 net.cpp:290] pool1 -> pool1
I0905 20:54:49.965358 32474 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:54:49.965363 32474 net.cpp:125] pool1 needs backward computation.
I0905 20:54:49.965370 32474 net.cpp:66] Creating Layer norm1
I0905 20:54:49.965376 32474 net.cpp:329] norm1 <- pool1
I0905 20:54:49.965384 32474 net.cpp:290] norm1 -> norm1
I0905 20:54:49.965394 32474 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:54:49.965399 32474 net.cpp:125] norm1 needs backward computation.
I0905 20:54:49.965406 32474 net.cpp:66] Creating Layer conv2
I0905 20:54:49.965412 32474 net.cpp:329] conv2 <- norm1
I0905 20:54:49.965420 32474 net.cpp:290] conv2 -> conv2
I0905 20:54:49.974808 32474 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:54:49.974824 32474 net.cpp:125] conv2 needs backward computation.
I0905 20:54:49.974831 32474 net.cpp:66] Creating Layer relu2
I0905 20:54:49.974838 32474 net.cpp:329] relu2 <- conv2
I0905 20:54:49.974844 32474 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:54:49.974853 32474 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:54:49.974858 32474 net.cpp:125] relu2 needs backward computation.
I0905 20:54:49.974864 32474 net.cpp:66] Creating Layer pool2
I0905 20:54:49.974869 32474 net.cpp:329] pool2 <- conv2
I0905 20:54:49.974882 32474 net.cpp:290] pool2 -> pool2
I0905 20:54:49.974891 32474 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:54:49.974897 32474 net.cpp:125] pool2 needs backward computation.
I0905 20:54:49.974906 32474 net.cpp:66] Creating Layer fc7
I0905 20:54:49.974912 32474 net.cpp:329] fc7 <- pool2
I0905 20:54:49.974920 32474 net.cpp:290] fc7 -> fc7
I0905 20:54:50.625990 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.626035 32474 net.cpp:125] fc7 needs backward computation.
I0905 20:54:50.626049 32474 net.cpp:66] Creating Layer relu7
I0905 20:54:50.626055 32474 net.cpp:329] relu7 <- fc7
I0905 20:54:50.626065 32474 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:54:50.626075 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.626080 32474 net.cpp:125] relu7 needs backward computation.
I0905 20:54:50.626087 32474 net.cpp:66] Creating Layer drop7
I0905 20:54:50.626092 32474 net.cpp:329] drop7 <- fc7
I0905 20:54:50.626099 32474 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:54:50.626111 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.626116 32474 net.cpp:125] drop7 needs backward computation.
I0905 20:54:50.626124 32474 net.cpp:66] Creating Layer fc8
I0905 20:54:50.626129 32474 net.cpp:329] fc8 <- fc7
I0905 20:54:50.626138 32474 net.cpp:290] fc8 -> fc8
I0905 20:54:50.633918 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.633930 32474 net.cpp:125] fc8 needs backward computation.
I0905 20:54:50.633937 32474 net.cpp:66] Creating Layer relu8
I0905 20:54:50.633942 32474 net.cpp:329] relu8 <- fc8
I0905 20:54:50.633950 32474 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:54:50.633957 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.633963 32474 net.cpp:125] relu8 needs backward computation.
I0905 20:54:50.633970 32474 net.cpp:66] Creating Layer drop8
I0905 20:54:50.633975 32474 net.cpp:329] drop8 <- fc8
I0905 20:54:50.633981 32474 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:54:50.633988 32474 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:54:50.633993 32474 net.cpp:125] drop8 needs backward computation.
I0905 20:54:50.634002 32474 net.cpp:66] Creating Layer fc9
I0905 20:54:50.634008 32474 net.cpp:329] fc9 <- fc8
I0905 20:54:50.634016 32474 net.cpp:290] fc9 -> fc9
I0905 20:54:50.634390 32474 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:54:50.634402 32474 net.cpp:125] fc9 needs backward computation.
I0905 20:54:50.634410 32474 net.cpp:66] Creating Layer fc10
I0905 20:54:50.634415 32474 net.cpp:329] fc10 <- fc9
I0905 20:54:50.634424 32474 net.cpp:290] fc10 -> fc10
I0905 20:54:50.634436 32474 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:54:50.634444 32474 net.cpp:125] fc10 needs backward computation.
I0905 20:54:50.634450 32474 net.cpp:66] Creating Layer prob
I0905 20:54:50.634456 32474 net.cpp:329] prob <- fc10
I0905 20:54:50.634464 32474 net.cpp:290] prob -> prob
I0905 20:54:50.634474 32474 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:54:50.634479 32474 net.cpp:125] prob needs backward computation.
I0905 20:54:50.634484 32474 net.cpp:156] This network produces output prob
I0905 20:54:50.634496 32474 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:54:50.634505 32474 net.cpp:167] Network initialization done.
I0905 20:54:50.634510 32474 net.cpp:168] Memory required for data: 6183480
Classifying 483 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 83, in predict
    dtype=np.float32)
MemoryError
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 20:59:07.128473 32493 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 20:59:07.128686 32493 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 20:59:07.153884 32493 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 20:59:07.154050 32493 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 20:59:07.154124 32493 net.cpp:292] Input 0 -> data
I0905 20:59:07.154151 32493 net.cpp:66] Creating Layer conv1
I0905 20:59:07.154158 32493 net.cpp:329] conv1 <- data
I0905 20:59:07.154168 32493 net.cpp:290] conv1 -> conv1
I0905 20:59:07.206660 32493 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:59:07.206815 32493 net.cpp:125] conv1 needs backward computation.
I0905 20:59:07.206897 32493 net.cpp:66] Creating Layer relu1
I0905 20:59:07.206948 32493 net.cpp:329] relu1 <- conv1
I0905 20:59:07.207016 32493 net.cpp:280] relu1 -> conv1 (in-place)
I0905 20:59:07.207093 32493 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 20:59:07.207141 32493 net.cpp:125] relu1 needs backward computation.
I0905 20:59:07.207233 32493 net.cpp:66] Creating Layer pool1
I0905 20:59:07.207288 32493 net.cpp:329] pool1 <- conv1
I0905 20:59:07.207351 32493 net.cpp:290] pool1 -> pool1
I0905 20:59:07.207448 32493 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:59:07.207497 32493 net.cpp:125] pool1 needs backward computation.
I0905 20:59:07.207561 32493 net.cpp:66] Creating Layer norm1
I0905 20:59:07.207615 32493 net.cpp:329] norm1 <- pool1
I0905 20:59:07.207679 32493 net.cpp:290] norm1 -> norm1
I0905 20:59:07.207756 32493 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 20:59:07.207810 32493 net.cpp:125] norm1 needs backward computation.
I0905 20:59:07.207880 32493 net.cpp:66] Creating Layer conv2
I0905 20:59:07.207929 32493 net.cpp:329] conv2 <- norm1
I0905 20:59:07.207998 32493 net.cpp:290] conv2 -> conv2
I0905 20:59:07.224015 32493 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:59:07.224040 32493 net.cpp:125] conv2 needs backward computation.
I0905 20:59:07.224050 32493 net.cpp:66] Creating Layer relu2
I0905 20:59:07.224057 32493 net.cpp:329] relu2 <- conv2
I0905 20:59:07.224066 32493 net.cpp:280] relu2 -> conv2 (in-place)
I0905 20:59:07.224073 32493 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 20:59:07.224081 32493 net.cpp:125] relu2 needs backward computation.
I0905 20:59:07.224087 32493 net.cpp:66] Creating Layer pool2
I0905 20:59:07.224093 32493 net.cpp:329] pool2 <- conv2
I0905 20:59:07.224102 32493 net.cpp:290] pool2 -> pool2
I0905 20:59:07.224110 32493 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 20:59:07.224117 32493 net.cpp:125] pool2 needs backward computation.
I0905 20:59:07.224128 32493 net.cpp:66] Creating Layer fc7
I0905 20:59:07.224135 32493 net.cpp:329] fc7 <- pool2
I0905 20:59:07.224143 32493 net.cpp:290] fc7 -> fc7
I0905 20:59:07.873719 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.873767 32493 net.cpp:125] fc7 needs backward computation.
I0905 20:59:07.873780 32493 net.cpp:66] Creating Layer relu7
I0905 20:59:07.873792 32493 net.cpp:329] relu7 <- fc7
I0905 20:59:07.873802 32493 net.cpp:280] relu7 -> fc7 (in-place)
I0905 20:59:07.873813 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.873819 32493 net.cpp:125] relu7 needs backward computation.
I0905 20:59:07.873826 32493 net.cpp:66] Creating Layer drop7
I0905 20:59:07.873832 32493 net.cpp:329] drop7 <- fc7
I0905 20:59:07.873838 32493 net.cpp:280] drop7 -> fc7 (in-place)
I0905 20:59:07.873850 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.873857 32493 net.cpp:125] drop7 needs backward computation.
I0905 20:59:07.873864 32493 net.cpp:66] Creating Layer fc8
I0905 20:59:07.873870 32493 net.cpp:329] fc8 <- fc7
I0905 20:59:07.873879 32493 net.cpp:290] fc8 -> fc8
I0905 20:59:07.881659 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.881671 32493 net.cpp:125] fc8 needs backward computation.
I0905 20:59:07.881680 32493 net.cpp:66] Creating Layer relu8
I0905 20:59:07.881685 32493 net.cpp:329] relu8 <- fc8
I0905 20:59:07.881692 32493 net.cpp:280] relu8 -> fc8 (in-place)
I0905 20:59:07.881700 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.881706 32493 net.cpp:125] relu8 needs backward computation.
I0905 20:59:07.881713 32493 net.cpp:66] Creating Layer drop8
I0905 20:59:07.881719 32493 net.cpp:329] drop8 <- fc8
I0905 20:59:07.881726 32493 net.cpp:280] drop8 -> fc8 (in-place)
I0905 20:59:07.881732 32493 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 20:59:07.881738 32493 net.cpp:125] drop8 needs backward computation.
I0905 20:59:07.881747 32493 net.cpp:66] Creating Layer fc9
I0905 20:59:07.881753 32493 net.cpp:329] fc9 <- fc8
I0905 20:59:07.881762 32493 net.cpp:290] fc9 -> fc9
I0905 20:59:07.882138 32493 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 20:59:07.882150 32493 net.cpp:125] fc9 needs backward computation.
I0905 20:59:07.882158 32493 net.cpp:66] Creating Layer fc10
I0905 20:59:07.882164 32493 net.cpp:329] fc10 <- fc9
I0905 20:59:07.882174 32493 net.cpp:290] fc10 -> fc10
I0905 20:59:07.882185 32493 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:59:07.882202 32493 net.cpp:125] fc10 needs backward computation.
I0905 20:59:07.882210 32493 net.cpp:66] Creating Layer prob
I0905 20:59:07.882215 32493 net.cpp:329] prob <- fc10
I0905 20:59:07.882223 32493 net.cpp:290] prob -> prob
I0905 20:59:07.939877 32493 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 20:59:07.939894 32493 net.cpp:125] prob needs backward computation.
I0905 20:59:07.939900 32493 net.cpp:156] This network produces output prob
I0905 20:59:07.939919 32493 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 20:59:07.939929 32493 net.cpp:167] Network initialization done.
I0905 20:59:07.939934 32493 net.cpp:168] Memory required for data: 6183480
Classifying 89 inputs.
Done in 54.98 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:00:09.344015 32499 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:00:09.344158 32499 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:00:09.344168 32499 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:00:09.344321 32499 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:00:09.344385 32499 net.cpp:292] Input 0 -> data
I0905 21:00:09.344411 32499 net.cpp:66] Creating Layer conv1
I0905 21:00:09.344419 32499 net.cpp:329] conv1 <- data
I0905 21:00:09.344429 32499 net.cpp:290] conv1 -> conv1
I0905 21:00:09.345851 32499 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:00:09.345870 32499 net.cpp:125] conv1 needs backward computation.
I0905 21:00:09.345880 32499 net.cpp:66] Creating Layer relu1
I0905 21:00:09.345886 32499 net.cpp:329] relu1 <- conv1
I0905 21:00:09.345893 32499 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:00:09.345902 32499 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:00:09.345908 32499 net.cpp:125] relu1 needs backward computation.
I0905 21:00:09.345916 32499 net.cpp:66] Creating Layer pool1
I0905 21:00:09.345921 32499 net.cpp:329] pool1 <- conv1
I0905 21:00:09.345928 32499 net.cpp:290] pool1 -> pool1
I0905 21:00:09.345939 32499 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:00:09.345945 32499 net.cpp:125] pool1 needs backward computation.
I0905 21:00:09.345953 32499 net.cpp:66] Creating Layer norm1
I0905 21:00:09.345958 32499 net.cpp:329] norm1 <- pool1
I0905 21:00:09.345965 32499 net.cpp:290] norm1 -> norm1
I0905 21:00:09.345975 32499 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:00:09.345981 32499 net.cpp:125] norm1 needs backward computation.
I0905 21:00:09.345989 32499 net.cpp:66] Creating Layer conv2
I0905 21:00:09.345995 32499 net.cpp:329] conv2 <- norm1
I0905 21:00:09.346002 32499 net.cpp:290] conv2 -> conv2
I0905 21:00:09.355113 32499 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:00:09.355126 32499 net.cpp:125] conv2 needs backward computation.
I0905 21:00:09.355134 32499 net.cpp:66] Creating Layer relu2
I0905 21:00:09.355139 32499 net.cpp:329] relu2 <- conv2
I0905 21:00:09.355146 32499 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:00:09.355154 32499 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:00:09.355159 32499 net.cpp:125] relu2 needs backward computation.
I0905 21:00:09.355166 32499 net.cpp:66] Creating Layer pool2
I0905 21:00:09.355171 32499 net.cpp:329] pool2 <- conv2
I0905 21:00:09.355178 32499 net.cpp:290] pool2 -> pool2
I0905 21:00:09.355186 32499 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:00:09.355192 32499 net.cpp:125] pool2 needs backward computation.
I0905 21:00:09.355202 32499 net.cpp:66] Creating Layer fc7
I0905 21:00:09.355208 32499 net.cpp:329] fc7 <- pool2
I0905 21:00:09.355216 32499 net.cpp:290] fc7 -> fc7
I0905 21:00:10.001847 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.001888 32499 net.cpp:125] fc7 needs backward computation.
I0905 21:00:10.001901 32499 net.cpp:66] Creating Layer relu7
I0905 21:00:10.001909 32499 net.cpp:329] relu7 <- fc7
I0905 21:00:10.001919 32499 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:00:10.001930 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.001936 32499 net.cpp:125] relu7 needs backward computation.
I0905 21:00:10.001943 32499 net.cpp:66] Creating Layer drop7
I0905 21:00:10.001950 32499 net.cpp:329] drop7 <- fc7
I0905 21:00:10.001956 32499 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:00:10.001967 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.001973 32499 net.cpp:125] drop7 needs backward computation.
I0905 21:00:10.001982 32499 net.cpp:66] Creating Layer fc8
I0905 21:00:10.001988 32499 net.cpp:329] fc8 <- fc7
I0905 21:00:10.001997 32499 net.cpp:290] fc8 -> fc8
I0905 21:00:10.010000 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.010013 32499 net.cpp:125] fc8 needs backward computation.
I0905 21:00:10.010020 32499 net.cpp:66] Creating Layer relu8
I0905 21:00:10.010026 32499 net.cpp:329] relu8 <- fc8
I0905 21:00:10.010046 32499 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:00:10.010053 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.010059 32499 net.cpp:125] relu8 needs backward computation.
I0905 21:00:10.010066 32499 net.cpp:66] Creating Layer drop8
I0905 21:00:10.010072 32499 net.cpp:329] drop8 <- fc8
I0905 21:00:10.010078 32499 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:00:10.010087 32499 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:00:10.010092 32499 net.cpp:125] drop8 needs backward computation.
I0905 21:00:10.010102 32499 net.cpp:66] Creating Layer fc9
I0905 21:00:10.010108 32499 net.cpp:329] fc9 <- fc8
I0905 21:00:10.010117 32499 net.cpp:290] fc9 -> fc9
I0905 21:00:10.010500 32499 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:00:10.010514 32499 net.cpp:125] fc9 needs backward computation.
I0905 21:00:10.010521 32499 net.cpp:66] Creating Layer fc10
I0905 21:00:10.010527 32499 net.cpp:329] fc10 <- fc9
I0905 21:00:10.010536 32499 net.cpp:290] fc10 -> fc10
I0905 21:00:10.010548 32499 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:00:10.010557 32499 net.cpp:125] fc10 needs backward computation.
I0905 21:00:10.010565 32499 net.cpp:66] Creating Layer prob
I0905 21:00:10.010571 32499 net.cpp:329] prob <- fc10
I0905 21:00:10.010578 32499 net.cpp:290] prob -> prob
I0905 21:00:10.010588 32499 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:00:10.010594 32499 net.cpp:125] prob needs backward computation.
I0905 21:00:10.010599 32499 net.cpp:156] This network produces output prob
I0905 21:00:10.010612 32499 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:00:10.010622 32499 net.cpp:167] Network initialization done.
I0905 21:00:10.010627 32499 net.cpp:168] Memory required for data: 6183480
Classifying 290 inputs.
Done in 175.38 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 90 is out of bounds for axis 0 with size 90
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:03:11.817860 32506 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:03:11.817999 32506 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:03:11.818008 32506 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:03:11.818156 32506 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:03:11.818219 32506 net.cpp:292] Input 0 -> data
I0905 21:03:11.818246 32506 net.cpp:66] Creating Layer conv1
I0905 21:03:11.818253 32506 net.cpp:329] conv1 <- data
I0905 21:03:11.818261 32506 net.cpp:290] conv1 -> conv1
I0905 21:03:11.819622 32506 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:03:11.819640 32506 net.cpp:125] conv1 needs backward computation.
I0905 21:03:11.819649 32506 net.cpp:66] Creating Layer relu1
I0905 21:03:11.819655 32506 net.cpp:329] relu1 <- conv1
I0905 21:03:11.819663 32506 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:03:11.819671 32506 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:03:11.819677 32506 net.cpp:125] relu1 needs backward computation.
I0905 21:03:11.819684 32506 net.cpp:66] Creating Layer pool1
I0905 21:03:11.819690 32506 net.cpp:329] pool1 <- conv1
I0905 21:03:11.819697 32506 net.cpp:290] pool1 -> pool1
I0905 21:03:11.819708 32506 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:03:11.819715 32506 net.cpp:125] pool1 needs backward computation.
I0905 21:03:11.819721 32506 net.cpp:66] Creating Layer norm1
I0905 21:03:11.819727 32506 net.cpp:329] norm1 <- pool1
I0905 21:03:11.819735 32506 net.cpp:290] norm1 -> norm1
I0905 21:03:11.819744 32506 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:03:11.819751 32506 net.cpp:125] norm1 needs backward computation.
I0905 21:03:11.819758 32506 net.cpp:66] Creating Layer conv2
I0905 21:03:11.819764 32506 net.cpp:329] conv2 <- norm1
I0905 21:03:11.819772 32506 net.cpp:290] conv2 -> conv2
I0905 21:03:11.828902 32506 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:03:11.828917 32506 net.cpp:125] conv2 needs backward computation.
I0905 21:03:11.828924 32506 net.cpp:66] Creating Layer relu2
I0905 21:03:11.828930 32506 net.cpp:329] relu2 <- conv2
I0905 21:03:11.828938 32506 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:03:11.828944 32506 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:03:11.828950 32506 net.cpp:125] relu2 needs backward computation.
I0905 21:03:11.828956 32506 net.cpp:66] Creating Layer pool2
I0905 21:03:11.828963 32506 net.cpp:329] pool2 <- conv2
I0905 21:03:11.828969 32506 net.cpp:290] pool2 -> pool2
I0905 21:03:11.828977 32506 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:03:11.828984 32506 net.cpp:125] pool2 needs backward computation.
I0905 21:03:11.828992 32506 net.cpp:66] Creating Layer fc7
I0905 21:03:11.829004 32506 net.cpp:329] fc7 <- pool2
I0905 21:03:11.829011 32506 net.cpp:290] fc7 -> fc7
I0905 21:03:12.479111 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.479156 32506 net.cpp:125] fc7 needs backward computation.
I0905 21:03:12.479168 32506 net.cpp:66] Creating Layer relu7
I0905 21:03:12.479176 32506 net.cpp:329] relu7 <- fc7
I0905 21:03:12.479185 32506 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:03:12.479195 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.479202 32506 net.cpp:125] relu7 needs backward computation.
I0905 21:03:12.479209 32506 net.cpp:66] Creating Layer drop7
I0905 21:03:12.479214 32506 net.cpp:329] drop7 <- fc7
I0905 21:03:12.479221 32506 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:03:12.479233 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.479238 32506 net.cpp:125] drop7 needs backward computation.
I0905 21:03:12.479246 32506 net.cpp:66] Creating Layer fc8
I0905 21:03:12.479253 32506 net.cpp:329] fc8 <- fc7
I0905 21:03:12.479261 32506 net.cpp:290] fc8 -> fc8
I0905 21:03:12.487040 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.487052 32506 net.cpp:125] fc8 needs backward computation.
I0905 21:03:12.487059 32506 net.cpp:66] Creating Layer relu8
I0905 21:03:12.487066 32506 net.cpp:329] relu8 <- fc8
I0905 21:03:12.487073 32506 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:03:12.487082 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.487087 32506 net.cpp:125] relu8 needs backward computation.
I0905 21:03:12.487093 32506 net.cpp:66] Creating Layer drop8
I0905 21:03:12.487099 32506 net.cpp:329] drop8 <- fc8
I0905 21:03:12.487105 32506 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:03:12.487112 32506 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:03:12.487118 32506 net.cpp:125] drop8 needs backward computation.
I0905 21:03:12.487128 32506 net.cpp:66] Creating Layer fc9
I0905 21:03:12.487133 32506 net.cpp:329] fc9 <- fc8
I0905 21:03:12.487140 32506 net.cpp:290] fc9 -> fc9
I0905 21:03:12.487517 32506 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:03:12.487529 32506 net.cpp:125] fc9 needs backward computation.
I0905 21:03:12.487537 32506 net.cpp:66] Creating Layer fc10
I0905 21:03:12.487543 32506 net.cpp:329] fc10 <- fc9
I0905 21:03:12.487552 32506 net.cpp:290] fc10 -> fc10
I0905 21:03:12.487563 32506 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:03:12.487571 32506 net.cpp:125] fc10 needs backward computation.
I0905 21:03:12.487578 32506 net.cpp:66] Creating Layer prob
I0905 21:03:12.487584 32506 net.cpp:329] prob <- fc10
I0905 21:03:12.487592 32506 net.cpp:290] prob -> prob
I0905 21:03:12.487601 32506 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:03:12.487607 32506 net.cpp:125] prob needs backward computation.
I0905 21:03:12.487613 32506 net.cpp:156] This network produces output prob
I0905 21:03:12.487625 32506 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:03:12.487634 32506 net.cpp:167] Network initialization done.
I0905 21:03:12.487639 32506 net.cpp:168] Memory required for data: 6183480
Classifying 349 inputs.
Done in 221.99 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 49 is out of bounds for axis 0 with size 49
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:07:00.159426 32515 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:07:00.159564 32515 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:07:00.159574 32515 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:07:00.159719 32515 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:07:00.159783 32515 net.cpp:292] Input 0 -> data
I0905 21:07:00.159809 32515 net.cpp:66] Creating Layer conv1
I0905 21:07:00.159816 32515 net.cpp:329] conv1 <- data
I0905 21:07:00.159824 32515 net.cpp:290] conv1 -> conv1
I0905 21:07:00.161185 32515 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:07:00.161203 32515 net.cpp:125] conv1 needs backward computation.
I0905 21:07:00.161212 32515 net.cpp:66] Creating Layer relu1
I0905 21:07:00.161218 32515 net.cpp:329] relu1 <- conv1
I0905 21:07:00.161226 32515 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:07:00.161234 32515 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:07:00.161241 32515 net.cpp:125] relu1 needs backward computation.
I0905 21:07:00.161247 32515 net.cpp:66] Creating Layer pool1
I0905 21:07:00.161253 32515 net.cpp:329] pool1 <- conv1
I0905 21:07:00.161260 32515 net.cpp:290] pool1 -> pool1
I0905 21:07:00.161272 32515 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:07:00.161278 32515 net.cpp:125] pool1 needs backward computation.
I0905 21:07:00.161284 32515 net.cpp:66] Creating Layer norm1
I0905 21:07:00.161295 32515 net.cpp:329] norm1 <- pool1
I0905 21:07:00.161303 32515 net.cpp:290] norm1 -> norm1
I0905 21:07:00.161312 32515 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:07:00.161319 32515 net.cpp:125] norm1 needs backward computation.
I0905 21:07:00.161325 32515 net.cpp:66] Creating Layer conv2
I0905 21:07:00.161331 32515 net.cpp:329] conv2 <- norm1
I0905 21:07:00.161339 32515 net.cpp:290] conv2 -> conv2
I0905 21:07:00.170802 32515 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:07:00.170824 32515 net.cpp:125] conv2 needs backward computation.
I0905 21:07:00.170831 32515 net.cpp:66] Creating Layer relu2
I0905 21:07:00.170837 32515 net.cpp:329] relu2 <- conv2
I0905 21:07:00.170845 32515 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:07:00.170853 32515 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:07:00.170860 32515 net.cpp:125] relu2 needs backward computation.
I0905 21:07:00.170869 32515 net.cpp:66] Creating Layer pool2
I0905 21:07:00.170876 32515 net.cpp:329] pool2 <- conv2
I0905 21:07:00.170882 32515 net.cpp:290] pool2 -> pool2
I0905 21:07:00.170892 32515 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:07:00.170897 32515 net.cpp:125] pool2 needs backward computation.
I0905 21:07:00.170904 32515 net.cpp:66] Creating Layer fc7
I0905 21:07:00.170910 32515 net.cpp:329] fc7 <- pool2
I0905 21:07:00.170918 32515 net.cpp:290] fc7 -> fc7
I0905 21:07:00.830289 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.830334 32515 net.cpp:125] fc7 needs backward computation.
I0905 21:07:00.830348 32515 net.cpp:66] Creating Layer relu7
I0905 21:07:00.830355 32515 net.cpp:329] relu7 <- fc7
I0905 21:07:00.830364 32515 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:07:00.830374 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.830380 32515 net.cpp:125] relu7 needs backward computation.
I0905 21:07:00.830389 32515 net.cpp:66] Creating Layer drop7
I0905 21:07:00.830394 32515 net.cpp:329] drop7 <- fc7
I0905 21:07:00.830400 32515 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:07:00.830411 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.830417 32515 net.cpp:125] drop7 needs backward computation.
I0905 21:07:00.830425 32515 net.cpp:66] Creating Layer fc8
I0905 21:07:00.830431 32515 net.cpp:329] fc8 <- fc7
I0905 21:07:00.830440 32515 net.cpp:290] fc8 -> fc8
I0905 21:07:00.838330 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.838342 32515 net.cpp:125] fc8 needs backward computation.
I0905 21:07:00.838349 32515 net.cpp:66] Creating Layer relu8
I0905 21:07:00.838356 32515 net.cpp:329] relu8 <- fc8
I0905 21:07:00.838363 32515 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:07:00.838371 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.838376 32515 net.cpp:125] relu8 needs backward computation.
I0905 21:07:00.838383 32515 net.cpp:66] Creating Layer drop8
I0905 21:07:00.838388 32515 net.cpp:329] drop8 <- fc8
I0905 21:07:00.838395 32515 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:07:00.838402 32515 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:07:00.838408 32515 net.cpp:125] drop8 needs backward computation.
I0905 21:07:00.838417 32515 net.cpp:66] Creating Layer fc9
I0905 21:07:00.838423 32515 net.cpp:329] fc9 <- fc8
I0905 21:07:00.838429 32515 net.cpp:290] fc9 -> fc9
I0905 21:07:00.838809 32515 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:07:00.838820 32515 net.cpp:125] fc9 needs backward computation.
I0905 21:07:00.838829 32515 net.cpp:66] Creating Layer fc10
I0905 21:07:00.838835 32515 net.cpp:329] fc10 <- fc9
I0905 21:07:00.838842 32515 net.cpp:290] fc10 -> fc10
I0905 21:07:00.838855 32515 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:07:00.838862 32515 net.cpp:125] fc10 needs backward computation.
I0905 21:07:00.838870 32515 net.cpp:66] Creating Layer prob
I0905 21:07:00.838875 32515 net.cpp:329] prob <- fc10
I0905 21:07:00.838882 32515 net.cpp:290] prob -> prob
I0905 21:07:00.838892 32515 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:07:00.838898 32515 net.cpp:125] prob needs backward computation.
I0905 21:07:00.838903 32515 net.cpp:156] This network produces output prob
I0905 21:07:00.838925 32515 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:07:00.838934 32515 net.cpp:167] Network initialization done.
I0905 21:07:00.838939 32515 net.cpp:168] Memory required for data: 6183480
Classifying 387 inputs.
Done in 241.69 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 87 is out of bounds for axis 0 with size 87
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:11:10.636963 32537 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:11:10.637104 32537 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:11:10.637114 32537 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:11:10.637259 32537 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:11:10.637323 32537 net.cpp:292] Input 0 -> data
I0905 21:11:10.637349 32537 net.cpp:66] Creating Layer conv1
I0905 21:11:10.637356 32537 net.cpp:329] conv1 <- data
I0905 21:11:10.637364 32537 net.cpp:290] conv1 -> conv1
I0905 21:11:10.638746 32537 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:11:10.638764 32537 net.cpp:125] conv1 needs backward computation.
I0905 21:11:10.638773 32537 net.cpp:66] Creating Layer relu1
I0905 21:11:10.638779 32537 net.cpp:329] relu1 <- conv1
I0905 21:11:10.638787 32537 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:11:10.638794 32537 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:11:10.638802 32537 net.cpp:125] relu1 needs backward computation.
I0905 21:11:10.638808 32537 net.cpp:66] Creating Layer pool1
I0905 21:11:10.638813 32537 net.cpp:329] pool1 <- conv1
I0905 21:11:10.638819 32537 net.cpp:290] pool1 -> pool1
I0905 21:11:10.638831 32537 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:11:10.638838 32537 net.cpp:125] pool1 needs backward computation.
I0905 21:11:10.638844 32537 net.cpp:66] Creating Layer norm1
I0905 21:11:10.638849 32537 net.cpp:329] norm1 <- pool1
I0905 21:11:10.638855 32537 net.cpp:290] norm1 -> norm1
I0905 21:11:10.638865 32537 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:11:10.638871 32537 net.cpp:125] norm1 needs backward computation.
I0905 21:11:10.638878 32537 net.cpp:66] Creating Layer conv2
I0905 21:11:10.638885 32537 net.cpp:329] conv2 <- norm1
I0905 21:11:10.638891 32537 net.cpp:290] conv2 -> conv2
I0905 21:11:10.648025 32537 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:11:10.648039 32537 net.cpp:125] conv2 needs backward computation.
I0905 21:11:10.648046 32537 net.cpp:66] Creating Layer relu2
I0905 21:11:10.648052 32537 net.cpp:329] relu2 <- conv2
I0905 21:11:10.648059 32537 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:11:10.648066 32537 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:11:10.648072 32537 net.cpp:125] relu2 needs backward computation.
I0905 21:11:10.648078 32537 net.cpp:66] Creating Layer pool2
I0905 21:11:10.648083 32537 net.cpp:329] pool2 <- conv2
I0905 21:11:10.648090 32537 net.cpp:290] pool2 -> pool2
I0905 21:11:10.648098 32537 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:11:10.648104 32537 net.cpp:125] pool2 needs backward computation.
I0905 21:11:10.648113 32537 net.cpp:66] Creating Layer fc7
I0905 21:11:10.648119 32537 net.cpp:329] fc7 <- pool2
I0905 21:11:10.648126 32537 net.cpp:290] fc7 -> fc7
I0905 21:11:11.296259 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.296299 32537 net.cpp:125] fc7 needs backward computation.
I0905 21:11:11.296313 32537 net.cpp:66] Creating Layer relu7
I0905 21:11:11.296319 32537 net.cpp:329] relu7 <- fc7
I0905 21:11:11.296330 32537 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:11:11.296341 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.296347 32537 net.cpp:125] relu7 needs backward computation.
I0905 21:11:11.296355 32537 net.cpp:66] Creating Layer drop7
I0905 21:11:11.296360 32537 net.cpp:329] drop7 <- fc7
I0905 21:11:11.296366 32537 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:11:11.296377 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.296383 32537 net.cpp:125] drop7 needs backward computation.
I0905 21:11:11.296391 32537 net.cpp:66] Creating Layer fc8
I0905 21:11:11.296396 32537 net.cpp:329] fc8 <- fc7
I0905 21:11:11.296406 32537 net.cpp:290] fc8 -> fc8
I0905 21:11:11.304189 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.304201 32537 net.cpp:125] fc8 needs backward computation.
I0905 21:11:11.304208 32537 net.cpp:66] Creating Layer relu8
I0905 21:11:11.304214 32537 net.cpp:329] relu8 <- fc8
I0905 21:11:11.304222 32537 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:11:11.304229 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.304244 32537 net.cpp:125] relu8 needs backward computation.
I0905 21:11:11.304252 32537 net.cpp:66] Creating Layer drop8
I0905 21:11:11.304257 32537 net.cpp:329] drop8 <- fc8
I0905 21:11:11.304263 32537 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:11:11.304270 32537 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:11:11.304276 32537 net.cpp:125] drop8 needs backward computation.
I0905 21:11:11.304285 32537 net.cpp:66] Creating Layer fc9
I0905 21:11:11.304291 32537 net.cpp:329] fc9 <- fc8
I0905 21:11:11.304298 32537 net.cpp:290] fc9 -> fc9
I0905 21:11:11.304672 32537 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:11:11.304683 32537 net.cpp:125] fc9 needs backward computation.
I0905 21:11:11.304692 32537 net.cpp:66] Creating Layer fc10
I0905 21:11:11.304697 32537 net.cpp:329] fc10 <- fc9
I0905 21:11:11.304705 32537 net.cpp:290] fc10 -> fc10
I0905 21:11:11.304718 32537 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:11:11.304725 32537 net.cpp:125] fc10 needs backward computation.
I0905 21:11:11.304731 32537 net.cpp:66] Creating Layer prob
I0905 21:11:11.304738 32537 net.cpp:329] prob <- fc10
I0905 21:11:11.304745 32537 net.cpp:290] prob -> prob
I0905 21:11:11.304754 32537 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:11:11.304760 32537 net.cpp:125] prob needs backward computation.
I0905 21:11:11.304765 32537 net.cpp:156] This network produces output prob
I0905 21:11:11.304777 32537 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:11:11.304785 32537 net.cpp:167] Network initialization done.
I0905 21:11:11.304790 32537 net.cpp:168] Memory required for data: 6183480
Classifying 287 inputs.
Done in 191.57 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 87 is out of bounds for axis 0 with size 87
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:14:28.592125 32544 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:14:28.592265 32544 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:14:28.592273 32544 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:14:28.592421 32544 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:14:28.592484 32544 net.cpp:292] Input 0 -> data
I0905 21:14:28.592509 32544 net.cpp:66] Creating Layer conv1
I0905 21:14:28.592516 32544 net.cpp:329] conv1 <- data
I0905 21:14:28.592525 32544 net.cpp:290] conv1 -> conv1
I0905 21:14:28.593920 32544 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:14:28.593940 32544 net.cpp:125] conv1 needs backward computation.
I0905 21:14:28.593950 32544 net.cpp:66] Creating Layer relu1
I0905 21:14:28.593955 32544 net.cpp:329] relu1 <- conv1
I0905 21:14:28.593962 32544 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:14:28.593971 32544 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:14:28.593976 32544 net.cpp:125] relu1 needs backward computation.
I0905 21:14:28.593983 32544 net.cpp:66] Creating Layer pool1
I0905 21:14:28.593989 32544 net.cpp:329] pool1 <- conv1
I0905 21:14:28.593996 32544 net.cpp:290] pool1 -> pool1
I0905 21:14:28.594007 32544 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:14:28.594012 32544 net.cpp:125] pool1 needs backward computation.
I0905 21:14:28.594019 32544 net.cpp:66] Creating Layer norm1
I0905 21:14:28.594025 32544 net.cpp:329] norm1 <- pool1
I0905 21:14:28.594033 32544 net.cpp:290] norm1 -> norm1
I0905 21:14:28.594041 32544 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:14:28.594048 32544 net.cpp:125] norm1 needs backward computation.
I0905 21:14:28.594055 32544 net.cpp:66] Creating Layer conv2
I0905 21:14:28.594060 32544 net.cpp:329] conv2 <- norm1
I0905 21:14:28.594069 32544 net.cpp:290] conv2 -> conv2
I0905 21:14:28.603198 32544 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:14:28.603214 32544 net.cpp:125] conv2 needs backward computation.
I0905 21:14:28.603220 32544 net.cpp:66] Creating Layer relu2
I0905 21:14:28.603226 32544 net.cpp:329] relu2 <- conv2
I0905 21:14:28.603234 32544 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:14:28.603240 32544 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:14:28.603246 32544 net.cpp:125] relu2 needs backward computation.
I0905 21:14:28.603252 32544 net.cpp:66] Creating Layer pool2
I0905 21:14:28.603258 32544 net.cpp:329] pool2 <- conv2
I0905 21:14:28.603266 32544 net.cpp:290] pool2 -> pool2
I0905 21:14:28.603273 32544 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:14:28.603279 32544 net.cpp:125] pool2 needs backward computation.
I0905 21:14:28.603288 32544 net.cpp:66] Creating Layer fc7
I0905 21:14:28.603294 32544 net.cpp:329] fc7 <- pool2
I0905 21:14:28.603302 32544 net.cpp:290] fc7 -> fc7
I0905 21:14:29.254410 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.254468 32544 net.cpp:125] fc7 needs backward computation.
I0905 21:14:29.254482 32544 net.cpp:66] Creating Layer relu7
I0905 21:14:29.254489 32544 net.cpp:329] relu7 <- fc7
I0905 21:14:29.254498 32544 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:14:29.254509 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.254515 32544 net.cpp:125] relu7 needs backward computation.
I0905 21:14:29.254523 32544 net.cpp:66] Creating Layer drop7
I0905 21:14:29.254528 32544 net.cpp:329] drop7 <- fc7
I0905 21:14:29.254535 32544 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:14:29.254546 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.254552 32544 net.cpp:125] drop7 needs backward computation.
I0905 21:14:29.254561 32544 net.cpp:66] Creating Layer fc8
I0905 21:14:29.254567 32544 net.cpp:329] fc8 <- fc7
I0905 21:14:29.254576 32544 net.cpp:290] fc8 -> fc8
I0905 21:14:29.262570 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.262585 32544 net.cpp:125] fc8 needs backward computation.
I0905 21:14:29.262593 32544 net.cpp:66] Creating Layer relu8
I0905 21:14:29.262598 32544 net.cpp:329] relu8 <- fc8
I0905 21:14:29.262606 32544 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:14:29.262614 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.262620 32544 net.cpp:125] relu8 needs backward computation.
I0905 21:14:29.262626 32544 net.cpp:66] Creating Layer drop8
I0905 21:14:29.262632 32544 net.cpp:329] drop8 <- fc8
I0905 21:14:29.262639 32544 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:14:29.262646 32544 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:14:29.262652 32544 net.cpp:125] drop8 needs backward computation.
I0905 21:14:29.262661 32544 net.cpp:66] Creating Layer fc9
I0905 21:14:29.262667 32544 net.cpp:329] fc9 <- fc8
I0905 21:14:29.262675 32544 net.cpp:290] fc9 -> fc9
I0905 21:14:29.263059 32544 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:14:29.263072 32544 net.cpp:125] fc9 needs backward computation.
I0905 21:14:29.263079 32544 net.cpp:66] Creating Layer fc10
I0905 21:14:29.263085 32544 net.cpp:329] fc10 <- fc9
I0905 21:14:29.263094 32544 net.cpp:290] fc10 -> fc10
I0905 21:14:29.263106 32544 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:14:29.263114 32544 net.cpp:125] fc10 needs backward computation.
I0905 21:14:29.263121 32544 net.cpp:66] Creating Layer prob
I0905 21:14:29.263128 32544 net.cpp:329] prob <- fc10
I0905 21:14:29.263135 32544 net.cpp:290] prob -> prob
I0905 21:14:29.263145 32544 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:14:29.263151 32544 net.cpp:125] prob needs backward computation.
I0905 21:14:29.263156 32544 net.cpp:156] This network produces output prob
I0905 21:14:29.263170 32544 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:14:29.263177 32544 net.cpp:167] Network initialization done.
I0905 21:14:29.263182 32544 net.cpp:168] Memory required for data: 6183480
Classifying 358 inputs.
Done in 238.83 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 58 is out of bounds for axis 0 with size 58
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:18:35.005491 32555 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:18:35.005645 32555 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:18:35.005656 32555 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:18:35.005800 32555 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:18:35.005863 32555 net.cpp:292] Input 0 -> data
I0905 21:18:35.005890 32555 net.cpp:66] Creating Layer conv1
I0905 21:18:35.005897 32555 net.cpp:329] conv1 <- data
I0905 21:18:35.005905 32555 net.cpp:290] conv1 -> conv1
I0905 21:18:35.007266 32555 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:18:35.007283 32555 net.cpp:125] conv1 needs backward computation.
I0905 21:18:35.007292 32555 net.cpp:66] Creating Layer relu1
I0905 21:18:35.007298 32555 net.cpp:329] relu1 <- conv1
I0905 21:18:35.007305 32555 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:18:35.007314 32555 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:18:35.007320 32555 net.cpp:125] relu1 needs backward computation.
I0905 21:18:35.007328 32555 net.cpp:66] Creating Layer pool1
I0905 21:18:35.007333 32555 net.cpp:329] pool1 <- conv1
I0905 21:18:35.007339 32555 net.cpp:290] pool1 -> pool1
I0905 21:18:35.007351 32555 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:18:35.007357 32555 net.cpp:125] pool1 needs backward computation.
I0905 21:18:35.007364 32555 net.cpp:66] Creating Layer norm1
I0905 21:18:35.007370 32555 net.cpp:329] norm1 <- pool1
I0905 21:18:35.007377 32555 net.cpp:290] norm1 -> norm1
I0905 21:18:35.007391 32555 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:18:35.007398 32555 net.cpp:125] norm1 needs backward computation.
I0905 21:18:35.007405 32555 net.cpp:66] Creating Layer conv2
I0905 21:18:35.007411 32555 net.cpp:329] conv2 <- norm1
I0905 21:18:35.007419 32555 net.cpp:290] conv2 -> conv2
I0905 21:18:35.016556 32555 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:18:35.016571 32555 net.cpp:125] conv2 needs backward computation.
I0905 21:18:35.016578 32555 net.cpp:66] Creating Layer relu2
I0905 21:18:35.016584 32555 net.cpp:329] relu2 <- conv2
I0905 21:18:35.016590 32555 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:18:35.016598 32555 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:18:35.016603 32555 net.cpp:125] relu2 needs backward computation.
I0905 21:18:35.016610 32555 net.cpp:66] Creating Layer pool2
I0905 21:18:35.016615 32555 net.cpp:329] pool2 <- conv2
I0905 21:18:35.016623 32555 net.cpp:290] pool2 -> pool2
I0905 21:18:35.016630 32555 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:18:35.016636 32555 net.cpp:125] pool2 needs backward computation.
I0905 21:18:35.016646 32555 net.cpp:66] Creating Layer fc7
I0905 21:18:35.016651 32555 net.cpp:329] fc7 <- pool2
I0905 21:18:35.016659 32555 net.cpp:290] fc7 -> fc7
I0905 21:18:35.666142 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.666187 32555 net.cpp:125] fc7 needs backward computation.
I0905 21:18:35.666198 32555 net.cpp:66] Creating Layer relu7
I0905 21:18:35.666206 32555 net.cpp:329] relu7 <- fc7
I0905 21:18:35.666215 32555 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:18:35.666225 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.666231 32555 net.cpp:125] relu7 needs backward computation.
I0905 21:18:35.666239 32555 net.cpp:66] Creating Layer drop7
I0905 21:18:35.666244 32555 net.cpp:329] drop7 <- fc7
I0905 21:18:35.666250 32555 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:18:35.666261 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.666267 32555 net.cpp:125] drop7 needs backward computation.
I0905 21:18:35.666275 32555 net.cpp:66] Creating Layer fc8
I0905 21:18:35.666280 32555 net.cpp:329] fc8 <- fc7
I0905 21:18:35.666290 32555 net.cpp:290] fc8 -> fc8
I0905 21:18:35.674057 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.674070 32555 net.cpp:125] fc8 needs backward computation.
I0905 21:18:35.674077 32555 net.cpp:66] Creating Layer relu8
I0905 21:18:35.674082 32555 net.cpp:329] relu8 <- fc8
I0905 21:18:35.674090 32555 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:18:35.674098 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.674104 32555 net.cpp:125] relu8 needs backward computation.
I0905 21:18:35.674110 32555 net.cpp:66] Creating Layer drop8
I0905 21:18:35.674116 32555 net.cpp:329] drop8 <- fc8
I0905 21:18:35.674123 32555 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:18:35.674129 32555 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:18:35.674135 32555 net.cpp:125] drop8 needs backward computation.
I0905 21:18:35.674144 32555 net.cpp:66] Creating Layer fc9
I0905 21:18:35.674150 32555 net.cpp:329] fc9 <- fc8
I0905 21:18:35.674157 32555 net.cpp:290] fc9 -> fc9
I0905 21:18:35.674530 32555 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:18:35.674541 32555 net.cpp:125] fc9 needs backward computation.
I0905 21:18:35.674551 32555 net.cpp:66] Creating Layer fc10
I0905 21:18:35.674556 32555 net.cpp:329] fc10 <- fc9
I0905 21:18:35.674564 32555 net.cpp:290] fc10 -> fc10
I0905 21:18:35.674576 32555 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:18:35.674584 32555 net.cpp:125] fc10 needs backward computation.
I0905 21:18:35.674590 32555 net.cpp:66] Creating Layer prob
I0905 21:18:35.674597 32555 net.cpp:329] prob <- fc10
I0905 21:18:35.674604 32555 net.cpp:290] prob -> prob
I0905 21:18:35.674613 32555 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:18:35.674620 32555 net.cpp:125] prob needs backward computation.
I0905 21:18:35.674625 32555 net.cpp:156] This network produces output prob
I0905 21:18:35.674639 32555 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:18:35.674657 32555 net.cpp:167] Network initialization done.
I0905 21:18:35.674671 32555 net.cpp:168] Memory required for data: 6183480
Classifying 399 inputs.
Done in 250.01 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 99 is out of bounds for axis 0 with size 99
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:22:57.441356 32567 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:22:57.441495 32567 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:22:57.441504 32567 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:22:57.441685 32567 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:22:57.441751 32567 net.cpp:292] Input 0 -> data
I0905 21:22:57.441776 32567 net.cpp:66] Creating Layer conv1
I0905 21:22:57.441783 32567 net.cpp:329] conv1 <- data
I0905 21:22:57.441792 32567 net.cpp:290] conv1 -> conv1
I0905 21:22:57.443155 32567 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:22:57.443172 32567 net.cpp:125] conv1 needs backward computation.
I0905 21:22:57.443181 32567 net.cpp:66] Creating Layer relu1
I0905 21:22:57.443187 32567 net.cpp:329] relu1 <- conv1
I0905 21:22:57.443193 32567 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:22:57.443202 32567 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:22:57.443208 32567 net.cpp:125] relu1 needs backward computation.
I0905 21:22:57.443215 32567 net.cpp:66] Creating Layer pool1
I0905 21:22:57.443220 32567 net.cpp:329] pool1 <- conv1
I0905 21:22:57.443228 32567 net.cpp:290] pool1 -> pool1
I0905 21:22:57.443238 32567 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:22:57.443244 32567 net.cpp:125] pool1 needs backward computation.
I0905 21:22:57.443251 32567 net.cpp:66] Creating Layer norm1
I0905 21:22:57.443256 32567 net.cpp:329] norm1 <- pool1
I0905 21:22:57.443264 32567 net.cpp:290] norm1 -> norm1
I0905 21:22:57.443274 32567 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:22:57.443279 32567 net.cpp:125] norm1 needs backward computation.
I0905 21:22:57.443286 32567 net.cpp:66] Creating Layer conv2
I0905 21:22:57.443291 32567 net.cpp:329] conv2 <- norm1
I0905 21:22:57.443299 32567 net.cpp:290] conv2 -> conv2
I0905 21:22:57.452450 32567 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:22:57.452466 32567 net.cpp:125] conv2 needs backward computation.
I0905 21:22:57.452472 32567 net.cpp:66] Creating Layer relu2
I0905 21:22:57.452478 32567 net.cpp:329] relu2 <- conv2
I0905 21:22:57.452484 32567 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:22:57.452491 32567 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:22:57.452497 32567 net.cpp:125] relu2 needs backward computation.
I0905 21:22:57.452503 32567 net.cpp:66] Creating Layer pool2
I0905 21:22:57.452509 32567 net.cpp:329] pool2 <- conv2
I0905 21:22:57.452515 32567 net.cpp:290] pool2 -> pool2
I0905 21:22:57.452523 32567 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:22:57.452529 32567 net.cpp:125] pool2 needs backward computation.
I0905 21:22:57.452538 32567 net.cpp:66] Creating Layer fc7
I0905 21:22:57.452544 32567 net.cpp:329] fc7 <- pool2
I0905 21:22:57.452551 32567 net.cpp:290] fc7 -> fc7
I0905 21:22:58.101521 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.101565 32567 net.cpp:125] fc7 needs backward computation.
I0905 21:22:58.101582 32567 net.cpp:66] Creating Layer relu7
I0905 21:22:58.101590 32567 net.cpp:329] relu7 <- fc7
I0905 21:22:58.101599 32567 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:22:58.101609 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.101615 32567 net.cpp:125] relu7 needs backward computation.
I0905 21:22:58.101624 32567 net.cpp:66] Creating Layer drop7
I0905 21:22:58.101629 32567 net.cpp:329] drop7 <- fc7
I0905 21:22:58.101634 32567 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:22:58.101645 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.101651 32567 net.cpp:125] drop7 needs backward computation.
I0905 21:22:58.101660 32567 net.cpp:66] Creating Layer fc8
I0905 21:22:58.101665 32567 net.cpp:329] fc8 <- fc7
I0905 21:22:58.101675 32567 net.cpp:290] fc8 -> fc8
I0905 21:22:58.109463 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.109475 32567 net.cpp:125] fc8 needs backward computation.
I0905 21:22:58.109483 32567 net.cpp:66] Creating Layer relu8
I0905 21:22:58.109488 32567 net.cpp:329] relu8 <- fc8
I0905 21:22:58.109496 32567 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:22:58.109503 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.109508 32567 net.cpp:125] relu8 needs backward computation.
I0905 21:22:58.109515 32567 net.cpp:66] Creating Layer drop8
I0905 21:22:58.109531 32567 net.cpp:329] drop8 <- fc8
I0905 21:22:58.109539 32567 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:22:58.109545 32567 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:22:58.109550 32567 net.cpp:125] drop8 needs backward computation.
I0905 21:22:58.109560 32567 net.cpp:66] Creating Layer fc9
I0905 21:22:58.109565 32567 net.cpp:329] fc9 <- fc8
I0905 21:22:58.109572 32567 net.cpp:290] fc9 -> fc9
I0905 21:22:58.109956 32567 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:22:58.109969 32567 net.cpp:125] fc9 needs backward computation.
I0905 21:22:58.109977 32567 net.cpp:66] Creating Layer fc10
I0905 21:22:58.109983 32567 net.cpp:329] fc10 <- fc9
I0905 21:22:58.109992 32567 net.cpp:290] fc10 -> fc10
I0905 21:22:58.110003 32567 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:22:58.110012 32567 net.cpp:125] fc10 needs backward computation.
I0905 21:22:58.110018 32567 net.cpp:66] Creating Layer prob
I0905 21:22:58.110023 32567 net.cpp:329] prob <- fc10
I0905 21:22:58.110031 32567 net.cpp:290] prob -> prob
I0905 21:22:58.110041 32567 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:22:58.110046 32567 net.cpp:125] prob needs backward computation.
I0905 21:22:58.110051 32567 net.cpp:156] This network produces output prob
I0905 21:22:58.110064 32567 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:22:58.110072 32567 net.cpp:167] Network initialization done.
I0905 21:22:58.110077 32567 net.cpp:168] Memory required for data: 6183480
Classifying 131 inputs.
Done in 85.17 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 31 is out of bounds for axis 0 with size 31
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:24:25.980857 32573 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:24:25.980998 32573 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:24:25.981006 32573 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:24:25.981153 32573 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:24:25.981217 32573 net.cpp:292] Input 0 -> data
I0905 21:24:25.981243 32573 net.cpp:66] Creating Layer conv1
I0905 21:24:25.981251 32573 net.cpp:329] conv1 <- data
I0905 21:24:25.981258 32573 net.cpp:290] conv1 -> conv1
I0905 21:24:25.982633 32573 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:24:25.982652 32573 net.cpp:125] conv1 needs backward computation.
I0905 21:24:25.982661 32573 net.cpp:66] Creating Layer relu1
I0905 21:24:25.982667 32573 net.cpp:329] relu1 <- conv1
I0905 21:24:25.982674 32573 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:24:25.982683 32573 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:24:25.982689 32573 net.cpp:125] relu1 needs backward computation.
I0905 21:24:25.982697 32573 net.cpp:66] Creating Layer pool1
I0905 21:24:25.982702 32573 net.cpp:329] pool1 <- conv1
I0905 21:24:25.982708 32573 net.cpp:290] pool1 -> pool1
I0905 21:24:25.982720 32573 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:24:25.982727 32573 net.cpp:125] pool1 needs backward computation.
I0905 21:24:25.982733 32573 net.cpp:66] Creating Layer norm1
I0905 21:24:25.982738 32573 net.cpp:329] norm1 <- pool1
I0905 21:24:25.982745 32573 net.cpp:290] norm1 -> norm1
I0905 21:24:25.982755 32573 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:24:25.982761 32573 net.cpp:125] norm1 needs backward computation.
I0905 21:24:25.982769 32573 net.cpp:66] Creating Layer conv2
I0905 21:24:25.982774 32573 net.cpp:329] conv2 <- norm1
I0905 21:24:25.982782 32573 net.cpp:290] conv2 -> conv2
I0905 21:24:25.991953 32573 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:24:25.991969 32573 net.cpp:125] conv2 needs backward computation.
I0905 21:24:25.991976 32573 net.cpp:66] Creating Layer relu2
I0905 21:24:25.991982 32573 net.cpp:329] relu2 <- conv2
I0905 21:24:25.991989 32573 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:24:25.991997 32573 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:24:25.992002 32573 net.cpp:125] relu2 needs backward computation.
I0905 21:24:25.992012 32573 net.cpp:66] Creating Layer pool2
I0905 21:24:25.992017 32573 net.cpp:329] pool2 <- conv2
I0905 21:24:25.992024 32573 net.cpp:290] pool2 -> pool2
I0905 21:24:25.992033 32573 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:24:25.992038 32573 net.cpp:125] pool2 needs backward computation.
I0905 21:24:25.992045 32573 net.cpp:66] Creating Layer fc7
I0905 21:24:25.992050 32573 net.cpp:329] fc7 <- pool2
I0905 21:24:25.992058 32573 net.cpp:290] fc7 -> fc7
I0905 21:24:26.639996 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.640039 32573 net.cpp:125] fc7 needs backward computation.
I0905 21:24:26.640053 32573 net.cpp:66] Creating Layer relu7
I0905 21:24:26.640070 32573 net.cpp:329] relu7 <- fc7
I0905 21:24:26.640080 32573 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:24:26.640090 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.640096 32573 net.cpp:125] relu7 needs backward computation.
I0905 21:24:26.640104 32573 net.cpp:66] Creating Layer drop7
I0905 21:24:26.640110 32573 net.cpp:329] drop7 <- fc7
I0905 21:24:26.640115 32573 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:24:26.640126 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.640132 32573 net.cpp:125] drop7 needs backward computation.
I0905 21:24:26.640141 32573 net.cpp:66] Creating Layer fc8
I0905 21:24:26.640146 32573 net.cpp:329] fc8 <- fc7
I0905 21:24:26.640156 32573 net.cpp:290] fc8 -> fc8
I0905 21:24:26.648149 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.648161 32573 net.cpp:125] fc8 needs backward computation.
I0905 21:24:26.648169 32573 net.cpp:66] Creating Layer relu8
I0905 21:24:26.648175 32573 net.cpp:329] relu8 <- fc8
I0905 21:24:26.648182 32573 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:24:26.648190 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.648196 32573 net.cpp:125] relu8 needs backward computation.
I0905 21:24:26.648202 32573 net.cpp:66] Creating Layer drop8
I0905 21:24:26.648208 32573 net.cpp:329] drop8 <- fc8
I0905 21:24:26.648216 32573 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:24:26.648222 32573 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:26.648228 32573 net.cpp:125] drop8 needs backward computation.
I0905 21:24:26.648237 32573 net.cpp:66] Creating Layer fc9
I0905 21:24:26.648243 32573 net.cpp:329] fc9 <- fc8
I0905 21:24:26.648250 32573 net.cpp:290] fc9 -> fc9
I0905 21:24:26.648634 32573 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:24:26.648648 32573 net.cpp:125] fc9 needs backward computation.
I0905 21:24:26.648655 32573 net.cpp:66] Creating Layer fc10
I0905 21:24:26.648661 32573 net.cpp:329] fc10 <- fc9
I0905 21:24:26.648669 32573 net.cpp:290] fc10 -> fc10
I0905 21:24:26.648682 32573 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:24:26.648690 32573 net.cpp:125] fc10 needs backward computation.
I0905 21:24:26.648697 32573 net.cpp:66] Creating Layer prob
I0905 21:24:26.648704 32573 net.cpp:329] prob <- fc10
I0905 21:24:26.648711 32573 net.cpp:290] prob -> prob
I0905 21:24:26.648721 32573 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:24:26.648727 32573 net.cpp:125] prob needs backward computation.
I0905 21:24:26.648732 32573 net.cpp:156] This network produces output prob
I0905 21:24:26.648746 32573 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:24:26.648754 32573 net.cpp:167] Network initialization done.
I0905 21:24:26.648759 32573 net.cpp:168] Memory required for data: 6183480
Classifying 8 inputs.
Done in 5.60 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:24:33.063099 32577 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:24:33.063238 32577 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:24:33.063247 32577 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:24:33.063395 32577 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:24:33.063458 32577 net.cpp:292] Input 0 -> data
I0905 21:24:33.063484 32577 net.cpp:66] Creating Layer conv1
I0905 21:24:33.063491 32577 net.cpp:329] conv1 <- data
I0905 21:24:33.063499 32577 net.cpp:290] conv1 -> conv1
I0905 21:24:33.064863 32577 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:24:33.064882 32577 net.cpp:125] conv1 needs backward computation.
I0905 21:24:33.064892 32577 net.cpp:66] Creating Layer relu1
I0905 21:24:33.064898 32577 net.cpp:329] relu1 <- conv1
I0905 21:24:33.064904 32577 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:24:33.064913 32577 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:24:33.064918 32577 net.cpp:125] relu1 needs backward computation.
I0905 21:24:33.064925 32577 net.cpp:66] Creating Layer pool1
I0905 21:24:33.064931 32577 net.cpp:329] pool1 <- conv1
I0905 21:24:33.064939 32577 net.cpp:290] pool1 -> pool1
I0905 21:24:33.064949 32577 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:24:33.064956 32577 net.cpp:125] pool1 needs backward computation.
I0905 21:24:33.064963 32577 net.cpp:66] Creating Layer norm1
I0905 21:24:33.064968 32577 net.cpp:329] norm1 <- pool1
I0905 21:24:33.064975 32577 net.cpp:290] norm1 -> norm1
I0905 21:24:33.064985 32577 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:24:33.064991 32577 net.cpp:125] norm1 needs backward computation.
I0905 21:24:33.064998 32577 net.cpp:66] Creating Layer conv2
I0905 21:24:33.065004 32577 net.cpp:329] conv2 <- norm1
I0905 21:24:33.065011 32577 net.cpp:290] conv2 -> conv2
I0905 21:24:33.074194 32577 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:24:33.074213 32577 net.cpp:125] conv2 needs backward computation.
I0905 21:24:33.074225 32577 net.cpp:66] Creating Layer relu2
I0905 21:24:33.074231 32577 net.cpp:329] relu2 <- conv2
I0905 21:24:33.074239 32577 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:24:33.074245 32577 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:24:33.074251 32577 net.cpp:125] relu2 needs backward computation.
I0905 21:24:33.074260 32577 net.cpp:66] Creating Layer pool2
I0905 21:24:33.074266 32577 net.cpp:329] pool2 <- conv2
I0905 21:24:33.074273 32577 net.cpp:290] pool2 -> pool2
I0905 21:24:33.074281 32577 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:24:33.074287 32577 net.cpp:125] pool2 needs backward computation.
I0905 21:24:33.074295 32577 net.cpp:66] Creating Layer fc7
I0905 21:24:33.074300 32577 net.cpp:329] fc7 <- pool2
I0905 21:24:33.074307 32577 net.cpp:290] fc7 -> fc7
I0905 21:24:33.725895 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.725941 32577 net.cpp:125] fc7 needs backward computation.
I0905 21:24:33.725955 32577 net.cpp:66] Creating Layer relu7
I0905 21:24:33.725961 32577 net.cpp:329] relu7 <- fc7
I0905 21:24:33.725971 32577 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:24:33.725981 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.725987 32577 net.cpp:125] relu7 needs backward computation.
I0905 21:24:33.725994 32577 net.cpp:66] Creating Layer drop7
I0905 21:24:33.726001 32577 net.cpp:329] drop7 <- fc7
I0905 21:24:33.726006 32577 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:24:33.726018 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.726024 32577 net.cpp:125] drop7 needs backward computation.
I0905 21:24:33.726032 32577 net.cpp:66] Creating Layer fc8
I0905 21:24:33.726038 32577 net.cpp:329] fc8 <- fc7
I0905 21:24:33.726047 32577 net.cpp:290] fc8 -> fc8
I0905 21:24:33.734074 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.734087 32577 net.cpp:125] fc8 needs backward computation.
I0905 21:24:33.734094 32577 net.cpp:66] Creating Layer relu8
I0905 21:24:33.734100 32577 net.cpp:329] relu8 <- fc8
I0905 21:24:33.734108 32577 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:24:33.734117 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.734122 32577 net.cpp:125] relu8 needs backward computation.
I0905 21:24:33.734128 32577 net.cpp:66] Creating Layer drop8
I0905 21:24:33.734134 32577 net.cpp:329] drop8 <- fc8
I0905 21:24:33.734140 32577 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:24:33.734148 32577 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:24:33.734154 32577 net.cpp:125] drop8 needs backward computation.
I0905 21:24:33.734165 32577 net.cpp:66] Creating Layer fc9
I0905 21:24:33.734171 32577 net.cpp:329] fc9 <- fc8
I0905 21:24:33.734179 32577 net.cpp:290] fc9 -> fc9
I0905 21:24:33.734565 32577 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:24:33.734576 32577 net.cpp:125] fc9 needs backward computation.
I0905 21:24:33.734586 32577 net.cpp:66] Creating Layer fc10
I0905 21:24:33.734591 32577 net.cpp:329] fc10 <- fc9
I0905 21:24:33.734599 32577 net.cpp:290] fc10 -> fc10
I0905 21:24:33.734611 32577 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:24:33.734621 32577 net.cpp:125] fc10 needs backward computation.
I0905 21:24:33.734627 32577 net.cpp:66] Creating Layer prob
I0905 21:24:33.734632 32577 net.cpp:329] prob <- fc10
I0905 21:24:33.734642 32577 net.cpp:290] prob -> prob
I0905 21:24:33.734650 32577 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:24:33.734657 32577 net.cpp:125] prob needs backward computation.
I0905 21:24:33.734663 32577 net.cpp:156] This network produces output prob
I0905 21:24:33.734675 32577 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:24:33.734684 32577 net.cpp:167] Network initialization done.
I0905 21:24:33.734689 32577 net.cpp:168] Memory required for data: 6183480
Classifying 410 inputs.
Done in 260.48 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 10 is out of bounds for axis 0 with size 10
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:29:10.623674 32589 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:29:10.623833 32589 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:29:10.623842 32589 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:29:10.623992 32589 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:29:10.624044 32589 net.cpp:292] Input 0 -> data
I0905 21:29:10.624070 32589 net.cpp:66] Creating Layer conv1
I0905 21:29:10.624078 32589 net.cpp:329] conv1 <- data
I0905 21:29:10.624086 32589 net.cpp:290] conv1 -> conv1
I0905 21:29:10.625447 32589 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:29:10.625474 32589 net.cpp:125] conv1 needs backward computation.
I0905 21:29:10.625483 32589 net.cpp:66] Creating Layer relu1
I0905 21:29:10.625490 32589 net.cpp:329] relu1 <- conv1
I0905 21:29:10.625497 32589 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:29:10.625505 32589 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:29:10.625511 32589 net.cpp:125] relu1 needs backward computation.
I0905 21:29:10.625519 32589 net.cpp:66] Creating Layer pool1
I0905 21:29:10.625524 32589 net.cpp:329] pool1 <- conv1
I0905 21:29:10.625531 32589 net.cpp:290] pool1 -> pool1
I0905 21:29:10.625542 32589 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:29:10.625548 32589 net.cpp:125] pool1 needs backward computation.
I0905 21:29:10.625555 32589 net.cpp:66] Creating Layer norm1
I0905 21:29:10.625561 32589 net.cpp:329] norm1 <- pool1
I0905 21:29:10.625568 32589 net.cpp:290] norm1 -> norm1
I0905 21:29:10.625587 32589 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:29:10.625596 32589 net.cpp:125] norm1 needs backward computation.
I0905 21:29:10.625610 32589 net.cpp:66] Creating Layer conv2
I0905 21:29:10.625617 32589 net.cpp:329] conv2 <- norm1
I0905 21:29:10.625624 32589 net.cpp:290] conv2 -> conv2
I0905 21:29:10.634750 32589 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:29:10.634765 32589 net.cpp:125] conv2 needs backward computation.
I0905 21:29:10.634773 32589 net.cpp:66] Creating Layer relu2
I0905 21:29:10.634779 32589 net.cpp:329] relu2 <- conv2
I0905 21:29:10.634785 32589 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:29:10.634793 32589 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:29:10.634799 32589 net.cpp:125] relu2 needs backward computation.
I0905 21:29:10.634805 32589 net.cpp:66] Creating Layer pool2
I0905 21:29:10.634810 32589 net.cpp:329] pool2 <- conv2
I0905 21:29:10.634817 32589 net.cpp:290] pool2 -> pool2
I0905 21:29:10.634825 32589 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:29:10.634831 32589 net.cpp:125] pool2 needs backward computation.
I0905 21:29:10.634841 32589 net.cpp:66] Creating Layer fc7
I0905 21:29:10.634847 32589 net.cpp:329] fc7 <- pool2
I0905 21:29:10.634855 32589 net.cpp:290] fc7 -> fc7
I0905 21:29:11.281723 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.281762 32589 net.cpp:125] fc7 needs backward computation.
I0905 21:29:11.281776 32589 net.cpp:66] Creating Layer relu7
I0905 21:29:11.281785 32589 net.cpp:329] relu7 <- fc7
I0905 21:29:11.281793 32589 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:29:11.281805 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.281810 32589 net.cpp:125] relu7 needs backward computation.
I0905 21:29:11.281818 32589 net.cpp:66] Creating Layer drop7
I0905 21:29:11.281824 32589 net.cpp:329] drop7 <- fc7
I0905 21:29:11.281831 32589 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:29:11.281841 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.281847 32589 net.cpp:125] drop7 needs backward computation.
I0905 21:29:11.281857 32589 net.cpp:66] Creating Layer fc8
I0905 21:29:11.281862 32589 net.cpp:329] fc8 <- fc7
I0905 21:29:11.281872 32589 net.cpp:290] fc8 -> fc8
I0905 21:29:11.289770 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.289783 32589 net.cpp:125] fc8 needs backward computation.
I0905 21:29:11.289790 32589 net.cpp:66] Creating Layer relu8
I0905 21:29:11.289796 32589 net.cpp:329] relu8 <- fc8
I0905 21:29:11.289804 32589 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:29:11.289811 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.289818 32589 net.cpp:125] relu8 needs backward computation.
I0905 21:29:11.289824 32589 net.cpp:66] Creating Layer drop8
I0905 21:29:11.289829 32589 net.cpp:329] drop8 <- fc8
I0905 21:29:11.289835 32589 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:29:11.289842 32589 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:29:11.289849 32589 net.cpp:125] drop8 needs backward computation.
I0905 21:29:11.289857 32589 net.cpp:66] Creating Layer fc9
I0905 21:29:11.289863 32589 net.cpp:329] fc9 <- fc8
I0905 21:29:11.289870 32589 net.cpp:290] fc9 -> fc9
I0905 21:29:11.290254 32589 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:29:11.290266 32589 net.cpp:125] fc9 needs backward computation.
I0905 21:29:11.290274 32589 net.cpp:66] Creating Layer fc10
I0905 21:29:11.290280 32589 net.cpp:329] fc10 <- fc9
I0905 21:29:11.290289 32589 net.cpp:290] fc10 -> fc10
I0905 21:29:11.290302 32589 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:29:11.290309 32589 net.cpp:125] fc10 needs backward computation.
I0905 21:29:11.290316 32589 net.cpp:66] Creating Layer prob
I0905 21:29:11.290323 32589 net.cpp:329] prob <- fc10
I0905 21:29:11.290330 32589 net.cpp:290] prob -> prob
I0905 21:29:11.290339 32589 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:29:11.290345 32589 net.cpp:125] prob needs backward computation.
I0905 21:29:11.290350 32589 net.cpp:156] This network produces output prob
I0905 21:29:11.290364 32589 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:29:11.290371 32589 net.cpp:167] Network initialization done.
I0905 21:29:11.290376 32589 net.cpp:168] Memory required for data: 6183480
Classifying 131 inputs.
Done in 86.76 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 31 is out of bounds for axis 0 with size 31
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:30:42.413238 32594 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:30:42.413377 32594 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:30:42.413385 32594 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:30:42.413532 32594 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:30:42.413612 32594 net.cpp:292] Input 0 -> data
I0905 21:30:42.413640 32594 net.cpp:66] Creating Layer conv1
I0905 21:30:42.413647 32594 net.cpp:329] conv1 <- data
I0905 21:30:42.413656 32594 net.cpp:290] conv1 -> conv1
I0905 21:30:42.415035 32594 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:30:42.415053 32594 net.cpp:125] conv1 needs backward computation.
I0905 21:30:42.415063 32594 net.cpp:66] Creating Layer relu1
I0905 21:30:42.415069 32594 net.cpp:329] relu1 <- conv1
I0905 21:30:42.415076 32594 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:30:42.415084 32594 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:30:42.415091 32594 net.cpp:125] relu1 needs backward computation.
I0905 21:30:42.415097 32594 net.cpp:66] Creating Layer pool1
I0905 21:30:42.415103 32594 net.cpp:329] pool1 <- conv1
I0905 21:30:42.415109 32594 net.cpp:290] pool1 -> pool1
I0905 21:30:42.415122 32594 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:30:42.415127 32594 net.cpp:125] pool1 needs backward computation.
I0905 21:30:42.415134 32594 net.cpp:66] Creating Layer norm1
I0905 21:30:42.415139 32594 net.cpp:329] norm1 <- pool1
I0905 21:30:42.415146 32594 net.cpp:290] norm1 -> norm1
I0905 21:30:42.415156 32594 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:30:42.415163 32594 net.cpp:125] norm1 needs backward computation.
I0905 21:30:42.415169 32594 net.cpp:66] Creating Layer conv2
I0905 21:30:42.415175 32594 net.cpp:329] conv2 <- norm1
I0905 21:30:42.415182 32594 net.cpp:290] conv2 -> conv2
I0905 21:30:42.424290 32594 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:30:42.424305 32594 net.cpp:125] conv2 needs backward computation.
I0905 21:30:42.424312 32594 net.cpp:66] Creating Layer relu2
I0905 21:30:42.424319 32594 net.cpp:329] relu2 <- conv2
I0905 21:30:42.424325 32594 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:30:42.424332 32594 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:30:42.424338 32594 net.cpp:125] relu2 needs backward computation.
I0905 21:30:42.424346 32594 net.cpp:66] Creating Layer pool2
I0905 21:30:42.424352 32594 net.cpp:329] pool2 <- conv2
I0905 21:30:42.424360 32594 net.cpp:290] pool2 -> pool2
I0905 21:30:42.424367 32594 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:30:42.424373 32594 net.cpp:125] pool2 needs backward computation.
I0905 21:30:42.424381 32594 net.cpp:66] Creating Layer fc7
I0905 21:30:42.424386 32594 net.cpp:329] fc7 <- pool2
I0905 21:30:42.424392 32594 net.cpp:290] fc7 -> fc7
I0905 21:30:43.073850 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.073894 32594 net.cpp:125] fc7 needs backward computation.
I0905 21:30:43.073907 32594 net.cpp:66] Creating Layer relu7
I0905 21:30:43.073915 32594 net.cpp:329] relu7 <- fc7
I0905 21:30:43.073925 32594 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:30:43.073935 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.073940 32594 net.cpp:125] relu7 needs backward computation.
I0905 21:30:43.073948 32594 net.cpp:66] Creating Layer drop7
I0905 21:30:43.073953 32594 net.cpp:329] drop7 <- fc7
I0905 21:30:43.073971 32594 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:30:43.073983 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.073989 32594 net.cpp:125] drop7 needs backward computation.
I0905 21:30:43.073998 32594 net.cpp:66] Creating Layer fc8
I0905 21:30:43.074004 32594 net.cpp:329] fc8 <- fc7
I0905 21:30:43.074015 32594 net.cpp:290] fc8 -> fc8
I0905 21:30:43.082021 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.082034 32594 net.cpp:125] fc8 needs backward computation.
I0905 21:30:43.082042 32594 net.cpp:66] Creating Layer relu8
I0905 21:30:43.082047 32594 net.cpp:329] relu8 <- fc8
I0905 21:30:43.082056 32594 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:30:43.082063 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.082069 32594 net.cpp:125] relu8 needs backward computation.
I0905 21:30:43.082075 32594 net.cpp:66] Creating Layer drop8
I0905 21:30:43.082082 32594 net.cpp:329] drop8 <- fc8
I0905 21:30:43.082087 32594 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:30:43.082094 32594 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:43.082100 32594 net.cpp:125] drop8 needs backward computation.
I0905 21:30:43.082108 32594 net.cpp:66] Creating Layer fc9
I0905 21:30:43.082114 32594 net.cpp:329] fc9 <- fc8
I0905 21:30:43.082123 32594 net.cpp:290] fc9 -> fc9
I0905 21:30:43.082506 32594 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:30:43.082519 32594 net.cpp:125] fc9 needs backward computation.
I0905 21:30:43.082526 32594 net.cpp:66] Creating Layer fc10
I0905 21:30:43.082532 32594 net.cpp:329] fc10 <- fc9
I0905 21:30:43.082541 32594 net.cpp:290] fc10 -> fc10
I0905 21:30:43.082553 32594 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:30:43.082561 32594 net.cpp:125] fc10 needs backward computation.
I0905 21:30:43.082568 32594 net.cpp:66] Creating Layer prob
I0905 21:30:43.082574 32594 net.cpp:329] prob <- fc10
I0905 21:30:43.082582 32594 net.cpp:290] prob -> prob
I0905 21:30:43.082592 32594 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:30:43.082597 32594 net.cpp:125] prob needs backward computation.
I0905 21:30:43.082602 32594 net.cpp:156] This network produces output prob
I0905 21:30:43.082617 32594 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:30:43.082624 32594 net.cpp:167] Network initialization done.
I0905 21:30:43.082629 32594 net.cpp:168] Memory required for data: 6183480
Classifying 16 inputs.
Done in 10.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:30:54.473789 32598 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:30:54.473928 32598 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:30:54.473937 32598 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:30:54.474084 32598 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:30:54.474148 32598 net.cpp:292] Input 0 -> data
I0905 21:30:54.474174 32598 net.cpp:66] Creating Layer conv1
I0905 21:30:54.474180 32598 net.cpp:329] conv1 <- data
I0905 21:30:54.474189 32598 net.cpp:290] conv1 -> conv1
I0905 21:30:54.475548 32598 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:30:54.475566 32598 net.cpp:125] conv1 needs backward computation.
I0905 21:30:54.475575 32598 net.cpp:66] Creating Layer relu1
I0905 21:30:54.475581 32598 net.cpp:329] relu1 <- conv1
I0905 21:30:54.475589 32598 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:30:54.475596 32598 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:30:54.475603 32598 net.cpp:125] relu1 needs backward computation.
I0905 21:30:54.475610 32598 net.cpp:66] Creating Layer pool1
I0905 21:30:54.475615 32598 net.cpp:329] pool1 <- conv1
I0905 21:30:54.475622 32598 net.cpp:290] pool1 -> pool1
I0905 21:30:54.475633 32598 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:30:54.475641 32598 net.cpp:125] pool1 needs backward computation.
I0905 21:30:54.475646 32598 net.cpp:66] Creating Layer norm1
I0905 21:30:54.475652 32598 net.cpp:329] norm1 <- pool1
I0905 21:30:54.475659 32598 net.cpp:290] norm1 -> norm1
I0905 21:30:54.475669 32598 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:30:54.475675 32598 net.cpp:125] norm1 needs backward computation.
I0905 21:30:54.475682 32598 net.cpp:66] Creating Layer conv2
I0905 21:30:54.475688 32598 net.cpp:329] conv2 <- norm1
I0905 21:30:54.475695 32598 net.cpp:290] conv2 -> conv2
I0905 21:30:54.484823 32598 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:30:54.484838 32598 net.cpp:125] conv2 needs backward computation.
I0905 21:30:54.484846 32598 net.cpp:66] Creating Layer relu2
I0905 21:30:54.484853 32598 net.cpp:329] relu2 <- conv2
I0905 21:30:54.484859 32598 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:30:54.484866 32598 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:30:54.484872 32598 net.cpp:125] relu2 needs backward computation.
I0905 21:30:54.484879 32598 net.cpp:66] Creating Layer pool2
I0905 21:30:54.484889 32598 net.cpp:329] pool2 <- conv2
I0905 21:30:54.484896 32598 net.cpp:290] pool2 -> pool2
I0905 21:30:54.484905 32598 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:30:54.484911 32598 net.cpp:125] pool2 needs backward computation.
I0905 21:30:54.484920 32598 net.cpp:66] Creating Layer fc7
I0905 21:30:54.484926 32598 net.cpp:329] fc7 <- pool2
I0905 21:30:54.484933 32598 net.cpp:290] fc7 -> fc7
I0905 21:30:55.135079 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.135125 32598 net.cpp:125] fc7 needs backward computation.
I0905 21:30:55.135138 32598 net.cpp:66] Creating Layer relu7
I0905 21:30:55.135145 32598 net.cpp:329] relu7 <- fc7
I0905 21:30:55.135154 32598 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:30:55.135165 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.135171 32598 net.cpp:125] relu7 needs backward computation.
I0905 21:30:55.135179 32598 net.cpp:66] Creating Layer drop7
I0905 21:30:55.135185 32598 net.cpp:329] drop7 <- fc7
I0905 21:30:55.135190 32598 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:30:55.135201 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.135207 32598 net.cpp:125] drop7 needs backward computation.
I0905 21:30:55.135215 32598 net.cpp:66] Creating Layer fc8
I0905 21:30:55.135221 32598 net.cpp:329] fc8 <- fc7
I0905 21:30:55.135231 32598 net.cpp:290] fc8 -> fc8
I0905 21:30:55.143000 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.143013 32598 net.cpp:125] fc8 needs backward computation.
I0905 21:30:55.143019 32598 net.cpp:66] Creating Layer relu8
I0905 21:30:55.143025 32598 net.cpp:329] relu8 <- fc8
I0905 21:30:55.143033 32598 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:30:55.143040 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.143046 32598 net.cpp:125] relu8 needs backward computation.
I0905 21:30:55.143054 32598 net.cpp:66] Creating Layer drop8
I0905 21:30:55.143059 32598 net.cpp:329] drop8 <- fc8
I0905 21:30:55.143064 32598 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:30:55.143071 32598 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:30:55.143077 32598 net.cpp:125] drop8 needs backward computation.
I0905 21:30:55.143086 32598 net.cpp:66] Creating Layer fc9
I0905 21:30:55.143092 32598 net.cpp:329] fc9 <- fc8
I0905 21:30:55.143100 32598 net.cpp:290] fc9 -> fc9
I0905 21:30:55.143489 32598 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:30:55.143502 32598 net.cpp:125] fc9 needs backward computation.
I0905 21:30:55.143510 32598 net.cpp:66] Creating Layer fc10
I0905 21:30:55.143517 32598 net.cpp:329] fc10 <- fc9
I0905 21:30:55.143525 32598 net.cpp:290] fc10 -> fc10
I0905 21:30:55.143537 32598 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:30:55.143545 32598 net.cpp:125] fc10 needs backward computation.
I0905 21:30:55.143551 32598 net.cpp:66] Creating Layer prob
I0905 21:30:55.143558 32598 net.cpp:329] prob <- fc10
I0905 21:30:55.143566 32598 net.cpp:290] prob -> prob
I0905 21:30:55.143576 32598 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:30:55.143582 32598 net.cpp:125] prob needs backward computation.
I0905 21:30:55.143587 32598 net.cpp:156] This network produces output prob
I0905 21:30:55.143600 32598 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:30:55.143609 32598 net.cpp:167] Network initialization done.
I0905 21:30:55.143615 32598 net.cpp:168] Memory required for data: 6183480
Classifying 26 inputs.
Done in 15.98 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:31:12.151976 32603 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:31:12.152114 32603 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:31:12.152123 32603 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:31:12.152269 32603 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:31:12.152331 32603 net.cpp:292] Input 0 -> data
I0905 21:31:12.152358 32603 net.cpp:66] Creating Layer conv1
I0905 21:31:12.152364 32603 net.cpp:329] conv1 <- data
I0905 21:31:12.152372 32603 net.cpp:290] conv1 -> conv1
I0905 21:31:12.153744 32603 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:31:12.153764 32603 net.cpp:125] conv1 needs backward computation.
I0905 21:31:12.153772 32603 net.cpp:66] Creating Layer relu1
I0905 21:31:12.153779 32603 net.cpp:329] relu1 <- conv1
I0905 21:31:12.153785 32603 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:31:12.153794 32603 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:31:12.153800 32603 net.cpp:125] relu1 needs backward computation.
I0905 21:31:12.153806 32603 net.cpp:66] Creating Layer pool1
I0905 21:31:12.153812 32603 net.cpp:329] pool1 <- conv1
I0905 21:31:12.153818 32603 net.cpp:290] pool1 -> pool1
I0905 21:31:12.153831 32603 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:31:12.153836 32603 net.cpp:125] pool1 needs backward computation.
I0905 21:31:12.153847 32603 net.cpp:66] Creating Layer norm1
I0905 21:31:12.153853 32603 net.cpp:329] norm1 <- pool1
I0905 21:31:12.153861 32603 net.cpp:290] norm1 -> norm1
I0905 21:31:12.153870 32603 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:31:12.153877 32603 net.cpp:125] norm1 needs backward computation.
I0905 21:31:12.153883 32603 net.cpp:66] Creating Layer conv2
I0905 21:31:12.153889 32603 net.cpp:329] conv2 <- norm1
I0905 21:31:12.153897 32603 net.cpp:290] conv2 -> conv2
I0905 21:31:12.163036 32603 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:31:12.163051 32603 net.cpp:125] conv2 needs backward computation.
I0905 21:31:12.163058 32603 net.cpp:66] Creating Layer relu2
I0905 21:31:12.163064 32603 net.cpp:329] relu2 <- conv2
I0905 21:31:12.163071 32603 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:31:12.163079 32603 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:31:12.163084 32603 net.cpp:125] relu2 needs backward computation.
I0905 21:31:12.163090 32603 net.cpp:66] Creating Layer pool2
I0905 21:31:12.163096 32603 net.cpp:329] pool2 <- conv2
I0905 21:31:12.163102 32603 net.cpp:290] pool2 -> pool2
I0905 21:31:12.163110 32603 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:31:12.163116 32603 net.cpp:125] pool2 needs backward computation.
I0905 21:31:12.163125 32603 net.cpp:66] Creating Layer fc7
I0905 21:31:12.163131 32603 net.cpp:329] fc7 <- pool2
I0905 21:31:12.163139 32603 net.cpp:290] fc7 -> fc7
I0905 21:31:12.811017 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.811063 32603 net.cpp:125] fc7 needs backward computation.
I0905 21:31:12.811074 32603 net.cpp:66] Creating Layer relu7
I0905 21:31:12.811082 32603 net.cpp:329] relu7 <- fc7
I0905 21:31:12.811091 32603 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:31:12.811101 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.811107 32603 net.cpp:125] relu7 needs backward computation.
I0905 21:31:12.811115 32603 net.cpp:66] Creating Layer drop7
I0905 21:31:12.811120 32603 net.cpp:329] drop7 <- fc7
I0905 21:31:12.811126 32603 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:31:12.811136 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.811142 32603 net.cpp:125] drop7 needs backward computation.
I0905 21:31:12.811151 32603 net.cpp:66] Creating Layer fc8
I0905 21:31:12.811156 32603 net.cpp:329] fc8 <- fc7
I0905 21:31:12.811166 32603 net.cpp:290] fc8 -> fc8
I0905 21:31:12.818959 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.818972 32603 net.cpp:125] fc8 needs backward computation.
I0905 21:31:12.818979 32603 net.cpp:66] Creating Layer relu8
I0905 21:31:12.818984 32603 net.cpp:329] relu8 <- fc8
I0905 21:31:12.818994 32603 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:31:12.819001 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.819006 32603 net.cpp:125] relu8 needs backward computation.
I0905 21:31:12.819013 32603 net.cpp:66] Creating Layer drop8
I0905 21:31:12.819018 32603 net.cpp:329] drop8 <- fc8
I0905 21:31:12.819025 32603 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:31:12.819032 32603 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:31:12.819037 32603 net.cpp:125] drop8 needs backward computation.
I0905 21:31:12.819046 32603 net.cpp:66] Creating Layer fc9
I0905 21:31:12.819051 32603 net.cpp:329] fc9 <- fc8
I0905 21:31:12.819058 32603 net.cpp:290] fc9 -> fc9
I0905 21:31:12.819432 32603 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:31:12.819443 32603 net.cpp:125] fc9 needs backward computation.
I0905 21:31:12.819452 32603 net.cpp:66] Creating Layer fc10
I0905 21:31:12.819458 32603 net.cpp:329] fc10 <- fc9
I0905 21:31:12.819465 32603 net.cpp:290] fc10 -> fc10
I0905 21:31:12.819478 32603 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:31:12.819485 32603 net.cpp:125] fc10 needs backward computation.
I0905 21:31:12.819491 32603 net.cpp:66] Creating Layer prob
I0905 21:31:12.819497 32603 net.cpp:329] prob <- fc10
I0905 21:31:12.819504 32603 net.cpp:290] prob -> prob
I0905 21:31:12.819514 32603 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:31:12.819530 32603 net.cpp:125] prob needs backward computation.
I0905 21:31:12.819535 32603 net.cpp:156] This network produces output prob
I0905 21:31:12.819548 32603 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:31:12.819557 32603 net.cpp:167] Network initialization done.
I0905 21:31:12.819562 32603 net.cpp:168] Memory required for data: 6183480
Classifying 644 inputs.
Done in 633.10 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 44 is out of bounds for axis 0 with size 44
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:42:13.926292 32633 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:42:13.926439 32633 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:42:13.926447 32633 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:42:13.926595 32633 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:42:13.926656 32633 net.cpp:292] Input 0 -> data
I0905 21:42:13.926682 32633 net.cpp:66] Creating Layer conv1
I0905 21:42:13.926689 32633 net.cpp:329] conv1 <- data
I0905 21:42:13.926697 32633 net.cpp:290] conv1 -> conv1
I0905 21:42:13.951596 32633 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:42:13.951622 32633 net.cpp:125] conv1 needs backward computation.
I0905 21:42:13.951632 32633 net.cpp:66] Creating Layer relu1
I0905 21:42:13.951638 32633 net.cpp:329] relu1 <- conv1
I0905 21:42:13.951645 32633 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:42:13.951653 32633 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:42:13.951659 32633 net.cpp:125] relu1 needs backward computation.
I0905 21:42:13.951666 32633 net.cpp:66] Creating Layer pool1
I0905 21:42:13.951673 32633 net.cpp:329] pool1 <- conv1
I0905 21:42:13.951678 32633 net.cpp:290] pool1 -> pool1
I0905 21:42:13.951689 32633 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:42:13.951695 32633 net.cpp:125] pool1 needs backward computation.
I0905 21:42:13.951702 32633 net.cpp:66] Creating Layer norm1
I0905 21:42:13.951709 32633 net.cpp:329] norm1 <- pool1
I0905 21:42:13.951714 32633 net.cpp:290] norm1 -> norm1
I0905 21:42:13.951725 32633 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:42:13.951730 32633 net.cpp:125] norm1 needs backward computation.
I0905 21:42:13.951737 32633 net.cpp:66] Creating Layer conv2
I0905 21:42:13.951743 32633 net.cpp:329] conv2 <- norm1
I0905 21:42:13.951750 32633 net.cpp:290] conv2 -> conv2
I0905 21:42:13.960856 32633 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:42:13.960871 32633 net.cpp:125] conv2 needs backward computation.
I0905 21:42:13.960878 32633 net.cpp:66] Creating Layer relu2
I0905 21:42:13.960885 32633 net.cpp:329] relu2 <- conv2
I0905 21:42:13.960891 32633 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:42:13.960897 32633 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:42:13.960903 32633 net.cpp:125] relu2 needs backward computation.
I0905 21:42:13.960909 32633 net.cpp:66] Creating Layer pool2
I0905 21:42:13.960914 32633 net.cpp:329] pool2 <- conv2
I0905 21:42:13.960922 32633 net.cpp:290] pool2 -> pool2
I0905 21:42:13.960929 32633 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:42:13.960934 32633 net.cpp:125] pool2 needs backward computation.
I0905 21:42:13.960943 32633 net.cpp:66] Creating Layer fc7
I0905 21:42:13.960949 32633 net.cpp:329] fc7 <- pool2
I0905 21:42:13.960958 32633 net.cpp:290] fc7 -> fc7
I0905 21:42:14.609558 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.609612 32633 net.cpp:125] fc7 needs backward computation.
I0905 21:42:14.609626 32633 net.cpp:66] Creating Layer relu7
I0905 21:42:14.609632 32633 net.cpp:329] relu7 <- fc7
I0905 21:42:14.609644 32633 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:42:14.609654 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.609660 32633 net.cpp:125] relu7 needs backward computation.
I0905 21:42:14.609668 32633 net.cpp:66] Creating Layer drop7
I0905 21:42:14.609673 32633 net.cpp:329] drop7 <- fc7
I0905 21:42:14.609680 32633 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:42:14.609691 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.609697 32633 net.cpp:125] drop7 needs backward computation.
I0905 21:42:14.609706 32633 net.cpp:66] Creating Layer fc8
I0905 21:42:14.609712 32633 net.cpp:329] fc8 <- fc7
I0905 21:42:14.609724 32633 net.cpp:290] fc8 -> fc8
I0905 21:42:14.617631 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.617643 32633 net.cpp:125] fc8 needs backward computation.
I0905 21:42:14.617650 32633 net.cpp:66] Creating Layer relu8
I0905 21:42:14.617656 32633 net.cpp:329] relu8 <- fc8
I0905 21:42:14.617665 32633 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:42:14.617683 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.617689 32633 net.cpp:125] relu8 needs backward computation.
I0905 21:42:14.617696 32633 net.cpp:66] Creating Layer drop8
I0905 21:42:14.617702 32633 net.cpp:329] drop8 <- fc8
I0905 21:42:14.617707 32633 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:42:14.617714 32633 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:42:14.617719 32633 net.cpp:125] drop8 needs backward computation.
I0905 21:42:14.617728 32633 net.cpp:66] Creating Layer fc9
I0905 21:42:14.617734 32633 net.cpp:329] fc9 <- fc8
I0905 21:42:14.617740 32633 net.cpp:290] fc9 -> fc9
I0905 21:42:14.618114 32633 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:42:14.618125 32633 net.cpp:125] fc9 needs backward computation.
I0905 21:42:14.618134 32633 net.cpp:66] Creating Layer fc10
I0905 21:42:14.618139 32633 net.cpp:329] fc10 <- fc9
I0905 21:42:14.618149 32633 net.cpp:290] fc10 -> fc10
I0905 21:42:14.618160 32633 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:42:14.618167 32633 net.cpp:125] fc10 needs backward computation.
I0905 21:42:14.618175 32633 net.cpp:66] Creating Layer prob
I0905 21:42:14.618180 32633 net.cpp:329] prob <- fc10
I0905 21:42:14.618188 32633 net.cpp:290] prob -> prob
I0905 21:42:14.618197 32633 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:42:14.618203 32633 net.cpp:125] prob needs backward computation.
I0905 21:42:14.618208 32633 net.cpp:156] This network produces output prob
I0905 21:42:14.618221 32633 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:42:14.618229 32633 net.cpp:167] Network initialization done.
I0905 21:42:14.618234 32633 net.cpp:168] Memory required for data: 6183480
Classifying 70 inputs.
Done in 47.07 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:43:05.699867 32638 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:43:05.700006 32638 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:43:05.700014 32638 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:43:05.700162 32638 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:43:05.700224 32638 net.cpp:292] Input 0 -> data
I0905 21:43:05.700251 32638 net.cpp:66] Creating Layer conv1
I0905 21:43:05.700258 32638 net.cpp:329] conv1 <- data
I0905 21:43:05.700266 32638 net.cpp:290] conv1 -> conv1
I0905 21:43:05.701637 32638 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:43:05.701655 32638 net.cpp:125] conv1 needs backward computation.
I0905 21:43:05.701664 32638 net.cpp:66] Creating Layer relu1
I0905 21:43:05.701671 32638 net.cpp:329] relu1 <- conv1
I0905 21:43:05.701678 32638 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:43:05.701686 32638 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:43:05.701692 32638 net.cpp:125] relu1 needs backward computation.
I0905 21:43:05.701699 32638 net.cpp:66] Creating Layer pool1
I0905 21:43:05.701704 32638 net.cpp:329] pool1 <- conv1
I0905 21:43:05.701711 32638 net.cpp:290] pool1 -> pool1
I0905 21:43:05.701722 32638 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:43:05.701728 32638 net.cpp:125] pool1 needs backward computation.
I0905 21:43:05.701735 32638 net.cpp:66] Creating Layer norm1
I0905 21:43:05.701740 32638 net.cpp:329] norm1 <- pool1
I0905 21:43:05.701747 32638 net.cpp:290] norm1 -> norm1
I0905 21:43:05.701757 32638 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:43:05.701763 32638 net.cpp:125] norm1 needs backward computation.
I0905 21:43:05.701771 32638 net.cpp:66] Creating Layer conv2
I0905 21:43:05.701776 32638 net.cpp:329] conv2 <- norm1
I0905 21:43:05.701783 32638 net.cpp:290] conv2 -> conv2
I0905 21:43:05.710917 32638 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:43:05.710932 32638 net.cpp:125] conv2 needs backward computation.
I0905 21:43:05.710938 32638 net.cpp:66] Creating Layer relu2
I0905 21:43:05.710944 32638 net.cpp:329] relu2 <- conv2
I0905 21:43:05.710952 32638 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:43:05.710958 32638 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:43:05.710964 32638 net.cpp:125] relu2 needs backward computation.
I0905 21:43:05.710970 32638 net.cpp:66] Creating Layer pool2
I0905 21:43:05.710975 32638 net.cpp:329] pool2 <- conv2
I0905 21:43:05.710983 32638 net.cpp:290] pool2 -> pool2
I0905 21:43:05.710989 32638 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:43:05.710995 32638 net.cpp:125] pool2 needs backward computation.
I0905 21:43:05.711004 32638 net.cpp:66] Creating Layer fc7
I0905 21:43:05.711010 32638 net.cpp:329] fc7 <- pool2
I0905 21:43:05.711017 32638 net.cpp:290] fc7 -> fc7
I0905 21:43:06.356899 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.356945 32638 net.cpp:125] fc7 needs backward computation.
I0905 21:43:06.356958 32638 net.cpp:66] Creating Layer relu7
I0905 21:43:06.356966 32638 net.cpp:329] relu7 <- fc7
I0905 21:43:06.356986 32638 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:43:06.356997 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.357002 32638 net.cpp:125] relu7 needs backward computation.
I0905 21:43:06.357009 32638 net.cpp:66] Creating Layer drop7
I0905 21:43:06.357015 32638 net.cpp:329] drop7 <- fc7
I0905 21:43:06.357022 32638 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:43:06.357033 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.357038 32638 net.cpp:125] drop7 needs backward computation.
I0905 21:43:06.357046 32638 net.cpp:66] Creating Layer fc8
I0905 21:43:06.357053 32638 net.cpp:329] fc8 <- fc7
I0905 21:43:06.357061 32638 net.cpp:290] fc8 -> fc8
I0905 21:43:06.364831 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.364845 32638 net.cpp:125] fc8 needs backward computation.
I0905 21:43:06.364851 32638 net.cpp:66] Creating Layer relu8
I0905 21:43:06.364856 32638 net.cpp:329] relu8 <- fc8
I0905 21:43:06.364864 32638 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:43:06.364871 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.364877 32638 net.cpp:125] relu8 needs backward computation.
I0905 21:43:06.364883 32638 net.cpp:66] Creating Layer drop8
I0905 21:43:06.364889 32638 net.cpp:329] drop8 <- fc8
I0905 21:43:06.364895 32638 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:43:06.364902 32638 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:43:06.364907 32638 net.cpp:125] drop8 needs backward computation.
I0905 21:43:06.364915 32638 net.cpp:66] Creating Layer fc9
I0905 21:43:06.364922 32638 net.cpp:329] fc9 <- fc8
I0905 21:43:06.364928 32638 net.cpp:290] fc9 -> fc9
I0905 21:43:06.365303 32638 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:43:06.365314 32638 net.cpp:125] fc9 needs backward computation.
I0905 21:43:06.365324 32638 net.cpp:66] Creating Layer fc10
I0905 21:43:06.365329 32638 net.cpp:329] fc10 <- fc9
I0905 21:43:06.365337 32638 net.cpp:290] fc10 -> fc10
I0905 21:43:06.365350 32638 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:43:06.365357 32638 net.cpp:125] fc10 needs backward computation.
I0905 21:43:06.365363 32638 net.cpp:66] Creating Layer prob
I0905 21:43:06.365370 32638 net.cpp:329] prob <- fc10
I0905 21:43:06.365376 32638 net.cpp:290] prob -> prob
I0905 21:43:06.365386 32638 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:43:06.365392 32638 net.cpp:125] prob needs backward computation.
I0905 21:43:06.365397 32638 net.cpp:156] This network produces output prob
I0905 21:43:06.365409 32638 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:43:06.365418 32638 net.cpp:167] Network initialization done.
I0905 21:43:06.365423 32638 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:47:05.659055 32647 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:47:05.659258 32647 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:47:05.659268 32647 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:47:05.659497 32647 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:47:05.659559 32647 net.cpp:292] Input 0 -> data
I0905 21:47:05.659603 32647 net.cpp:66] Creating Layer conv1
I0905 21:47:05.659611 32647 net.cpp:329] conv1 <- data
I0905 21:47:05.659620 32647 net.cpp:290] conv1 -> conv1
I0905 21:47:05.691735 32647 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:47:05.691759 32647 net.cpp:125] conv1 needs backward computation.
I0905 21:47:05.691769 32647 net.cpp:66] Creating Layer relu1
I0905 21:47:05.691776 32647 net.cpp:329] relu1 <- conv1
I0905 21:47:05.691782 32647 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:47:05.691792 32647 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:47:05.691797 32647 net.cpp:125] relu1 needs backward computation.
I0905 21:47:05.691804 32647 net.cpp:66] Creating Layer pool1
I0905 21:47:05.691810 32647 net.cpp:329] pool1 <- conv1
I0905 21:47:05.691817 32647 net.cpp:290] pool1 -> pool1
I0905 21:47:05.691828 32647 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:47:05.691834 32647 net.cpp:125] pool1 needs backward computation.
I0905 21:47:05.691840 32647 net.cpp:66] Creating Layer norm1
I0905 21:47:05.691846 32647 net.cpp:329] norm1 <- pool1
I0905 21:47:05.691854 32647 net.cpp:290] norm1 -> norm1
I0905 21:47:05.691864 32647 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:47:05.691869 32647 net.cpp:125] norm1 needs backward computation.
I0905 21:47:05.691876 32647 net.cpp:66] Creating Layer conv2
I0905 21:47:05.691881 32647 net.cpp:329] conv2 <- norm1
I0905 21:47:05.691890 32647 net.cpp:290] conv2 -> conv2
I0905 21:47:05.701006 32647 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:47:05.701022 32647 net.cpp:125] conv2 needs backward computation.
I0905 21:47:05.701030 32647 net.cpp:66] Creating Layer relu2
I0905 21:47:05.701040 32647 net.cpp:329] relu2 <- conv2
I0905 21:47:05.701047 32647 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:47:05.701056 32647 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:47:05.701061 32647 net.cpp:125] relu2 needs backward computation.
I0905 21:47:05.701069 32647 net.cpp:66] Creating Layer pool2
I0905 21:47:05.701074 32647 net.cpp:329] pool2 <- conv2
I0905 21:47:05.701081 32647 net.cpp:290] pool2 -> pool2
I0905 21:47:05.701089 32647 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:47:05.701095 32647 net.cpp:125] pool2 needs backward computation.
I0905 21:47:05.701102 32647 net.cpp:66] Creating Layer fc7
I0905 21:47:05.701108 32647 net.cpp:329] fc7 <- pool2
I0905 21:47:05.701115 32647 net.cpp:290] fc7 -> fc7
I0905 21:47:06.351974 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.352022 32647 net.cpp:125] fc7 needs backward computation.
I0905 21:47:06.352035 32647 net.cpp:66] Creating Layer relu7
I0905 21:47:06.352043 32647 net.cpp:329] relu7 <- fc7
I0905 21:47:06.352052 32647 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:47:06.352063 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.352069 32647 net.cpp:125] relu7 needs backward computation.
I0905 21:47:06.352077 32647 net.cpp:66] Creating Layer drop7
I0905 21:47:06.352083 32647 net.cpp:329] drop7 <- fc7
I0905 21:47:06.352089 32647 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:47:06.352104 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.352112 32647 net.cpp:125] drop7 needs backward computation.
I0905 21:47:06.352120 32647 net.cpp:66] Creating Layer fc8
I0905 21:47:06.352125 32647 net.cpp:329] fc8 <- fc7
I0905 21:47:06.352134 32647 net.cpp:290] fc8 -> fc8
I0905 21:47:06.360139 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.360152 32647 net.cpp:125] fc8 needs backward computation.
I0905 21:47:06.360159 32647 net.cpp:66] Creating Layer relu8
I0905 21:47:06.360165 32647 net.cpp:329] relu8 <- fc8
I0905 21:47:06.360173 32647 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:47:06.360182 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.360187 32647 net.cpp:125] relu8 needs backward computation.
I0905 21:47:06.360193 32647 net.cpp:66] Creating Layer drop8
I0905 21:47:06.360199 32647 net.cpp:329] drop8 <- fc8
I0905 21:47:06.360205 32647 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:47:06.360213 32647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:47:06.360219 32647 net.cpp:125] drop8 needs backward computation.
I0905 21:47:06.360227 32647 net.cpp:66] Creating Layer fc9
I0905 21:47:06.360234 32647 net.cpp:329] fc9 <- fc8
I0905 21:47:06.360240 32647 net.cpp:290] fc9 -> fc9
I0905 21:47:06.360625 32647 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:47:06.360637 32647 net.cpp:125] fc9 needs backward computation.
I0905 21:47:06.360646 32647 net.cpp:66] Creating Layer fc10
I0905 21:47:06.360651 32647 net.cpp:329] fc10 <- fc9
I0905 21:47:06.360661 32647 net.cpp:290] fc10 -> fc10
I0905 21:47:06.360672 32647 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:47:06.360680 32647 net.cpp:125] fc10 needs backward computation.
I0905 21:47:06.360687 32647 net.cpp:66] Creating Layer prob
I0905 21:47:06.360693 32647 net.cpp:329] prob <- fc10
I0905 21:47:06.360702 32647 net.cpp:290] prob -> prob
I0905 21:47:06.360710 32647 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:47:06.360718 32647 net.cpp:125] prob needs backward computation.
I0905 21:47:06.360723 32647 net.cpp:156] This network produces output prob
I0905 21:47:06.360735 32647 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:47:06.360743 32647 net.cpp:167] Network initialization done.
I0905 21:47:06.360749 32647 net.cpp:168] Memory required for data: 6183480
Classifying 241 inputs.
Done in 161.44 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 41 is out of bounds for axis 0 with size 41
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:50:01.449676 32657 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:50:01.449826 32657 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:50:01.449834 32657 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:50:01.449980 32657 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:50:01.450032 32657 net.cpp:292] Input 0 -> data
I0905 21:50:01.450058 32657 net.cpp:66] Creating Layer conv1
I0905 21:50:01.450064 32657 net.cpp:329] conv1 <- data
I0905 21:50:01.450073 32657 net.cpp:290] conv1 -> conv1
I0905 21:50:01.451483 32657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:50:01.451500 32657 net.cpp:125] conv1 needs backward computation.
I0905 21:50:01.451515 32657 net.cpp:66] Creating Layer relu1
I0905 21:50:01.451521 32657 net.cpp:329] relu1 <- conv1
I0905 21:50:01.451529 32657 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:50:01.451537 32657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:50:01.451545 32657 net.cpp:125] relu1 needs backward computation.
I0905 21:50:01.451551 32657 net.cpp:66] Creating Layer pool1
I0905 21:50:01.451556 32657 net.cpp:329] pool1 <- conv1
I0905 21:50:01.451563 32657 net.cpp:290] pool1 -> pool1
I0905 21:50:01.451575 32657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:50:01.451581 32657 net.cpp:125] pool1 needs backward computation.
I0905 21:50:01.451588 32657 net.cpp:66] Creating Layer norm1
I0905 21:50:01.451594 32657 net.cpp:329] norm1 <- pool1
I0905 21:50:01.451601 32657 net.cpp:290] norm1 -> norm1
I0905 21:50:01.451611 32657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:50:01.451617 32657 net.cpp:125] norm1 needs backward computation.
I0905 21:50:01.451624 32657 net.cpp:66] Creating Layer conv2
I0905 21:50:01.451630 32657 net.cpp:329] conv2 <- norm1
I0905 21:50:01.451638 32657 net.cpp:290] conv2 -> conv2
I0905 21:50:01.461025 32657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:50:01.461041 32657 net.cpp:125] conv2 needs backward computation.
I0905 21:50:01.461050 32657 net.cpp:66] Creating Layer relu2
I0905 21:50:01.461055 32657 net.cpp:329] relu2 <- conv2
I0905 21:50:01.461062 32657 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:50:01.461069 32657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:50:01.461076 32657 net.cpp:125] relu2 needs backward computation.
I0905 21:50:01.461084 32657 net.cpp:66] Creating Layer pool2
I0905 21:50:01.461091 32657 net.cpp:329] pool2 <- conv2
I0905 21:50:01.461097 32657 net.cpp:290] pool2 -> pool2
I0905 21:50:01.461105 32657 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:50:01.461112 32657 net.cpp:125] pool2 needs backward computation.
I0905 21:50:01.461118 32657 net.cpp:66] Creating Layer fc7
I0905 21:50:01.461124 32657 net.cpp:329] fc7 <- pool2
I0905 21:50:01.461133 32657 net.cpp:290] fc7 -> fc7
I0905 21:50:02.112468 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.112512 32657 net.cpp:125] fc7 needs backward computation.
I0905 21:50:02.112525 32657 net.cpp:66] Creating Layer relu7
I0905 21:50:02.112534 32657 net.cpp:329] relu7 <- fc7
I0905 21:50:02.112542 32657 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:50:02.112552 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.112558 32657 net.cpp:125] relu7 needs backward computation.
I0905 21:50:02.112566 32657 net.cpp:66] Creating Layer drop7
I0905 21:50:02.112571 32657 net.cpp:329] drop7 <- fc7
I0905 21:50:02.112578 32657 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:50:02.112589 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.112596 32657 net.cpp:125] drop7 needs backward computation.
I0905 21:50:02.112604 32657 net.cpp:66] Creating Layer fc8
I0905 21:50:02.112611 32657 net.cpp:329] fc8 <- fc7
I0905 21:50:02.112619 32657 net.cpp:290] fc8 -> fc8
I0905 21:50:02.120616 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.120630 32657 net.cpp:125] fc8 needs backward computation.
I0905 21:50:02.120636 32657 net.cpp:66] Creating Layer relu8
I0905 21:50:02.120642 32657 net.cpp:329] relu8 <- fc8
I0905 21:50:02.120650 32657 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:50:02.120656 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.120662 32657 net.cpp:125] relu8 needs backward computation.
I0905 21:50:02.120668 32657 net.cpp:66] Creating Layer drop8
I0905 21:50:02.120674 32657 net.cpp:329] drop8 <- fc8
I0905 21:50:02.120681 32657 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:50:02.120687 32657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:50:02.120693 32657 net.cpp:125] drop8 needs backward computation.
I0905 21:50:02.120702 32657 net.cpp:66] Creating Layer fc9
I0905 21:50:02.120707 32657 net.cpp:329] fc9 <- fc8
I0905 21:50:02.120714 32657 net.cpp:290] fc9 -> fc9
I0905 21:50:02.121086 32657 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:50:02.121109 32657 net.cpp:125] fc9 needs backward computation.
I0905 21:50:02.121117 32657 net.cpp:66] Creating Layer fc10
I0905 21:50:02.121124 32657 net.cpp:329] fc10 <- fc9
I0905 21:50:02.121131 32657 net.cpp:290] fc10 -> fc10
I0905 21:50:02.121143 32657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:50:02.121151 32657 net.cpp:125] fc10 needs backward computation.
I0905 21:50:02.121158 32657 net.cpp:66] Creating Layer prob
I0905 21:50:02.121165 32657 net.cpp:329] prob <- fc10
I0905 21:50:02.121172 32657 net.cpp:290] prob -> prob
I0905 21:50:02.121181 32657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:50:02.121187 32657 net.cpp:125] prob needs backward computation.
I0905 21:50:02.121192 32657 net.cpp:156] This network produces output prob
I0905 21:50:02.121204 32657 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:50:02.121213 32657 net.cpp:167] Network initialization done.
I0905 21:50:02.121218 32657 net.cpp:168] Memory required for data: 6183480
Classifying 356 inputs.
Done in 230.45 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 56 is out of bounds for axis 0 with size 56
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:54:09.142784 32665 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:54:09.142926 32665 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:54:09.142935 32665 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:54:09.143082 32665 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:54:09.143146 32665 net.cpp:292] Input 0 -> data
I0905 21:54:09.143172 32665 net.cpp:66] Creating Layer conv1
I0905 21:54:09.143179 32665 net.cpp:329] conv1 <- data
I0905 21:54:09.143187 32665 net.cpp:290] conv1 -> conv1
I0905 21:54:09.144551 32665 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:54:09.144568 32665 net.cpp:125] conv1 needs backward computation.
I0905 21:54:09.144577 32665 net.cpp:66] Creating Layer relu1
I0905 21:54:09.144583 32665 net.cpp:329] relu1 <- conv1
I0905 21:54:09.144590 32665 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:54:09.144599 32665 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:54:09.144605 32665 net.cpp:125] relu1 needs backward computation.
I0905 21:54:09.144613 32665 net.cpp:66] Creating Layer pool1
I0905 21:54:09.144618 32665 net.cpp:329] pool1 <- conv1
I0905 21:54:09.144625 32665 net.cpp:290] pool1 -> pool1
I0905 21:54:09.144637 32665 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:54:09.144644 32665 net.cpp:125] pool1 needs backward computation.
I0905 21:54:09.144650 32665 net.cpp:66] Creating Layer norm1
I0905 21:54:09.144656 32665 net.cpp:329] norm1 <- pool1
I0905 21:54:09.144662 32665 net.cpp:290] norm1 -> norm1
I0905 21:54:09.144673 32665 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:54:09.144680 32665 net.cpp:125] norm1 needs backward computation.
I0905 21:54:09.144686 32665 net.cpp:66] Creating Layer conv2
I0905 21:54:09.144692 32665 net.cpp:329] conv2 <- norm1
I0905 21:54:09.144701 32665 net.cpp:290] conv2 -> conv2
I0905 21:54:09.153864 32665 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:54:09.153880 32665 net.cpp:125] conv2 needs backward computation.
I0905 21:54:09.153888 32665 net.cpp:66] Creating Layer relu2
I0905 21:54:09.153894 32665 net.cpp:329] relu2 <- conv2
I0905 21:54:09.153902 32665 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:54:09.153908 32665 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:54:09.153914 32665 net.cpp:125] relu2 needs backward computation.
I0905 21:54:09.153921 32665 net.cpp:66] Creating Layer pool2
I0905 21:54:09.153926 32665 net.cpp:329] pool2 <- conv2
I0905 21:54:09.153934 32665 net.cpp:290] pool2 -> pool2
I0905 21:54:09.153942 32665 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:54:09.153949 32665 net.cpp:125] pool2 needs backward computation.
I0905 21:54:09.153957 32665 net.cpp:66] Creating Layer fc7
I0905 21:54:09.153964 32665 net.cpp:329] fc7 <- pool2
I0905 21:54:09.153971 32665 net.cpp:290] fc7 -> fc7
I0905 21:54:09.798450 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.798496 32665 net.cpp:125] fc7 needs backward computation.
I0905 21:54:09.798509 32665 net.cpp:66] Creating Layer relu7
I0905 21:54:09.798516 32665 net.cpp:329] relu7 <- fc7
I0905 21:54:09.798526 32665 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:54:09.798537 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.798542 32665 net.cpp:125] relu7 needs backward computation.
I0905 21:54:09.798549 32665 net.cpp:66] Creating Layer drop7
I0905 21:54:09.798555 32665 net.cpp:329] drop7 <- fc7
I0905 21:54:09.798562 32665 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:54:09.798573 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.798590 32665 net.cpp:125] drop7 needs backward computation.
I0905 21:54:09.798599 32665 net.cpp:66] Creating Layer fc8
I0905 21:54:09.798605 32665 net.cpp:329] fc8 <- fc7
I0905 21:54:09.798614 32665 net.cpp:290] fc8 -> fc8
I0905 21:54:09.806373 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.806386 32665 net.cpp:125] fc8 needs backward computation.
I0905 21:54:09.806393 32665 net.cpp:66] Creating Layer relu8
I0905 21:54:09.806399 32665 net.cpp:329] relu8 <- fc8
I0905 21:54:09.806407 32665 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:54:09.806416 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.806421 32665 net.cpp:125] relu8 needs backward computation.
I0905 21:54:09.806427 32665 net.cpp:66] Creating Layer drop8
I0905 21:54:09.806433 32665 net.cpp:329] drop8 <- fc8
I0905 21:54:09.806440 32665 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:54:09.806447 32665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:09.806453 32665 net.cpp:125] drop8 needs backward computation.
I0905 21:54:09.806463 32665 net.cpp:66] Creating Layer fc9
I0905 21:54:09.806468 32665 net.cpp:329] fc9 <- fc8
I0905 21:54:09.806475 32665 net.cpp:290] fc9 -> fc9
I0905 21:54:09.806849 32665 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:54:09.806861 32665 net.cpp:125] fc9 needs backward computation.
I0905 21:54:09.806870 32665 net.cpp:66] Creating Layer fc10
I0905 21:54:09.806876 32665 net.cpp:329] fc10 <- fc9
I0905 21:54:09.806885 32665 net.cpp:290] fc10 -> fc10
I0905 21:54:09.806896 32665 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:54:09.806905 32665 net.cpp:125] fc10 needs backward computation.
I0905 21:54:09.806911 32665 net.cpp:66] Creating Layer prob
I0905 21:54:09.806917 32665 net.cpp:329] prob <- fc10
I0905 21:54:09.806926 32665 net.cpp:290] prob -> prob
I0905 21:54:09.806934 32665 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:54:09.806941 32665 net.cpp:125] prob needs backward computation.
I0905 21:54:09.806946 32665 net.cpp:156] This network produces output prob
I0905 21:54:09.806958 32665 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:54:09.806967 32665 net.cpp:167] Network initialization done.
I0905 21:54:09.806972 32665 net.cpp:168] Memory required for data: 6183480
Classifying 29 inputs.
Done in 19.09 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:54:30.966334 32669 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:54:30.966471 32669 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:54:30.966480 32669 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:54:30.966627 32669 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:54:30.966689 32669 net.cpp:292] Input 0 -> data
I0905 21:54:30.966717 32669 net.cpp:66] Creating Layer conv1
I0905 21:54:30.966723 32669 net.cpp:329] conv1 <- data
I0905 21:54:30.966732 32669 net.cpp:290] conv1 -> conv1
I0905 21:54:30.968091 32669 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:54:30.968109 32669 net.cpp:125] conv1 needs backward computation.
I0905 21:54:30.968118 32669 net.cpp:66] Creating Layer relu1
I0905 21:54:30.968124 32669 net.cpp:329] relu1 <- conv1
I0905 21:54:30.968130 32669 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:54:30.968139 32669 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:54:30.968145 32669 net.cpp:125] relu1 needs backward computation.
I0905 21:54:30.968152 32669 net.cpp:66] Creating Layer pool1
I0905 21:54:30.968158 32669 net.cpp:329] pool1 <- conv1
I0905 21:54:30.968164 32669 net.cpp:290] pool1 -> pool1
I0905 21:54:30.968175 32669 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:54:30.968181 32669 net.cpp:125] pool1 needs backward computation.
I0905 21:54:30.968189 32669 net.cpp:66] Creating Layer norm1
I0905 21:54:30.968194 32669 net.cpp:329] norm1 <- pool1
I0905 21:54:30.968200 32669 net.cpp:290] norm1 -> norm1
I0905 21:54:30.968210 32669 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:54:30.968216 32669 net.cpp:125] norm1 needs backward computation.
I0905 21:54:30.968224 32669 net.cpp:66] Creating Layer conv2
I0905 21:54:30.968230 32669 net.cpp:329] conv2 <- norm1
I0905 21:54:30.968236 32669 net.cpp:290] conv2 -> conv2
I0905 21:54:30.977376 32669 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:54:30.977391 32669 net.cpp:125] conv2 needs backward computation.
I0905 21:54:30.977398 32669 net.cpp:66] Creating Layer relu2
I0905 21:54:30.977404 32669 net.cpp:329] relu2 <- conv2
I0905 21:54:30.977411 32669 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:54:30.977417 32669 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:54:30.977423 32669 net.cpp:125] relu2 needs backward computation.
I0905 21:54:30.977429 32669 net.cpp:66] Creating Layer pool2
I0905 21:54:30.977434 32669 net.cpp:329] pool2 <- conv2
I0905 21:54:30.977447 32669 net.cpp:290] pool2 -> pool2
I0905 21:54:30.977455 32669 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:54:30.977460 32669 net.cpp:125] pool2 needs backward computation.
I0905 21:54:30.977469 32669 net.cpp:66] Creating Layer fc7
I0905 21:54:30.977475 32669 net.cpp:329] fc7 <- pool2
I0905 21:54:30.977483 32669 net.cpp:290] fc7 -> fc7
I0905 21:54:31.629608 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.629652 32669 net.cpp:125] fc7 needs backward computation.
I0905 21:54:31.629664 32669 net.cpp:66] Creating Layer relu7
I0905 21:54:31.629672 32669 net.cpp:329] relu7 <- fc7
I0905 21:54:31.629680 32669 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:54:31.629690 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.629696 32669 net.cpp:125] relu7 needs backward computation.
I0905 21:54:31.629703 32669 net.cpp:66] Creating Layer drop7
I0905 21:54:31.629709 32669 net.cpp:329] drop7 <- fc7
I0905 21:54:31.629715 32669 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:54:31.629725 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.629731 32669 net.cpp:125] drop7 needs backward computation.
I0905 21:54:31.629739 32669 net.cpp:66] Creating Layer fc8
I0905 21:54:31.629745 32669 net.cpp:329] fc8 <- fc7
I0905 21:54:31.629755 32669 net.cpp:290] fc8 -> fc8
I0905 21:54:31.637570 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.637590 32669 net.cpp:125] fc8 needs backward computation.
I0905 21:54:31.637599 32669 net.cpp:66] Creating Layer relu8
I0905 21:54:31.637604 32669 net.cpp:329] relu8 <- fc8
I0905 21:54:31.637612 32669 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:54:31.637620 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.637626 32669 net.cpp:125] relu8 needs backward computation.
I0905 21:54:31.637632 32669 net.cpp:66] Creating Layer drop8
I0905 21:54:31.637637 32669 net.cpp:329] drop8 <- fc8
I0905 21:54:31.637645 32669 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:54:31.637650 32669 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:54:31.637656 32669 net.cpp:125] drop8 needs backward computation.
I0905 21:54:31.637665 32669 net.cpp:66] Creating Layer fc9
I0905 21:54:31.637670 32669 net.cpp:329] fc9 <- fc8
I0905 21:54:31.637678 32669 net.cpp:290] fc9 -> fc9
I0905 21:54:31.638051 32669 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:54:31.638063 32669 net.cpp:125] fc9 needs backward computation.
I0905 21:54:31.638072 32669 net.cpp:66] Creating Layer fc10
I0905 21:54:31.638077 32669 net.cpp:329] fc10 <- fc9
I0905 21:54:31.638085 32669 net.cpp:290] fc10 -> fc10
I0905 21:54:31.638098 32669 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:54:31.638105 32669 net.cpp:125] fc10 needs backward computation.
I0905 21:54:31.638113 32669 net.cpp:66] Creating Layer prob
I0905 21:54:31.638118 32669 net.cpp:329] prob <- fc10
I0905 21:54:31.638125 32669 net.cpp:290] prob -> prob
I0905 21:54:31.638134 32669 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:54:31.638140 32669 net.cpp:125] prob needs backward computation.
I0905 21:54:31.638145 32669 net.cpp:156] This network produces output prob
I0905 21:54:31.638159 32669 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:54:31.638166 32669 net.cpp:167] Network initialization done.
I0905 21:54:31.638172 32669 net.cpp:168] Memory required for data: 6183480
Classifying 166 inputs.
Done in 101.01 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 66 is out of bounds for axis 0 with size 66
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:56:17.448123 32676 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:56:17.448261 32676 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:56:17.448271 32676 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:56:17.448428 32676 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:56:17.448482 32676 net.cpp:292] Input 0 -> data
I0905 21:56:17.448506 32676 net.cpp:66] Creating Layer conv1
I0905 21:56:17.448513 32676 net.cpp:329] conv1 <- data
I0905 21:56:17.448523 32676 net.cpp:290] conv1 -> conv1
I0905 21:56:17.449908 32676 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:56:17.449928 32676 net.cpp:125] conv1 needs backward computation.
I0905 21:56:17.449936 32676 net.cpp:66] Creating Layer relu1
I0905 21:56:17.449942 32676 net.cpp:329] relu1 <- conv1
I0905 21:56:17.449949 32676 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:56:17.449959 32676 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:56:17.449965 32676 net.cpp:125] relu1 needs backward computation.
I0905 21:56:17.449971 32676 net.cpp:66] Creating Layer pool1
I0905 21:56:17.449976 32676 net.cpp:329] pool1 <- conv1
I0905 21:56:17.449988 32676 net.cpp:290] pool1 -> pool1
I0905 21:56:17.450001 32676 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:56:17.450006 32676 net.cpp:125] pool1 needs backward computation.
I0905 21:56:17.450013 32676 net.cpp:66] Creating Layer norm1
I0905 21:56:17.450019 32676 net.cpp:329] norm1 <- pool1
I0905 21:56:17.450026 32676 net.cpp:290] norm1 -> norm1
I0905 21:56:17.450037 32676 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:56:17.450042 32676 net.cpp:125] norm1 needs backward computation.
I0905 21:56:17.450050 32676 net.cpp:66] Creating Layer conv2
I0905 21:56:17.450055 32676 net.cpp:329] conv2 <- norm1
I0905 21:56:17.450063 32676 net.cpp:290] conv2 -> conv2
I0905 21:56:17.459339 32676 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:56:17.459354 32676 net.cpp:125] conv2 needs backward computation.
I0905 21:56:17.459362 32676 net.cpp:66] Creating Layer relu2
I0905 21:56:17.459368 32676 net.cpp:329] relu2 <- conv2
I0905 21:56:17.459375 32676 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:56:17.459383 32676 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:56:17.459388 32676 net.cpp:125] relu2 needs backward computation.
I0905 21:56:17.459394 32676 net.cpp:66] Creating Layer pool2
I0905 21:56:17.459399 32676 net.cpp:329] pool2 <- conv2
I0905 21:56:17.459406 32676 net.cpp:290] pool2 -> pool2
I0905 21:56:17.459414 32676 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:56:17.459419 32676 net.cpp:125] pool2 needs backward computation.
I0905 21:56:17.459429 32676 net.cpp:66] Creating Layer fc7
I0905 21:56:17.459435 32676 net.cpp:329] fc7 <- pool2
I0905 21:56:17.459444 32676 net.cpp:290] fc7 -> fc7
I0905 21:56:18.110085 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.110132 32676 net.cpp:125] fc7 needs backward computation.
I0905 21:56:18.110146 32676 net.cpp:66] Creating Layer relu7
I0905 21:56:18.110152 32676 net.cpp:329] relu7 <- fc7
I0905 21:56:18.110162 32676 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:56:18.110172 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.110178 32676 net.cpp:125] relu7 needs backward computation.
I0905 21:56:18.110185 32676 net.cpp:66] Creating Layer drop7
I0905 21:56:18.110191 32676 net.cpp:329] drop7 <- fc7
I0905 21:56:18.110198 32676 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:56:18.110208 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.110214 32676 net.cpp:125] drop7 needs backward computation.
I0905 21:56:18.110223 32676 net.cpp:66] Creating Layer fc8
I0905 21:56:18.110229 32676 net.cpp:329] fc8 <- fc7
I0905 21:56:18.110237 32676 net.cpp:290] fc8 -> fc8
I0905 21:56:18.118010 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.118022 32676 net.cpp:125] fc8 needs backward computation.
I0905 21:56:18.118029 32676 net.cpp:66] Creating Layer relu8
I0905 21:56:18.118036 32676 net.cpp:329] relu8 <- fc8
I0905 21:56:18.118043 32676 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:56:18.118051 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.118057 32676 net.cpp:125] relu8 needs backward computation.
I0905 21:56:18.118063 32676 net.cpp:66] Creating Layer drop8
I0905 21:56:18.118068 32676 net.cpp:329] drop8 <- fc8
I0905 21:56:18.118074 32676 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:56:18.118082 32676 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:18.118088 32676 net.cpp:125] drop8 needs backward computation.
I0905 21:56:18.118095 32676 net.cpp:66] Creating Layer fc9
I0905 21:56:18.118101 32676 net.cpp:329] fc9 <- fc8
I0905 21:56:18.118108 32676 net.cpp:290] fc9 -> fc9
I0905 21:56:18.118499 32676 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:56:18.118510 32676 net.cpp:125] fc9 needs backward computation.
I0905 21:56:18.118518 32676 net.cpp:66] Creating Layer fc10
I0905 21:56:18.118525 32676 net.cpp:329] fc10 <- fc9
I0905 21:56:18.118533 32676 net.cpp:290] fc10 -> fc10
I0905 21:56:18.118546 32676 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:56:18.118553 32676 net.cpp:125] fc10 needs backward computation.
I0905 21:56:18.118561 32676 net.cpp:66] Creating Layer prob
I0905 21:56:18.118577 32676 net.cpp:329] prob <- fc10
I0905 21:56:18.118587 32676 net.cpp:290] prob -> prob
I0905 21:56:18.118595 32676 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:56:18.118602 32676 net.cpp:125] prob needs backward computation.
I0905 21:56:18.118607 32676 net.cpp:156] This network produces output prob
I0905 21:56:18.118619 32676 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:56:18.118628 32676 net.cpp:167] Network initialization done.
I0905 21:56:18.118633 32676 net.cpp:168] Memory required for data: 6183480
Classifying 8 inputs.
Done in 5.31 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:56:24.237257 32680 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:56:24.237396 32680 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:56:24.237404 32680 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:56:24.237551 32680 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:56:24.237629 32680 net.cpp:292] Input 0 -> data
I0905 21:56:24.237656 32680 net.cpp:66] Creating Layer conv1
I0905 21:56:24.237664 32680 net.cpp:329] conv1 <- data
I0905 21:56:24.237673 32680 net.cpp:290] conv1 -> conv1
I0905 21:56:24.239032 32680 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:56:24.239050 32680 net.cpp:125] conv1 needs backward computation.
I0905 21:56:24.239059 32680 net.cpp:66] Creating Layer relu1
I0905 21:56:24.239065 32680 net.cpp:329] relu1 <- conv1
I0905 21:56:24.239073 32680 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:56:24.239081 32680 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:56:24.239087 32680 net.cpp:125] relu1 needs backward computation.
I0905 21:56:24.239094 32680 net.cpp:66] Creating Layer pool1
I0905 21:56:24.239099 32680 net.cpp:329] pool1 <- conv1
I0905 21:56:24.239106 32680 net.cpp:290] pool1 -> pool1
I0905 21:56:24.239116 32680 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:56:24.239122 32680 net.cpp:125] pool1 needs backward computation.
I0905 21:56:24.239130 32680 net.cpp:66] Creating Layer norm1
I0905 21:56:24.239135 32680 net.cpp:329] norm1 <- pool1
I0905 21:56:24.239141 32680 net.cpp:290] norm1 -> norm1
I0905 21:56:24.239151 32680 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:56:24.239157 32680 net.cpp:125] norm1 needs backward computation.
I0905 21:56:24.239164 32680 net.cpp:66] Creating Layer conv2
I0905 21:56:24.239171 32680 net.cpp:329] conv2 <- norm1
I0905 21:56:24.239177 32680 net.cpp:290] conv2 -> conv2
I0905 21:56:24.248322 32680 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:56:24.248337 32680 net.cpp:125] conv2 needs backward computation.
I0905 21:56:24.248344 32680 net.cpp:66] Creating Layer relu2
I0905 21:56:24.248350 32680 net.cpp:329] relu2 <- conv2
I0905 21:56:24.248358 32680 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:56:24.248364 32680 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:56:24.248370 32680 net.cpp:125] relu2 needs backward computation.
I0905 21:56:24.248376 32680 net.cpp:66] Creating Layer pool2
I0905 21:56:24.248381 32680 net.cpp:329] pool2 <- conv2
I0905 21:56:24.248389 32680 net.cpp:290] pool2 -> pool2
I0905 21:56:24.248396 32680 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:56:24.248401 32680 net.cpp:125] pool2 needs backward computation.
I0905 21:56:24.248411 32680 net.cpp:66] Creating Layer fc7
I0905 21:56:24.248416 32680 net.cpp:329] fc7 <- pool2
I0905 21:56:24.248425 32680 net.cpp:290] fc7 -> fc7
I0905 21:56:24.897734 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.897789 32680 net.cpp:125] fc7 needs backward computation.
I0905 21:56:24.897802 32680 net.cpp:66] Creating Layer relu7
I0905 21:56:24.897809 32680 net.cpp:329] relu7 <- fc7
I0905 21:56:24.897819 32680 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:56:24.897830 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.897843 32680 net.cpp:125] relu7 needs backward computation.
I0905 21:56:24.897850 32680 net.cpp:66] Creating Layer drop7
I0905 21:56:24.897855 32680 net.cpp:329] drop7 <- fc7
I0905 21:56:24.897862 32680 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:56:24.897873 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.897878 32680 net.cpp:125] drop7 needs backward computation.
I0905 21:56:24.897887 32680 net.cpp:66] Creating Layer fc8
I0905 21:56:24.897892 32680 net.cpp:329] fc8 <- fc7
I0905 21:56:24.897902 32680 net.cpp:290] fc8 -> fc8
I0905 21:56:24.905664 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.905678 32680 net.cpp:125] fc8 needs backward computation.
I0905 21:56:24.905684 32680 net.cpp:66] Creating Layer relu8
I0905 21:56:24.905690 32680 net.cpp:329] relu8 <- fc8
I0905 21:56:24.905699 32680 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:56:24.905705 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.905722 32680 net.cpp:125] relu8 needs backward computation.
I0905 21:56:24.905729 32680 net.cpp:66] Creating Layer drop8
I0905 21:56:24.905735 32680 net.cpp:329] drop8 <- fc8
I0905 21:56:24.905740 32680 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:56:24.905747 32680 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:56:24.905752 32680 net.cpp:125] drop8 needs backward computation.
I0905 21:56:24.905761 32680 net.cpp:66] Creating Layer fc9
I0905 21:56:24.905767 32680 net.cpp:329] fc9 <- fc8
I0905 21:56:24.905774 32680 net.cpp:290] fc9 -> fc9
I0905 21:56:24.906147 32680 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:56:24.906158 32680 net.cpp:125] fc9 needs backward computation.
I0905 21:56:24.906167 32680 net.cpp:66] Creating Layer fc10
I0905 21:56:24.906172 32680 net.cpp:329] fc10 <- fc9
I0905 21:56:24.906180 32680 net.cpp:290] fc10 -> fc10
I0905 21:56:24.906193 32680 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:56:24.906200 32680 net.cpp:125] fc10 needs backward computation.
I0905 21:56:24.906208 32680 net.cpp:66] Creating Layer prob
I0905 21:56:24.906213 32680 net.cpp:329] prob <- fc10
I0905 21:56:24.906220 32680 net.cpp:290] prob -> prob
I0905 21:56:24.906229 32680 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:56:24.906235 32680 net.cpp:125] prob needs backward computation.
I0905 21:56:24.906240 32680 net.cpp:156] This network produces output prob
I0905 21:56:24.906254 32680 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:56:24.906261 32680 net.cpp:167] Network initialization done.
I0905 21:56:24.906266 32680 net.cpp:168] Memory required for data: 6183480
Classifying 246 inputs.
Done in 155.01 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 46 is out of bounds for axis 0 with size 46
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 21:59:04.950808 32688 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 21:59:04.950947 32688 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 21:59:04.950955 32688 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 21:59:04.951102 32688 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 21:59:04.951167 32688 net.cpp:292] Input 0 -> data
I0905 21:59:04.951194 32688 net.cpp:66] Creating Layer conv1
I0905 21:59:04.951201 32688 net.cpp:329] conv1 <- data
I0905 21:59:04.951210 32688 net.cpp:290] conv1 -> conv1
I0905 21:59:04.952569 32688 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:59:04.952587 32688 net.cpp:125] conv1 needs backward computation.
I0905 21:59:04.952596 32688 net.cpp:66] Creating Layer relu1
I0905 21:59:04.952602 32688 net.cpp:329] relu1 <- conv1
I0905 21:59:04.952610 32688 net.cpp:280] relu1 -> conv1 (in-place)
I0905 21:59:04.952618 32688 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 21:59:04.952625 32688 net.cpp:125] relu1 needs backward computation.
I0905 21:59:04.952631 32688 net.cpp:66] Creating Layer pool1
I0905 21:59:04.952637 32688 net.cpp:329] pool1 <- conv1
I0905 21:59:04.952643 32688 net.cpp:290] pool1 -> pool1
I0905 21:59:04.952654 32688 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:59:04.952661 32688 net.cpp:125] pool1 needs backward computation.
I0905 21:59:04.952667 32688 net.cpp:66] Creating Layer norm1
I0905 21:59:04.952673 32688 net.cpp:329] norm1 <- pool1
I0905 21:59:04.952680 32688 net.cpp:290] norm1 -> norm1
I0905 21:59:04.952690 32688 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 21:59:04.952697 32688 net.cpp:125] norm1 needs backward computation.
I0905 21:59:04.952703 32688 net.cpp:66] Creating Layer conv2
I0905 21:59:04.952709 32688 net.cpp:329] conv2 <- norm1
I0905 21:59:04.952716 32688 net.cpp:290] conv2 -> conv2
I0905 21:59:04.961869 32688 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:59:04.961884 32688 net.cpp:125] conv2 needs backward computation.
I0905 21:59:04.961892 32688 net.cpp:66] Creating Layer relu2
I0905 21:59:04.961899 32688 net.cpp:329] relu2 <- conv2
I0905 21:59:04.961905 32688 net.cpp:280] relu2 -> conv2 (in-place)
I0905 21:59:04.961912 32688 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 21:59:04.961918 32688 net.cpp:125] relu2 needs backward computation.
I0905 21:59:04.961926 32688 net.cpp:66] Creating Layer pool2
I0905 21:59:04.961932 32688 net.cpp:329] pool2 <- conv2
I0905 21:59:04.961940 32688 net.cpp:290] pool2 -> pool2
I0905 21:59:04.961948 32688 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 21:59:04.961954 32688 net.cpp:125] pool2 needs backward computation.
I0905 21:59:04.961961 32688 net.cpp:66] Creating Layer fc7
I0905 21:59:04.961967 32688 net.cpp:329] fc7 <- pool2
I0905 21:59:04.961974 32688 net.cpp:290] fc7 -> fc7
I0905 21:59:05.608342 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.608398 32688 net.cpp:125] fc7 needs backward computation.
I0905 21:59:05.608412 32688 net.cpp:66] Creating Layer relu7
I0905 21:59:05.608419 32688 net.cpp:329] relu7 <- fc7
I0905 21:59:05.608429 32688 net.cpp:280] relu7 -> fc7 (in-place)
I0905 21:59:05.608439 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.608445 32688 net.cpp:125] relu7 needs backward computation.
I0905 21:59:05.608453 32688 net.cpp:66] Creating Layer drop7
I0905 21:59:05.608459 32688 net.cpp:329] drop7 <- fc7
I0905 21:59:05.608465 32688 net.cpp:280] drop7 -> fc7 (in-place)
I0905 21:59:05.608477 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.608484 32688 net.cpp:125] drop7 needs backward computation.
I0905 21:59:05.608491 32688 net.cpp:66] Creating Layer fc8
I0905 21:59:05.608497 32688 net.cpp:329] fc8 <- fc7
I0905 21:59:05.608506 32688 net.cpp:290] fc8 -> fc8
I0905 21:59:05.616305 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.616318 32688 net.cpp:125] fc8 needs backward computation.
I0905 21:59:05.616327 32688 net.cpp:66] Creating Layer relu8
I0905 21:59:05.616333 32688 net.cpp:329] relu8 <- fc8
I0905 21:59:05.616340 32688 net.cpp:280] relu8 -> fc8 (in-place)
I0905 21:59:05.616348 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.616353 32688 net.cpp:125] relu8 needs backward computation.
I0905 21:59:05.616360 32688 net.cpp:66] Creating Layer drop8
I0905 21:59:05.616365 32688 net.cpp:329] drop8 <- fc8
I0905 21:59:05.616372 32688 net.cpp:280] drop8 -> fc8 (in-place)
I0905 21:59:05.616379 32688 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 21:59:05.616385 32688 net.cpp:125] drop8 needs backward computation.
I0905 21:59:05.616394 32688 net.cpp:66] Creating Layer fc9
I0905 21:59:05.616400 32688 net.cpp:329] fc9 <- fc8
I0905 21:59:05.616406 32688 net.cpp:290] fc9 -> fc9
I0905 21:59:05.616780 32688 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 21:59:05.616792 32688 net.cpp:125] fc9 needs backward computation.
I0905 21:59:05.616801 32688 net.cpp:66] Creating Layer fc10
I0905 21:59:05.616806 32688 net.cpp:329] fc10 <- fc9
I0905 21:59:05.616816 32688 net.cpp:290] fc10 -> fc10
I0905 21:59:05.616827 32688 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:59:05.616835 32688 net.cpp:125] fc10 needs backward computation.
I0905 21:59:05.616842 32688 net.cpp:66] Creating Layer prob
I0905 21:59:05.616847 32688 net.cpp:329] prob <- fc10
I0905 21:59:05.616855 32688 net.cpp:290] prob -> prob
I0905 21:59:05.616865 32688 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 21:59:05.616871 32688 net.cpp:125] prob needs backward computation.
I0905 21:59:05.616876 32688 net.cpp:156] This network produces output prob
I0905 21:59:05.616889 32688 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 21:59:05.616897 32688 net.cpp:167] Network initialization done.
I0905 21:59:05.616904 32688 net.cpp:168] Memory required for data: 6183480
Classifying 232 inputs.
Done in 150.58 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 32 is out of bounds for axis 0 with size 32
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:01:40.649494 32694 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:01:40.649651 32694 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:01:40.649662 32694 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:01:40.649808 32694 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:01:40.649873 32694 net.cpp:292] Input 0 -> data
I0905 22:01:40.649899 32694 net.cpp:66] Creating Layer conv1
I0905 22:01:40.649906 32694 net.cpp:329] conv1 <- data
I0905 22:01:40.649914 32694 net.cpp:290] conv1 -> conv1
I0905 22:01:40.651274 32694 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:01:40.651291 32694 net.cpp:125] conv1 needs backward computation.
I0905 22:01:40.651300 32694 net.cpp:66] Creating Layer relu1
I0905 22:01:40.651306 32694 net.cpp:329] relu1 <- conv1
I0905 22:01:40.651314 32694 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:01:40.651322 32694 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:01:40.651329 32694 net.cpp:125] relu1 needs backward computation.
I0905 22:01:40.651335 32694 net.cpp:66] Creating Layer pool1
I0905 22:01:40.651340 32694 net.cpp:329] pool1 <- conv1
I0905 22:01:40.651347 32694 net.cpp:290] pool1 -> pool1
I0905 22:01:40.651358 32694 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:01:40.651365 32694 net.cpp:125] pool1 needs backward computation.
I0905 22:01:40.651371 32694 net.cpp:66] Creating Layer norm1
I0905 22:01:40.651376 32694 net.cpp:329] norm1 <- pool1
I0905 22:01:40.651383 32694 net.cpp:290] norm1 -> norm1
I0905 22:01:40.651397 32694 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:01:40.651404 32694 net.cpp:125] norm1 needs backward computation.
I0905 22:01:40.651412 32694 net.cpp:66] Creating Layer conv2
I0905 22:01:40.651417 32694 net.cpp:329] conv2 <- norm1
I0905 22:01:40.651424 32694 net.cpp:290] conv2 -> conv2
I0905 22:01:40.660572 32694 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:01:40.660586 32694 net.cpp:125] conv2 needs backward computation.
I0905 22:01:40.660593 32694 net.cpp:66] Creating Layer relu2
I0905 22:01:40.660599 32694 net.cpp:329] relu2 <- conv2
I0905 22:01:40.660606 32694 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:01:40.660614 32694 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:01:40.660619 32694 net.cpp:125] relu2 needs backward computation.
I0905 22:01:40.660625 32694 net.cpp:66] Creating Layer pool2
I0905 22:01:40.660631 32694 net.cpp:329] pool2 <- conv2
I0905 22:01:40.660637 32694 net.cpp:290] pool2 -> pool2
I0905 22:01:40.660645 32694 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:01:40.660651 32694 net.cpp:125] pool2 needs backward computation.
I0905 22:01:40.660660 32694 net.cpp:66] Creating Layer fc7
I0905 22:01:40.660666 32694 net.cpp:329] fc7 <- pool2
I0905 22:01:40.660673 32694 net.cpp:290] fc7 -> fc7
I0905 22:01:41.308720 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.308765 32694 net.cpp:125] fc7 needs backward computation.
I0905 22:01:41.308779 32694 net.cpp:66] Creating Layer relu7
I0905 22:01:41.308785 32694 net.cpp:329] relu7 <- fc7
I0905 22:01:41.308795 32694 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:01:41.308805 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.308811 32694 net.cpp:125] relu7 needs backward computation.
I0905 22:01:41.308818 32694 net.cpp:66] Creating Layer drop7
I0905 22:01:41.308825 32694 net.cpp:329] drop7 <- fc7
I0905 22:01:41.308830 32694 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:01:41.308841 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.308847 32694 net.cpp:125] drop7 needs backward computation.
I0905 22:01:41.308856 32694 net.cpp:66] Creating Layer fc8
I0905 22:01:41.308861 32694 net.cpp:329] fc8 <- fc7
I0905 22:01:41.308871 32694 net.cpp:290] fc8 -> fc8
I0905 22:01:41.316861 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.316875 32694 net.cpp:125] fc8 needs backward computation.
I0905 22:01:41.316882 32694 net.cpp:66] Creating Layer relu8
I0905 22:01:41.316889 32694 net.cpp:329] relu8 <- fc8
I0905 22:01:41.316898 32694 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:01:41.316905 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.316911 32694 net.cpp:125] relu8 needs backward computation.
I0905 22:01:41.316918 32694 net.cpp:66] Creating Layer drop8
I0905 22:01:41.316925 32694 net.cpp:329] drop8 <- fc8
I0905 22:01:41.316931 32694 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:01:41.316938 32694 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:01:41.316943 32694 net.cpp:125] drop8 needs backward computation.
I0905 22:01:41.316952 32694 net.cpp:66] Creating Layer fc9
I0905 22:01:41.316958 32694 net.cpp:329] fc9 <- fc8
I0905 22:01:41.316967 32694 net.cpp:290] fc9 -> fc9
I0905 22:01:41.317350 32694 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:01:41.317363 32694 net.cpp:125] fc9 needs backward computation.
I0905 22:01:41.317371 32694 net.cpp:66] Creating Layer fc10
I0905 22:01:41.317378 32694 net.cpp:329] fc10 <- fc9
I0905 22:01:41.317385 32694 net.cpp:290] fc10 -> fc10
I0905 22:01:41.317397 32694 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:01:41.317406 32694 net.cpp:125] fc10 needs backward computation.
I0905 22:01:41.317412 32694 net.cpp:66] Creating Layer prob
I0905 22:01:41.317419 32694 net.cpp:329] prob <- fc10
I0905 22:01:41.317426 32694 net.cpp:290] prob -> prob
I0905 22:01:41.317436 32694 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:01:41.317442 32694 net.cpp:125] prob needs backward computation.
I0905 22:01:41.317447 32694 net.cpp:156] This network produces output prob
I0905 22:01:41.317461 32694 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:01:41.317478 32694 net.cpp:167] Network initialization done.
I0905 22:01:41.317484 32694 net.cpp:168] Memory required for data: 6183480
Classifying 92 inputs.
Done in 56.11 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:02:40.046991 32701 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:02:40.047129 32701 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:02:40.047138 32701 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:02:40.047286 32701 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:02:40.047339 32701 net.cpp:292] Input 0 -> data
I0905 22:02:40.047365 32701 net.cpp:66] Creating Layer conv1
I0905 22:02:40.047372 32701 net.cpp:329] conv1 <- data
I0905 22:02:40.047391 32701 net.cpp:290] conv1 -> conv1
I0905 22:02:40.048750 32701 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:02:40.048768 32701 net.cpp:125] conv1 needs backward computation.
I0905 22:02:40.048776 32701 net.cpp:66] Creating Layer relu1
I0905 22:02:40.048782 32701 net.cpp:329] relu1 <- conv1
I0905 22:02:40.048789 32701 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:02:40.048799 32701 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:02:40.048804 32701 net.cpp:125] relu1 needs backward computation.
I0905 22:02:40.048811 32701 net.cpp:66] Creating Layer pool1
I0905 22:02:40.048816 32701 net.cpp:329] pool1 <- conv1
I0905 22:02:40.048823 32701 net.cpp:290] pool1 -> pool1
I0905 22:02:40.048835 32701 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:02:40.048840 32701 net.cpp:125] pool1 needs backward computation.
I0905 22:02:40.048847 32701 net.cpp:66] Creating Layer norm1
I0905 22:02:40.048853 32701 net.cpp:329] norm1 <- pool1
I0905 22:02:40.048859 32701 net.cpp:290] norm1 -> norm1
I0905 22:02:40.048869 32701 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:02:40.048876 32701 net.cpp:125] norm1 needs backward computation.
I0905 22:02:40.048882 32701 net.cpp:66] Creating Layer conv2
I0905 22:02:40.048888 32701 net.cpp:329] conv2 <- norm1
I0905 22:02:40.048895 32701 net.cpp:290] conv2 -> conv2
I0905 22:02:40.058068 32701 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:02:40.058082 32701 net.cpp:125] conv2 needs backward computation.
I0905 22:02:40.058090 32701 net.cpp:66] Creating Layer relu2
I0905 22:02:40.058095 32701 net.cpp:329] relu2 <- conv2
I0905 22:02:40.058102 32701 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:02:40.058109 32701 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:02:40.058115 32701 net.cpp:125] relu2 needs backward computation.
I0905 22:02:40.058123 32701 net.cpp:66] Creating Layer pool2
I0905 22:02:40.058130 32701 net.cpp:329] pool2 <- conv2
I0905 22:02:40.058136 32701 net.cpp:290] pool2 -> pool2
I0905 22:02:40.058145 32701 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:02:40.058151 32701 net.cpp:125] pool2 needs backward computation.
I0905 22:02:40.058157 32701 net.cpp:66] Creating Layer fc7
I0905 22:02:40.058162 32701 net.cpp:329] fc7 <- pool2
I0905 22:02:40.058169 32701 net.cpp:290] fc7 -> fc7
I0905 22:02:40.707268 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.707314 32701 net.cpp:125] fc7 needs backward computation.
I0905 22:02:40.707327 32701 net.cpp:66] Creating Layer relu7
I0905 22:02:40.707335 32701 net.cpp:329] relu7 <- fc7
I0905 22:02:40.707345 32701 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:02:40.707355 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.707360 32701 net.cpp:125] relu7 needs backward computation.
I0905 22:02:40.707367 32701 net.cpp:66] Creating Layer drop7
I0905 22:02:40.707373 32701 net.cpp:329] drop7 <- fc7
I0905 22:02:40.707379 32701 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:02:40.707391 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.707396 32701 net.cpp:125] drop7 needs backward computation.
I0905 22:02:40.707404 32701 net.cpp:66] Creating Layer fc8
I0905 22:02:40.707409 32701 net.cpp:329] fc8 <- fc7
I0905 22:02:40.707418 32701 net.cpp:290] fc8 -> fc8
I0905 22:02:40.715206 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.715219 32701 net.cpp:125] fc8 needs backward computation.
I0905 22:02:40.715227 32701 net.cpp:66] Creating Layer relu8
I0905 22:02:40.715232 32701 net.cpp:329] relu8 <- fc8
I0905 22:02:40.715240 32701 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:02:40.715248 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.715253 32701 net.cpp:125] relu8 needs backward computation.
I0905 22:02:40.715260 32701 net.cpp:66] Creating Layer drop8
I0905 22:02:40.715265 32701 net.cpp:329] drop8 <- fc8
I0905 22:02:40.715272 32701 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:02:40.715279 32701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:02:40.715284 32701 net.cpp:125] drop8 needs backward computation.
I0905 22:02:40.715292 32701 net.cpp:66] Creating Layer fc9
I0905 22:02:40.715308 32701 net.cpp:329] fc9 <- fc8
I0905 22:02:40.715317 32701 net.cpp:290] fc9 -> fc9
I0905 22:02:40.715689 32701 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:02:40.715701 32701 net.cpp:125] fc9 needs backward computation.
I0905 22:02:40.715709 32701 net.cpp:66] Creating Layer fc10
I0905 22:02:40.715715 32701 net.cpp:329] fc10 <- fc9
I0905 22:02:40.715723 32701 net.cpp:290] fc10 -> fc10
I0905 22:02:40.715735 32701 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:02:40.715744 32701 net.cpp:125] fc10 needs backward computation.
I0905 22:02:40.715750 32701 net.cpp:66] Creating Layer prob
I0905 22:02:40.715755 32701 net.cpp:329] prob <- fc10
I0905 22:02:40.715764 32701 net.cpp:290] prob -> prob
I0905 22:02:40.715773 32701 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:02:40.715780 32701 net.cpp:125] prob needs backward computation.
I0905 22:02:40.715785 32701 net.cpp:156] This network produces output prob
I0905 22:02:40.715797 32701 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:02:40.715806 32701 net.cpp:167] Network initialization done.
I0905 22:02:40.715811 32701 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 243 inputs.
Done in 166.02 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 43 is out of bounds for axis 0 with size 43
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:05:32.296311 32708 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:05:32.296449 32708 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:05:32.296459 32708 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:05:32.296605 32708 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:05:32.296669 32708 net.cpp:292] Input 0 -> data
I0905 22:05:32.296694 32708 net.cpp:66] Creating Layer conv1
I0905 22:05:32.296700 32708 net.cpp:329] conv1 <- data
I0905 22:05:32.296710 32708 net.cpp:290] conv1 -> conv1
I0905 22:05:32.298084 32708 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:05:32.298104 32708 net.cpp:125] conv1 needs backward computation.
I0905 22:05:32.298113 32708 net.cpp:66] Creating Layer relu1
I0905 22:05:32.298120 32708 net.cpp:329] relu1 <- conv1
I0905 22:05:32.298125 32708 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:05:32.298135 32708 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:05:32.298141 32708 net.cpp:125] relu1 needs backward computation.
I0905 22:05:32.298146 32708 net.cpp:66] Creating Layer pool1
I0905 22:05:32.298152 32708 net.cpp:329] pool1 <- conv1
I0905 22:05:32.298159 32708 net.cpp:290] pool1 -> pool1
I0905 22:05:32.298171 32708 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:05:32.298177 32708 net.cpp:125] pool1 needs backward computation.
I0905 22:05:32.298183 32708 net.cpp:66] Creating Layer norm1
I0905 22:05:32.298188 32708 net.cpp:329] norm1 <- pool1
I0905 22:05:32.298195 32708 net.cpp:290] norm1 -> norm1
I0905 22:05:32.298205 32708 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:05:32.298212 32708 net.cpp:125] norm1 needs backward computation.
I0905 22:05:32.298218 32708 net.cpp:66] Creating Layer conv2
I0905 22:05:32.298224 32708 net.cpp:329] conv2 <- norm1
I0905 22:05:32.298231 32708 net.cpp:290] conv2 -> conv2
I0905 22:05:32.307361 32708 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:05:32.307376 32708 net.cpp:125] conv2 needs backward computation.
I0905 22:05:32.307384 32708 net.cpp:66] Creating Layer relu2
I0905 22:05:32.307389 32708 net.cpp:329] relu2 <- conv2
I0905 22:05:32.307396 32708 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:05:32.307404 32708 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:05:32.307409 32708 net.cpp:125] relu2 needs backward computation.
I0905 22:05:32.307415 32708 net.cpp:66] Creating Layer pool2
I0905 22:05:32.307420 32708 net.cpp:329] pool2 <- conv2
I0905 22:05:32.307426 32708 net.cpp:290] pool2 -> pool2
I0905 22:05:32.307435 32708 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:05:32.307440 32708 net.cpp:125] pool2 needs backward computation.
I0905 22:05:32.307449 32708 net.cpp:66] Creating Layer fc7
I0905 22:05:32.307456 32708 net.cpp:329] fc7 <- pool2
I0905 22:05:32.307462 32708 net.cpp:290] fc7 -> fc7
I0905 22:05:32.953434 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.953479 32708 net.cpp:125] fc7 needs backward computation.
I0905 22:05:32.953490 32708 net.cpp:66] Creating Layer relu7
I0905 22:05:32.953498 32708 net.cpp:329] relu7 <- fc7
I0905 22:05:32.953510 32708 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:05:32.953531 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.953537 32708 net.cpp:125] relu7 needs backward computation.
I0905 22:05:32.953546 32708 net.cpp:66] Creating Layer drop7
I0905 22:05:32.953551 32708 net.cpp:329] drop7 <- fc7
I0905 22:05:32.953557 32708 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:05:32.953568 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.953574 32708 net.cpp:125] drop7 needs backward computation.
I0905 22:05:32.953585 32708 net.cpp:66] Creating Layer fc8
I0905 22:05:32.953593 32708 net.cpp:329] fc8 <- fc7
I0905 22:05:32.953601 32708 net.cpp:290] fc8 -> fc8
I0905 22:05:32.961381 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.961395 32708 net.cpp:125] fc8 needs backward computation.
I0905 22:05:32.961402 32708 net.cpp:66] Creating Layer relu8
I0905 22:05:32.961408 32708 net.cpp:329] relu8 <- fc8
I0905 22:05:32.961416 32708 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:05:32.961423 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.961428 32708 net.cpp:125] relu8 needs backward computation.
I0905 22:05:32.961436 32708 net.cpp:66] Creating Layer drop8
I0905 22:05:32.961441 32708 net.cpp:329] drop8 <- fc8
I0905 22:05:32.961447 32708 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:05:32.961453 32708 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:32.961458 32708 net.cpp:125] drop8 needs backward computation.
I0905 22:05:32.961467 32708 net.cpp:66] Creating Layer fc9
I0905 22:05:32.961473 32708 net.cpp:329] fc9 <- fc8
I0905 22:05:32.961480 32708 net.cpp:290] fc9 -> fc9
I0905 22:05:32.961856 32708 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:05:32.961869 32708 net.cpp:125] fc9 needs backward computation.
I0905 22:05:32.961877 32708 net.cpp:66] Creating Layer fc10
I0905 22:05:32.961884 32708 net.cpp:329] fc10 <- fc9
I0905 22:05:32.961891 32708 net.cpp:290] fc10 -> fc10
I0905 22:05:32.961904 32708 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:05:32.961911 32708 net.cpp:125] fc10 needs backward computation.
I0905 22:05:32.961917 32708 net.cpp:66] Creating Layer prob
I0905 22:05:32.961923 32708 net.cpp:329] prob <- fc10
I0905 22:05:32.961930 32708 net.cpp:290] prob -> prob
I0905 22:05:32.961941 32708 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:05:32.961946 32708 net.cpp:125] prob needs backward computation.
I0905 22:05:32.961951 32708 net.cpp:156] This network produces output prob
I0905 22:05:32.961963 32708 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:05:32.961972 32708 net.cpp:167] Network initialization done.
I0905 22:05:32.961977 32708 net.cpp:168] Memory required for data: 6183480
Classifying 36 inputs.
Done in 23.13 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:05:57.632202 32713 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:05:57.632340 32713 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:05:57.632349 32713 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:05:57.632496 32713 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:05:57.632560 32713 net.cpp:292] Input 0 -> data
I0905 22:05:57.632586 32713 net.cpp:66] Creating Layer conv1
I0905 22:05:57.632592 32713 net.cpp:329] conv1 <- data
I0905 22:05:57.632601 32713 net.cpp:290] conv1 -> conv1
I0905 22:05:57.633976 32713 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:05:57.633996 32713 net.cpp:125] conv1 needs backward computation.
I0905 22:05:57.634006 32713 net.cpp:66] Creating Layer relu1
I0905 22:05:57.634011 32713 net.cpp:329] relu1 <- conv1
I0905 22:05:57.634018 32713 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:05:57.634027 32713 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:05:57.634033 32713 net.cpp:125] relu1 needs backward computation.
I0905 22:05:57.634040 32713 net.cpp:66] Creating Layer pool1
I0905 22:05:57.634045 32713 net.cpp:329] pool1 <- conv1
I0905 22:05:57.634052 32713 net.cpp:290] pool1 -> pool1
I0905 22:05:57.634063 32713 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:05:57.634069 32713 net.cpp:125] pool1 needs backward computation.
I0905 22:05:57.634076 32713 net.cpp:66] Creating Layer norm1
I0905 22:05:57.634083 32713 net.cpp:329] norm1 <- pool1
I0905 22:05:57.634089 32713 net.cpp:290] norm1 -> norm1
I0905 22:05:57.634099 32713 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:05:57.634104 32713 net.cpp:125] norm1 needs backward computation.
I0905 22:05:57.634112 32713 net.cpp:66] Creating Layer conv2
I0905 22:05:57.634119 32713 net.cpp:329] conv2 <- norm1
I0905 22:05:57.634125 32713 net.cpp:290] conv2 -> conv2
I0905 22:05:57.643259 32713 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:05:57.643275 32713 net.cpp:125] conv2 needs backward computation.
I0905 22:05:57.643281 32713 net.cpp:66] Creating Layer relu2
I0905 22:05:57.643293 32713 net.cpp:329] relu2 <- conv2
I0905 22:05:57.643301 32713 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:05:57.643308 32713 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:05:57.643314 32713 net.cpp:125] relu2 needs backward computation.
I0905 22:05:57.643321 32713 net.cpp:66] Creating Layer pool2
I0905 22:05:57.643326 32713 net.cpp:329] pool2 <- conv2
I0905 22:05:57.643332 32713 net.cpp:290] pool2 -> pool2
I0905 22:05:57.643342 32713 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:05:57.643347 32713 net.cpp:125] pool2 needs backward computation.
I0905 22:05:57.643357 32713 net.cpp:66] Creating Layer fc7
I0905 22:05:57.643362 32713 net.cpp:329] fc7 <- pool2
I0905 22:05:57.643369 32713 net.cpp:290] fc7 -> fc7
I0905 22:05:58.294697 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.294742 32713 net.cpp:125] fc7 needs backward computation.
I0905 22:05:58.294755 32713 net.cpp:66] Creating Layer relu7
I0905 22:05:58.294764 32713 net.cpp:329] relu7 <- fc7
I0905 22:05:58.294773 32713 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:05:58.294783 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.294790 32713 net.cpp:125] relu7 needs backward computation.
I0905 22:05:58.294797 32713 net.cpp:66] Creating Layer drop7
I0905 22:05:58.294803 32713 net.cpp:329] drop7 <- fc7
I0905 22:05:58.294809 32713 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:05:58.294821 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.294827 32713 net.cpp:125] drop7 needs backward computation.
I0905 22:05:58.294836 32713 net.cpp:66] Creating Layer fc8
I0905 22:05:58.294842 32713 net.cpp:329] fc8 <- fc7
I0905 22:05:58.294852 32713 net.cpp:290] fc8 -> fc8
I0905 22:05:58.302716 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.302727 32713 net.cpp:125] fc8 needs backward computation.
I0905 22:05:58.302734 32713 net.cpp:66] Creating Layer relu8
I0905 22:05:58.302741 32713 net.cpp:329] relu8 <- fc8
I0905 22:05:58.302748 32713 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:05:58.302755 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.302762 32713 net.cpp:125] relu8 needs backward computation.
I0905 22:05:58.302768 32713 net.cpp:66] Creating Layer drop8
I0905 22:05:58.302773 32713 net.cpp:329] drop8 <- fc8
I0905 22:05:58.302779 32713 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:05:58.302786 32713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:05:58.302793 32713 net.cpp:125] drop8 needs backward computation.
I0905 22:05:58.302800 32713 net.cpp:66] Creating Layer fc9
I0905 22:05:58.302806 32713 net.cpp:329] fc9 <- fc8
I0905 22:05:58.302814 32713 net.cpp:290] fc9 -> fc9
I0905 22:05:58.303187 32713 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:05:58.303200 32713 net.cpp:125] fc9 needs backward computation.
I0905 22:05:58.303208 32713 net.cpp:66] Creating Layer fc10
I0905 22:05:58.303213 32713 net.cpp:329] fc10 <- fc9
I0905 22:05:58.303222 32713 net.cpp:290] fc10 -> fc10
I0905 22:05:58.303235 32713 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:05:58.303242 32713 net.cpp:125] fc10 needs backward computation.
I0905 22:05:58.303248 32713 net.cpp:66] Creating Layer prob
I0905 22:05:58.303254 32713 net.cpp:329] prob <- fc10
I0905 22:05:58.303262 32713 net.cpp:290] prob -> prob
I0905 22:05:58.303272 32713 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:05:58.303277 32713 net.cpp:125] prob needs backward computation.
I0905 22:05:58.303283 32713 net.cpp:156] This network produces output prob
I0905 22:05:58.303295 32713 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:05:58.303304 32713 net.cpp:167] Network initialization done.
I0905 22:05:58.303309 32713 net.cpp:168] Memory required for data: 6183480
Classifying 80 inputs.
Done in 51.72 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:06:52.807149 32718 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:06:52.807287 32718 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:06:52.807307 32718 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:06:52.807454 32718 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:06:52.807507 32718 net.cpp:292] Input 0 -> data
I0905 22:06:52.807533 32718 net.cpp:66] Creating Layer conv1
I0905 22:06:52.807539 32718 net.cpp:329] conv1 <- data
I0905 22:06:52.807548 32718 net.cpp:290] conv1 -> conv1
I0905 22:06:52.808907 32718 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:06:52.808925 32718 net.cpp:125] conv1 needs backward computation.
I0905 22:06:52.808934 32718 net.cpp:66] Creating Layer relu1
I0905 22:06:52.808940 32718 net.cpp:329] relu1 <- conv1
I0905 22:06:52.808948 32718 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:06:52.808955 32718 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:06:52.808961 32718 net.cpp:125] relu1 needs backward computation.
I0905 22:06:52.808974 32718 net.cpp:66] Creating Layer pool1
I0905 22:06:52.808979 32718 net.cpp:329] pool1 <- conv1
I0905 22:06:52.808986 32718 net.cpp:290] pool1 -> pool1
I0905 22:06:52.808997 32718 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:06:52.809003 32718 net.cpp:125] pool1 needs backward computation.
I0905 22:06:52.809010 32718 net.cpp:66] Creating Layer norm1
I0905 22:06:52.809015 32718 net.cpp:329] norm1 <- pool1
I0905 22:06:52.809022 32718 net.cpp:290] norm1 -> norm1
I0905 22:06:52.809032 32718 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:06:52.809038 32718 net.cpp:125] norm1 needs backward computation.
I0905 22:06:52.809046 32718 net.cpp:66] Creating Layer conv2
I0905 22:06:52.809051 32718 net.cpp:329] conv2 <- norm1
I0905 22:06:52.809065 32718 net.cpp:290] conv2 -> conv2
I0905 22:06:52.818214 32718 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:06:52.818229 32718 net.cpp:125] conv2 needs backward computation.
I0905 22:06:52.818236 32718 net.cpp:66] Creating Layer relu2
I0905 22:06:52.818243 32718 net.cpp:329] relu2 <- conv2
I0905 22:06:52.818248 32718 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:06:52.818255 32718 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:06:52.818261 32718 net.cpp:125] relu2 needs backward computation.
I0905 22:06:52.818269 32718 net.cpp:66] Creating Layer pool2
I0905 22:06:52.818275 32718 net.cpp:329] pool2 <- conv2
I0905 22:06:52.818282 32718 net.cpp:290] pool2 -> pool2
I0905 22:06:52.818290 32718 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:06:52.818296 32718 net.cpp:125] pool2 needs backward computation.
I0905 22:06:52.818303 32718 net.cpp:66] Creating Layer fc7
I0905 22:06:52.818308 32718 net.cpp:329] fc7 <- pool2
I0905 22:06:52.818315 32718 net.cpp:290] fc7 -> fc7
I0905 22:06:53.470432 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.470476 32718 net.cpp:125] fc7 needs backward computation.
I0905 22:06:53.470489 32718 net.cpp:66] Creating Layer relu7
I0905 22:06:53.470497 32718 net.cpp:329] relu7 <- fc7
I0905 22:06:53.470506 32718 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:06:53.470516 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.470522 32718 net.cpp:125] relu7 needs backward computation.
I0905 22:06:53.470530 32718 net.cpp:66] Creating Layer drop7
I0905 22:06:53.470536 32718 net.cpp:329] drop7 <- fc7
I0905 22:06:53.470542 32718 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:06:53.470553 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.470561 32718 net.cpp:125] drop7 needs backward computation.
I0905 22:06:53.470569 32718 net.cpp:66] Creating Layer fc8
I0905 22:06:53.470574 32718 net.cpp:329] fc8 <- fc7
I0905 22:06:53.470587 32718 net.cpp:290] fc8 -> fc8
I0905 22:06:53.478603 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.478616 32718 net.cpp:125] fc8 needs backward computation.
I0905 22:06:53.478624 32718 net.cpp:66] Creating Layer relu8
I0905 22:06:53.478629 32718 net.cpp:329] relu8 <- fc8
I0905 22:06:53.478637 32718 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:06:53.478646 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.478652 32718 net.cpp:125] relu8 needs backward computation.
I0905 22:06:53.478657 32718 net.cpp:66] Creating Layer drop8
I0905 22:06:53.478663 32718 net.cpp:329] drop8 <- fc8
I0905 22:06:53.478669 32718 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:06:53.478677 32718 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:06:53.478682 32718 net.cpp:125] drop8 needs backward computation.
I0905 22:06:53.478692 32718 net.cpp:66] Creating Layer fc9
I0905 22:06:53.478696 32718 net.cpp:329] fc9 <- fc8
I0905 22:06:53.478704 32718 net.cpp:290] fc9 -> fc9
I0905 22:06:53.479089 32718 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:06:53.479101 32718 net.cpp:125] fc9 needs backward computation.
I0905 22:06:53.479110 32718 net.cpp:66] Creating Layer fc10
I0905 22:06:53.479115 32718 net.cpp:329] fc10 <- fc9
I0905 22:06:53.479125 32718 net.cpp:290] fc10 -> fc10
I0905 22:06:53.479136 32718 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:06:53.479154 32718 net.cpp:125] fc10 needs backward computation.
I0905 22:06:53.479162 32718 net.cpp:66] Creating Layer prob
I0905 22:06:53.479168 32718 net.cpp:329] prob <- fc10
I0905 22:06:53.479176 32718 net.cpp:290] prob -> prob
I0905 22:06:53.479187 32718 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:06:53.479192 32718 net.cpp:125] prob needs backward computation.
I0905 22:06:53.479197 32718 net.cpp:156] This network produces output prob
I0905 22:06:53.479210 32718 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:06:53.479218 32718 net.cpp:167] Network initialization done.
I0905 22:06:53.479223 32718 net.cpp:168] Memory required for data: 6183480
Classifying 14 inputs.
Done in 9.07 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:07:03.638842 32722 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:07:03.638979 32722 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:07:03.638988 32722 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:07:03.639135 32722 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:07:03.639199 32722 net.cpp:292] Input 0 -> data
I0905 22:07:03.639225 32722 net.cpp:66] Creating Layer conv1
I0905 22:07:03.639231 32722 net.cpp:329] conv1 <- data
I0905 22:07:03.639240 32722 net.cpp:290] conv1 -> conv1
I0905 22:07:03.640596 32722 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:07:03.640614 32722 net.cpp:125] conv1 needs backward computation.
I0905 22:07:03.640625 32722 net.cpp:66] Creating Layer relu1
I0905 22:07:03.640630 32722 net.cpp:329] relu1 <- conv1
I0905 22:07:03.640637 32722 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:07:03.640645 32722 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:07:03.640651 32722 net.cpp:125] relu1 needs backward computation.
I0905 22:07:03.640658 32722 net.cpp:66] Creating Layer pool1
I0905 22:07:03.640665 32722 net.cpp:329] pool1 <- conv1
I0905 22:07:03.640671 32722 net.cpp:290] pool1 -> pool1
I0905 22:07:03.640681 32722 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:07:03.640687 32722 net.cpp:125] pool1 needs backward computation.
I0905 22:07:03.640694 32722 net.cpp:66] Creating Layer norm1
I0905 22:07:03.640700 32722 net.cpp:329] norm1 <- pool1
I0905 22:07:03.640707 32722 net.cpp:290] norm1 -> norm1
I0905 22:07:03.640717 32722 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:07:03.640723 32722 net.cpp:125] norm1 needs backward computation.
I0905 22:07:03.640730 32722 net.cpp:66] Creating Layer conv2
I0905 22:07:03.640736 32722 net.cpp:329] conv2 <- norm1
I0905 22:07:03.640743 32722 net.cpp:290] conv2 -> conv2
I0905 22:07:03.649935 32722 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:07:03.649950 32722 net.cpp:125] conv2 needs backward computation.
I0905 22:07:03.649956 32722 net.cpp:66] Creating Layer relu2
I0905 22:07:03.649962 32722 net.cpp:329] relu2 <- conv2
I0905 22:07:03.649970 32722 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:07:03.649976 32722 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:07:03.649982 32722 net.cpp:125] relu2 needs backward computation.
I0905 22:07:03.649988 32722 net.cpp:66] Creating Layer pool2
I0905 22:07:03.649994 32722 net.cpp:329] pool2 <- conv2
I0905 22:07:03.650002 32722 net.cpp:290] pool2 -> pool2
I0905 22:07:03.650009 32722 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:07:03.650014 32722 net.cpp:125] pool2 needs backward computation.
I0905 22:07:03.650024 32722 net.cpp:66] Creating Layer fc7
I0905 22:07:03.650029 32722 net.cpp:329] fc7 <- pool2
I0905 22:07:03.650038 32722 net.cpp:290] fc7 -> fc7
I0905 22:07:04.307993 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.308039 32722 net.cpp:125] fc7 needs backward computation.
I0905 22:07:04.308053 32722 net.cpp:66] Creating Layer relu7
I0905 22:07:04.308059 32722 net.cpp:329] relu7 <- fc7
I0905 22:07:04.308069 32722 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:07:04.308079 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.308085 32722 net.cpp:125] relu7 needs backward computation.
I0905 22:07:04.308092 32722 net.cpp:66] Creating Layer drop7
I0905 22:07:04.308099 32722 net.cpp:329] drop7 <- fc7
I0905 22:07:04.308104 32722 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:07:04.308115 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.308121 32722 net.cpp:125] drop7 needs backward computation.
I0905 22:07:04.308130 32722 net.cpp:66] Creating Layer fc8
I0905 22:07:04.308135 32722 net.cpp:329] fc8 <- fc7
I0905 22:07:04.308145 32722 net.cpp:290] fc8 -> fc8
I0905 22:07:04.315930 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.315943 32722 net.cpp:125] fc8 needs backward computation.
I0905 22:07:04.315950 32722 net.cpp:66] Creating Layer relu8
I0905 22:07:04.315956 32722 net.cpp:329] relu8 <- fc8
I0905 22:07:04.315964 32722 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:07:04.315984 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.315990 32722 net.cpp:125] relu8 needs backward computation.
I0905 22:07:04.315996 32722 net.cpp:66] Creating Layer drop8
I0905 22:07:04.316002 32722 net.cpp:329] drop8 <- fc8
I0905 22:07:04.316009 32722 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:07:04.316015 32722 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:07:04.316020 32722 net.cpp:125] drop8 needs backward computation.
I0905 22:07:04.316030 32722 net.cpp:66] Creating Layer fc9
I0905 22:07:04.316035 32722 net.cpp:329] fc9 <- fc8
I0905 22:07:04.316042 32722 net.cpp:290] fc9 -> fc9
I0905 22:07:04.316414 32722 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:07:04.316426 32722 net.cpp:125] fc9 needs backward computation.
I0905 22:07:04.316436 32722 net.cpp:66] Creating Layer fc10
I0905 22:07:04.316442 32722 net.cpp:329] fc10 <- fc9
I0905 22:07:04.316449 32722 net.cpp:290] fc10 -> fc10
I0905 22:07:04.316462 32722 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:07:04.316469 32722 net.cpp:125] fc10 needs backward computation.
I0905 22:07:04.316476 32722 net.cpp:66] Creating Layer prob
I0905 22:07:04.316483 32722 net.cpp:329] prob <- fc10
I0905 22:07:04.316489 32722 net.cpp:290] prob -> prob
I0905 22:07:04.316499 32722 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:07:04.316505 32722 net.cpp:125] prob needs backward computation.
I0905 22:07:04.316510 32722 net.cpp:156] This network produces output prob
I0905 22:07:04.316524 32722 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:07:04.316531 32722 net.cpp:167] Network initialization done.
I0905 22:07:04.316537 32722 net.cpp:168] Memory required for data: 6183480
Classifying 105 inputs.
Done in 68.39 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 5 is out of bounds for axis 0 with size 5
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:08:15.376191 32727 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:08:15.376329 32727 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:08:15.376338 32727 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:08:15.376487 32727 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:08:15.376550 32727 net.cpp:292] Input 0 -> data
I0905 22:08:15.376577 32727 net.cpp:66] Creating Layer conv1
I0905 22:08:15.376585 32727 net.cpp:329] conv1 <- data
I0905 22:08:15.376593 32727 net.cpp:290] conv1 -> conv1
I0905 22:08:15.377969 32727 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:08:15.377987 32727 net.cpp:125] conv1 needs backward computation.
I0905 22:08:15.377996 32727 net.cpp:66] Creating Layer relu1
I0905 22:08:15.378002 32727 net.cpp:329] relu1 <- conv1
I0905 22:08:15.378010 32727 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:08:15.378018 32727 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:08:15.378025 32727 net.cpp:125] relu1 needs backward computation.
I0905 22:08:15.378031 32727 net.cpp:66] Creating Layer pool1
I0905 22:08:15.378037 32727 net.cpp:329] pool1 <- conv1
I0905 22:08:15.378043 32727 net.cpp:290] pool1 -> pool1
I0905 22:08:15.378056 32727 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:08:15.378062 32727 net.cpp:125] pool1 needs backward computation.
I0905 22:08:15.378068 32727 net.cpp:66] Creating Layer norm1
I0905 22:08:15.378073 32727 net.cpp:329] norm1 <- pool1
I0905 22:08:15.378080 32727 net.cpp:290] norm1 -> norm1
I0905 22:08:15.378090 32727 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:08:15.378096 32727 net.cpp:125] norm1 needs backward computation.
I0905 22:08:15.378104 32727 net.cpp:66] Creating Layer conv2
I0905 22:08:15.378110 32727 net.cpp:329] conv2 <- norm1
I0905 22:08:15.378118 32727 net.cpp:290] conv2 -> conv2
I0905 22:08:15.387244 32727 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:08:15.387259 32727 net.cpp:125] conv2 needs backward computation.
I0905 22:08:15.387267 32727 net.cpp:66] Creating Layer relu2
I0905 22:08:15.387274 32727 net.cpp:329] relu2 <- conv2
I0905 22:08:15.387280 32727 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:08:15.387287 32727 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:08:15.387294 32727 net.cpp:125] relu2 needs backward computation.
I0905 22:08:15.387300 32727 net.cpp:66] Creating Layer pool2
I0905 22:08:15.387305 32727 net.cpp:329] pool2 <- conv2
I0905 22:08:15.387311 32727 net.cpp:290] pool2 -> pool2
I0905 22:08:15.387320 32727 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:08:15.387326 32727 net.cpp:125] pool2 needs backward computation.
I0905 22:08:15.387334 32727 net.cpp:66] Creating Layer fc7
I0905 22:08:15.387341 32727 net.cpp:329] fc7 <- pool2
I0905 22:08:15.387353 32727 net.cpp:290] fc7 -> fc7
I0905 22:08:16.035470 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.035516 32727 net.cpp:125] fc7 needs backward computation.
I0905 22:08:16.035528 32727 net.cpp:66] Creating Layer relu7
I0905 22:08:16.035537 32727 net.cpp:329] relu7 <- fc7
I0905 22:08:16.035547 32727 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:08:16.035557 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.035562 32727 net.cpp:125] relu7 needs backward computation.
I0905 22:08:16.035570 32727 net.cpp:66] Creating Layer drop7
I0905 22:08:16.035575 32727 net.cpp:329] drop7 <- fc7
I0905 22:08:16.035583 32727 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:08:16.035593 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.035599 32727 net.cpp:125] drop7 needs backward computation.
I0905 22:08:16.035609 32727 net.cpp:66] Creating Layer fc8
I0905 22:08:16.035614 32727 net.cpp:329] fc8 <- fc7
I0905 22:08:16.035624 32727 net.cpp:290] fc8 -> fc8
I0905 22:08:16.043406 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.043417 32727 net.cpp:125] fc8 needs backward computation.
I0905 22:08:16.043426 32727 net.cpp:66] Creating Layer relu8
I0905 22:08:16.043431 32727 net.cpp:329] relu8 <- fc8
I0905 22:08:16.043439 32727 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:08:16.043447 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.043452 32727 net.cpp:125] relu8 needs backward computation.
I0905 22:08:16.043459 32727 net.cpp:66] Creating Layer drop8
I0905 22:08:16.043465 32727 net.cpp:329] drop8 <- fc8
I0905 22:08:16.043472 32727 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:08:16.043478 32727 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:08:16.043484 32727 net.cpp:125] drop8 needs backward computation.
I0905 22:08:16.043493 32727 net.cpp:66] Creating Layer fc9
I0905 22:08:16.043499 32727 net.cpp:329] fc9 <- fc8
I0905 22:08:16.043506 32727 net.cpp:290] fc9 -> fc9
I0905 22:08:16.043879 32727 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:08:16.043891 32727 net.cpp:125] fc9 needs backward computation.
I0905 22:08:16.043900 32727 net.cpp:66] Creating Layer fc10
I0905 22:08:16.043906 32727 net.cpp:329] fc10 <- fc9
I0905 22:08:16.043915 32727 net.cpp:290] fc10 -> fc10
I0905 22:08:16.043926 32727 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:08:16.043934 32727 net.cpp:125] fc10 needs backward computation.
I0905 22:08:16.043941 32727 net.cpp:66] Creating Layer prob
I0905 22:08:16.043947 32727 net.cpp:329] prob <- fc10
I0905 22:08:16.043956 32727 net.cpp:290] prob -> prob
I0905 22:08:16.043966 32727 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:08:16.043972 32727 net.cpp:125] prob needs backward computation.
I0905 22:08:16.043977 32727 net.cpp:156] This network produces output prob
I0905 22:08:16.043989 32727 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:08:16.043998 32727 net.cpp:167] Network initialization done.
I0905 22:08:16.044003 32727 net.cpp:168] Memory required for data: 6183480
Classifying 421 inputs.
Done in 268.22 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 21 is out of bounds for axis 0 with size 21
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:13:03.797549 32750 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:13:03.797708 32750 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:13:03.797719 32750 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:13:03.797866 32750 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:13:03.806490 32750 net.cpp:292] Input 0 -> data
I0905 22:13:03.806519 32750 net.cpp:66] Creating Layer conv1
I0905 22:13:03.806527 32750 net.cpp:329] conv1 <- data
I0905 22:13:03.806535 32750 net.cpp:290] conv1 -> conv1
I0905 22:13:03.831146 32750 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:13:03.831171 32750 net.cpp:125] conv1 needs backward computation.
I0905 22:13:03.831182 32750 net.cpp:66] Creating Layer relu1
I0905 22:13:03.831188 32750 net.cpp:329] relu1 <- conv1
I0905 22:13:03.831197 32750 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:13:03.831205 32750 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:13:03.831212 32750 net.cpp:125] relu1 needs backward computation.
I0905 22:13:03.831218 32750 net.cpp:66] Creating Layer pool1
I0905 22:13:03.831224 32750 net.cpp:329] pool1 <- conv1
I0905 22:13:03.831231 32750 net.cpp:290] pool1 -> pool1
I0905 22:13:03.831243 32750 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:13:03.831249 32750 net.cpp:125] pool1 needs backward computation.
I0905 22:13:03.831256 32750 net.cpp:66] Creating Layer norm1
I0905 22:13:03.831267 32750 net.cpp:329] norm1 <- pool1
I0905 22:13:03.831275 32750 net.cpp:290] norm1 -> norm1
I0905 22:13:03.831285 32750 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:13:03.831291 32750 net.cpp:125] norm1 needs backward computation.
I0905 22:13:03.831300 32750 net.cpp:66] Creating Layer conv2
I0905 22:13:03.831305 32750 net.cpp:329] conv2 <- norm1
I0905 22:13:03.831312 32750 net.cpp:290] conv2 -> conv2
I0905 22:13:03.840416 32750 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:13:03.840431 32750 net.cpp:125] conv2 needs backward computation.
I0905 22:13:03.840438 32750 net.cpp:66] Creating Layer relu2
I0905 22:13:03.840445 32750 net.cpp:329] relu2 <- conv2
I0905 22:13:03.840451 32750 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:13:03.840459 32750 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:13:03.840464 32750 net.cpp:125] relu2 needs backward computation.
I0905 22:13:03.840471 32750 net.cpp:66] Creating Layer pool2
I0905 22:13:03.840477 32750 net.cpp:329] pool2 <- conv2
I0905 22:13:03.840484 32750 net.cpp:290] pool2 -> pool2
I0905 22:13:03.840492 32750 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:13:03.840498 32750 net.cpp:125] pool2 needs backward computation.
I0905 22:13:03.840507 32750 net.cpp:66] Creating Layer fc7
I0905 22:13:03.840513 32750 net.cpp:329] fc7 <- pool2
I0905 22:13:03.840522 32750 net.cpp:290] fc7 -> fc7
I0905 22:13:04.488116 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.488162 32750 net.cpp:125] fc7 needs backward computation.
I0905 22:13:04.488174 32750 net.cpp:66] Creating Layer relu7
I0905 22:13:04.488183 32750 net.cpp:329] relu7 <- fc7
I0905 22:13:04.488193 32750 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:13:04.488203 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.488209 32750 net.cpp:125] relu7 needs backward computation.
I0905 22:13:04.488216 32750 net.cpp:66] Creating Layer drop7
I0905 22:13:04.488221 32750 net.cpp:329] drop7 <- fc7
I0905 22:13:04.488229 32750 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:13:04.488240 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.488245 32750 net.cpp:125] drop7 needs backward computation.
I0905 22:13:04.488255 32750 net.cpp:66] Creating Layer fc8
I0905 22:13:04.488260 32750 net.cpp:329] fc8 <- fc7
I0905 22:13:04.488268 32750 net.cpp:290] fc8 -> fc8
I0905 22:13:04.496047 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.496060 32750 net.cpp:125] fc8 needs backward computation.
I0905 22:13:04.496068 32750 net.cpp:66] Creating Layer relu8
I0905 22:13:04.496073 32750 net.cpp:329] relu8 <- fc8
I0905 22:13:04.496081 32750 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:13:04.496089 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.496095 32750 net.cpp:125] relu8 needs backward computation.
I0905 22:13:04.496103 32750 net.cpp:66] Creating Layer drop8
I0905 22:13:04.496109 32750 net.cpp:329] drop8 <- fc8
I0905 22:13:04.496114 32750 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:13:04.496121 32750 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:04.496127 32750 net.cpp:125] drop8 needs backward computation.
I0905 22:13:04.496136 32750 net.cpp:66] Creating Layer fc9
I0905 22:13:04.496142 32750 net.cpp:329] fc9 <- fc8
I0905 22:13:04.496150 32750 net.cpp:290] fc9 -> fc9
I0905 22:13:04.496522 32750 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:13:04.496534 32750 net.cpp:125] fc9 needs backward computation.
I0905 22:13:04.496542 32750 net.cpp:66] Creating Layer fc10
I0905 22:13:04.496548 32750 net.cpp:329] fc10 <- fc9
I0905 22:13:04.496557 32750 net.cpp:290] fc10 -> fc10
I0905 22:13:04.496569 32750 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:13:04.496577 32750 net.cpp:125] fc10 needs backward computation.
I0905 22:13:04.496584 32750 net.cpp:66] Creating Layer prob
I0905 22:13:04.496590 32750 net.cpp:329] prob <- fc10
I0905 22:13:04.496599 32750 net.cpp:290] prob -> prob
I0905 22:13:04.496609 32750 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:13:04.496615 32750 net.cpp:125] prob needs backward computation.
I0905 22:13:04.496621 32750 net.cpp:156] This network produces output prob
I0905 22:13:04.496644 32750 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:13:04.496654 32750 net.cpp:167] Network initialization done.
I0905 22:13:04.496659 32750 net.cpp:168] Memory required for data: 6183480
Classifying 200 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:13:11.690796 32754 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:13:11.690933 32754 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:13:11.690943 32754 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:13:11.691087 32754 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:13:11.691150 32754 net.cpp:292] Input 0 -> data
I0905 22:13:11.691176 32754 net.cpp:66] Creating Layer conv1
I0905 22:13:11.691184 32754 net.cpp:329] conv1 <- data
I0905 22:13:11.691192 32754 net.cpp:290] conv1 -> conv1
I0905 22:13:11.692553 32754 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:13:11.692570 32754 net.cpp:125] conv1 needs backward computation.
I0905 22:13:11.692579 32754 net.cpp:66] Creating Layer relu1
I0905 22:13:11.692585 32754 net.cpp:329] relu1 <- conv1
I0905 22:13:11.692592 32754 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:13:11.692601 32754 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:13:11.692606 32754 net.cpp:125] relu1 needs backward computation.
I0905 22:13:11.692613 32754 net.cpp:66] Creating Layer pool1
I0905 22:13:11.692620 32754 net.cpp:329] pool1 <- conv1
I0905 22:13:11.692626 32754 net.cpp:290] pool1 -> pool1
I0905 22:13:11.692637 32754 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:13:11.692643 32754 net.cpp:125] pool1 needs backward computation.
I0905 22:13:11.692651 32754 net.cpp:66] Creating Layer norm1
I0905 22:13:11.692656 32754 net.cpp:329] norm1 <- pool1
I0905 22:13:11.692662 32754 net.cpp:290] norm1 -> norm1
I0905 22:13:11.692672 32754 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:13:11.692678 32754 net.cpp:125] norm1 needs backward computation.
I0905 22:13:11.692685 32754 net.cpp:66] Creating Layer conv2
I0905 22:13:11.692692 32754 net.cpp:329] conv2 <- norm1
I0905 22:13:11.692698 32754 net.cpp:290] conv2 -> conv2
I0905 22:13:11.701885 32754 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:13:11.701900 32754 net.cpp:125] conv2 needs backward computation.
I0905 22:13:11.701907 32754 net.cpp:66] Creating Layer relu2
I0905 22:13:11.701913 32754 net.cpp:329] relu2 <- conv2
I0905 22:13:11.701920 32754 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:13:11.701927 32754 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:13:11.701932 32754 net.cpp:125] relu2 needs backward computation.
I0905 22:13:11.701938 32754 net.cpp:66] Creating Layer pool2
I0905 22:13:11.701944 32754 net.cpp:329] pool2 <- conv2
I0905 22:13:11.701951 32754 net.cpp:290] pool2 -> pool2
I0905 22:13:11.701958 32754 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:13:11.701964 32754 net.cpp:125] pool2 needs backward computation.
I0905 22:13:11.701973 32754 net.cpp:66] Creating Layer fc7
I0905 22:13:11.701979 32754 net.cpp:329] fc7 <- pool2
I0905 22:13:11.701987 32754 net.cpp:290] fc7 -> fc7
I0905 22:13:12.348192 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.348237 32754 net.cpp:125] fc7 needs backward computation.
I0905 22:13:12.348249 32754 net.cpp:66] Creating Layer relu7
I0905 22:13:12.348258 32754 net.cpp:329] relu7 <- fc7
I0905 22:13:12.348266 32754 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:13:12.348276 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.348283 32754 net.cpp:125] relu7 needs backward computation.
I0905 22:13:12.348290 32754 net.cpp:66] Creating Layer drop7
I0905 22:13:12.348295 32754 net.cpp:329] drop7 <- fc7
I0905 22:13:12.348302 32754 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:13:12.348312 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.348319 32754 net.cpp:125] drop7 needs backward computation.
I0905 22:13:12.348327 32754 net.cpp:66] Creating Layer fc8
I0905 22:13:12.348332 32754 net.cpp:329] fc8 <- fc7
I0905 22:13:12.348341 32754 net.cpp:290] fc8 -> fc8
I0905 22:13:12.356118 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.356130 32754 net.cpp:125] fc8 needs backward computation.
I0905 22:13:12.356137 32754 net.cpp:66] Creating Layer relu8
I0905 22:13:12.356143 32754 net.cpp:329] relu8 <- fc8
I0905 22:13:12.356163 32754 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:13:12.356169 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.356175 32754 net.cpp:125] relu8 needs backward computation.
I0905 22:13:12.356183 32754 net.cpp:66] Creating Layer drop8
I0905 22:13:12.356187 32754 net.cpp:329] drop8 <- fc8
I0905 22:13:12.356194 32754 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:13:12.356201 32754 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:13:12.356207 32754 net.cpp:125] drop8 needs backward computation.
I0905 22:13:12.356215 32754 net.cpp:66] Creating Layer fc9
I0905 22:13:12.356221 32754 net.cpp:329] fc9 <- fc8
I0905 22:13:12.356228 32754 net.cpp:290] fc9 -> fc9
I0905 22:13:12.356602 32754 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:13:12.356614 32754 net.cpp:125] fc9 needs backward computation.
I0905 22:13:12.356622 32754 net.cpp:66] Creating Layer fc10
I0905 22:13:12.356628 32754 net.cpp:329] fc10 <- fc9
I0905 22:13:12.356637 32754 net.cpp:290] fc10 -> fc10
I0905 22:13:12.356648 32754 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:13:12.356657 32754 net.cpp:125] fc10 needs backward computation.
I0905 22:13:12.356663 32754 net.cpp:66] Creating Layer prob
I0905 22:13:12.356669 32754 net.cpp:329] prob <- fc10
I0905 22:13:12.356678 32754 net.cpp:290] prob -> prob
I0905 22:13:12.356686 32754 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:13:12.356693 32754 net.cpp:125] prob needs backward computation.
I0905 22:13:12.356698 32754 net.cpp:156] This network produces output prob
I0905 22:13:12.356710 32754 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:13:12.356719 32754 net.cpp:167] Network initialization done.
I0905 22:13:12.356724 32754 net.cpp:168] Memory required for data: 6183480
Classifying 652 inputs.
Done in 411.17 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 52 is out of bounds for axis 0 with size 52
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:20:16.884125   305 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:20:16.884264   305 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:20:16.884274   305 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:20:16.884423   305 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:20:16.884487   305 net.cpp:292] Input 0 -> data
I0905 22:20:16.884513   305 net.cpp:66] Creating Layer conv1
I0905 22:20:16.884521   305 net.cpp:329] conv1 <- data
I0905 22:20:16.884528   305 net.cpp:290] conv1 -> conv1
I0905 22:20:16.885901   305 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:20:16.885921   305 net.cpp:125] conv1 needs backward computation.
I0905 22:20:16.885929   305 net.cpp:66] Creating Layer relu1
I0905 22:20:16.885936   305 net.cpp:329] relu1 <- conv1
I0905 22:20:16.885942   305 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:20:16.885951   305 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:20:16.885957   305 net.cpp:125] relu1 needs backward computation.
I0905 22:20:16.885964   305 net.cpp:66] Creating Layer pool1
I0905 22:20:16.885970   305 net.cpp:329] pool1 <- conv1
I0905 22:20:16.885977   305 net.cpp:290] pool1 -> pool1
I0905 22:20:16.885988   305 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:20:16.885994   305 net.cpp:125] pool1 needs backward computation.
I0905 22:20:16.886001   305 net.cpp:66] Creating Layer norm1
I0905 22:20:16.886006   305 net.cpp:329] norm1 <- pool1
I0905 22:20:16.886013   305 net.cpp:290] norm1 -> norm1
I0905 22:20:16.886023   305 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:20:16.886029   305 net.cpp:125] norm1 needs backward computation.
I0905 22:20:16.886036   305 net.cpp:66] Creating Layer conv2
I0905 22:20:16.886042   305 net.cpp:329] conv2 <- norm1
I0905 22:20:16.886049   305 net.cpp:290] conv2 -> conv2
I0905 22:20:16.895329   305 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:20:16.895347   305 net.cpp:125] conv2 needs backward computation.
I0905 22:20:16.895355   305 net.cpp:66] Creating Layer relu2
I0905 22:20:16.895361   305 net.cpp:329] relu2 <- conv2
I0905 22:20:16.895369   305 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:20:16.895375   305 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:20:16.895381   305 net.cpp:125] relu2 needs backward computation.
I0905 22:20:16.895390   305 net.cpp:66] Creating Layer pool2
I0905 22:20:16.895396   305 net.cpp:329] pool2 <- conv2
I0905 22:20:16.895403   305 net.cpp:290] pool2 -> pool2
I0905 22:20:16.895412   305 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:20:16.895418   305 net.cpp:125] pool2 needs backward computation.
I0905 22:20:16.895426   305 net.cpp:66] Creating Layer fc7
I0905 22:20:16.895436   305 net.cpp:329] fc7 <- pool2
I0905 22:20:16.895444   305 net.cpp:290] fc7 -> fc7
I0905 22:20:17.546463   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.546509   305 net.cpp:125] fc7 needs backward computation.
I0905 22:20:17.546522   305 net.cpp:66] Creating Layer relu7
I0905 22:20:17.546530   305 net.cpp:329] relu7 <- fc7
I0905 22:20:17.546540   305 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:20:17.546550   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.546555   305 net.cpp:125] relu7 needs backward computation.
I0905 22:20:17.546562   305 net.cpp:66] Creating Layer drop7
I0905 22:20:17.546568   305 net.cpp:329] drop7 <- fc7
I0905 22:20:17.546574   305 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:20:17.546586   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.546591   305 net.cpp:125] drop7 needs backward computation.
I0905 22:20:17.546600   305 net.cpp:66] Creating Layer fc8
I0905 22:20:17.546605   305 net.cpp:329] fc8 <- fc7
I0905 22:20:17.546615   305 net.cpp:290] fc8 -> fc8
I0905 22:20:17.554394   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.554407   305 net.cpp:125] fc8 needs backward computation.
I0905 22:20:17.554414   305 net.cpp:66] Creating Layer relu8
I0905 22:20:17.554420   305 net.cpp:329] relu8 <- fc8
I0905 22:20:17.554429   305 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:20:17.554435   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.554441   305 net.cpp:125] relu8 needs backward computation.
I0905 22:20:17.554448   305 net.cpp:66] Creating Layer drop8
I0905 22:20:17.554453   305 net.cpp:329] drop8 <- fc8
I0905 22:20:17.554461   305 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:20:17.554467   305 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:20:17.554472   305 net.cpp:125] drop8 needs backward computation.
I0905 22:20:17.554481   305 net.cpp:66] Creating Layer fc9
I0905 22:20:17.554487   305 net.cpp:329] fc9 <- fc8
I0905 22:20:17.554494   305 net.cpp:290] fc9 -> fc9
I0905 22:20:17.554867   305 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:20:17.554879   305 net.cpp:125] fc9 needs backward computation.
I0905 22:20:17.554888   305 net.cpp:66] Creating Layer fc10
I0905 22:20:17.554894   305 net.cpp:329] fc10 <- fc9
I0905 22:20:17.554903   305 net.cpp:290] fc10 -> fc10
I0905 22:20:17.554914   305 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:20:17.554922   305 net.cpp:125] fc10 needs backward computation.
I0905 22:20:17.554929   305 net.cpp:66] Creating Layer prob
I0905 22:20:17.554935   305 net.cpp:329] prob <- fc10
I0905 22:20:17.554944   305 net.cpp:290] prob -> prob
I0905 22:20:17.554954   305 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:20:17.554960   305 net.cpp:125] prob needs backward computation.
I0905 22:20:17.554965   305 net.cpp:156] This network produces output prob
I0905 22:20:17.554976   305 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:20:17.554986   305 net.cpp:167] Network initialization done.
I0905 22:20:17.554991   305 net.cpp:168] Memory required for data: 6183480
Classifying 288 inputs.
Done in 179.51 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 88 is out of bounds for axis 0 with size 88
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:23:23.262086   314 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:23:23.262225   314 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:23:23.262234   314 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:23:23.262383   314 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:23:23.262446   314 net.cpp:292] Input 0 -> data
I0905 22:23:23.262473   314 net.cpp:66] Creating Layer conv1
I0905 22:23:23.262480   314 net.cpp:329] conv1 <- data
I0905 22:23:23.262488   314 net.cpp:290] conv1 -> conv1
I0905 22:23:23.263849   314 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:23:23.263867   314 net.cpp:125] conv1 needs backward computation.
I0905 22:23:23.263876   314 net.cpp:66] Creating Layer relu1
I0905 22:23:23.263882   314 net.cpp:329] relu1 <- conv1
I0905 22:23:23.263890   314 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:23:23.263898   314 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:23:23.263905   314 net.cpp:125] relu1 needs backward computation.
I0905 22:23:23.263911   314 net.cpp:66] Creating Layer pool1
I0905 22:23:23.263916   314 net.cpp:329] pool1 <- conv1
I0905 22:23:23.263923   314 net.cpp:290] pool1 -> pool1
I0905 22:23:23.263936   314 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:23:23.263942   314 net.cpp:125] pool1 needs backward computation.
I0905 22:23:23.263948   314 net.cpp:66] Creating Layer norm1
I0905 22:23:23.263958   314 net.cpp:329] norm1 <- pool1
I0905 22:23:23.263965   314 net.cpp:290] norm1 -> norm1
I0905 22:23:23.263975   314 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:23:23.263981   314 net.cpp:125] norm1 needs backward computation.
I0905 22:23:23.263989   314 net.cpp:66] Creating Layer conv2
I0905 22:23:23.263994   314 net.cpp:329] conv2 <- norm1
I0905 22:23:23.264003   314 net.cpp:290] conv2 -> conv2
I0905 22:23:23.273154   314 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:23:23.273169   314 net.cpp:125] conv2 needs backward computation.
I0905 22:23:23.273175   314 net.cpp:66] Creating Layer relu2
I0905 22:23:23.273180   314 net.cpp:329] relu2 <- conv2
I0905 22:23:23.273187   314 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:23:23.273195   314 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:23:23.273200   314 net.cpp:125] relu2 needs backward computation.
I0905 22:23:23.273206   314 net.cpp:66] Creating Layer pool2
I0905 22:23:23.273212   314 net.cpp:329] pool2 <- conv2
I0905 22:23:23.273218   314 net.cpp:290] pool2 -> pool2
I0905 22:23:23.273226   314 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:23:23.273232   314 net.cpp:125] pool2 needs backward computation.
I0905 22:23:23.273241   314 net.cpp:66] Creating Layer fc7
I0905 22:23:23.273247   314 net.cpp:329] fc7 <- pool2
I0905 22:23:23.273255   314 net.cpp:290] fc7 -> fc7
I0905 22:23:23.924012   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.924057   314 net.cpp:125] fc7 needs backward computation.
I0905 22:23:23.924068   314 net.cpp:66] Creating Layer relu7
I0905 22:23:23.924075   314 net.cpp:329] relu7 <- fc7
I0905 22:23:23.924085   314 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:23:23.924095   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.924101   314 net.cpp:125] relu7 needs backward computation.
I0905 22:23:23.924108   314 net.cpp:66] Creating Layer drop7
I0905 22:23:23.924114   314 net.cpp:329] drop7 <- fc7
I0905 22:23:23.924120   314 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:23:23.924139   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.924144   314 net.cpp:125] drop7 needs backward computation.
I0905 22:23:23.924154   314 net.cpp:66] Creating Layer fc8
I0905 22:23:23.924160   314 net.cpp:329] fc8 <- fc7
I0905 22:23:23.924168   314 net.cpp:290] fc8 -> fc8
I0905 22:23:23.931937   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.931951   314 net.cpp:125] fc8 needs backward computation.
I0905 22:23:23.931958   314 net.cpp:66] Creating Layer relu8
I0905 22:23:23.931964   314 net.cpp:329] relu8 <- fc8
I0905 22:23:23.931972   314 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:23:23.931979   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.931985   314 net.cpp:125] relu8 needs backward computation.
I0905 22:23:23.931993   314 net.cpp:66] Creating Layer drop8
I0905 22:23:23.931998   314 net.cpp:329] drop8 <- fc8
I0905 22:23:23.932004   314 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:23:23.932010   314 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:23.932016   314 net.cpp:125] drop8 needs backward computation.
I0905 22:23:23.932025   314 net.cpp:66] Creating Layer fc9
I0905 22:23:23.932030   314 net.cpp:329] fc9 <- fc8
I0905 22:23:23.932039   314 net.cpp:290] fc9 -> fc9
I0905 22:23:23.932412   314 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:23:23.932425   314 net.cpp:125] fc9 needs backward computation.
I0905 22:23:23.932432   314 net.cpp:66] Creating Layer fc10
I0905 22:23:23.932438   314 net.cpp:329] fc10 <- fc9
I0905 22:23:23.932447   314 net.cpp:290] fc10 -> fc10
I0905 22:23:23.932458   314 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:23:23.932466   314 net.cpp:125] fc10 needs backward computation.
I0905 22:23:23.932473   314 net.cpp:66] Creating Layer prob
I0905 22:23:23.932479   314 net.cpp:329] prob <- fc10
I0905 22:23:23.932487   314 net.cpp:290] prob -> prob
I0905 22:23:23.932497   314 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:23:23.932502   314 net.cpp:125] prob needs backward computation.
I0905 22:23:23.932518   314 net.cpp:156] This network produces output prob
I0905 22:23:23.932530   314 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:23:23.932539   314 net.cpp:167] Network initialization done.
I0905 22:23:23.932544   314 net.cpp:168] Memory required for data: 6183480
Classifying 50 inputs.
Done in 30.69 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:23:56.142472   320 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:23:56.142611   320 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:23:56.142621   320 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:23:56.142768   320 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:23:56.142832   320 net.cpp:292] Input 0 -> data
I0905 22:23:56.142858   320 net.cpp:66] Creating Layer conv1
I0905 22:23:56.142865   320 net.cpp:329] conv1 <- data
I0905 22:23:56.142874   320 net.cpp:290] conv1 -> conv1
I0905 22:23:56.144235   320 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:23:56.144253   320 net.cpp:125] conv1 needs backward computation.
I0905 22:23:56.144263   320 net.cpp:66] Creating Layer relu1
I0905 22:23:56.144268   320 net.cpp:329] relu1 <- conv1
I0905 22:23:56.144275   320 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:23:56.144284   320 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:23:56.144290   320 net.cpp:125] relu1 needs backward computation.
I0905 22:23:56.144297   320 net.cpp:66] Creating Layer pool1
I0905 22:23:56.144302   320 net.cpp:329] pool1 <- conv1
I0905 22:23:56.144309   320 net.cpp:290] pool1 -> pool1
I0905 22:23:56.144320   320 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:23:56.144326   320 net.cpp:125] pool1 needs backward computation.
I0905 22:23:56.144333   320 net.cpp:66] Creating Layer norm1
I0905 22:23:56.144340   320 net.cpp:329] norm1 <- pool1
I0905 22:23:56.144345   320 net.cpp:290] norm1 -> norm1
I0905 22:23:56.144356   320 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:23:56.144361   320 net.cpp:125] norm1 needs backward computation.
I0905 22:23:56.144368   320 net.cpp:66] Creating Layer conv2
I0905 22:23:56.144374   320 net.cpp:329] conv2 <- norm1
I0905 22:23:56.144381   320 net.cpp:290] conv2 -> conv2
I0905 22:23:56.153524   320 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:23:56.153539   320 net.cpp:125] conv2 needs backward computation.
I0905 22:23:56.153547   320 net.cpp:66] Creating Layer relu2
I0905 22:23:56.153553   320 net.cpp:329] relu2 <- conv2
I0905 22:23:56.153559   320 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:23:56.153566   320 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:23:56.153573   320 net.cpp:125] relu2 needs backward computation.
I0905 22:23:56.153587   320 net.cpp:66] Creating Layer pool2
I0905 22:23:56.153595   320 net.cpp:329] pool2 <- conv2
I0905 22:23:56.153609   320 net.cpp:290] pool2 -> pool2
I0905 22:23:56.153619   320 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:23:56.153625   320 net.cpp:125] pool2 needs backward computation.
I0905 22:23:56.153635   320 net.cpp:66] Creating Layer fc7
I0905 22:23:56.153640   320 net.cpp:329] fc7 <- pool2
I0905 22:23:56.153656   320 net.cpp:290] fc7 -> fc7
I0905 22:23:56.804280   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.804327   320 net.cpp:125] fc7 needs backward computation.
I0905 22:23:56.804339   320 net.cpp:66] Creating Layer relu7
I0905 22:23:56.804347   320 net.cpp:329] relu7 <- fc7
I0905 22:23:56.804358   320 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:23:56.804368   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.804373   320 net.cpp:125] relu7 needs backward computation.
I0905 22:23:56.804381   320 net.cpp:66] Creating Layer drop7
I0905 22:23:56.804388   320 net.cpp:329] drop7 <- fc7
I0905 22:23:56.804394   320 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:23:56.804404   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.804411   320 net.cpp:125] drop7 needs backward computation.
I0905 22:23:56.804419   320 net.cpp:66] Creating Layer fc8
I0905 22:23:56.804425   320 net.cpp:329] fc8 <- fc7
I0905 22:23:56.804435   320 net.cpp:290] fc8 -> fc8
I0905 22:23:56.812458   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.812469   320 net.cpp:125] fc8 needs backward computation.
I0905 22:23:56.812476   320 net.cpp:66] Creating Layer relu8
I0905 22:23:56.812482   320 net.cpp:329] relu8 <- fc8
I0905 22:23:56.812490   320 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:23:56.812499   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.812505   320 net.cpp:125] relu8 needs backward computation.
I0905 22:23:56.812511   320 net.cpp:66] Creating Layer drop8
I0905 22:23:56.812516   320 net.cpp:329] drop8 <- fc8
I0905 22:23:56.812523   320 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:23:56.812541   320 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:23:56.812547   320 net.cpp:125] drop8 needs backward computation.
I0905 22:23:56.812556   320 net.cpp:66] Creating Layer fc9
I0905 22:23:56.812562   320 net.cpp:329] fc9 <- fc8
I0905 22:23:56.812569   320 net.cpp:290] fc9 -> fc9
I0905 22:23:56.812954   320 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:23:56.812968   320 net.cpp:125] fc9 needs backward computation.
I0905 22:23:56.812975   320 net.cpp:66] Creating Layer fc10
I0905 22:23:56.812981   320 net.cpp:329] fc10 <- fc9
I0905 22:23:56.812990   320 net.cpp:290] fc10 -> fc10
I0905 22:23:56.813002   320 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:23:56.813010   320 net.cpp:125] fc10 needs backward computation.
I0905 22:23:56.813017   320 net.cpp:66] Creating Layer prob
I0905 22:23:56.813024   320 net.cpp:329] prob <- fc10
I0905 22:23:56.813031   320 net.cpp:290] prob -> prob
I0905 22:23:56.813041   320 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:23:56.813047   320 net.cpp:125] prob needs backward computation.
I0905 22:23:56.813052   320 net.cpp:156] This network produces output prob
I0905 22:23:56.813066   320 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:23:56.813074   320 net.cpp:167] Network initialization done.
I0905 22:23:56.813079   320 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 114 inputs.
Done in 72.00 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 14 is out of bounds for axis 0 with size 14
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:25:11.405927   325 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:25:11.406065   325 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:25:11.406075   325 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:25:11.406221   325 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:25:11.406286   325 net.cpp:292] Input 0 -> data
I0905 22:25:11.406312   325 net.cpp:66] Creating Layer conv1
I0905 22:25:11.406318   325 net.cpp:329] conv1 <- data
I0905 22:25:11.406327   325 net.cpp:290] conv1 -> conv1
I0905 22:25:11.407712   325 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:25:11.407730   325 net.cpp:125] conv1 needs backward computation.
I0905 22:25:11.407739   325 net.cpp:66] Creating Layer relu1
I0905 22:25:11.407745   325 net.cpp:329] relu1 <- conv1
I0905 22:25:11.407752   325 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:25:11.407762   325 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:25:11.407766   325 net.cpp:125] relu1 needs backward computation.
I0905 22:25:11.407773   325 net.cpp:66] Creating Layer pool1
I0905 22:25:11.407779   325 net.cpp:329] pool1 <- conv1
I0905 22:25:11.407785   325 net.cpp:290] pool1 -> pool1
I0905 22:25:11.407796   325 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:25:11.407802   325 net.cpp:125] pool1 needs backward computation.
I0905 22:25:11.407809   325 net.cpp:66] Creating Layer norm1
I0905 22:25:11.407815   325 net.cpp:329] norm1 <- pool1
I0905 22:25:11.407821   325 net.cpp:290] norm1 -> norm1
I0905 22:25:11.407831   325 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:25:11.407837   325 net.cpp:125] norm1 needs backward computation.
I0905 22:25:11.407845   325 net.cpp:66] Creating Layer conv2
I0905 22:25:11.407850   325 net.cpp:329] conv2 <- norm1
I0905 22:25:11.407857   325 net.cpp:290] conv2 -> conv2
I0905 22:25:11.416980   325 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:25:11.416995   325 net.cpp:125] conv2 needs backward computation.
I0905 22:25:11.417001   325 net.cpp:66] Creating Layer relu2
I0905 22:25:11.417007   325 net.cpp:329] relu2 <- conv2
I0905 22:25:11.417014   325 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:25:11.417021   325 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:25:11.417027   325 net.cpp:125] relu2 needs backward computation.
I0905 22:25:11.417033   325 net.cpp:66] Creating Layer pool2
I0905 22:25:11.417038   325 net.cpp:329] pool2 <- conv2
I0905 22:25:11.417045   325 net.cpp:290] pool2 -> pool2
I0905 22:25:11.417053   325 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:25:11.417059   325 net.cpp:125] pool2 needs backward computation.
I0905 22:25:11.417068   325 net.cpp:66] Creating Layer fc7
I0905 22:25:11.417074   325 net.cpp:329] fc7 <- pool2
I0905 22:25:11.417081   325 net.cpp:290] fc7 -> fc7
I0905 22:25:12.065107   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.065151   325 net.cpp:125] fc7 needs backward computation.
I0905 22:25:12.065176   325 net.cpp:66] Creating Layer relu7
I0905 22:25:12.065184   325 net.cpp:329] relu7 <- fc7
I0905 22:25:12.065193   325 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:25:12.065204   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.065210   325 net.cpp:125] relu7 needs backward computation.
I0905 22:25:12.065217   325 net.cpp:66] Creating Layer drop7
I0905 22:25:12.065223   325 net.cpp:329] drop7 <- fc7
I0905 22:25:12.065229   325 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:25:12.065240   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.065246   325 net.cpp:125] drop7 needs backward computation.
I0905 22:25:12.065254   325 net.cpp:66] Creating Layer fc8
I0905 22:25:12.065260   325 net.cpp:329] fc8 <- fc7
I0905 22:25:12.065269   325 net.cpp:290] fc8 -> fc8
I0905 22:25:12.073055   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.073067   325 net.cpp:125] fc8 needs backward computation.
I0905 22:25:12.073076   325 net.cpp:66] Creating Layer relu8
I0905 22:25:12.073081   325 net.cpp:329] relu8 <- fc8
I0905 22:25:12.073088   325 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:25:12.073096   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.073102   325 net.cpp:125] relu8 needs backward computation.
I0905 22:25:12.073109   325 net.cpp:66] Creating Layer drop8
I0905 22:25:12.073114   325 net.cpp:329] drop8 <- fc8
I0905 22:25:12.073122   325 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:25:12.073128   325 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:12.073133   325 net.cpp:125] drop8 needs backward computation.
I0905 22:25:12.073143   325 net.cpp:66] Creating Layer fc9
I0905 22:25:12.073148   325 net.cpp:329] fc9 <- fc8
I0905 22:25:12.073155   325 net.cpp:290] fc9 -> fc9
I0905 22:25:12.073529   325 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:25:12.073541   325 net.cpp:125] fc9 needs backward computation.
I0905 22:25:12.073549   325 net.cpp:66] Creating Layer fc10
I0905 22:25:12.073555   325 net.cpp:329] fc10 <- fc9
I0905 22:25:12.073565   325 net.cpp:290] fc10 -> fc10
I0905 22:25:12.073575   325 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:25:12.073587   325 net.cpp:125] fc10 needs backward computation.
I0905 22:25:12.073595   325 net.cpp:66] Creating Layer prob
I0905 22:25:12.073601   325 net.cpp:329] prob <- fc10
I0905 22:25:12.073608   325 net.cpp:290] prob -> prob
I0905 22:25:12.073618   325 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:25:12.073624   325 net.cpp:125] prob needs backward computation.
I0905 22:25:12.073629   325 net.cpp:156] This network produces output prob
I0905 22:25:12.073642   325 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:25:12.073650   325 net.cpp:167] Network initialization done.
I0905 22:25:12.073655   325 net.cpp:168] Memory required for data: 6183480
Classifying 11 inputs.
Done in 6.96 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:25:19.989651   329 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:25:19.989789   329 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:25:19.989797   329 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:25:19.989944   329 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:25:19.990007   329 net.cpp:292] Input 0 -> data
I0905 22:25:19.990033   329 net.cpp:66] Creating Layer conv1
I0905 22:25:19.990041   329 net.cpp:329] conv1 <- data
I0905 22:25:19.990048   329 net.cpp:290] conv1 -> conv1
I0905 22:25:19.991418   329 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:25:19.991436   329 net.cpp:125] conv1 needs backward computation.
I0905 22:25:19.991446   329 net.cpp:66] Creating Layer relu1
I0905 22:25:19.991451   329 net.cpp:329] relu1 <- conv1
I0905 22:25:19.991458   329 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:25:19.991467   329 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:25:19.991472   329 net.cpp:125] relu1 needs backward computation.
I0905 22:25:19.991479   329 net.cpp:66] Creating Layer pool1
I0905 22:25:19.991485   329 net.cpp:329] pool1 <- conv1
I0905 22:25:19.991492   329 net.cpp:290] pool1 -> pool1
I0905 22:25:19.991503   329 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:25:19.991508   329 net.cpp:125] pool1 needs backward computation.
I0905 22:25:19.991515   329 net.cpp:66] Creating Layer norm1
I0905 22:25:19.991520   329 net.cpp:329] norm1 <- pool1
I0905 22:25:19.991528   329 net.cpp:290] norm1 -> norm1
I0905 22:25:19.991538   329 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:25:19.991543   329 net.cpp:125] norm1 needs backward computation.
I0905 22:25:19.991550   329 net.cpp:66] Creating Layer conv2
I0905 22:25:19.991555   329 net.cpp:329] conv2 <- norm1
I0905 22:25:19.991562   329 net.cpp:290] conv2 -> conv2
I0905 22:25:20.001039   329 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:25:20.001062   329 net.cpp:125] conv2 needs backward computation.
I0905 22:25:20.001070   329 net.cpp:66] Creating Layer relu2
I0905 22:25:20.001076   329 net.cpp:329] relu2 <- conv2
I0905 22:25:20.001083   329 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:25:20.001091   329 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:25:20.001097   329 net.cpp:125] relu2 needs backward computation.
I0905 22:25:20.001106   329 net.cpp:66] Creating Layer pool2
I0905 22:25:20.001111   329 net.cpp:329] pool2 <- conv2
I0905 22:25:20.001118   329 net.cpp:290] pool2 -> pool2
I0905 22:25:20.001127   329 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:25:20.001132   329 net.cpp:125] pool2 needs backward computation.
I0905 22:25:20.001140   329 net.cpp:66] Creating Layer fc7
I0905 22:25:20.001145   329 net.cpp:329] fc7 <- pool2
I0905 22:25:20.001153   329 net.cpp:290] fc7 -> fc7
I0905 22:25:20.651237   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.651283   329 net.cpp:125] fc7 needs backward computation.
I0905 22:25:20.651295   329 net.cpp:66] Creating Layer relu7
I0905 22:25:20.651304   329 net.cpp:329] relu7 <- fc7
I0905 22:25:20.651312   329 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:25:20.651322   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.651329   329 net.cpp:125] relu7 needs backward computation.
I0905 22:25:20.651336   329 net.cpp:66] Creating Layer drop7
I0905 22:25:20.651342   329 net.cpp:329] drop7 <- fc7
I0905 22:25:20.651348   329 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:25:20.651360   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.651366   329 net.cpp:125] drop7 needs backward computation.
I0905 22:25:20.651375   329 net.cpp:66] Creating Layer fc8
I0905 22:25:20.651381   329 net.cpp:329] fc8 <- fc7
I0905 22:25:20.651389   329 net.cpp:290] fc8 -> fc8
I0905 22:25:20.659400   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.659412   329 net.cpp:125] fc8 needs backward computation.
I0905 22:25:20.659420   329 net.cpp:66] Creating Layer relu8
I0905 22:25:20.659425   329 net.cpp:329] relu8 <- fc8
I0905 22:25:20.659433   329 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:25:20.659441   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.659446   329 net.cpp:125] relu8 needs backward computation.
I0905 22:25:20.659453   329 net.cpp:66] Creating Layer drop8
I0905 22:25:20.659458   329 net.cpp:329] drop8 <- fc8
I0905 22:25:20.659464   329 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:25:20.659471   329 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:25:20.659477   329 net.cpp:125] drop8 needs backward computation.
I0905 22:25:20.659487   329 net.cpp:66] Creating Layer fc9
I0905 22:25:20.659492   329 net.cpp:329] fc9 <- fc8
I0905 22:25:20.659498   329 net.cpp:290] fc9 -> fc9
I0905 22:25:20.659873   329 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:25:20.659885   329 net.cpp:125] fc9 needs backward computation.
I0905 22:25:20.659893   329 net.cpp:66] Creating Layer fc10
I0905 22:25:20.659899   329 net.cpp:329] fc10 <- fc9
I0905 22:25:20.659907   329 net.cpp:290] fc10 -> fc10
I0905 22:25:20.659919   329 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:25:20.659927   329 net.cpp:125] fc10 needs backward computation.
I0905 22:25:20.659934   329 net.cpp:66] Creating Layer prob
I0905 22:25:20.659940   329 net.cpp:329] prob <- fc10
I0905 22:25:20.659947   329 net.cpp:290] prob -> prob
I0905 22:25:20.659957   329 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:25:20.659962   329 net.cpp:125] prob needs backward computation.
I0905 22:25:20.659967   329 net.cpp:156] This network produces output prob
I0905 22:25:20.659981   329 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:25:20.659988   329 net.cpp:167] Network initialization done.
I0905 22:25:20.659993   329 net.cpp:168] Memory required for data: 6183480
Classifying 75 inputs.
Done in 47.97 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:26:15.074723   334 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:26:15.074872   334 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:26:15.074882   334 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:26:15.075026   334 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:26:15.075078   334 net.cpp:292] Input 0 -> data
I0905 22:26:15.075104   334 net.cpp:66] Creating Layer conv1
I0905 22:26:15.075111   334 net.cpp:329] conv1 <- data
I0905 22:26:15.075120   334 net.cpp:290] conv1 -> conv1
I0905 22:26:15.076478   334 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:26:15.076496   334 net.cpp:125] conv1 needs backward computation.
I0905 22:26:15.076506   334 net.cpp:66] Creating Layer relu1
I0905 22:26:15.076513   334 net.cpp:329] relu1 <- conv1
I0905 22:26:15.076519   334 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:26:15.076532   334 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:26:15.076539   334 net.cpp:125] relu1 needs backward computation.
I0905 22:26:15.076546   334 net.cpp:66] Creating Layer pool1
I0905 22:26:15.076552   334 net.cpp:329] pool1 <- conv1
I0905 22:26:15.076560   334 net.cpp:290] pool1 -> pool1
I0905 22:26:15.076570   334 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:26:15.076576   334 net.cpp:125] pool1 needs backward computation.
I0905 22:26:15.076583   334 net.cpp:66] Creating Layer norm1
I0905 22:26:15.076589   334 net.cpp:329] norm1 <- pool1
I0905 22:26:15.076596   334 net.cpp:290] norm1 -> norm1
I0905 22:26:15.076606   334 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:26:15.076611   334 net.cpp:125] norm1 needs backward computation.
I0905 22:26:15.076619   334 net.cpp:66] Creating Layer conv2
I0905 22:26:15.076625   334 net.cpp:329] conv2 <- norm1
I0905 22:26:15.076632   334 net.cpp:290] conv2 -> conv2
I0905 22:26:15.085913   334 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:26:15.085949   334 net.cpp:125] conv2 needs backward computation.
I0905 22:26:15.085958   334 net.cpp:66] Creating Layer relu2
I0905 22:26:15.085964   334 net.cpp:329] relu2 <- conv2
I0905 22:26:15.085971   334 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:26:15.085978   334 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:26:15.085984   334 net.cpp:125] relu2 needs backward computation.
I0905 22:26:15.085991   334 net.cpp:66] Creating Layer pool2
I0905 22:26:15.085997   334 net.cpp:329] pool2 <- conv2
I0905 22:26:15.086004   334 net.cpp:290] pool2 -> pool2
I0905 22:26:15.086012   334 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:26:15.086019   334 net.cpp:125] pool2 needs backward computation.
I0905 22:26:15.086029   334 net.cpp:66] Creating Layer fc7
I0905 22:26:15.086035   334 net.cpp:329] fc7 <- pool2
I0905 22:26:15.086042   334 net.cpp:290] fc7 -> fc7
I0905 22:26:15.735888   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.735934   334 net.cpp:125] fc7 needs backward computation.
I0905 22:26:15.735946   334 net.cpp:66] Creating Layer relu7
I0905 22:26:15.735954   334 net.cpp:329] relu7 <- fc7
I0905 22:26:15.735963   334 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:26:15.735975   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.735980   334 net.cpp:125] relu7 needs backward computation.
I0905 22:26:15.735987   334 net.cpp:66] Creating Layer drop7
I0905 22:26:15.735993   334 net.cpp:329] drop7 <- fc7
I0905 22:26:15.736001   334 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:26:15.736011   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.736017   334 net.cpp:125] drop7 needs backward computation.
I0905 22:26:15.736026   334 net.cpp:66] Creating Layer fc8
I0905 22:26:15.736032   334 net.cpp:329] fc8 <- fc7
I0905 22:26:15.736040   334 net.cpp:290] fc8 -> fc8
I0905 22:26:15.743826   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.743839   334 net.cpp:125] fc8 needs backward computation.
I0905 22:26:15.743845   334 net.cpp:66] Creating Layer relu8
I0905 22:26:15.743851   334 net.cpp:329] relu8 <- fc8
I0905 22:26:15.743860   334 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:26:15.743867   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.743873   334 net.cpp:125] relu8 needs backward computation.
I0905 22:26:15.743880   334 net.cpp:66] Creating Layer drop8
I0905 22:26:15.743885   334 net.cpp:329] drop8 <- fc8
I0905 22:26:15.743892   334 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:26:15.743899   334 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:15.743906   334 net.cpp:125] drop8 needs backward computation.
I0905 22:26:15.743914   334 net.cpp:66] Creating Layer fc9
I0905 22:26:15.743921   334 net.cpp:329] fc9 <- fc8
I0905 22:26:15.743927   334 net.cpp:290] fc9 -> fc9
I0905 22:26:15.744302   334 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:26:15.744313   334 net.cpp:125] fc9 needs backward computation.
I0905 22:26:15.744321   334 net.cpp:66] Creating Layer fc10
I0905 22:26:15.744328   334 net.cpp:329] fc10 <- fc9
I0905 22:26:15.744348   334 net.cpp:290] fc10 -> fc10
I0905 22:26:15.744360   334 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:26:15.744369   334 net.cpp:125] fc10 needs backward computation.
I0905 22:26:15.744376   334 net.cpp:66] Creating Layer prob
I0905 22:26:15.744381   334 net.cpp:329] prob <- fc10
I0905 22:26:15.744390   334 net.cpp:290] prob -> prob
I0905 22:26:15.744400   334 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:26:15.744405   334 net.cpp:125] prob needs backward computation.
I0905 22:26:15.744410   334 net.cpp:156] This network produces output prob
I0905 22:26:15.744423   334 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:26:15.744432   334 net.cpp:167] Network initialization done.
I0905 22:26:15.744437   334 net.cpp:168] Memory required for data: 6183480
Classifying 12 inputs.
Done in 7.54 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:26:24.426156   338 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:26:24.426293   338 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:26:24.426302   338 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:26:24.426451   338 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:26:24.426515   338 net.cpp:292] Input 0 -> data
I0905 22:26:24.426542   338 net.cpp:66] Creating Layer conv1
I0905 22:26:24.426548   338 net.cpp:329] conv1 <- data
I0905 22:26:24.426556   338 net.cpp:290] conv1 -> conv1
I0905 22:26:24.427917   338 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:26:24.427934   338 net.cpp:125] conv1 needs backward computation.
I0905 22:26:24.427944   338 net.cpp:66] Creating Layer relu1
I0905 22:26:24.427950   338 net.cpp:329] relu1 <- conv1
I0905 22:26:24.427958   338 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:26:24.427966   338 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:26:24.427973   338 net.cpp:125] relu1 needs backward computation.
I0905 22:26:24.427979   338 net.cpp:66] Creating Layer pool1
I0905 22:26:24.427985   338 net.cpp:329] pool1 <- conv1
I0905 22:26:24.427992   338 net.cpp:290] pool1 -> pool1
I0905 22:26:24.428004   338 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:26:24.428009   338 net.cpp:125] pool1 needs backward computation.
I0905 22:26:24.428016   338 net.cpp:66] Creating Layer norm1
I0905 22:26:24.428022   338 net.cpp:329] norm1 <- pool1
I0905 22:26:24.428028   338 net.cpp:290] norm1 -> norm1
I0905 22:26:24.428038   338 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:26:24.428045   338 net.cpp:125] norm1 needs backward computation.
I0905 22:26:24.428052   338 net.cpp:66] Creating Layer conv2
I0905 22:26:24.428057   338 net.cpp:329] conv2 <- norm1
I0905 22:26:24.428066   338 net.cpp:290] conv2 -> conv2
I0905 22:26:24.437188   338 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:26:24.437204   338 net.cpp:125] conv2 needs backward computation.
I0905 22:26:24.437212   338 net.cpp:66] Creating Layer relu2
I0905 22:26:24.437218   338 net.cpp:329] relu2 <- conv2
I0905 22:26:24.437225   338 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:26:24.437232   338 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:26:24.437238   338 net.cpp:125] relu2 needs backward computation.
I0905 22:26:24.437247   338 net.cpp:66] Creating Layer pool2
I0905 22:26:24.437253   338 net.cpp:329] pool2 <- conv2
I0905 22:26:24.437260   338 net.cpp:290] pool2 -> pool2
I0905 22:26:24.437268   338 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:26:24.437274   338 net.cpp:125] pool2 needs backward computation.
I0905 22:26:24.437281   338 net.cpp:66] Creating Layer fc7
I0905 22:26:24.437288   338 net.cpp:329] fc7 <- pool2
I0905 22:26:24.437294   338 net.cpp:290] fc7 -> fc7
I0905 22:26:25.085106   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.085151   338 net.cpp:125] fc7 needs backward computation.
I0905 22:26:25.085165   338 net.cpp:66] Creating Layer relu7
I0905 22:26:25.085171   338 net.cpp:329] relu7 <- fc7
I0905 22:26:25.085181   338 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:26:25.085191   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.085197   338 net.cpp:125] relu7 needs backward computation.
I0905 22:26:25.085206   338 net.cpp:66] Creating Layer drop7
I0905 22:26:25.085211   338 net.cpp:329] drop7 <- fc7
I0905 22:26:25.085217   338 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:26:25.085228   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.085234   338 net.cpp:125] drop7 needs backward computation.
I0905 22:26:25.085243   338 net.cpp:66] Creating Layer fc8
I0905 22:26:25.085248   338 net.cpp:329] fc8 <- fc7
I0905 22:26:25.085258   338 net.cpp:290] fc8 -> fc8
I0905 22:26:25.093066   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.093080   338 net.cpp:125] fc8 needs backward computation.
I0905 22:26:25.093097   338 net.cpp:66] Creating Layer relu8
I0905 22:26:25.093103   338 net.cpp:329] relu8 <- fc8
I0905 22:26:25.093112   338 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:26:25.093119   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.093125   338 net.cpp:125] relu8 needs backward computation.
I0905 22:26:25.093132   338 net.cpp:66] Creating Layer drop8
I0905 22:26:25.093137   338 net.cpp:329] drop8 <- fc8
I0905 22:26:25.093144   338 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:26:25.093152   338 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:26:25.093157   338 net.cpp:125] drop8 needs backward computation.
I0905 22:26:25.093165   338 net.cpp:66] Creating Layer fc9
I0905 22:26:25.093171   338 net.cpp:329] fc9 <- fc8
I0905 22:26:25.093178   338 net.cpp:290] fc9 -> fc9
I0905 22:26:25.093554   338 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:26:25.093565   338 net.cpp:125] fc9 needs backward computation.
I0905 22:26:25.093575   338 net.cpp:66] Creating Layer fc10
I0905 22:26:25.093583   338 net.cpp:329] fc10 <- fc9
I0905 22:26:25.093593   338 net.cpp:290] fc10 -> fc10
I0905 22:26:25.093605   338 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:26:25.093615   338 net.cpp:125] fc10 needs backward computation.
I0905 22:26:25.093621   338 net.cpp:66] Creating Layer prob
I0905 22:26:25.093626   338 net.cpp:329] prob <- fc10
I0905 22:26:25.093634   338 net.cpp:290] prob -> prob
I0905 22:26:25.093644   338 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:26:25.093650   338 net.cpp:125] prob needs backward computation.
I0905 22:26:25.093655   338 net.cpp:156] This network produces output prob
I0905 22:26:25.093668   338 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:26:25.093677   338 net.cpp:167] Network initialization done.
I0905 22:26:25.093683   338 net.cpp:168] Memory required for data: 6183480
Classifying 242 inputs.
Done in 153.58 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 42 is out of bounds for axis 0 with size 42
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:29:03.986361   346 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:29:03.986502   346 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:29:03.986512   346 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:29:03.986665   346 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:29:03.986729   346 net.cpp:292] Input 0 -> data
I0905 22:29:03.986755   346 net.cpp:66] Creating Layer conv1
I0905 22:29:03.986763   346 net.cpp:329] conv1 <- data
I0905 22:29:03.986770   346 net.cpp:290] conv1 -> conv1
I0905 22:29:03.988172   346 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:29:03.988189   346 net.cpp:125] conv1 needs backward computation.
I0905 22:29:03.988199   346 net.cpp:66] Creating Layer relu1
I0905 22:29:03.988205   346 net.cpp:329] relu1 <- conv1
I0905 22:29:03.988212   346 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:29:03.988221   346 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:29:03.988227   346 net.cpp:125] relu1 needs backward computation.
I0905 22:29:03.988234   346 net.cpp:66] Creating Layer pool1
I0905 22:29:03.988240   346 net.cpp:329] pool1 <- conv1
I0905 22:29:03.988246   346 net.cpp:290] pool1 -> pool1
I0905 22:29:03.988257   346 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:29:03.988263   346 net.cpp:125] pool1 needs backward computation.
I0905 22:29:03.988270   346 net.cpp:66] Creating Layer norm1
I0905 22:29:03.988276   346 net.cpp:329] norm1 <- pool1
I0905 22:29:03.988282   346 net.cpp:290] norm1 -> norm1
I0905 22:29:03.988292   346 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:29:03.988298   346 net.cpp:125] norm1 needs backward computation.
I0905 22:29:03.988306   346 net.cpp:66] Creating Layer conv2
I0905 22:29:03.988312   346 net.cpp:329] conv2 <- norm1
I0905 22:29:03.988319   346 net.cpp:290] conv2 -> conv2
I0905 22:29:03.997704   346 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:29:03.997720   346 net.cpp:125] conv2 needs backward computation.
I0905 22:29:03.997727   346 net.cpp:66] Creating Layer relu2
I0905 22:29:03.997733   346 net.cpp:329] relu2 <- conv2
I0905 22:29:03.997740   346 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:29:03.997747   346 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:29:03.997753   346 net.cpp:125] relu2 needs backward computation.
I0905 22:29:03.997761   346 net.cpp:66] Creating Layer pool2
I0905 22:29:03.997766   346 net.cpp:329] pool2 <- conv2
I0905 22:29:03.997772   346 net.cpp:290] pool2 -> pool2
I0905 22:29:03.997781   346 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:29:03.997792   346 net.cpp:125] pool2 needs backward computation.
I0905 22:29:03.997802   346 net.cpp:66] Creating Layer fc7
I0905 22:29:03.997807   346 net.cpp:329] fc7 <- pool2
I0905 22:29:03.997815   346 net.cpp:290] fc7 -> fc7
I0905 22:29:04.644575   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.644620   346 net.cpp:125] fc7 needs backward computation.
I0905 22:29:04.644634   346 net.cpp:66] Creating Layer relu7
I0905 22:29:04.644641   346 net.cpp:329] relu7 <- fc7
I0905 22:29:04.644650   346 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:29:04.644660   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.644666   346 net.cpp:125] relu7 needs backward computation.
I0905 22:29:04.644673   346 net.cpp:66] Creating Layer drop7
I0905 22:29:04.644680   346 net.cpp:329] drop7 <- fc7
I0905 22:29:04.644685   346 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:29:04.644696   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.644702   346 net.cpp:125] drop7 needs backward computation.
I0905 22:29:04.644711   346 net.cpp:66] Creating Layer fc8
I0905 22:29:04.644716   346 net.cpp:329] fc8 <- fc7
I0905 22:29:04.644726   346 net.cpp:290] fc8 -> fc8
I0905 22:29:04.652497   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.652508   346 net.cpp:125] fc8 needs backward computation.
I0905 22:29:04.652515   346 net.cpp:66] Creating Layer relu8
I0905 22:29:04.652521   346 net.cpp:329] relu8 <- fc8
I0905 22:29:04.652529   346 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:29:04.652536   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.652542   346 net.cpp:125] relu8 needs backward computation.
I0905 22:29:04.652549   346 net.cpp:66] Creating Layer drop8
I0905 22:29:04.652554   346 net.cpp:329] drop8 <- fc8
I0905 22:29:04.652560   346 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:29:04.652567   346 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:04.652572   346 net.cpp:125] drop8 needs backward computation.
I0905 22:29:04.652581   346 net.cpp:66] Creating Layer fc9
I0905 22:29:04.652587   346 net.cpp:329] fc9 <- fc8
I0905 22:29:04.652595   346 net.cpp:290] fc9 -> fc9
I0905 22:29:04.652968   346 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:29:04.652981   346 net.cpp:125] fc9 needs backward computation.
I0905 22:29:04.652988   346 net.cpp:66] Creating Layer fc10
I0905 22:29:04.652994   346 net.cpp:329] fc10 <- fc9
I0905 22:29:04.653002   346 net.cpp:290] fc10 -> fc10
I0905 22:29:04.653014   346 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:29:04.653023   346 net.cpp:125] fc10 needs backward computation.
I0905 22:29:04.653028   346 net.cpp:66] Creating Layer prob
I0905 22:29:04.653034   346 net.cpp:329] prob <- fc10
I0905 22:29:04.653043   346 net.cpp:290] prob -> prob
I0905 22:29:04.653051   346 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:29:04.653058   346 net.cpp:125] prob needs backward computation.
I0905 22:29:04.653062   346 net.cpp:156] This network produces output prob
I0905 22:29:04.653075   346 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:29:04.653084   346 net.cpp:167] Network initialization done.
I0905 22:29:04.653089   346 net.cpp:168] Memory required for data: 6183480
Classifying 68 inputs.
Done in 43.72 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:29:52.787370   352 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:29:52.787509   352 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:29:52.787518   352 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:29:52.787664   352 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:29:52.787727   352 net.cpp:292] Input 0 -> data
I0905 22:29:52.787753   352 net.cpp:66] Creating Layer conv1
I0905 22:29:52.787760   352 net.cpp:329] conv1 <- data
I0905 22:29:52.787768   352 net.cpp:290] conv1 -> conv1
I0905 22:29:52.789132   352 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:29:52.789150   352 net.cpp:125] conv1 needs backward computation.
I0905 22:29:52.789160   352 net.cpp:66] Creating Layer relu1
I0905 22:29:52.789165   352 net.cpp:329] relu1 <- conv1
I0905 22:29:52.789172   352 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:29:52.789180   352 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:29:52.789186   352 net.cpp:125] relu1 needs backward computation.
I0905 22:29:52.789193   352 net.cpp:66] Creating Layer pool1
I0905 22:29:52.789198   352 net.cpp:329] pool1 <- conv1
I0905 22:29:52.789206   352 net.cpp:290] pool1 -> pool1
I0905 22:29:52.789216   352 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:29:52.789222   352 net.cpp:125] pool1 needs backward computation.
I0905 22:29:52.789228   352 net.cpp:66] Creating Layer norm1
I0905 22:29:52.789234   352 net.cpp:329] norm1 <- pool1
I0905 22:29:52.789240   352 net.cpp:290] norm1 -> norm1
I0905 22:29:52.789255   352 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:29:52.789261   352 net.cpp:125] norm1 needs backward computation.
I0905 22:29:52.789269   352 net.cpp:66] Creating Layer conv2
I0905 22:29:52.789274   352 net.cpp:329] conv2 <- norm1
I0905 22:29:52.789281   352 net.cpp:290] conv2 -> conv2
I0905 22:29:52.798507   352 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:29:52.798524   352 net.cpp:125] conv2 needs backward computation.
I0905 22:29:52.798532   352 net.cpp:66] Creating Layer relu2
I0905 22:29:52.798538   352 net.cpp:329] relu2 <- conv2
I0905 22:29:52.798545   352 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:29:52.798552   352 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:29:52.798557   352 net.cpp:125] relu2 needs backward computation.
I0905 22:29:52.798564   352 net.cpp:66] Creating Layer pool2
I0905 22:29:52.798569   352 net.cpp:329] pool2 <- conv2
I0905 22:29:52.798576   352 net.cpp:290] pool2 -> pool2
I0905 22:29:52.798584   352 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:29:52.798589   352 net.cpp:125] pool2 needs backward computation.
I0905 22:29:52.798599   352 net.cpp:66] Creating Layer fc7
I0905 22:29:52.798604   352 net.cpp:329] fc7 <- pool2
I0905 22:29:52.798612   352 net.cpp:290] fc7 -> fc7
I0905 22:29:53.447590   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.447635   352 net.cpp:125] fc7 needs backward computation.
I0905 22:29:53.447648   352 net.cpp:66] Creating Layer relu7
I0905 22:29:53.447654   352 net.cpp:329] relu7 <- fc7
I0905 22:29:53.447664   352 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:29:53.447674   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.447680   352 net.cpp:125] relu7 needs backward computation.
I0905 22:29:53.447687   352 net.cpp:66] Creating Layer drop7
I0905 22:29:53.447692   352 net.cpp:329] drop7 <- fc7
I0905 22:29:53.447700   352 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:29:53.447710   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.447715   352 net.cpp:125] drop7 needs backward computation.
I0905 22:29:53.447724   352 net.cpp:66] Creating Layer fc8
I0905 22:29:53.447729   352 net.cpp:329] fc8 <- fc7
I0905 22:29:53.447738   352 net.cpp:290] fc8 -> fc8
I0905 22:29:53.455528   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.455539   352 net.cpp:125] fc8 needs backward computation.
I0905 22:29:53.455546   352 net.cpp:66] Creating Layer relu8
I0905 22:29:53.455553   352 net.cpp:329] relu8 <- fc8
I0905 22:29:53.455559   352 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:29:53.455567   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.455572   352 net.cpp:125] relu8 needs backward computation.
I0905 22:29:53.455579   352 net.cpp:66] Creating Layer drop8
I0905 22:29:53.455585   352 net.cpp:329] drop8 <- fc8
I0905 22:29:53.455590   352 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:29:53.455597   352 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:29:53.455602   352 net.cpp:125] drop8 needs backward computation.
I0905 22:29:53.455611   352 net.cpp:66] Creating Layer fc9
I0905 22:29:53.455617   352 net.cpp:329] fc9 <- fc8
I0905 22:29:53.455624   352 net.cpp:290] fc9 -> fc9
I0905 22:29:53.455997   352 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:29:53.456010   352 net.cpp:125] fc9 needs backward computation.
I0905 22:29:53.456018   352 net.cpp:66] Creating Layer fc10
I0905 22:29:53.456023   352 net.cpp:329] fc10 <- fc9
I0905 22:29:53.456032   352 net.cpp:290] fc10 -> fc10
I0905 22:29:53.456043   352 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:29:53.456051   352 net.cpp:125] fc10 needs backward computation.
I0905 22:29:53.456058   352 net.cpp:66] Creating Layer prob
I0905 22:29:53.456063   352 net.cpp:329] prob <- fc10
I0905 22:29:53.456071   352 net.cpp:290] prob -> prob
I0905 22:29:53.456080   352 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:29:53.456086   352 net.cpp:125] prob needs backward computation.
I0905 22:29:53.456091   352 net.cpp:156] This network produces output prob
I0905 22:29:53.456104   352 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:29:53.456122   352 net.cpp:167] Network initialization done.
I0905 22:29:53.456128   352 net.cpp:168] Memory required for data: 6183480
Classifying 121 inputs.
Done in 75.93 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 21 is out of bounds for axis 0 with size 21
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:31:11.816117   359 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:31:11.816256   359 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:31:11.816265   359 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:31:11.816412   359 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:31:11.816476   359 net.cpp:292] Input 0 -> data
I0905 22:31:11.816503   359 net.cpp:66] Creating Layer conv1
I0905 22:31:11.816509   359 net.cpp:329] conv1 <- data
I0905 22:31:11.816517   359 net.cpp:290] conv1 -> conv1
I0905 22:31:11.817917   359 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:31:11.817936   359 net.cpp:125] conv1 needs backward computation.
I0905 22:31:11.817945   359 net.cpp:66] Creating Layer relu1
I0905 22:31:11.817951   359 net.cpp:329] relu1 <- conv1
I0905 22:31:11.817957   359 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:31:11.817966   359 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:31:11.817972   359 net.cpp:125] relu1 needs backward computation.
I0905 22:31:11.817978   359 net.cpp:66] Creating Layer pool1
I0905 22:31:11.817984   359 net.cpp:329] pool1 <- conv1
I0905 22:31:11.817991   359 net.cpp:290] pool1 -> pool1
I0905 22:31:11.818001   359 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:31:11.818007   359 net.cpp:125] pool1 needs backward computation.
I0905 22:31:11.818014   359 net.cpp:66] Creating Layer norm1
I0905 22:31:11.818019   359 net.cpp:329] norm1 <- pool1
I0905 22:31:11.818027   359 net.cpp:290] norm1 -> norm1
I0905 22:31:11.818035   359 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:31:11.818042   359 net.cpp:125] norm1 needs backward computation.
I0905 22:31:11.818048   359 net.cpp:66] Creating Layer conv2
I0905 22:31:11.818054   359 net.cpp:329] conv2 <- norm1
I0905 22:31:11.818061   359 net.cpp:290] conv2 -> conv2
I0905 22:31:11.827281   359 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:31:11.827296   359 net.cpp:125] conv2 needs backward computation.
I0905 22:31:11.827302   359 net.cpp:66] Creating Layer relu2
I0905 22:31:11.827308   359 net.cpp:329] relu2 <- conv2
I0905 22:31:11.827316   359 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:31:11.827322   359 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:31:11.827327   359 net.cpp:125] relu2 needs backward computation.
I0905 22:31:11.827337   359 net.cpp:66] Creating Layer pool2
I0905 22:31:11.827342   359 net.cpp:329] pool2 <- conv2
I0905 22:31:11.827348   359 net.cpp:290] pool2 -> pool2
I0905 22:31:11.827357   359 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:31:11.827363   359 net.cpp:125] pool2 needs backward computation.
I0905 22:31:11.827369   359 net.cpp:66] Creating Layer fc7
I0905 22:31:11.827374   359 net.cpp:329] fc7 <- pool2
I0905 22:31:11.827381   359 net.cpp:290] fc7 -> fc7
I0905 22:31:12.476549   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.476595   359 net.cpp:125] fc7 needs backward computation.
I0905 22:31:12.476608   359 net.cpp:66] Creating Layer relu7
I0905 22:31:12.476616   359 net.cpp:329] relu7 <- fc7
I0905 22:31:12.476625   359 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:31:12.476635   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.476641   359 net.cpp:125] relu7 needs backward computation.
I0905 22:31:12.476649   359 net.cpp:66] Creating Layer drop7
I0905 22:31:12.476655   359 net.cpp:329] drop7 <- fc7
I0905 22:31:12.476661   359 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:31:12.476672   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.476678   359 net.cpp:125] drop7 needs backward computation.
I0905 22:31:12.476687   359 net.cpp:66] Creating Layer fc8
I0905 22:31:12.476693   359 net.cpp:329] fc8 <- fc7
I0905 22:31:12.476702   359 net.cpp:290] fc8 -> fc8
I0905 22:31:12.484663   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.484675   359 net.cpp:125] fc8 needs backward computation.
I0905 22:31:12.484683   359 net.cpp:66] Creating Layer relu8
I0905 22:31:12.484688   359 net.cpp:329] relu8 <- fc8
I0905 22:31:12.484696   359 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:31:12.484704   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.484709   359 net.cpp:125] relu8 needs backward computation.
I0905 22:31:12.484715   359 net.cpp:66] Creating Layer drop8
I0905 22:31:12.484720   359 net.cpp:329] drop8 <- fc8
I0905 22:31:12.484737   359 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:31:12.484745   359 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:31:12.484750   359 net.cpp:125] drop8 needs backward computation.
I0905 22:31:12.484760   359 net.cpp:66] Creating Layer fc9
I0905 22:31:12.484766   359 net.cpp:329] fc9 <- fc8
I0905 22:31:12.484772   359 net.cpp:290] fc9 -> fc9
I0905 22:31:12.485146   359 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:31:12.485157   359 net.cpp:125] fc9 needs backward computation.
I0905 22:31:12.485165   359 net.cpp:66] Creating Layer fc10
I0905 22:31:12.485172   359 net.cpp:329] fc10 <- fc9
I0905 22:31:12.485179   359 net.cpp:290] fc10 -> fc10
I0905 22:31:12.485191   359 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:31:12.485198   359 net.cpp:125] fc10 needs backward computation.
I0905 22:31:12.485205   359 net.cpp:66] Creating Layer prob
I0905 22:31:12.485211   359 net.cpp:329] prob <- fc10
I0905 22:31:12.485219   359 net.cpp:290] prob -> prob
I0905 22:31:12.485229   359 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:31:12.485234   359 net.cpp:125] prob needs backward computation.
I0905 22:31:12.485239   359 net.cpp:156] This network produces output prob
I0905 22:31:12.485252   359 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:31:12.485260   359 net.cpp:167] Network initialization done.
I0905 22:31:12.485265   359 net.cpp:168] Memory required for data: 6183480
Classifying 80 inputs.
Done in 50.50 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:32:05.884233   364 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:32:05.884374   364 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:32:05.884383   364 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:32:05.884533   364 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:32:05.884598   364 net.cpp:292] Input 0 -> data
I0905 22:32:05.884625   364 net.cpp:66] Creating Layer conv1
I0905 22:32:05.884632   364 net.cpp:329] conv1 <- data
I0905 22:32:05.884640   364 net.cpp:290] conv1 -> conv1
I0905 22:32:05.886015   364 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:32:05.886035   364 net.cpp:125] conv1 needs backward computation.
I0905 22:32:05.886044   364 net.cpp:66] Creating Layer relu1
I0905 22:32:05.886050   364 net.cpp:329] relu1 <- conv1
I0905 22:32:05.886057   364 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:32:05.886066   364 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:32:05.886072   364 net.cpp:125] relu1 needs backward computation.
I0905 22:32:05.886080   364 net.cpp:66] Creating Layer pool1
I0905 22:32:05.886085   364 net.cpp:329] pool1 <- conv1
I0905 22:32:05.886092   364 net.cpp:290] pool1 -> pool1
I0905 22:32:05.886103   364 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:32:05.886109   364 net.cpp:125] pool1 needs backward computation.
I0905 22:32:05.886116   364 net.cpp:66] Creating Layer norm1
I0905 22:32:05.886122   364 net.cpp:329] norm1 <- pool1
I0905 22:32:05.886129   364 net.cpp:290] norm1 -> norm1
I0905 22:32:05.886139   364 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:32:05.886145   364 net.cpp:125] norm1 needs backward computation.
I0905 22:32:05.886152   364 net.cpp:66] Creating Layer conv2
I0905 22:32:05.886158   364 net.cpp:329] conv2 <- norm1
I0905 22:32:05.886165   364 net.cpp:290] conv2 -> conv2
I0905 22:32:05.895308   364 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:32:05.895323   364 net.cpp:125] conv2 needs backward computation.
I0905 22:32:05.895331   364 net.cpp:66] Creating Layer relu2
I0905 22:32:05.895337   364 net.cpp:329] relu2 <- conv2
I0905 22:32:05.895344   364 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:32:05.895351   364 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:32:05.895357   364 net.cpp:125] relu2 needs backward computation.
I0905 22:32:05.895365   364 net.cpp:66] Creating Layer pool2
I0905 22:32:05.895371   364 net.cpp:329] pool2 <- conv2
I0905 22:32:05.895378   364 net.cpp:290] pool2 -> pool2
I0905 22:32:05.895386   364 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:32:05.895392   364 net.cpp:125] pool2 needs backward computation.
I0905 22:32:05.895401   364 net.cpp:66] Creating Layer fc7
I0905 22:32:05.895406   364 net.cpp:329] fc7 <- pool2
I0905 22:32:05.895413   364 net.cpp:290] fc7 -> fc7
I0905 22:32:06.542482   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.542531   364 net.cpp:125] fc7 needs backward computation.
I0905 22:32:06.542543   364 net.cpp:66] Creating Layer relu7
I0905 22:32:06.542551   364 net.cpp:329] relu7 <- fc7
I0905 22:32:06.542561   364 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:32:06.542572   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.542577   364 net.cpp:125] relu7 needs backward computation.
I0905 22:32:06.542595   364 net.cpp:66] Creating Layer drop7
I0905 22:32:06.542601   364 net.cpp:329] drop7 <- fc7
I0905 22:32:06.542608   364 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:32:06.542619   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.542625   364 net.cpp:125] drop7 needs backward computation.
I0905 22:32:06.542634   364 net.cpp:66] Creating Layer fc8
I0905 22:32:06.542640   364 net.cpp:329] fc8 <- fc7
I0905 22:32:06.542649   364 net.cpp:290] fc8 -> fc8
I0905 22:32:06.550442   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.550456   364 net.cpp:125] fc8 needs backward computation.
I0905 22:32:06.550462   364 net.cpp:66] Creating Layer relu8
I0905 22:32:06.550468   364 net.cpp:329] relu8 <- fc8
I0905 22:32:06.550477   364 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:32:06.550484   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.550490   364 net.cpp:125] relu8 needs backward computation.
I0905 22:32:06.550498   364 net.cpp:66] Creating Layer drop8
I0905 22:32:06.550503   364 net.cpp:329] drop8 <- fc8
I0905 22:32:06.550509   364 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:32:06.550516   364 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:06.550523   364 net.cpp:125] drop8 needs backward computation.
I0905 22:32:06.550530   364 net.cpp:66] Creating Layer fc9
I0905 22:32:06.550536   364 net.cpp:329] fc9 <- fc8
I0905 22:32:06.550544   364 net.cpp:290] fc9 -> fc9
I0905 22:32:06.550920   364 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:32:06.550931   364 net.cpp:125] fc9 needs backward computation.
I0905 22:32:06.550940   364 net.cpp:66] Creating Layer fc10
I0905 22:32:06.550945   364 net.cpp:329] fc10 <- fc9
I0905 22:32:06.550954   364 net.cpp:290] fc10 -> fc10
I0905 22:32:06.550966   364 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:32:06.550974   364 net.cpp:125] fc10 needs backward computation.
I0905 22:32:06.550981   364 net.cpp:66] Creating Layer prob
I0905 22:32:06.550987   364 net.cpp:329] prob <- fc10
I0905 22:32:06.550994   364 net.cpp:290] prob -> prob
I0905 22:32:06.551004   364 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:32:06.551010   364 net.cpp:125] prob needs backward computation.
I0905 22:32:06.551015   364 net.cpp:156] This network produces output prob
I0905 22:32:06.551028   364 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:32:06.551038   364 net.cpp:167] Network initialization done.
I0905 22:32:06.551043   364 net.cpp:168] Memory required for data: 6183480
Classifying 26 inputs.
Done in 16.56 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:32:24.315646   368 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:32:24.315784   368 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:32:24.315793   368 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:32:24.315942   368 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:32:24.316005   368 net.cpp:292] Input 0 -> data
I0905 22:32:24.316032   368 net.cpp:66] Creating Layer conv1
I0905 22:32:24.316040   368 net.cpp:329] conv1 <- data
I0905 22:32:24.316048   368 net.cpp:290] conv1 -> conv1
I0905 22:32:24.317437   368 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:32:24.317456   368 net.cpp:125] conv1 needs backward computation.
I0905 22:32:24.317464   368 net.cpp:66] Creating Layer relu1
I0905 22:32:24.317471   368 net.cpp:329] relu1 <- conv1
I0905 22:32:24.317477   368 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:32:24.317486   368 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:32:24.317492   368 net.cpp:125] relu1 needs backward computation.
I0905 22:32:24.317498   368 net.cpp:66] Creating Layer pool1
I0905 22:32:24.317503   368 net.cpp:329] pool1 <- conv1
I0905 22:32:24.317510   368 net.cpp:290] pool1 -> pool1
I0905 22:32:24.317522   368 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:32:24.317528   368 net.cpp:125] pool1 needs backward computation.
I0905 22:32:24.317534   368 net.cpp:66] Creating Layer norm1
I0905 22:32:24.317539   368 net.cpp:329] norm1 <- pool1
I0905 22:32:24.317546   368 net.cpp:290] norm1 -> norm1
I0905 22:32:24.317555   368 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:32:24.317561   368 net.cpp:125] norm1 needs backward computation.
I0905 22:32:24.317569   368 net.cpp:66] Creating Layer conv2
I0905 22:32:24.317574   368 net.cpp:329] conv2 <- norm1
I0905 22:32:24.317600   368 net.cpp:290] conv2 -> conv2
I0905 22:32:24.326747   368 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:32:24.326761   368 net.cpp:125] conv2 needs backward computation.
I0905 22:32:24.326768   368 net.cpp:66] Creating Layer relu2
I0905 22:32:24.326774   368 net.cpp:329] relu2 <- conv2
I0905 22:32:24.326781   368 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:32:24.326788   368 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:32:24.326798   368 net.cpp:125] relu2 needs backward computation.
I0905 22:32:24.326807   368 net.cpp:66] Creating Layer pool2
I0905 22:32:24.326812   368 net.cpp:329] pool2 <- conv2
I0905 22:32:24.326818   368 net.cpp:290] pool2 -> pool2
I0905 22:32:24.326827   368 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:32:24.326831   368 net.cpp:125] pool2 needs backward computation.
I0905 22:32:24.326840   368 net.cpp:66] Creating Layer fc7
I0905 22:32:24.326846   368 net.cpp:329] fc7 <- pool2
I0905 22:32:24.326853   368 net.cpp:290] fc7 -> fc7
I0905 22:32:24.977224   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.977269   368 net.cpp:125] fc7 needs backward computation.
I0905 22:32:24.977283   368 net.cpp:66] Creating Layer relu7
I0905 22:32:24.977289   368 net.cpp:329] relu7 <- fc7
I0905 22:32:24.977298   368 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:32:24.977308   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.977314   368 net.cpp:125] relu7 needs backward computation.
I0905 22:32:24.977321   368 net.cpp:66] Creating Layer drop7
I0905 22:32:24.977327   368 net.cpp:329] drop7 <- fc7
I0905 22:32:24.977334   368 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:32:24.977344   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.977350   368 net.cpp:125] drop7 needs backward computation.
I0905 22:32:24.977358   368 net.cpp:66] Creating Layer fc8
I0905 22:32:24.977363   368 net.cpp:329] fc8 <- fc7
I0905 22:32:24.977372   368 net.cpp:290] fc8 -> fc8
I0905 22:32:24.985311   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.985323   368 net.cpp:125] fc8 needs backward computation.
I0905 22:32:24.985330   368 net.cpp:66] Creating Layer relu8
I0905 22:32:24.985337   368 net.cpp:329] relu8 <- fc8
I0905 22:32:24.985344   368 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:32:24.985352   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.985358   368 net.cpp:125] relu8 needs backward computation.
I0905 22:32:24.985365   368 net.cpp:66] Creating Layer drop8
I0905 22:32:24.985370   368 net.cpp:329] drop8 <- fc8
I0905 22:32:24.985378   368 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:32:24.985384   368 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:32:24.985390   368 net.cpp:125] drop8 needs backward computation.
I0905 22:32:24.985399   368 net.cpp:66] Creating Layer fc9
I0905 22:32:24.985405   368 net.cpp:329] fc9 <- fc8
I0905 22:32:24.985412   368 net.cpp:290] fc9 -> fc9
I0905 22:32:24.985801   368 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:32:24.985815   368 net.cpp:125] fc9 needs backward computation.
I0905 22:32:24.985823   368 net.cpp:66] Creating Layer fc10
I0905 22:32:24.985829   368 net.cpp:329] fc10 <- fc9
I0905 22:32:24.985838   368 net.cpp:290] fc10 -> fc10
I0905 22:32:24.985851   368 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:32:24.985858   368 net.cpp:125] fc10 needs backward computation.
I0905 22:32:24.985865   368 net.cpp:66] Creating Layer prob
I0905 22:32:24.985870   368 net.cpp:329] prob <- fc10
I0905 22:32:24.985878   368 net.cpp:290] prob -> prob
I0905 22:32:24.985888   368 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:32:24.985894   368 net.cpp:125] prob needs backward computation.
I0905 22:32:24.985899   368 net.cpp:156] This network produces output prob
I0905 22:32:24.985913   368 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:32:24.985921   368 net.cpp:167] Network initialization done.
I0905 22:32:24.985926   368 net.cpp:168] Memory required for data: 6183480
Classifying 217 inputs.
Done in 416.00 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 17 is out of bounds for axis 0 with size 17
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:39:59.250715   394 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:39:59.250869   394 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:39:59.250877   394 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:39:59.251026   394 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:39:59.251080   394 net.cpp:292] Input 0 -> data
I0905 22:39:59.251104   394 net.cpp:66] Creating Layer conv1
I0905 22:39:59.251111   394 net.cpp:329] conv1 <- data
I0905 22:39:59.251119   394 net.cpp:290] conv1 -> conv1
I0905 22:39:59.260149   394 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:39:59.260179   394 net.cpp:125] conv1 needs backward computation.
I0905 22:39:59.260192   394 net.cpp:66] Creating Layer relu1
I0905 22:39:59.260201   394 net.cpp:329] relu1 <- conv1
I0905 22:39:59.260211   394 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:39:59.260231   394 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:39:59.260241   394 net.cpp:125] relu1 needs backward computation.
I0905 22:39:59.260251   394 net.cpp:66] Creating Layer pool1
I0905 22:39:59.260259   394 net.cpp:329] pool1 <- conv1
I0905 22:39:59.260269   394 net.cpp:290] pool1 -> pool1
I0905 22:39:59.260284   394 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:39:59.260293   394 net.cpp:125] pool1 needs backward computation.
I0905 22:39:59.260303   394 net.cpp:66] Creating Layer norm1
I0905 22:39:59.260311   394 net.cpp:329] norm1 <- pool1
I0905 22:39:59.260321   394 net.cpp:290] norm1 -> norm1
I0905 22:39:59.260334   394 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:39:59.260344   394 net.cpp:125] norm1 needs backward computation.
I0905 22:39:59.260354   394 net.cpp:66] Creating Layer conv2
I0905 22:39:59.260362   394 net.cpp:329] conv2 <- norm1
I0905 22:39:59.260372   394 net.cpp:290] conv2 -> conv2
I0905 22:39:59.270470   394 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:39:59.270488   394 net.cpp:125] conv2 needs backward computation.
I0905 22:39:59.270495   394 net.cpp:66] Creating Layer relu2
I0905 22:39:59.270501   394 net.cpp:329] relu2 <- conv2
I0905 22:39:59.270509   394 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:39:59.270516   394 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:39:59.270522   394 net.cpp:125] relu2 needs backward computation.
I0905 22:39:59.270529   394 net.cpp:66] Creating Layer pool2
I0905 22:39:59.270534   394 net.cpp:329] pool2 <- conv2
I0905 22:39:59.270541   394 net.cpp:290] pool2 -> pool2
I0905 22:39:59.270550   394 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:39:59.270556   394 net.cpp:125] pool2 needs backward computation.
I0905 22:39:59.270565   394 net.cpp:66] Creating Layer fc7
I0905 22:39:59.270572   394 net.cpp:329] fc7 <- pool2
I0905 22:39:59.270580   394 net.cpp:290] fc7 -> fc7
I0905 22:39:59.919884   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.919931   394 net.cpp:125] fc7 needs backward computation.
I0905 22:39:59.919945   394 net.cpp:66] Creating Layer relu7
I0905 22:39:59.919951   394 net.cpp:329] relu7 <- fc7
I0905 22:39:59.919961   394 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:39:59.919971   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.919977   394 net.cpp:125] relu7 needs backward computation.
I0905 22:39:59.919984   394 net.cpp:66] Creating Layer drop7
I0905 22:39:59.919989   394 net.cpp:329] drop7 <- fc7
I0905 22:39:59.919996   394 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:39:59.920007   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.920013   394 net.cpp:125] drop7 needs backward computation.
I0905 22:39:59.920022   394 net.cpp:66] Creating Layer fc8
I0905 22:39:59.920027   394 net.cpp:329] fc8 <- fc7
I0905 22:39:59.920035   394 net.cpp:290] fc8 -> fc8
I0905 22:39:59.927834   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.927846   394 net.cpp:125] fc8 needs backward computation.
I0905 22:39:59.927853   394 net.cpp:66] Creating Layer relu8
I0905 22:39:59.927858   394 net.cpp:329] relu8 <- fc8
I0905 22:39:59.927866   394 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:39:59.927875   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.927880   394 net.cpp:125] relu8 needs backward computation.
I0905 22:39:59.927886   394 net.cpp:66] Creating Layer drop8
I0905 22:39:59.927891   394 net.cpp:329] drop8 <- fc8
I0905 22:39:59.927897   394 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:39:59.927904   394 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:39:59.927911   394 net.cpp:125] drop8 needs backward computation.
I0905 22:39:59.927918   394 net.cpp:66] Creating Layer fc9
I0905 22:39:59.927924   394 net.cpp:329] fc9 <- fc8
I0905 22:39:59.927930   394 net.cpp:290] fc9 -> fc9
I0905 22:39:59.928304   394 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:39:59.928315   394 net.cpp:125] fc9 needs backward computation.
I0905 22:39:59.928324   394 net.cpp:66] Creating Layer fc10
I0905 22:39:59.928329   394 net.cpp:329] fc10 <- fc9
I0905 22:39:59.928338   394 net.cpp:290] fc10 -> fc10
I0905 22:39:59.928359   394 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:39:59.928369   394 net.cpp:125] fc10 needs backward computation.
I0905 22:39:59.928375   394 net.cpp:66] Creating Layer prob
I0905 22:39:59.928380   394 net.cpp:329] prob <- fc10
I0905 22:39:59.928388   394 net.cpp:290] prob -> prob
I0905 22:39:59.945698   394 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:39:59.945714   394 net.cpp:125] prob needs backward computation.
I0905 22:39:59.945724   394 net.cpp:156] This network produces output prob
I0905 22:39:59.945745   394 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:39:59.945762   394 net.cpp:167] Network initialization done.
I0905 22:39:59.945766   394 net.cpp:168] Memory required for data: 6183480
Classifying 444 inputs.
Done in 278.12 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 44 is out of bounds for axis 0 with size 44
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:44:50.993018   404 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:44:50.993170   404 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:44:50.993180   404 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:44:50.993326   404 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:44:50.993389   404 net.cpp:292] Input 0 -> data
I0905 22:44:50.993417   404 net.cpp:66] Creating Layer conv1
I0905 22:44:50.993423   404 net.cpp:329] conv1 <- data
I0905 22:44:50.993432   404 net.cpp:290] conv1 -> conv1
I0905 22:44:50.994810   404 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:44:50.994830   404 net.cpp:125] conv1 needs backward computation.
I0905 22:44:50.994839   404 net.cpp:66] Creating Layer relu1
I0905 22:44:50.994845   404 net.cpp:329] relu1 <- conv1
I0905 22:44:50.994851   404 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:44:50.994860   404 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:44:50.994866   404 net.cpp:125] relu1 needs backward computation.
I0905 22:44:50.994874   404 net.cpp:66] Creating Layer pool1
I0905 22:44:50.994879   404 net.cpp:329] pool1 <- conv1
I0905 22:44:50.994885   404 net.cpp:290] pool1 -> pool1
I0905 22:44:50.994896   404 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:44:50.994902   404 net.cpp:125] pool1 needs backward computation.
I0905 22:44:50.994909   404 net.cpp:66] Creating Layer norm1
I0905 22:44:50.994915   404 net.cpp:329] norm1 <- pool1
I0905 22:44:50.994920   404 net.cpp:290] norm1 -> norm1
I0905 22:44:50.994930   404 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:44:50.994935   404 net.cpp:125] norm1 needs backward computation.
I0905 22:44:50.994942   404 net.cpp:66] Creating Layer conv2
I0905 22:44:50.994948   404 net.cpp:329] conv2 <- norm1
I0905 22:44:50.994956   404 net.cpp:290] conv2 -> conv2
I0905 22:44:51.004125   404 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:44:51.004140   404 net.cpp:125] conv2 needs backward computation.
I0905 22:44:51.004148   404 net.cpp:66] Creating Layer relu2
I0905 22:44:51.004153   404 net.cpp:329] relu2 <- conv2
I0905 22:44:51.004159   404 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:44:51.004168   404 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:44:51.004173   404 net.cpp:125] relu2 needs backward computation.
I0905 22:44:51.004179   404 net.cpp:66] Creating Layer pool2
I0905 22:44:51.004184   404 net.cpp:329] pool2 <- conv2
I0905 22:44:51.004190   404 net.cpp:290] pool2 -> pool2
I0905 22:44:51.004199   404 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:44:51.004204   404 net.cpp:125] pool2 needs backward computation.
I0905 22:44:51.004214   404 net.cpp:66] Creating Layer fc7
I0905 22:44:51.004220   404 net.cpp:329] fc7 <- pool2
I0905 22:44:51.004226   404 net.cpp:290] fc7 -> fc7
I0905 22:44:51.652719   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.652765   404 net.cpp:125] fc7 needs backward computation.
I0905 22:44:51.652776   404 net.cpp:66] Creating Layer relu7
I0905 22:44:51.652784   404 net.cpp:329] relu7 <- fc7
I0905 22:44:51.652793   404 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:44:51.652803   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.652809   404 net.cpp:125] relu7 needs backward computation.
I0905 22:44:51.652817   404 net.cpp:66] Creating Layer drop7
I0905 22:44:51.652822   404 net.cpp:329] drop7 <- fc7
I0905 22:44:51.652827   404 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:44:51.652838   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.652844   404 net.cpp:125] drop7 needs backward computation.
I0905 22:44:51.652853   404 net.cpp:66] Creating Layer fc8
I0905 22:44:51.652869   404 net.cpp:329] fc8 <- fc7
I0905 22:44:51.652879   404 net.cpp:290] fc8 -> fc8
I0905 22:44:51.660892   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.660904   404 net.cpp:125] fc8 needs backward computation.
I0905 22:44:51.660912   404 net.cpp:66] Creating Layer relu8
I0905 22:44:51.660917   404 net.cpp:329] relu8 <- fc8
I0905 22:44:51.660925   404 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:44:51.660933   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.660939   404 net.cpp:125] relu8 needs backward computation.
I0905 22:44:51.660946   404 net.cpp:66] Creating Layer drop8
I0905 22:44:51.660951   404 net.cpp:329] drop8 <- fc8
I0905 22:44:51.660958   404 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:44:51.660964   404 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:44:51.660970   404 net.cpp:125] drop8 needs backward computation.
I0905 22:44:51.660979   404 net.cpp:66] Creating Layer fc9
I0905 22:44:51.660985   404 net.cpp:329] fc9 <- fc8
I0905 22:44:51.660992   404 net.cpp:290] fc9 -> fc9
I0905 22:44:51.661380   404 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:44:51.661391   404 net.cpp:125] fc9 needs backward computation.
I0905 22:44:51.661399   404 net.cpp:66] Creating Layer fc10
I0905 22:44:51.661406   404 net.cpp:329] fc10 <- fc9
I0905 22:44:51.661413   404 net.cpp:290] fc10 -> fc10
I0905 22:44:51.661425   404 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:44:51.661433   404 net.cpp:125] fc10 needs backward computation.
I0905 22:44:51.661440   404 net.cpp:66] Creating Layer prob
I0905 22:44:51.661447   404 net.cpp:329] prob <- fc10
I0905 22:44:51.661453   404 net.cpp:290] prob -> prob
I0905 22:44:51.661463   404 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:44:51.661469   404 net.cpp:125] prob needs backward computation.
I0905 22:44:51.661474   404 net.cpp:156] This network produces output prob
I0905 22:44:51.661487   404 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:44:51.661496   404 net.cpp:167] Network initialization done.
I0905 22:44:51.661501   404 net.cpp:168] Memory required for data: 6183480
Classifying 52 inputs.
Done in 35.45 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:45:29.490293   408 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:45:29.490432   408 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:45:29.490442   408 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:45:29.490588   408 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:45:29.490653   408 net.cpp:292] Input 0 -> data
I0905 22:45:29.490679   408 net.cpp:66] Creating Layer conv1
I0905 22:45:29.490685   408 net.cpp:329] conv1 <- data
I0905 22:45:29.490694   408 net.cpp:290] conv1 -> conv1
I0905 22:45:29.492076   408 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:45:29.492094   408 net.cpp:125] conv1 needs backward computation.
I0905 22:45:29.492104   408 net.cpp:66] Creating Layer relu1
I0905 22:45:29.492110   408 net.cpp:329] relu1 <- conv1
I0905 22:45:29.492116   408 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:45:29.492125   408 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:45:29.492130   408 net.cpp:125] relu1 needs backward computation.
I0905 22:45:29.492137   408 net.cpp:66] Creating Layer pool1
I0905 22:45:29.492143   408 net.cpp:329] pool1 <- conv1
I0905 22:45:29.492149   408 net.cpp:290] pool1 -> pool1
I0905 22:45:29.492161   408 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:45:29.492166   408 net.cpp:125] pool1 needs backward computation.
I0905 22:45:29.492173   408 net.cpp:66] Creating Layer norm1
I0905 22:45:29.492178   408 net.cpp:329] norm1 <- pool1
I0905 22:45:29.492185   408 net.cpp:290] norm1 -> norm1
I0905 22:45:29.492195   408 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:45:29.492200   408 net.cpp:125] norm1 needs backward computation.
I0905 22:45:29.492208   408 net.cpp:66] Creating Layer conv2
I0905 22:45:29.492213   408 net.cpp:329] conv2 <- norm1
I0905 22:45:29.492220   408 net.cpp:290] conv2 -> conv2
I0905 22:45:29.501370   408 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:45:29.501386   408 net.cpp:125] conv2 needs backward computation.
I0905 22:45:29.501394   408 net.cpp:66] Creating Layer relu2
I0905 22:45:29.501399   408 net.cpp:329] relu2 <- conv2
I0905 22:45:29.501406   408 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:45:29.501413   408 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:45:29.501420   408 net.cpp:125] relu2 needs backward computation.
I0905 22:45:29.501425   408 net.cpp:66] Creating Layer pool2
I0905 22:45:29.501430   408 net.cpp:329] pool2 <- conv2
I0905 22:45:29.501437   408 net.cpp:290] pool2 -> pool2
I0905 22:45:29.501446   408 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:45:29.501451   408 net.cpp:125] pool2 needs backward computation.
I0905 22:45:29.501466   408 net.cpp:66] Creating Layer fc7
I0905 22:45:29.501472   408 net.cpp:329] fc7 <- pool2
I0905 22:45:29.501479   408 net.cpp:290] fc7 -> fc7
I0905 22:45:30.152444   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.152490   408 net.cpp:125] fc7 needs backward computation.
I0905 22:45:30.152503   408 net.cpp:66] Creating Layer relu7
I0905 22:45:30.152510   408 net.cpp:329] relu7 <- fc7
I0905 22:45:30.152520   408 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:45:30.152530   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.152536   408 net.cpp:125] relu7 needs backward computation.
I0905 22:45:30.152544   408 net.cpp:66] Creating Layer drop7
I0905 22:45:30.152549   408 net.cpp:329] drop7 <- fc7
I0905 22:45:30.152556   408 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:45:30.152567   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.152573   408 net.cpp:125] drop7 needs backward computation.
I0905 22:45:30.152582   408 net.cpp:66] Creating Layer fc8
I0905 22:45:30.152587   408 net.cpp:329] fc8 <- fc7
I0905 22:45:30.152596   408 net.cpp:290] fc8 -> fc8
I0905 22:45:30.160614   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.160627   408 net.cpp:125] fc8 needs backward computation.
I0905 22:45:30.160635   408 net.cpp:66] Creating Layer relu8
I0905 22:45:30.160641   408 net.cpp:329] relu8 <- fc8
I0905 22:45:30.160650   408 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:45:30.160657   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.160662   408 net.cpp:125] relu8 needs backward computation.
I0905 22:45:30.160670   408 net.cpp:66] Creating Layer drop8
I0905 22:45:30.160676   408 net.cpp:329] drop8 <- fc8
I0905 22:45:30.160682   408 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:45:30.160688   408 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:45:30.160694   408 net.cpp:125] drop8 needs backward computation.
I0905 22:45:30.160703   408 net.cpp:66] Creating Layer fc9
I0905 22:45:30.160709   408 net.cpp:329] fc9 <- fc8
I0905 22:45:30.160717   408 net.cpp:290] fc9 -> fc9
I0905 22:45:30.161101   408 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:45:30.161114   408 net.cpp:125] fc9 needs backward computation.
I0905 22:45:30.161123   408 net.cpp:66] Creating Layer fc10
I0905 22:45:30.161128   408 net.cpp:329] fc10 <- fc9
I0905 22:45:30.161137   408 net.cpp:290] fc10 -> fc10
I0905 22:45:30.161149   408 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:45:30.161157   408 net.cpp:125] fc10 needs backward computation.
I0905 22:45:30.161164   408 net.cpp:66] Creating Layer prob
I0905 22:45:30.161170   408 net.cpp:329] prob <- fc10
I0905 22:45:30.161180   408 net.cpp:290] prob -> prob
I0905 22:45:30.161190   408 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:45:30.161196   408 net.cpp:125] prob needs backward computation.
I0905 22:45:30.161201   408 net.cpp:156] This network produces output prob
I0905 22:45:30.161214   408 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:45:30.161223   408 net.cpp:167] Network initialization done.
I0905 22:45:30.161228   408 net.cpp:168] Memory required for data: 6183480
Classifying 210 inputs.
Done in 134.28 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 10 is out of bounds for axis 0 with size 10
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:47:49.663600   416 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:47:49.663745   416 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:47:49.663754   416 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:47:49.663908   416 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:47:49.663975   416 net.cpp:292] Input 0 -> data
I0905 22:47:49.664001   416 net.cpp:66] Creating Layer conv1
I0905 22:47:49.664008   416 net.cpp:329] conv1 <- data
I0905 22:47:49.664017   416 net.cpp:290] conv1 -> conv1
I0905 22:47:49.665421   416 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:47:49.665441   416 net.cpp:125] conv1 needs backward computation.
I0905 22:47:49.665451   416 net.cpp:66] Creating Layer relu1
I0905 22:47:49.665457   416 net.cpp:329] relu1 <- conv1
I0905 22:47:49.665464   416 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:47:49.665473   416 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:47:49.665479   416 net.cpp:125] relu1 needs backward computation.
I0905 22:47:49.665488   416 net.cpp:66] Creating Layer pool1
I0905 22:47:49.665493   416 net.cpp:329] pool1 <- conv1
I0905 22:47:49.665500   416 net.cpp:290] pool1 -> pool1
I0905 22:47:49.665511   416 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:47:49.665518   416 net.cpp:125] pool1 needs backward computation.
I0905 22:47:49.665530   416 net.cpp:66] Creating Layer norm1
I0905 22:47:49.665536   416 net.cpp:329] norm1 <- pool1
I0905 22:47:49.665544   416 net.cpp:290] norm1 -> norm1
I0905 22:47:49.665555   416 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:47:49.665560   416 net.cpp:125] norm1 needs backward computation.
I0905 22:47:49.665568   416 net.cpp:66] Creating Layer conv2
I0905 22:47:49.665575   416 net.cpp:329] conv2 <- norm1
I0905 22:47:49.665596   416 net.cpp:290] conv2 -> conv2
I0905 22:47:49.674990   416 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:47:49.675006   416 net.cpp:125] conv2 needs backward computation.
I0905 22:47:49.675014   416 net.cpp:66] Creating Layer relu2
I0905 22:47:49.675021   416 net.cpp:329] relu2 <- conv2
I0905 22:47:49.675029   416 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:47:49.675035   416 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:47:49.675042   416 net.cpp:125] relu2 needs backward computation.
I0905 22:47:49.675051   416 net.cpp:66] Creating Layer pool2
I0905 22:47:49.675057   416 net.cpp:329] pool2 <- conv2
I0905 22:47:49.675065   416 net.cpp:290] pool2 -> pool2
I0905 22:47:49.675073   416 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:47:49.675079   416 net.cpp:125] pool2 needs backward computation.
I0905 22:47:49.675087   416 net.cpp:66] Creating Layer fc7
I0905 22:47:49.675093   416 net.cpp:329] fc7 <- pool2
I0905 22:47:49.675101   416 net.cpp:290] fc7 -> fc7
I0905 22:47:50.323285   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.323329   416 net.cpp:125] fc7 needs backward computation.
I0905 22:47:50.323343   416 net.cpp:66] Creating Layer relu7
I0905 22:47:50.323350   416 net.cpp:329] relu7 <- fc7
I0905 22:47:50.323359   416 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:47:50.323370   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.323376   416 net.cpp:125] relu7 needs backward computation.
I0905 22:47:50.323384   416 net.cpp:66] Creating Layer drop7
I0905 22:47:50.323390   416 net.cpp:329] drop7 <- fc7
I0905 22:47:50.323396   416 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:47:50.323407   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.323413   416 net.cpp:125] drop7 needs backward computation.
I0905 22:47:50.323421   416 net.cpp:66] Creating Layer fc8
I0905 22:47:50.323427   416 net.cpp:329] fc8 <- fc7
I0905 22:47:50.323436   416 net.cpp:290] fc8 -> fc8
I0905 22:47:50.331233   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.331246   416 net.cpp:125] fc8 needs backward computation.
I0905 22:47:50.331254   416 net.cpp:66] Creating Layer relu8
I0905 22:47:50.331259   416 net.cpp:329] relu8 <- fc8
I0905 22:47:50.331267   416 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:47:50.331275   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.331280   416 net.cpp:125] relu8 needs backward computation.
I0905 22:47:50.331287   416 net.cpp:66] Creating Layer drop8
I0905 22:47:50.331292   416 net.cpp:329] drop8 <- fc8
I0905 22:47:50.331300   416 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:47:50.331306   416 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:47:50.331312   416 net.cpp:125] drop8 needs backward computation.
I0905 22:47:50.331321   416 net.cpp:66] Creating Layer fc9
I0905 22:47:50.331327   416 net.cpp:329] fc9 <- fc8
I0905 22:47:50.331334   416 net.cpp:290] fc9 -> fc9
I0905 22:47:50.331709   416 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:47:50.331722   416 net.cpp:125] fc9 needs backward computation.
I0905 22:47:50.331730   416 net.cpp:66] Creating Layer fc10
I0905 22:47:50.331737   416 net.cpp:329] fc10 <- fc9
I0905 22:47:50.331745   416 net.cpp:290] fc10 -> fc10
I0905 22:47:50.331758   416 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:47:50.331765   416 net.cpp:125] fc10 needs backward computation.
I0905 22:47:50.331773   416 net.cpp:66] Creating Layer prob
I0905 22:47:50.331779   416 net.cpp:329] prob <- fc10
I0905 22:47:50.331786   416 net.cpp:290] prob -> prob
I0905 22:47:50.331795   416 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:47:50.331812   416 net.cpp:125] prob needs backward computation.
I0905 22:47:50.331817   416 net.cpp:156] This network produces output prob
I0905 22:47:50.331830   416 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:47:50.331840   416 net.cpp:167] Network initialization done.
I0905 22:47:50.331845   416 net.cpp:168] Memory required for data: 6183480
Classifying 97 inputs.
Done in 66.80 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:48:59.994719   421 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:48:59.994858   421 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:48:59.994866   421 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:48:59.995013   421 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:48:59.995079   421 net.cpp:292] Input 0 -> data
I0905 22:48:59.995105   421 net.cpp:66] Creating Layer conv1
I0905 22:48:59.995110   421 net.cpp:329] conv1 <- data
I0905 22:48:59.995120   421 net.cpp:290] conv1 -> conv1
I0905 22:48:59.996484   421 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:48:59.996501   421 net.cpp:125] conv1 needs backward computation.
I0905 22:48:59.996510   421 net.cpp:66] Creating Layer relu1
I0905 22:48:59.996516   421 net.cpp:329] relu1 <- conv1
I0905 22:48:59.996523   421 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:48:59.996531   421 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:48:59.996537   421 net.cpp:125] relu1 needs backward computation.
I0905 22:48:59.996544   421 net.cpp:66] Creating Layer pool1
I0905 22:48:59.996551   421 net.cpp:329] pool1 <- conv1
I0905 22:48:59.996556   421 net.cpp:290] pool1 -> pool1
I0905 22:48:59.996567   421 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:48:59.996573   421 net.cpp:125] pool1 needs backward computation.
I0905 22:48:59.996580   421 net.cpp:66] Creating Layer norm1
I0905 22:48:59.996587   421 net.cpp:329] norm1 <- pool1
I0905 22:48:59.996593   421 net.cpp:290] norm1 -> norm1
I0905 22:48:59.996603   421 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:48:59.996608   421 net.cpp:125] norm1 needs backward computation.
I0905 22:48:59.996616   421 net.cpp:66] Creating Layer conv2
I0905 22:48:59.996621   421 net.cpp:329] conv2 <- norm1
I0905 22:48:59.996629   421 net.cpp:290] conv2 -> conv2
I0905 22:49:00.005821   421 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:49:00.005838   421 net.cpp:125] conv2 needs backward computation.
I0905 22:49:00.005846   421 net.cpp:66] Creating Layer relu2
I0905 22:49:00.005852   421 net.cpp:329] relu2 <- conv2
I0905 22:49:00.005859   421 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:49:00.005867   421 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:49:00.005872   421 net.cpp:125] relu2 needs backward computation.
I0905 22:49:00.005882   421 net.cpp:66] Creating Layer pool2
I0905 22:49:00.005887   421 net.cpp:329] pool2 <- conv2
I0905 22:49:00.005894   421 net.cpp:290] pool2 -> pool2
I0905 22:49:00.005903   421 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:49:00.005909   421 net.cpp:125] pool2 needs backward computation.
I0905 22:49:00.005916   421 net.cpp:66] Creating Layer fc7
I0905 22:49:00.005923   421 net.cpp:329] fc7 <- pool2
I0905 22:49:00.005929   421 net.cpp:290] fc7 -> fc7
I0905 22:49:00.657438   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.657485   421 net.cpp:125] fc7 needs backward computation.
I0905 22:49:00.657498   421 net.cpp:66] Creating Layer relu7
I0905 22:49:00.657505   421 net.cpp:329] relu7 <- fc7
I0905 22:49:00.657515   421 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:49:00.657524   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.657531   421 net.cpp:125] relu7 needs backward computation.
I0905 22:49:00.657537   421 net.cpp:66] Creating Layer drop7
I0905 22:49:00.657543   421 net.cpp:329] drop7 <- fc7
I0905 22:49:00.657549   421 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:49:00.657560   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.657567   421 net.cpp:125] drop7 needs backward computation.
I0905 22:49:00.657574   421 net.cpp:66] Creating Layer fc8
I0905 22:49:00.657584   421 net.cpp:329] fc8 <- fc7
I0905 22:49:00.657596   421 net.cpp:290] fc8 -> fc8
I0905 22:49:00.665359   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.665371   421 net.cpp:125] fc8 needs backward computation.
I0905 22:49:00.665379   421 net.cpp:66] Creating Layer relu8
I0905 22:49:00.665385   421 net.cpp:329] relu8 <- fc8
I0905 22:49:00.665392   421 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:49:00.665400   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.665405   421 net.cpp:125] relu8 needs backward computation.
I0905 22:49:00.665411   421 net.cpp:66] Creating Layer drop8
I0905 22:49:00.665417   421 net.cpp:329] drop8 <- fc8
I0905 22:49:00.665433   421 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:49:00.665441   421 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:00.665446   421 net.cpp:125] drop8 needs backward computation.
I0905 22:49:00.665455   421 net.cpp:66] Creating Layer fc9
I0905 22:49:00.665462   421 net.cpp:329] fc9 <- fc8
I0905 22:49:00.665468   421 net.cpp:290] fc9 -> fc9
I0905 22:49:00.665848   421 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:49:00.665861   421 net.cpp:125] fc9 needs backward computation.
I0905 22:49:00.665870   421 net.cpp:66] Creating Layer fc10
I0905 22:49:00.665876   421 net.cpp:329] fc10 <- fc9
I0905 22:49:00.665884   421 net.cpp:290] fc10 -> fc10
I0905 22:49:00.665896   421 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:49:00.665904   421 net.cpp:125] fc10 needs backward computation.
I0905 22:49:00.665911   421 net.cpp:66] Creating Layer prob
I0905 22:49:00.665916   421 net.cpp:329] prob <- fc10
I0905 22:49:00.665925   421 net.cpp:290] prob -> prob
I0905 22:49:00.665935   421 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:49:00.665940   421 net.cpp:125] prob needs backward computation.
I0905 22:49:00.665946   421 net.cpp:156] This network produces output prob
I0905 22:49:00.665957   421 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:49:00.665966   421 net.cpp:167] Network initialization done.
I0905 22:49:00.665971   421 net.cpp:168] Memory required for data: 6183480
Classifying 24 inputs.
Done in 17.85 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:49:21.409308   426 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:49:21.409446   426 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:49:21.409454   426 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:49:21.409627   426 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:49:21.409693   426 net.cpp:292] Input 0 -> data
I0905 22:49:21.409720   426 net.cpp:66] Creating Layer conv1
I0905 22:49:21.409728   426 net.cpp:329] conv1 <- data
I0905 22:49:21.409736   426 net.cpp:290] conv1 -> conv1
I0905 22:49:21.411139   426 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:49:21.411157   426 net.cpp:125] conv1 needs backward computation.
I0905 22:49:21.411167   426 net.cpp:66] Creating Layer relu1
I0905 22:49:21.411173   426 net.cpp:329] relu1 <- conv1
I0905 22:49:21.411180   426 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:49:21.411188   426 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:49:21.411195   426 net.cpp:125] relu1 needs backward computation.
I0905 22:49:21.411201   426 net.cpp:66] Creating Layer pool1
I0905 22:49:21.411207   426 net.cpp:329] pool1 <- conv1
I0905 22:49:21.411214   426 net.cpp:290] pool1 -> pool1
I0905 22:49:21.411226   426 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:49:21.411231   426 net.cpp:125] pool1 needs backward computation.
I0905 22:49:21.411238   426 net.cpp:66] Creating Layer norm1
I0905 22:49:21.411243   426 net.cpp:329] norm1 <- pool1
I0905 22:49:21.411250   426 net.cpp:290] norm1 -> norm1
I0905 22:49:21.411262   426 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:49:21.411267   426 net.cpp:125] norm1 needs backward computation.
I0905 22:49:21.411274   426 net.cpp:66] Creating Layer conv2
I0905 22:49:21.411280   426 net.cpp:329] conv2 <- norm1
I0905 22:49:21.411288   426 net.cpp:290] conv2 -> conv2
I0905 22:49:21.420636   426 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:49:21.420653   426 net.cpp:125] conv2 needs backward computation.
I0905 22:49:21.420660   426 net.cpp:66] Creating Layer relu2
I0905 22:49:21.420666   426 net.cpp:329] relu2 <- conv2
I0905 22:49:21.420673   426 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:49:21.420680   426 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:49:21.420686   426 net.cpp:125] relu2 needs backward computation.
I0905 22:49:21.420692   426 net.cpp:66] Creating Layer pool2
I0905 22:49:21.420697   426 net.cpp:329] pool2 <- conv2
I0905 22:49:21.420704   426 net.cpp:290] pool2 -> pool2
I0905 22:49:21.420712   426 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:49:21.420717   426 net.cpp:125] pool2 needs backward computation.
I0905 22:49:21.420727   426 net.cpp:66] Creating Layer fc7
I0905 22:49:21.420733   426 net.cpp:329] fc7 <- pool2
I0905 22:49:21.420740   426 net.cpp:290] fc7 -> fc7
I0905 22:49:22.069818   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.069864   426 net.cpp:125] fc7 needs backward computation.
I0905 22:49:22.069875   426 net.cpp:66] Creating Layer relu7
I0905 22:49:22.069882   426 net.cpp:329] relu7 <- fc7
I0905 22:49:22.069892   426 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:49:22.069901   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.069907   426 net.cpp:125] relu7 needs backward computation.
I0905 22:49:22.069916   426 net.cpp:66] Creating Layer drop7
I0905 22:49:22.069931   426 net.cpp:329] drop7 <- fc7
I0905 22:49:22.069937   426 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:49:22.069948   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.069954   426 net.cpp:125] drop7 needs backward computation.
I0905 22:49:22.069962   426 net.cpp:66] Creating Layer fc8
I0905 22:49:22.069967   426 net.cpp:329] fc8 <- fc7
I0905 22:49:22.069977   426 net.cpp:290] fc8 -> fc8
I0905 22:49:22.077782   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.077795   426 net.cpp:125] fc8 needs backward computation.
I0905 22:49:22.077801   426 net.cpp:66] Creating Layer relu8
I0905 22:49:22.077807   426 net.cpp:329] relu8 <- fc8
I0905 22:49:22.077816   426 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:49:22.077822   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.077827   426 net.cpp:125] relu8 needs backward computation.
I0905 22:49:22.077834   426 net.cpp:66] Creating Layer drop8
I0905 22:49:22.077839   426 net.cpp:329] drop8 <- fc8
I0905 22:49:22.077846   426 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:49:22.077852   426 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:49:22.077858   426 net.cpp:125] drop8 needs backward computation.
I0905 22:49:22.077867   426 net.cpp:66] Creating Layer fc9
I0905 22:49:22.077872   426 net.cpp:329] fc9 <- fc8
I0905 22:49:22.077879   426 net.cpp:290] fc9 -> fc9
I0905 22:49:22.078253   426 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:49:22.078264   426 net.cpp:125] fc9 needs backward computation.
I0905 22:49:22.078272   426 net.cpp:66] Creating Layer fc10
I0905 22:49:22.078279   426 net.cpp:329] fc10 <- fc9
I0905 22:49:22.078287   426 net.cpp:290] fc10 -> fc10
I0905 22:49:22.078299   426 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:49:22.078306   426 net.cpp:125] fc10 needs backward computation.
I0905 22:49:22.078313   426 net.cpp:66] Creating Layer prob
I0905 22:49:22.078318   426 net.cpp:329] prob <- fc10
I0905 22:49:22.078326   426 net.cpp:290] prob -> prob
I0905 22:49:22.078336   426 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:49:22.078342   426 net.cpp:125] prob needs backward computation.
I0905 22:49:22.078346   426 net.cpp:156] This network produces output prob
I0905 22:49:22.078359   426 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:49:22.078368   426 net.cpp:167] Network initialization done.
I0905 22:49:22.078373   426 net.cpp:168] Memory required for data: 6183480
Classifying 65 inputs.
Done in 41.11 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:50:05.322278   431 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:50:05.322420   431 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:50:05.322430   431 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:50:05.322582   431 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:50:05.322646   431 net.cpp:292] Input 0 -> data
I0905 22:50:05.322674   431 net.cpp:66] Creating Layer conv1
I0905 22:50:05.322681   431 net.cpp:329] conv1 <- data
I0905 22:50:05.322690   431 net.cpp:290] conv1 -> conv1
I0905 22:50:05.324095   431 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:50:05.324115   431 net.cpp:125] conv1 needs backward computation.
I0905 22:50:05.324123   431 net.cpp:66] Creating Layer relu1
I0905 22:50:05.324129   431 net.cpp:329] relu1 <- conv1
I0905 22:50:05.324136   431 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:50:05.324146   431 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:50:05.324151   431 net.cpp:125] relu1 needs backward computation.
I0905 22:50:05.324158   431 net.cpp:66] Creating Layer pool1
I0905 22:50:05.324164   431 net.cpp:329] pool1 <- conv1
I0905 22:50:05.324172   431 net.cpp:290] pool1 -> pool1
I0905 22:50:05.324183   431 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:50:05.324189   431 net.cpp:125] pool1 needs backward computation.
I0905 22:50:05.324197   431 net.cpp:66] Creating Layer norm1
I0905 22:50:05.324203   431 net.cpp:329] norm1 <- pool1
I0905 22:50:05.324209   431 net.cpp:290] norm1 -> norm1
I0905 22:50:05.324219   431 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:50:05.324225   431 net.cpp:125] norm1 needs backward computation.
I0905 22:50:05.324234   431 net.cpp:66] Creating Layer conv2
I0905 22:50:05.324239   431 net.cpp:329] conv2 <- norm1
I0905 22:50:05.324246   431 net.cpp:290] conv2 -> conv2
I0905 22:50:05.333636   431 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:50:05.333650   431 net.cpp:125] conv2 needs backward computation.
I0905 22:50:05.333658   431 net.cpp:66] Creating Layer relu2
I0905 22:50:05.333664   431 net.cpp:329] relu2 <- conv2
I0905 22:50:05.333672   431 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:50:05.333678   431 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:50:05.333690   431 net.cpp:125] relu2 needs backward computation.
I0905 22:50:05.333698   431 net.cpp:66] Creating Layer pool2
I0905 22:50:05.333703   431 net.cpp:329] pool2 <- conv2
I0905 22:50:05.333710   431 net.cpp:290] pool2 -> pool2
I0905 22:50:05.333719   431 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:50:05.333724   431 net.cpp:125] pool2 needs backward computation.
I0905 22:50:05.333734   431 net.cpp:66] Creating Layer fc7
I0905 22:50:05.333740   431 net.cpp:329] fc7 <- pool2
I0905 22:50:05.333748   431 net.cpp:290] fc7 -> fc7
I0905 22:50:05.982942   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.982987   431 net.cpp:125] fc7 needs backward computation.
I0905 22:50:05.983000   431 net.cpp:66] Creating Layer relu7
I0905 22:50:05.983007   431 net.cpp:329] relu7 <- fc7
I0905 22:50:05.983017   431 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:50:05.983027   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.983033   431 net.cpp:125] relu7 needs backward computation.
I0905 22:50:05.983041   431 net.cpp:66] Creating Layer drop7
I0905 22:50:05.983047   431 net.cpp:329] drop7 <- fc7
I0905 22:50:05.983052   431 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:50:05.983063   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.983068   431 net.cpp:125] drop7 needs backward computation.
I0905 22:50:05.983078   431 net.cpp:66] Creating Layer fc8
I0905 22:50:05.983083   431 net.cpp:329] fc8 <- fc7
I0905 22:50:05.983091   431 net.cpp:290] fc8 -> fc8
I0905 22:50:05.990874   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.990886   431 net.cpp:125] fc8 needs backward computation.
I0905 22:50:05.990893   431 net.cpp:66] Creating Layer relu8
I0905 22:50:05.990898   431 net.cpp:329] relu8 <- fc8
I0905 22:50:05.990906   431 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:50:05.990914   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.990919   431 net.cpp:125] relu8 needs backward computation.
I0905 22:50:05.990926   431 net.cpp:66] Creating Layer drop8
I0905 22:50:05.990931   431 net.cpp:329] drop8 <- fc8
I0905 22:50:05.990938   431 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:50:05.990946   431 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:50:05.990950   431 net.cpp:125] drop8 needs backward computation.
I0905 22:50:05.990959   431 net.cpp:66] Creating Layer fc9
I0905 22:50:05.990965   431 net.cpp:329] fc9 <- fc8
I0905 22:50:05.990972   431 net.cpp:290] fc9 -> fc9
I0905 22:50:05.991348   431 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:50:05.991359   431 net.cpp:125] fc9 needs backward computation.
I0905 22:50:05.991369   431 net.cpp:66] Creating Layer fc10
I0905 22:50:05.991374   431 net.cpp:329] fc10 <- fc9
I0905 22:50:05.991382   431 net.cpp:290] fc10 -> fc10
I0905 22:50:05.991394   431 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:50:05.991401   431 net.cpp:125] fc10 needs backward computation.
I0905 22:50:05.991408   431 net.cpp:66] Creating Layer prob
I0905 22:50:05.991415   431 net.cpp:329] prob <- fc10
I0905 22:50:05.991421   431 net.cpp:290] prob -> prob
I0905 22:50:05.991431   431 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:50:05.991437   431 net.cpp:125] prob needs backward computation.
I0905 22:50:05.991442   431 net.cpp:156] This network produces output prob
I0905 22:50:05.991454   431 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:50:05.991463   431 net.cpp:167] Network initialization done.
I0905 22:50:05.991468   431 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 248 inputs.
Done in 151.99 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 48 is out of bounds for axis 0 with size 48
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:52:43.453564   438 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:52:43.453728   438 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:52:43.453738   438 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:52:43.453884   438 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:52:43.453938   438 net.cpp:292] Input 0 -> data
I0905 22:52:43.453963   438 net.cpp:66] Creating Layer conv1
I0905 22:52:43.453969   438 net.cpp:329] conv1 <- data
I0905 22:52:43.453979   438 net.cpp:290] conv1 -> conv1
I0905 22:52:43.455340   438 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:52:43.455358   438 net.cpp:125] conv1 needs backward computation.
I0905 22:52:43.455368   438 net.cpp:66] Creating Layer relu1
I0905 22:52:43.455374   438 net.cpp:329] relu1 <- conv1
I0905 22:52:43.455385   438 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:52:43.455395   438 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:52:43.455400   438 net.cpp:125] relu1 needs backward computation.
I0905 22:52:43.455407   438 net.cpp:66] Creating Layer pool1
I0905 22:52:43.455413   438 net.cpp:329] pool1 <- conv1
I0905 22:52:43.455420   438 net.cpp:290] pool1 -> pool1
I0905 22:52:43.455431   438 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:52:43.455437   438 net.cpp:125] pool1 needs backward computation.
I0905 22:52:43.455445   438 net.cpp:66] Creating Layer norm1
I0905 22:52:43.455449   438 net.cpp:329] norm1 <- pool1
I0905 22:52:43.455456   438 net.cpp:290] norm1 -> norm1
I0905 22:52:43.455466   438 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:52:43.455472   438 net.cpp:125] norm1 needs backward computation.
I0905 22:52:43.455479   438 net.cpp:66] Creating Layer conv2
I0905 22:52:43.455485   438 net.cpp:329] conv2 <- norm1
I0905 22:52:43.455492   438 net.cpp:290] conv2 -> conv2
I0905 22:52:43.464629   438 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:52:43.464645   438 net.cpp:125] conv2 needs backward computation.
I0905 22:52:43.464653   438 net.cpp:66] Creating Layer relu2
I0905 22:52:43.464658   438 net.cpp:329] relu2 <- conv2
I0905 22:52:43.464665   438 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:52:43.464673   438 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:52:43.464679   438 net.cpp:125] relu2 needs backward computation.
I0905 22:52:43.464687   438 net.cpp:66] Creating Layer pool2
I0905 22:52:43.464694   438 net.cpp:329] pool2 <- conv2
I0905 22:52:43.464699   438 net.cpp:290] pool2 -> pool2
I0905 22:52:43.464707   438 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:52:43.464714   438 net.cpp:125] pool2 needs backward computation.
I0905 22:52:43.464720   438 net.cpp:66] Creating Layer fc7
I0905 22:52:43.464726   438 net.cpp:329] fc7 <- pool2
I0905 22:52:43.464733   438 net.cpp:290] fc7 -> fc7
I0905 22:52:44.111346   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.111392   438 net.cpp:125] fc7 needs backward computation.
I0905 22:52:44.111404   438 net.cpp:66] Creating Layer relu7
I0905 22:52:44.111413   438 net.cpp:329] relu7 <- fc7
I0905 22:52:44.111421   438 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:52:44.111431   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.111438   438 net.cpp:125] relu7 needs backward computation.
I0905 22:52:44.111445   438 net.cpp:66] Creating Layer drop7
I0905 22:52:44.111450   438 net.cpp:329] drop7 <- fc7
I0905 22:52:44.111457   438 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:52:44.111467   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.111474   438 net.cpp:125] drop7 needs backward computation.
I0905 22:52:44.111482   438 net.cpp:66] Creating Layer fc8
I0905 22:52:44.111487   438 net.cpp:329] fc8 <- fc7
I0905 22:52:44.111500   438 net.cpp:290] fc8 -> fc8
I0905 22:52:44.119313   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.119324   438 net.cpp:125] fc8 needs backward computation.
I0905 22:52:44.119331   438 net.cpp:66] Creating Layer relu8
I0905 22:52:44.119338   438 net.cpp:329] relu8 <- fc8
I0905 22:52:44.119345   438 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:52:44.119352   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.119359   438 net.cpp:125] relu8 needs backward computation.
I0905 22:52:44.119365   438 net.cpp:66] Creating Layer drop8
I0905 22:52:44.119370   438 net.cpp:329] drop8 <- fc8
I0905 22:52:44.119376   438 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:52:44.119384   438 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:52:44.119390   438 net.cpp:125] drop8 needs backward computation.
I0905 22:52:44.119398   438 net.cpp:66] Creating Layer fc9
I0905 22:52:44.119403   438 net.cpp:329] fc9 <- fc8
I0905 22:52:44.119410   438 net.cpp:290] fc9 -> fc9
I0905 22:52:44.119784   438 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:52:44.119796   438 net.cpp:125] fc9 needs backward computation.
I0905 22:52:44.119806   438 net.cpp:66] Creating Layer fc10
I0905 22:52:44.119820   438 net.cpp:329] fc10 <- fc9
I0905 22:52:44.119829   438 net.cpp:290] fc10 -> fc10
I0905 22:52:44.119842   438 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:52:44.119849   438 net.cpp:125] fc10 needs backward computation.
I0905 22:52:44.119856   438 net.cpp:66] Creating Layer prob
I0905 22:52:44.119861   438 net.cpp:329] prob <- fc10
I0905 22:52:44.119869   438 net.cpp:290] prob -> prob
I0905 22:52:44.119879   438 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:52:44.119884   438 net.cpp:125] prob needs backward computation.
I0905 22:52:44.119889   438 net.cpp:156] This network produces output prob
I0905 22:52:44.119902   438 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:52:44.119910   438 net.cpp:167] Network initialization done.
I0905 22:52:44.119916   438 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 205 inputs.
Done in 129.88 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 5 is out of bounds for axis 0 with size 5
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:54:58.419102   446 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:54:58.419241   446 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:54:58.419250   446 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:54:58.419396   446 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:54:58.419461   446 net.cpp:292] Input 0 -> data
I0905 22:54:58.419486   446 net.cpp:66] Creating Layer conv1
I0905 22:54:58.419493   446 net.cpp:329] conv1 <- data
I0905 22:54:58.419502   446 net.cpp:290] conv1 -> conv1
I0905 22:54:58.420863   446 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:54:58.420881   446 net.cpp:125] conv1 needs backward computation.
I0905 22:54:58.420891   446 net.cpp:66] Creating Layer relu1
I0905 22:54:58.420897   446 net.cpp:329] relu1 <- conv1
I0905 22:54:58.420903   446 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:54:58.420912   446 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:54:58.420918   446 net.cpp:125] relu1 needs backward computation.
I0905 22:54:58.420925   446 net.cpp:66] Creating Layer pool1
I0905 22:54:58.420932   446 net.cpp:329] pool1 <- conv1
I0905 22:54:58.420938   446 net.cpp:290] pool1 -> pool1
I0905 22:54:58.420948   446 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:54:58.420954   446 net.cpp:125] pool1 needs backward computation.
I0905 22:54:58.420961   446 net.cpp:66] Creating Layer norm1
I0905 22:54:58.420966   446 net.cpp:329] norm1 <- pool1
I0905 22:54:58.420974   446 net.cpp:290] norm1 -> norm1
I0905 22:54:58.420984   446 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:54:58.420989   446 net.cpp:125] norm1 needs backward computation.
I0905 22:54:58.420996   446 net.cpp:66] Creating Layer conv2
I0905 22:54:58.421002   446 net.cpp:329] conv2 <- norm1
I0905 22:54:58.421010   446 net.cpp:290] conv2 -> conv2
I0905 22:54:58.430184   446 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:54:58.430201   446 net.cpp:125] conv2 needs backward computation.
I0905 22:54:58.430207   446 net.cpp:66] Creating Layer relu2
I0905 22:54:58.430212   446 net.cpp:329] relu2 <- conv2
I0905 22:54:58.430219   446 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:54:58.430227   446 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:54:58.430233   446 net.cpp:125] relu2 needs backward computation.
I0905 22:54:58.430240   446 net.cpp:66] Creating Layer pool2
I0905 22:54:58.430246   446 net.cpp:329] pool2 <- conv2
I0905 22:54:58.430253   446 net.cpp:290] pool2 -> pool2
I0905 22:54:58.430261   446 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:54:58.430268   446 net.cpp:125] pool2 needs backward computation.
I0905 22:54:58.430274   446 net.cpp:66] Creating Layer fc7
I0905 22:54:58.430279   446 net.cpp:329] fc7 <- pool2
I0905 22:54:58.430286   446 net.cpp:290] fc7 -> fc7
I0905 22:54:59.080205   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.080251   446 net.cpp:125] fc7 needs backward computation.
I0905 22:54:59.080265   446 net.cpp:66] Creating Layer relu7
I0905 22:54:59.080272   446 net.cpp:329] relu7 <- fc7
I0905 22:54:59.080282   446 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:54:59.080292   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.080298   446 net.cpp:125] relu7 needs backward computation.
I0905 22:54:59.080307   446 net.cpp:66] Creating Layer drop7
I0905 22:54:59.080312   446 net.cpp:329] drop7 <- fc7
I0905 22:54:59.080330   446 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:54:59.080342   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.080348   446 net.cpp:125] drop7 needs backward computation.
I0905 22:54:59.080358   446 net.cpp:66] Creating Layer fc8
I0905 22:54:59.080363   446 net.cpp:329] fc8 <- fc7
I0905 22:54:59.080373   446 net.cpp:290] fc8 -> fc8
I0905 22:54:59.088296   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.088310   446 net.cpp:125] fc8 needs backward computation.
I0905 22:54:59.088316   446 net.cpp:66] Creating Layer relu8
I0905 22:54:59.088322   446 net.cpp:329] relu8 <- fc8
I0905 22:54:59.088330   446 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:54:59.088337   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.088343   446 net.cpp:125] relu8 needs backward computation.
I0905 22:54:59.088349   446 net.cpp:66] Creating Layer drop8
I0905 22:54:59.088354   446 net.cpp:329] drop8 <- fc8
I0905 22:54:59.088361   446 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:54:59.088367   446 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:54:59.088373   446 net.cpp:125] drop8 needs backward computation.
I0905 22:54:59.088382   446 net.cpp:66] Creating Layer fc9
I0905 22:54:59.088388   446 net.cpp:329] fc9 <- fc8
I0905 22:54:59.088395   446 net.cpp:290] fc9 -> fc9
I0905 22:54:59.088769   446 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:54:59.088781   446 net.cpp:125] fc9 needs backward computation.
I0905 22:54:59.088789   446 net.cpp:66] Creating Layer fc10
I0905 22:54:59.088795   446 net.cpp:329] fc10 <- fc9
I0905 22:54:59.088804   446 net.cpp:290] fc10 -> fc10
I0905 22:54:59.088815   446 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:54:59.088824   446 net.cpp:125] fc10 needs backward computation.
I0905 22:54:59.088830   446 net.cpp:66] Creating Layer prob
I0905 22:54:59.088835   446 net.cpp:329] prob <- fc10
I0905 22:54:59.088843   446 net.cpp:290] prob -> prob
I0905 22:54:59.088853   446 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:54:59.088860   446 net.cpp:125] prob needs backward computation.
I0905 22:54:59.088865   446 net.cpp:156] This network produces output prob
I0905 22:54:59.088877   446 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:54:59.088886   446 net.cpp:167] Network initialization done.
I0905 22:54:59.088891   446 net.cpp:168] Memory required for data: 6183480
Classifying 66 inputs.
Done in 41.22 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:55:42.686596   460 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:55:42.686735   460 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:55:42.686744   460 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:55:42.686892   460 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:55:42.686956   460 net.cpp:292] Input 0 -> data
I0905 22:55:42.686982   460 net.cpp:66] Creating Layer conv1
I0905 22:55:42.686990   460 net.cpp:329] conv1 <- data
I0905 22:55:42.686997   460 net.cpp:290] conv1 -> conv1
I0905 22:55:42.688359   460 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:55:42.688377   460 net.cpp:125] conv1 needs backward computation.
I0905 22:55:42.688386   460 net.cpp:66] Creating Layer relu1
I0905 22:55:42.688392   460 net.cpp:329] relu1 <- conv1
I0905 22:55:42.688400   460 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:55:42.688407   460 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:55:42.688413   460 net.cpp:125] relu1 needs backward computation.
I0905 22:55:42.688421   460 net.cpp:66] Creating Layer pool1
I0905 22:55:42.688426   460 net.cpp:329] pool1 <- conv1
I0905 22:55:42.688432   460 net.cpp:290] pool1 -> pool1
I0905 22:55:42.688443   460 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:55:42.688449   460 net.cpp:125] pool1 needs backward computation.
I0905 22:55:42.688457   460 net.cpp:66] Creating Layer norm1
I0905 22:55:42.688462   460 net.cpp:329] norm1 <- pool1
I0905 22:55:42.688468   460 net.cpp:290] norm1 -> norm1
I0905 22:55:42.688478   460 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:55:42.688484   460 net.cpp:125] norm1 needs backward computation.
I0905 22:55:42.688491   460 net.cpp:66] Creating Layer conv2
I0905 22:55:42.688496   460 net.cpp:329] conv2 <- norm1
I0905 22:55:42.688503   460 net.cpp:290] conv2 -> conv2
I0905 22:55:42.697702   460 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:55:42.697718   460 net.cpp:125] conv2 needs backward computation.
I0905 22:55:42.697726   460 net.cpp:66] Creating Layer relu2
I0905 22:55:42.697731   460 net.cpp:329] relu2 <- conv2
I0905 22:55:42.697737   460 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:55:42.697746   460 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:55:42.697751   460 net.cpp:125] relu2 needs backward computation.
I0905 22:55:42.697759   460 net.cpp:66] Creating Layer pool2
I0905 22:55:42.697770   460 net.cpp:329] pool2 <- conv2
I0905 22:55:42.697777   460 net.cpp:290] pool2 -> pool2
I0905 22:55:42.697785   460 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:55:42.697792   460 net.cpp:125] pool2 needs backward computation.
I0905 22:55:42.697798   460 net.cpp:66] Creating Layer fc7
I0905 22:55:42.697804   460 net.cpp:329] fc7 <- pool2
I0905 22:55:42.697811   460 net.cpp:290] fc7 -> fc7
I0905 22:55:43.347151   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.347193   460 net.cpp:125] fc7 needs backward computation.
I0905 22:55:43.347205   460 net.cpp:66] Creating Layer relu7
I0905 22:55:43.347213   460 net.cpp:329] relu7 <- fc7
I0905 22:55:43.347223   460 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:55:43.347232   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.347239   460 net.cpp:125] relu7 needs backward computation.
I0905 22:55:43.347245   460 net.cpp:66] Creating Layer drop7
I0905 22:55:43.347250   460 net.cpp:329] drop7 <- fc7
I0905 22:55:43.347257   460 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:55:43.347267   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.347273   460 net.cpp:125] drop7 needs backward computation.
I0905 22:55:43.347282   460 net.cpp:66] Creating Layer fc8
I0905 22:55:43.347287   460 net.cpp:329] fc8 <- fc7
I0905 22:55:43.347296   460 net.cpp:290] fc8 -> fc8
I0905 22:55:43.355088   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.355100   460 net.cpp:125] fc8 needs backward computation.
I0905 22:55:43.355108   460 net.cpp:66] Creating Layer relu8
I0905 22:55:43.355113   460 net.cpp:329] relu8 <- fc8
I0905 22:55:43.355121   460 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:55:43.355129   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.355134   460 net.cpp:125] relu8 needs backward computation.
I0905 22:55:43.355140   460 net.cpp:66] Creating Layer drop8
I0905 22:55:43.355145   460 net.cpp:329] drop8 <- fc8
I0905 22:55:43.355152   460 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:55:43.355159   460 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:55:43.355165   460 net.cpp:125] drop8 needs backward computation.
I0905 22:55:43.355172   460 net.cpp:66] Creating Layer fc9
I0905 22:55:43.355178   460 net.cpp:329] fc9 <- fc8
I0905 22:55:43.355185   460 net.cpp:290] fc9 -> fc9
I0905 22:55:43.355559   460 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:55:43.355571   460 net.cpp:125] fc9 needs backward computation.
I0905 22:55:43.355579   460 net.cpp:66] Creating Layer fc10
I0905 22:55:43.355586   460 net.cpp:329] fc10 <- fc9
I0905 22:55:43.355593   460 net.cpp:290] fc10 -> fc10
I0905 22:55:43.355605   460 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:55:43.355613   460 net.cpp:125] fc10 needs backward computation.
I0905 22:55:43.355619   460 net.cpp:66] Creating Layer prob
I0905 22:55:43.355625   460 net.cpp:329] prob <- fc10
I0905 22:55:43.355633   460 net.cpp:290] prob -> prob
I0905 22:55:43.355643   460 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:55:43.355648   460 net.cpp:125] prob needs backward computation.
I0905 22:55:43.355654   460 net.cpp:156] This network produces output prob
I0905 22:55:43.355666   460 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:55:43.355674   460 net.cpp:167] Network initialization done.
I0905 22:55:43.355679   460 net.cpp:168] Memory required for data: 6183480
Classifying 196 inputs.
Done in 120.03 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 96 is out of bounds for axis 0 with size 96
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:57:49.769428   466 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:57:49.769567   466 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:57:49.769613   466 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:57:49.769775   466 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:57:49.769829   466 net.cpp:292] Input 0 -> data
I0905 22:57:49.769855   466 net.cpp:66] Creating Layer conv1
I0905 22:57:49.769861   466 net.cpp:329] conv1 <- data
I0905 22:57:49.769868   466 net.cpp:290] conv1 -> conv1
I0905 22:57:49.771241   466 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:57:49.771260   466 net.cpp:125] conv1 needs backward computation.
I0905 22:57:49.771268   466 net.cpp:66] Creating Layer relu1
I0905 22:57:49.771275   466 net.cpp:329] relu1 <- conv1
I0905 22:57:49.771281   466 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:57:49.771289   466 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:57:49.771296   466 net.cpp:125] relu1 needs backward computation.
I0905 22:57:49.771307   466 net.cpp:66] Creating Layer pool1
I0905 22:57:49.771313   466 net.cpp:329] pool1 <- conv1
I0905 22:57:49.771320   466 net.cpp:290] pool1 -> pool1
I0905 22:57:49.771332   466 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:57:49.771337   466 net.cpp:125] pool1 needs backward computation.
I0905 22:57:49.771343   466 net.cpp:66] Creating Layer norm1
I0905 22:57:49.771348   466 net.cpp:329] norm1 <- pool1
I0905 22:57:49.771355   466 net.cpp:290] norm1 -> norm1
I0905 22:57:49.771365   466 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:57:49.771370   466 net.cpp:125] norm1 needs backward computation.
I0905 22:57:49.771378   466 net.cpp:66] Creating Layer conv2
I0905 22:57:49.771384   466 net.cpp:329] conv2 <- norm1
I0905 22:57:49.771390   466 net.cpp:290] conv2 -> conv2
I0905 22:57:49.780527   466 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:57:49.780544   466 net.cpp:125] conv2 needs backward computation.
I0905 22:57:49.780550   466 net.cpp:66] Creating Layer relu2
I0905 22:57:49.780555   466 net.cpp:329] relu2 <- conv2
I0905 22:57:49.780562   466 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:57:49.780570   466 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:57:49.780575   466 net.cpp:125] relu2 needs backward computation.
I0905 22:57:49.780583   466 net.cpp:66] Creating Layer pool2
I0905 22:57:49.780588   466 net.cpp:329] pool2 <- conv2
I0905 22:57:49.780596   466 net.cpp:290] pool2 -> pool2
I0905 22:57:49.780603   466 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:57:49.780609   466 net.cpp:125] pool2 needs backward computation.
I0905 22:57:49.780616   466 net.cpp:66] Creating Layer fc7
I0905 22:57:49.780622   466 net.cpp:329] fc7 <- pool2
I0905 22:57:49.780628   466 net.cpp:290] fc7 -> fc7
I0905 22:57:50.431299   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.431345   466 net.cpp:125] fc7 needs backward computation.
I0905 22:57:50.431359   466 net.cpp:66] Creating Layer relu7
I0905 22:57:50.431365   466 net.cpp:329] relu7 <- fc7
I0905 22:57:50.431375   466 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:57:50.431385   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.431391   466 net.cpp:125] relu7 needs backward computation.
I0905 22:57:50.431398   466 net.cpp:66] Creating Layer drop7
I0905 22:57:50.431404   466 net.cpp:329] drop7 <- fc7
I0905 22:57:50.431411   466 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:57:50.431421   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.431427   466 net.cpp:125] drop7 needs backward computation.
I0905 22:57:50.431434   466 net.cpp:66] Creating Layer fc8
I0905 22:57:50.431440   466 net.cpp:329] fc8 <- fc7
I0905 22:57:50.431449   466 net.cpp:290] fc8 -> fc8
I0905 22:57:50.439244   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.439255   466 net.cpp:125] fc8 needs backward computation.
I0905 22:57:50.439262   466 net.cpp:66] Creating Layer relu8
I0905 22:57:50.439267   466 net.cpp:329] relu8 <- fc8
I0905 22:57:50.439275   466 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:57:50.439282   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.439288   466 net.cpp:125] relu8 needs backward computation.
I0905 22:57:50.439295   466 net.cpp:66] Creating Layer drop8
I0905 22:57:50.439299   466 net.cpp:329] drop8 <- fc8
I0905 22:57:50.439306   466 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:57:50.439313   466 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:57:50.439318   466 net.cpp:125] drop8 needs backward computation.
I0905 22:57:50.439327   466 net.cpp:66] Creating Layer fc9
I0905 22:57:50.439332   466 net.cpp:329] fc9 <- fc8
I0905 22:57:50.439339   466 net.cpp:290] fc9 -> fc9
I0905 22:57:50.439730   466 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:57:50.439743   466 net.cpp:125] fc9 needs backward computation.
I0905 22:57:50.439750   466 net.cpp:66] Creating Layer fc10
I0905 22:57:50.439756   466 net.cpp:329] fc10 <- fc9
I0905 22:57:50.439764   466 net.cpp:290] fc10 -> fc10
I0905 22:57:50.439776   466 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:57:50.439784   466 net.cpp:125] fc10 needs backward computation.
I0905 22:57:50.439800   466 net.cpp:66] Creating Layer prob
I0905 22:57:50.439806   466 net.cpp:329] prob <- fc10
I0905 22:57:50.439815   466 net.cpp:290] prob -> prob
I0905 22:57:50.439823   466 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:57:50.439829   466 net.cpp:125] prob needs backward computation.
I0905 22:57:50.439833   466 net.cpp:156] This network produces output prob
I0905 22:57:50.439846   466 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:57:50.439856   466 net.cpp:167] Network initialization done.
I0905 22:57:50.439860   466 net.cpp:168] Memory required for data: 6183480
Classifying 126 inputs.
Done in 76.28 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 26 is out of bounds for axis 0 with size 26
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 22:59:10.212877   473 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 22:59:10.213018   473 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 22:59:10.213027   473 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 22:59:10.213174   473 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 22:59:10.213238   473 net.cpp:292] Input 0 -> data
I0905 22:59:10.213264   473 net.cpp:66] Creating Layer conv1
I0905 22:59:10.213271   473 net.cpp:329] conv1 <- data
I0905 22:59:10.213279   473 net.cpp:290] conv1 -> conv1
I0905 22:59:10.214654   473 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:59:10.214673   473 net.cpp:125] conv1 needs backward computation.
I0905 22:59:10.214682   473 net.cpp:66] Creating Layer relu1
I0905 22:59:10.214689   473 net.cpp:329] relu1 <- conv1
I0905 22:59:10.214695   473 net.cpp:280] relu1 -> conv1 (in-place)
I0905 22:59:10.214704   473 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 22:59:10.214710   473 net.cpp:125] relu1 needs backward computation.
I0905 22:59:10.214716   473 net.cpp:66] Creating Layer pool1
I0905 22:59:10.214722   473 net.cpp:329] pool1 <- conv1
I0905 22:59:10.214730   473 net.cpp:290] pool1 -> pool1
I0905 22:59:10.214740   473 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:59:10.214746   473 net.cpp:125] pool1 needs backward computation.
I0905 22:59:10.214752   473 net.cpp:66] Creating Layer norm1
I0905 22:59:10.214758   473 net.cpp:329] norm1 <- pool1
I0905 22:59:10.214764   473 net.cpp:290] norm1 -> norm1
I0905 22:59:10.214774   473 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 22:59:10.214781   473 net.cpp:125] norm1 needs backward computation.
I0905 22:59:10.214787   473 net.cpp:66] Creating Layer conv2
I0905 22:59:10.214793   473 net.cpp:329] conv2 <- norm1
I0905 22:59:10.214800   473 net.cpp:290] conv2 -> conv2
I0905 22:59:10.223970   473 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:59:10.223985   473 net.cpp:125] conv2 needs backward computation.
I0905 22:59:10.223994   473 net.cpp:66] Creating Layer relu2
I0905 22:59:10.223999   473 net.cpp:329] relu2 <- conv2
I0905 22:59:10.224005   473 net.cpp:280] relu2 -> conv2 (in-place)
I0905 22:59:10.224014   473 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 22:59:10.224019   473 net.cpp:125] relu2 needs backward computation.
I0905 22:59:10.224025   473 net.cpp:66] Creating Layer pool2
I0905 22:59:10.224030   473 net.cpp:329] pool2 <- conv2
I0905 22:59:10.224036   473 net.cpp:290] pool2 -> pool2
I0905 22:59:10.224045   473 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 22:59:10.224050   473 net.cpp:125] pool2 needs backward computation.
I0905 22:59:10.224061   473 net.cpp:66] Creating Layer fc7
I0905 22:59:10.224066   473 net.cpp:329] fc7 <- pool2
I0905 22:59:10.224072   473 net.cpp:290] fc7 -> fc7
I0905 22:59:10.872578   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.872623   473 net.cpp:125] fc7 needs backward computation.
I0905 22:59:10.872637   473 net.cpp:66] Creating Layer relu7
I0905 22:59:10.872643   473 net.cpp:329] relu7 <- fc7
I0905 22:59:10.872653   473 net.cpp:280] relu7 -> fc7 (in-place)
I0905 22:59:10.872663   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.872668   473 net.cpp:125] relu7 needs backward computation.
I0905 22:59:10.872675   473 net.cpp:66] Creating Layer drop7
I0905 22:59:10.872680   473 net.cpp:329] drop7 <- fc7
I0905 22:59:10.872688   473 net.cpp:280] drop7 -> fc7 (in-place)
I0905 22:59:10.872697   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.872704   473 net.cpp:125] drop7 needs backward computation.
I0905 22:59:10.872712   473 net.cpp:66] Creating Layer fc8
I0905 22:59:10.872717   473 net.cpp:329] fc8 <- fc7
I0905 22:59:10.872730   473 net.cpp:290] fc8 -> fc8
I0905 22:59:10.880609   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.880632   473 net.cpp:125] fc8 needs backward computation.
I0905 22:59:10.880640   473 net.cpp:66] Creating Layer relu8
I0905 22:59:10.880645   473 net.cpp:329] relu8 <- fc8
I0905 22:59:10.880653   473 net.cpp:280] relu8 -> fc8 (in-place)
I0905 22:59:10.880661   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.880667   473 net.cpp:125] relu8 needs backward computation.
I0905 22:59:10.880673   473 net.cpp:66] Creating Layer drop8
I0905 22:59:10.880679   473 net.cpp:329] drop8 <- fc8
I0905 22:59:10.880686   473 net.cpp:280] drop8 -> fc8 (in-place)
I0905 22:59:10.880692   473 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 22:59:10.880698   473 net.cpp:125] drop8 needs backward computation.
I0905 22:59:10.880707   473 net.cpp:66] Creating Layer fc9
I0905 22:59:10.880713   473 net.cpp:329] fc9 <- fc8
I0905 22:59:10.880720   473 net.cpp:290] fc9 -> fc9
I0905 22:59:10.881105   473 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 22:59:10.881117   473 net.cpp:125] fc9 needs backward computation.
I0905 22:59:10.881125   473 net.cpp:66] Creating Layer fc10
I0905 22:59:10.881131   473 net.cpp:329] fc10 <- fc9
I0905 22:59:10.881139   473 net.cpp:290] fc10 -> fc10
I0905 22:59:10.881151   473 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:59:10.881160   473 net.cpp:125] fc10 needs backward computation.
I0905 22:59:10.881166   473 net.cpp:66] Creating Layer prob
I0905 22:59:10.881172   473 net.cpp:329] prob <- fc10
I0905 22:59:10.881181   473 net.cpp:290] prob -> prob
I0905 22:59:10.881191   473 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 22:59:10.881196   473 net.cpp:125] prob needs backward computation.
I0905 22:59:10.881201   473 net.cpp:156] This network produces output prob
I0905 22:59:10.881214   473 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 22:59:10.881222   473 net.cpp:167] Network initialization done.
I0905 22:59:10.881228   473 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:03:13.838733   483 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:03:13.838937   483 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:03:13.838948   483 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:03:13.839176   483 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:03:13.839239   483 net.cpp:292] Input 0 -> data
I0905 23:03:13.839284   483 net.cpp:66] Creating Layer conv1
I0905 23:03:13.839293   483 net.cpp:329] conv1 <- data
I0905 23:03:13.839303   483 net.cpp:290] conv1 -> conv1
I0905 23:03:13.871459   483 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:03:13.871484   483 net.cpp:125] conv1 needs backward computation.
I0905 23:03:13.871495   483 net.cpp:66] Creating Layer relu1
I0905 23:03:13.871500   483 net.cpp:329] relu1 <- conv1
I0905 23:03:13.871507   483 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:03:13.871516   483 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:03:13.871522   483 net.cpp:125] relu1 needs backward computation.
I0905 23:03:13.871529   483 net.cpp:66] Creating Layer pool1
I0905 23:03:13.871534   483 net.cpp:329] pool1 <- conv1
I0905 23:03:13.871541   483 net.cpp:290] pool1 -> pool1
I0905 23:03:13.871552   483 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:03:13.871558   483 net.cpp:125] pool1 needs backward computation.
I0905 23:03:13.871565   483 net.cpp:66] Creating Layer norm1
I0905 23:03:13.871570   483 net.cpp:329] norm1 <- pool1
I0905 23:03:13.871577   483 net.cpp:290] norm1 -> norm1
I0905 23:03:13.871587   483 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:03:13.871592   483 net.cpp:125] norm1 needs backward computation.
I0905 23:03:13.871600   483 net.cpp:66] Creating Layer conv2
I0905 23:03:13.871606   483 net.cpp:329] conv2 <- norm1
I0905 23:03:13.871613   483 net.cpp:290] conv2 -> conv2
I0905 23:03:13.880735   483 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:03:13.880751   483 net.cpp:125] conv2 needs backward computation.
I0905 23:03:13.880759   483 net.cpp:66] Creating Layer relu2
I0905 23:03:13.880764   483 net.cpp:329] relu2 <- conv2
I0905 23:03:13.880771   483 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:03:13.880779   483 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:03:13.880784   483 net.cpp:125] relu2 needs backward computation.
I0905 23:03:13.880790   483 net.cpp:66] Creating Layer pool2
I0905 23:03:13.880796   483 net.cpp:329] pool2 <- conv2
I0905 23:03:13.880802   483 net.cpp:290] pool2 -> pool2
I0905 23:03:13.880810   483 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:03:13.880816   483 net.cpp:125] pool2 needs backward computation.
I0905 23:03:13.880825   483 net.cpp:66] Creating Layer fc7
I0905 23:03:13.880831   483 net.cpp:329] fc7 <- pool2
I0905 23:03:13.880839   483 net.cpp:290] fc7 -> fc7
I0905 23:03:14.559916   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.559962   483 net.cpp:125] fc7 needs backward computation.
I0905 23:03:14.559974   483 net.cpp:66] Creating Layer relu7
I0905 23:03:14.559983   483 net.cpp:329] relu7 <- fc7
I0905 23:03:14.559991   483 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:03:14.560003   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.560008   483 net.cpp:125] relu7 needs backward computation.
I0905 23:03:14.560015   483 net.cpp:66] Creating Layer drop7
I0905 23:03:14.560020   483 net.cpp:329] drop7 <- fc7
I0905 23:03:14.560027   483 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:03:14.560037   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.560044   483 net.cpp:125] drop7 needs backward computation.
I0905 23:03:14.560052   483 net.cpp:66] Creating Layer fc8
I0905 23:03:14.560057   483 net.cpp:329] fc8 <- fc7
I0905 23:03:14.560066   483 net.cpp:290] fc8 -> fc8
I0905 23:03:14.568192   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.568203   483 net.cpp:125] fc8 needs backward computation.
I0905 23:03:14.568210   483 net.cpp:66] Creating Layer relu8
I0905 23:03:14.568217   483 net.cpp:329] relu8 <- fc8
I0905 23:03:14.568224   483 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:03:14.568231   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.568238   483 net.cpp:125] relu8 needs backward computation.
I0905 23:03:14.568243   483 net.cpp:66] Creating Layer drop8
I0905 23:03:14.568249   483 net.cpp:329] drop8 <- fc8
I0905 23:03:14.568255   483 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:03:14.568261   483 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:03:14.568267   483 net.cpp:125] drop8 needs backward computation.
I0905 23:03:14.568276   483 net.cpp:66] Creating Layer fc9
I0905 23:03:14.568282   483 net.cpp:329] fc9 <- fc8
I0905 23:03:14.568289   483 net.cpp:290] fc9 -> fc9
I0905 23:03:14.568681   483 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:03:14.568693   483 net.cpp:125] fc9 needs backward computation.
I0905 23:03:14.568701   483 net.cpp:66] Creating Layer fc10
I0905 23:03:14.568707   483 net.cpp:329] fc10 <- fc9
I0905 23:03:14.568716   483 net.cpp:290] fc10 -> fc10
I0905 23:03:14.568728   483 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:03:14.568737   483 net.cpp:125] fc10 needs backward computation.
I0905 23:03:14.568742   483 net.cpp:66] Creating Layer prob
I0905 23:03:14.568748   483 net.cpp:329] prob <- fc10
I0905 23:03:14.568755   483 net.cpp:290] prob -> prob
I0905 23:03:14.568765   483 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:03:14.568771   483 net.cpp:125] prob needs backward computation.
I0905 23:03:14.568776   483 net.cpp:156] This network produces output prob
I0905 23:03:14.568789   483 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:03:14.568797   483 net.cpp:167] Network initialization done.
I0905 23:03:14.568804   483 net.cpp:168] Memory required for data: 6183480
Classifying 59 inputs.
Done in 39.78 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:03:59.711007   490 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:03:59.711144   490 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:03:59.711153   490 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:03:59.711299   490 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:03:59.711364   490 net.cpp:292] Input 0 -> data
I0905 23:03:59.711390   490 net.cpp:66] Creating Layer conv1
I0905 23:03:59.711396   490 net.cpp:329] conv1 <- data
I0905 23:03:59.711405   490 net.cpp:290] conv1 -> conv1
I0905 23:03:59.712765   490 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:03:59.712784   490 net.cpp:125] conv1 needs backward computation.
I0905 23:03:59.712792   490 net.cpp:66] Creating Layer relu1
I0905 23:03:59.712798   490 net.cpp:329] relu1 <- conv1
I0905 23:03:59.712805   490 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:03:59.712815   490 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:03:59.712820   490 net.cpp:125] relu1 needs backward computation.
I0905 23:03:59.712826   490 net.cpp:66] Creating Layer pool1
I0905 23:03:59.712832   490 net.cpp:329] pool1 <- conv1
I0905 23:03:59.712839   490 net.cpp:290] pool1 -> pool1
I0905 23:03:59.712851   490 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:03:59.712857   490 net.cpp:125] pool1 needs backward computation.
I0905 23:03:59.712863   490 net.cpp:66] Creating Layer norm1
I0905 23:03:59.712868   490 net.cpp:329] norm1 <- pool1
I0905 23:03:59.712875   490 net.cpp:290] norm1 -> norm1
I0905 23:03:59.712885   490 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:03:59.712891   490 net.cpp:125] norm1 needs backward computation.
I0905 23:03:59.712898   490 net.cpp:66] Creating Layer conv2
I0905 23:03:59.712909   490 net.cpp:329] conv2 <- norm1
I0905 23:03:59.712916   490 net.cpp:290] conv2 -> conv2
I0905 23:03:59.722225   490 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:03:59.722241   490 net.cpp:125] conv2 needs backward computation.
I0905 23:03:59.722249   490 net.cpp:66] Creating Layer relu2
I0905 23:03:59.722254   490 net.cpp:329] relu2 <- conv2
I0905 23:03:59.722261   490 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:03:59.722270   490 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:03:59.722275   490 net.cpp:125] relu2 needs backward computation.
I0905 23:03:59.722281   490 net.cpp:66] Creating Layer pool2
I0905 23:03:59.722287   490 net.cpp:329] pool2 <- conv2
I0905 23:03:59.722295   490 net.cpp:290] pool2 -> pool2
I0905 23:03:59.722302   490 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:03:59.722308   490 net.cpp:125] pool2 needs backward computation.
I0905 23:03:59.722317   490 net.cpp:66] Creating Layer fc7
I0905 23:03:59.722323   490 net.cpp:329] fc7 <- pool2
I0905 23:03:59.722332   490 net.cpp:290] fc7 -> fc7
I0905 23:04:00.433845   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.433897   490 net.cpp:125] fc7 needs backward computation.
I0905 23:04:00.433910   490 net.cpp:66] Creating Layer relu7
I0905 23:04:00.433917   490 net.cpp:329] relu7 <- fc7
I0905 23:04:00.433928   490 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:04:00.433938   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.433944   490 net.cpp:125] relu7 needs backward computation.
I0905 23:04:00.433951   490 net.cpp:66] Creating Layer drop7
I0905 23:04:00.433958   490 net.cpp:329] drop7 <- fc7
I0905 23:04:00.433964   490 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:04:00.433974   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.433980   490 net.cpp:125] drop7 needs backward computation.
I0905 23:04:00.433990   490 net.cpp:66] Creating Layer fc8
I0905 23:04:00.433995   490 net.cpp:329] fc8 <- fc7
I0905 23:04:00.434003   490 net.cpp:290] fc8 -> fc8
I0905 23:04:00.441768   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.441781   490 net.cpp:125] fc8 needs backward computation.
I0905 23:04:00.441788   490 net.cpp:66] Creating Layer relu8
I0905 23:04:00.441794   490 net.cpp:329] relu8 <- fc8
I0905 23:04:00.441802   490 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:04:00.441809   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.441815   490 net.cpp:125] relu8 needs backward computation.
I0905 23:04:00.441822   490 net.cpp:66] Creating Layer drop8
I0905 23:04:00.441828   490 net.cpp:329] drop8 <- fc8
I0905 23:04:00.441833   490 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:04:00.441840   490 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:00.441845   490 net.cpp:125] drop8 needs backward computation.
I0905 23:04:00.441855   490 net.cpp:66] Creating Layer fc9
I0905 23:04:00.441861   490 net.cpp:329] fc9 <- fc8
I0905 23:04:00.441869   490 net.cpp:290] fc9 -> fc9
I0905 23:04:00.442240   490 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:04:00.442252   490 net.cpp:125] fc9 needs backward computation.
I0905 23:04:00.442260   490 net.cpp:66] Creating Layer fc10
I0905 23:04:00.442266   490 net.cpp:329] fc10 <- fc9
I0905 23:04:00.442275   490 net.cpp:290] fc10 -> fc10
I0905 23:04:00.442286   490 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:04:00.442293   490 net.cpp:125] fc10 needs backward computation.
I0905 23:04:00.442301   490 net.cpp:66] Creating Layer prob
I0905 23:04:00.442306   490 net.cpp:329] prob <- fc10
I0905 23:04:00.442314   490 net.cpp:290] prob -> prob
I0905 23:04:00.442323   490 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:04:00.442329   490 net.cpp:125] prob needs backward computation.
I0905 23:04:00.442334   490 net.cpp:156] This network produces output prob
I0905 23:04:00.442347   490 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:04:00.442355   490 net.cpp:167] Network initialization done.
I0905 23:04:00.442360   490 net.cpp:168] Memory required for data: 6183480
Classifying 10 inputs.
Done in 6.62 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:04:07.936662   495 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:04:07.936810   495 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:04:07.936818   495 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:04:07.936965   495 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:04:07.937016   495 net.cpp:292] Input 0 -> data
I0905 23:04:07.937042   495 net.cpp:66] Creating Layer conv1
I0905 23:04:07.937049   495 net.cpp:329] conv1 <- data
I0905 23:04:07.937057   495 net.cpp:290] conv1 -> conv1
I0905 23:04:07.938453   495 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:04:07.938473   495 net.cpp:125] conv1 needs backward computation.
I0905 23:04:07.938488   495 net.cpp:66] Creating Layer relu1
I0905 23:04:07.938493   495 net.cpp:329] relu1 <- conv1
I0905 23:04:07.938500   495 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:04:07.938508   495 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:04:07.938514   495 net.cpp:125] relu1 needs backward computation.
I0905 23:04:07.938521   495 net.cpp:66] Creating Layer pool1
I0905 23:04:07.938526   495 net.cpp:329] pool1 <- conv1
I0905 23:04:07.938534   495 net.cpp:290] pool1 -> pool1
I0905 23:04:07.938544   495 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:04:07.938550   495 net.cpp:125] pool1 needs backward computation.
I0905 23:04:07.938557   495 net.cpp:66] Creating Layer norm1
I0905 23:04:07.938562   495 net.cpp:329] norm1 <- pool1
I0905 23:04:07.938570   495 net.cpp:290] norm1 -> norm1
I0905 23:04:07.938580   495 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:04:07.938585   495 net.cpp:125] norm1 needs backward computation.
I0905 23:04:07.938592   495 net.cpp:66] Creating Layer conv2
I0905 23:04:07.938597   495 net.cpp:329] conv2 <- norm1
I0905 23:04:07.938604   495 net.cpp:290] conv2 -> conv2
I0905 23:04:07.947726   495 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:04:07.947741   495 net.cpp:125] conv2 needs backward computation.
I0905 23:04:07.947747   495 net.cpp:66] Creating Layer relu2
I0905 23:04:07.947753   495 net.cpp:329] relu2 <- conv2
I0905 23:04:07.947760   495 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:04:07.947767   495 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:04:07.947773   495 net.cpp:125] relu2 needs backward computation.
I0905 23:04:07.947779   495 net.cpp:66] Creating Layer pool2
I0905 23:04:07.947784   495 net.cpp:329] pool2 <- conv2
I0905 23:04:07.947790   495 net.cpp:290] pool2 -> pool2
I0905 23:04:07.947798   495 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:04:07.947804   495 net.cpp:125] pool2 needs backward computation.
I0905 23:04:07.947813   495 net.cpp:66] Creating Layer fc7
I0905 23:04:07.947819   495 net.cpp:329] fc7 <- pool2
I0905 23:04:07.947826   495 net.cpp:290] fc7 -> fc7
I0905 23:04:08.598294   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.598337   495 net.cpp:125] fc7 needs backward computation.
I0905 23:04:08.598351   495 net.cpp:66] Creating Layer relu7
I0905 23:04:08.598357   495 net.cpp:329] relu7 <- fc7
I0905 23:04:08.598366   495 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:04:08.598376   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.598382   495 net.cpp:125] relu7 needs backward computation.
I0905 23:04:08.598389   495 net.cpp:66] Creating Layer drop7
I0905 23:04:08.598395   495 net.cpp:329] drop7 <- fc7
I0905 23:04:08.598402   495 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:04:08.598412   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.598417   495 net.cpp:125] drop7 needs backward computation.
I0905 23:04:08.598426   495 net.cpp:66] Creating Layer fc8
I0905 23:04:08.598431   495 net.cpp:329] fc8 <- fc7
I0905 23:04:08.598440   495 net.cpp:290] fc8 -> fc8
I0905 23:04:08.606219   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.606230   495 net.cpp:125] fc8 needs backward computation.
I0905 23:04:08.606237   495 net.cpp:66] Creating Layer relu8
I0905 23:04:08.606243   495 net.cpp:329] relu8 <- fc8
I0905 23:04:08.606252   495 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:04:08.606259   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.606266   495 net.cpp:125] relu8 needs backward computation.
I0905 23:04:08.606272   495 net.cpp:66] Creating Layer drop8
I0905 23:04:08.606277   495 net.cpp:329] drop8 <- fc8
I0905 23:04:08.606283   495 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:04:08.606290   495 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:04:08.606295   495 net.cpp:125] drop8 needs backward computation.
I0905 23:04:08.606304   495 net.cpp:66] Creating Layer fc9
I0905 23:04:08.606310   495 net.cpp:329] fc9 <- fc8
I0905 23:04:08.606317   495 net.cpp:290] fc9 -> fc9
I0905 23:04:08.606689   495 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:04:08.606710   495 net.cpp:125] fc9 needs backward computation.
I0905 23:04:08.606719   495 net.cpp:66] Creating Layer fc10
I0905 23:04:08.606725   495 net.cpp:329] fc10 <- fc9
I0905 23:04:08.606734   495 net.cpp:290] fc10 -> fc10
I0905 23:04:08.606745   495 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:04:08.606752   495 net.cpp:125] fc10 needs backward computation.
I0905 23:04:08.606760   495 net.cpp:66] Creating Layer prob
I0905 23:04:08.606765   495 net.cpp:329] prob <- fc10
I0905 23:04:08.606772   495 net.cpp:290] prob -> prob
I0905 23:04:08.606782   495 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:04:08.606787   495 net.cpp:125] prob needs backward computation.
I0905 23:04:08.606792   495 net.cpp:156] This network produces output prob
I0905 23:04:08.606806   495 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:04:08.606813   495 net.cpp:167] Network initialization done.
I0905 23:04:08.606818   495 net.cpp:168] Memory required for data: 6183480
Classifying 478 inputs.
Done in 459.34 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 78 is out of bounds for axis 0 with size 78
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:12:14.598273   523 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:12:14.598487   523 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:12:14.598501   523 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:12:14.598733   523 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:12:14.598830   523 net.cpp:292] Input 0 -> data
I0905 23:12:14.598870   523 net.cpp:66] Creating Layer conv1
I0905 23:12:14.598881   523 net.cpp:329] conv1 <- data
I0905 23:12:14.598896   523 net.cpp:290] conv1 -> conv1
I0905 23:12:14.615042   523 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:12:14.615067   523 net.cpp:125] conv1 needs backward computation.
I0905 23:12:14.615077   523 net.cpp:66] Creating Layer relu1
I0905 23:12:14.615083   523 net.cpp:329] relu1 <- conv1
I0905 23:12:14.615090   523 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:12:14.615099   523 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:12:14.615105   523 net.cpp:125] relu1 needs backward computation.
I0905 23:12:14.615113   523 net.cpp:66] Creating Layer pool1
I0905 23:12:14.615118   523 net.cpp:329] pool1 <- conv1
I0905 23:12:14.615125   523 net.cpp:290] pool1 -> pool1
I0905 23:12:14.615136   523 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:12:14.615142   523 net.cpp:125] pool1 needs backward computation.
I0905 23:12:14.615149   523 net.cpp:66] Creating Layer norm1
I0905 23:12:14.615154   523 net.cpp:329] norm1 <- pool1
I0905 23:12:14.615161   523 net.cpp:290] norm1 -> norm1
I0905 23:12:14.615171   523 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:12:14.615177   523 net.cpp:125] norm1 needs backward computation.
I0905 23:12:14.615185   523 net.cpp:66] Creating Layer conv2
I0905 23:12:14.615190   523 net.cpp:329] conv2 <- norm1
I0905 23:12:14.615197   523 net.cpp:290] conv2 -> conv2
I0905 23:12:14.624531   523 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:12:14.624547   523 net.cpp:125] conv2 needs backward computation.
I0905 23:12:14.624554   523 net.cpp:66] Creating Layer relu2
I0905 23:12:14.624560   523 net.cpp:329] relu2 <- conv2
I0905 23:12:14.624567   523 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:12:14.624575   523 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:12:14.624582   523 net.cpp:125] relu2 needs backward computation.
I0905 23:12:14.624588   523 net.cpp:66] Creating Layer pool2
I0905 23:12:14.624593   523 net.cpp:329] pool2 <- conv2
I0905 23:12:14.624600   523 net.cpp:290] pool2 -> pool2
I0905 23:12:14.624608   523 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:12:14.624614   523 net.cpp:125] pool2 needs backward computation.
I0905 23:12:14.624624   523 net.cpp:66] Creating Layer fc7
I0905 23:12:14.624629   523 net.cpp:329] fc7 <- pool2
I0905 23:12:14.624637   523 net.cpp:290] fc7 -> fc7
I0905 23:12:15.270797   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.270843   523 net.cpp:125] fc7 needs backward computation.
I0905 23:12:15.270856   523 net.cpp:66] Creating Layer relu7
I0905 23:12:15.270864   523 net.cpp:329] relu7 <- fc7
I0905 23:12:15.270872   523 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:12:15.270882   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.270889   523 net.cpp:125] relu7 needs backward computation.
I0905 23:12:15.270896   523 net.cpp:66] Creating Layer drop7
I0905 23:12:15.270901   523 net.cpp:329] drop7 <- fc7
I0905 23:12:15.270907   523 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:12:15.270931   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.270936   523 net.cpp:125] drop7 needs backward computation.
I0905 23:12:15.270944   523 net.cpp:66] Creating Layer fc8
I0905 23:12:15.270951   523 net.cpp:329] fc8 <- fc7
I0905 23:12:15.270959   523 net.cpp:290] fc8 -> fc8
I0905 23:12:15.278733   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.278745   523 net.cpp:125] fc8 needs backward computation.
I0905 23:12:15.278753   523 net.cpp:66] Creating Layer relu8
I0905 23:12:15.278758   523 net.cpp:329] relu8 <- fc8
I0905 23:12:15.278765   523 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:12:15.278774   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.278779   523 net.cpp:125] relu8 needs backward computation.
I0905 23:12:15.278785   523 net.cpp:66] Creating Layer drop8
I0905 23:12:15.278790   523 net.cpp:329] drop8 <- fc8
I0905 23:12:15.278796   523 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:12:15.278803   523 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:12:15.278808   523 net.cpp:125] drop8 needs backward computation.
I0905 23:12:15.278817   523 net.cpp:66] Creating Layer fc9
I0905 23:12:15.278823   523 net.cpp:329] fc9 <- fc8
I0905 23:12:15.278830   523 net.cpp:290] fc9 -> fc9
I0905 23:12:15.279203   523 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:12:15.279214   523 net.cpp:125] fc9 needs backward computation.
I0905 23:12:15.279222   523 net.cpp:66] Creating Layer fc10
I0905 23:12:15.279228   523 net.cpp:329] fc10 <- fc9
I0905 23:12:15.279237   523 net.cpp:290] fc10 -> fc10
I0905 23:12:15.279248   523 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:12:15.279255   523 net.cpp:125] fc10 needs backward computation.
I0905 23:12:15.279263   523 net.cpp:66] Creating Layer prob
I0905 23:12:15.279268   523 net.cpp:329] prob <- fc10
I0905 23:12:15.279276   523 net.cpp:290] prob -> prob
I0905 23:12:15.279286   523 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:12:15.279291   523 net.cpp:125] prob needs backward computation.
I0905 23:12:15.279296   523 net.cpp:156] This network produces output prob
I0905 23:12:15.279309   523 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:12:15.279317   523 net.cpp:167] Network initialization done.
I0905 23:12:15.279322   523 net.cpp:168] Memory required for data: 6183480
/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py:48: RuntimeWarning: invalid value encountered in divide
  im_std = (im - im_min) / (im_max - im_min)
Classifying 162 inputs.
Done in 105.56 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 62 is out of bounds for axis 0 with size 62
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:14:06.745115   529 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:14:06.745260   529 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:14:06.745268   529 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:14:06.745421   529 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:14:06.745487   529 net.cpp:292] Input 0 -> data
I0905 23:14:06.745515   529 net.cpp:66] Creating Layer conv1
I0905 23:14:06.745522   529 net.cpp:329] conv1 <- data
I0905 23:14:06.745530   529 net.cpp:290] conv1 -> conv1
I0905 23:14:06.746933   529 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:14:06.746953   529 net.cpp:125] conv1 needs backward computation.
I0905 23:14:06.746963   529 net.cpp:66] Creating Layer relu1
I0905 23:14:06.746968   529 net.cpp:329] relu1 <- conv1
I0905 23:14:06.746975   529 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:14:06.746984   529 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:14:06.746989   529 net.cpp:125] relu1 needs backward computation.
I0905 23:14:06.746996   529 net.cpp:66] Creating Layer pool1
I0905 23:14:06.747001   529 net.cpp:329] pool1 <- conv1
I0905 23:14:06.747009   529 net.cpp:290] pool1 -> pool1
I0905 23:14:06.747020   529 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:14:06.747025   529 net.cpp:125] pool1 needs backward computation.
I0905 23:14:06.747032   529 net.cpp:66] Creating Layer norm1
I0905 23:14:06.747037   529 net.cpp:329] norm1 <- pool1
I0905 23:14:06.747045   529 net.cpp:290] norm1 -> norm1
I0905 23:14:06.747053   529 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:14:06.747059   529 net.cpp:125] norm1 needs backward computation.
I0905 23:14:06.747067   529 net.cpp:66] Creating Layer conv2
I0905 23:14:06.747072   529 net.cpp:329] conv2 <- norm1
I0905 23:14:06.747079   529 net.cpp:290] conv2 -> conv2
I0905 23:14:06.756405   529 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:14:06.756422   529 net.cpp:125] conv2 needs backward computation.
I0905 23:14:06.756435   529 net.cpp:66] Creating Layer relu2
I0905 23:14:06.756441   529 net.cpp:329] relu2 <- conv2
I0905 23:14:06.756448   529 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:14:06.756455   529 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:14:06.756461   529 net.cpp:125] relu2 needs backward computation.
I0905 23:14:06.756467   529 net.cpp:66] Creating Layer pool2
I0905 23:14:06.756474   529 net.cpp:329] pool2 <- conv2
I0905 23:14:06.756479   529 net.cpp:290] pool2 -> pool2
I0905 23:14:06.756487   529 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:14:06.756494   529 net.cpp:125] pool2 needs backward computation.
I0905 23:14:06.756505   529 net.cpp:66] Creating Layer fc7
I0905 23:14:06.756510   529 net.cpp:329] fc7 <- pool2
I0905 23:14:06.756517   529 net.cpp:290] fc7 -> fc7
I0905 23:14:07.407614   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.407661   529 net.cpp:125] fc7 needs backward computation.
I0905 23:14:07.407673   529 net.cpp:66] Creating Layer relu7
I0905 23:14:07.407681   529 net.cpp:329] relu7 <- fc7
I0905 23:14:07.407691   529 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:14:07.407701   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.407706   529 net.cpp:125] relu7 needs backward computation.
I0905 23:14:07.407714   529 net.cpp:66] Creating Layer drop7
I0905 23:14:07.407719   529 net.cpp:329] drop7 <- fc7
I0905 23:14:07.407727   529 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:14:07.407737   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.407742   529 net.cpp:125] drop7 needs backward computation.
I0905 23:14:07.407752   529 net.cpp:66] Creating Layer fc8
I0905 23:14:07.407757   529 net.cpp:329] fc8 <- fc7
I0905 23:14:07.407766   529 net.cpp:290] fc8 -> fc8
I0905 23:14:07.415551   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.415565   529 net.cpp:125] fc8 needs backward computation.
I0905 23:14:07.415571   529 net.cpp:66] Creating Layer relu8
I0905 23:14:07.415577   529 net.cpp:329] relu8 <- fc8
I0905 23:14:07.415585   529 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:14:07.415592   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.415598   529 net.cpp:125] relu8 needs backward computation.
I0905 23:14:07.415604   529 net.cpp:66] Creating Layer drop8
I0905 23:14:07.415609   529 net.cpp:329] drop8 <- fc8
I0905 23:14:07.415616   529 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:14:07.415622   529 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:14:07.415628   529 net.cpp:125] drop8 needs backward computation.
I0905 23:14:07.415637   529 net.cpp:66] Creating Layer fc9
I0905 23:14:07.415643   529 net.cpp:329] fc9 <- fc8
I0905 23:14:07.415650   529 net.cpp:290] fc9 -> fc9
I0905 23:14:07.416024   529 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:14:07.416036   529 net.cpp:125] fc9 needs backward computation.
I0905 23:14:07.416044   529 net.cpp:66] Creating Layer fc10
I0905 23:14:07.416050   529 net.cpp:329] fc10 <- fc9
I0905 23:14:07.416059   529 net.cpp:290] fc10 -> fc10
I0905 23:14:07.416071   529 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:14:07.416079   529 net.cpp:125] fc10 needs backward computation.
I0905 23:14:07.416085   529 net.cpp:66] Creating Layer prob
I0905 23:14:07.416091   529 net.cpp:329] prob <- fc10
I0905 23:14:07.416098   529 net.cpp:290] prob -> prob
I0905 23:14:07.416108   529 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:14:07.416115   529 net.cpp:125] prob needs backward computation.
I0905 23:14:07.416120   529 net.cpp:156] This network produces output prob
I0905 23:14:07.416131   529 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:14:07.416141   529 net.cpp:167] Network initialization done.
I0905 23:14:07.416146   529 net.cpp:168] Memory required for data: 6183480
Classifying 385 inputs.
Done in 244.13 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 85 is out of bounds for axis 0 with size 85
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:18:18.609387   543 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:18:18.609526   543 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:18:18.609535   543 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:18:18.609695   543 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:18:18.609750   543 net.cpp:292] Input 0 -> data
I0905 23:18:18.609776   543 net.cpp:66] Creating Layer conv1
I0905 23:18:18.609782   543 net.cpp:329] conv1 <- data
I0905 23:18:18.609791   543 net.cpp:290] conv1 -> conv1
I0905 23:18:18.611150   543 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:18:18.611168   543 net.cpp:125] conv1 needs backward computation.
I0905 23:18:18.611187   543 net.cpp:66] Creating Layer relu1
I0905 23:18:18.611193   543 net.cpp:329] relu1 <- conv1
I0905 23:18:18.611201   543 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:18:18.611209   543 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:18:18.611215   543 net.cpp:125] relu1 needs backward computation.
I0905 23:18:18.611222   543 net.cpp:66] Creating Layer pool1
I0905 23:18:18.611227   543 net.cpp:329] pool1 <- conv1
I0905 23:18:18.611234   543 net.cpp:290] pool1 -> pool1
I0905 23:18:18.611245   543 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:18:18.611251   543 net.cpp:125] pool1 needs backward computation.
I0905 23:18:18.611258   543 net.cpp:66] Creating Layer norm1
I0905 23:18:18.611263   543 net.cpp:329] norm1 <- pool1
I0905 23:18:18.611270   543 net.cpp:290] norm1 -> norm1
I0905 23:18:18.611279   543 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:18:18.611285   543 net.cpp:125] norm1 needs backward computation.
I0905 23:18:18.611292   543 net.cpp:66] Creating Layer conv2
I0905 23:18:18.611299   543 net.cpp:329] conv2 <- norm1
I0905 23:18:18.611305   543 net.cpp:290] conv2 -> conv2
I0905 23:18:18.620448   543 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:18:18.620465   543 net.cpp:125] conv2 needs backward computation.
I0905 23:18:18.620472   543 net.cpp:66] Creating Layer relu2
I0905 23:18:18.620478   543 net.cpp:329] relu2 <- conv2
I0905 23:18:18.620486   543 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:18:18.620492   543 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:18:18.620497   543 net.cpp:125] relu2 needs backward computation.
I0905 23:18:18.620507   543 net.cpp:66] Creating Layer pool2
I0905 23:18:18.620512   543 net.cpp:329] pool2 <- conv2
I0905 23:18:18.620519   543 net.cpp:290] pool2 -> pool2
I0905 23:18:18.620527   543 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:18:18.620532   543 net.cpp:125] pool2 needs backward computation.
I0905 23:18:18.620540   543 net.cpp:66] Creating Layer fc7
I0905 23:18:18.620545   543 net.cpp:329] fc7 <- pool2
I0905 23:18:18.620553   543 net.cpp:290] fc7 -> fc7
I0905 23:18:19.282253   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.282299   543 net.cpp:125] fc7 needs backward computation.
I0905 23:18:19.282310   543 net.cpp:66] Creating Layer relu7
I0905 23:18:19.282318   543 net.cpp:329] relu7 <- fc7
I0905 23:18:19.282327   543 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:18:19.282337   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.282343   543 net.cpp:125] relu7 needs backward computation.
I0905 23:18:19.282351   543 net.cpp:66] Creating Layer drop7
I0905 23:18:19.282356   543 net.cpp:329] drop7 <- fc7
I0905 23:18:19.282362   543 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:18:19.282373   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.282379   543 net.cpp:125] drop7 needs backward computation.
I0905 23:18:19.282388   543 net.cpp:66] Creating Layer fc8
I0905 23:18:19.282393   543 net.cpp:329] fc8 <- fc7
I0905 23:18:19.282402   543 net.cpp:290] fc8 -> fc8
I0905 23:18:19.290345   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.290357   543 net.cpp:125] fc8 needs backward computation.
I0905 23:18:19.290364   543 net.cpp:66] Creating Layer relu8
I0905 23:18:19.290370   543 net.cpp:329] relu8 <- fc8
I0905 23:18:19.290379   543 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:18:19.290385   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.290391   543 net.cpp:125] relu8 needs backward computation.
I0905 23:18:19.290397   543 net.cpp:66] Creating Layer drop8
I0905 23:18:19.290403   543 net.cpp:329] drop8 <- fc8
I0905 23:18:19.290410   543 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:18:19.290416   543 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:18:19.290421   543 net.cpp:125] drop8 needs backward computation.
I0905 23:18:19.290431   543 net.cpp:66] Creating Layer fc9
I0905 23:18:19.290436   543 net.cpp:329] fc9 <- fc8
I0905 23:18:19.290442   543 net.cpp:290] fc9 -> fc9
I0905 23:18:19.290823   543 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:18:19.290848   543 net.cpp:125] fc9 needs backward computation.
I0905 23:18:19.290856   543 net.cpp:66] Creating Layer fc10
I0905 23:18:19.290863   543 net.cpp:329] fc10 <- fc9
I0905 23:18:19.290870   543 net.cpp:290] fc10 -> fc10
I0905 23:18:19.290882   543 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:18:19.290890   543 net.cpp:125] fc10 needs backward computation.
I0905 23:18:19.290896   543 net.cpp:66] Creating Layer prob
I0905 23:18:19.290902   543 net.cpp:329] prob <- fc10
I0905 23:18:19.290910   543 net.cpp:290] prob -> prob
I0905 23:18:19.290918   543 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:18:19.290925   543 net.cpp:125] prob needs backward computation.
I0905 23:18:19.290930   543 net.cpp:156] This network produces output prob
I0905 23:18:19.290942   543 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:18:19.290951   543 net.cpp:167] Network initialization done.
I0905 23:18:19.290956   543 net.cpp:168] Memory required for data: 6183480
Classifying 146 inputs.
Done in 87.63 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 46 is out of bounds for axis 0 with size 46
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:19:50.278383   549 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:19:50.278523   549 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:19:50.278532   549 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:19:50.278681   549 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:19:50.278743   549 net.cpp:292] Input 0 -> data
I0905 23:19:50.278770   549 net.cpp:66] Creating Layer conv1
I0905 23:19:50.278779   549 net.cpp:329] conv1 <- data
I0905 23:19:50.278786   549 net.cpp:290] conv1 -> conv1
I0905 23:19:50.280149   549 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:19:50.280167   549 net.cpp:125] conv1 needs backward computation.
I0905 23:19:50.280176   549 net.cpp:66] Creating Layer relu1
I0905 23:19:50.280182   549 net.cpp:329] relu1 <- conv1
I0905 23:19:50.280189   549 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:19:50.280199   549 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:19:50.280205   549 net.cpp:125] relu1 needs backward computation.
I0905 23:19:50.280211   549 net.cpp:66] Creating Layer pool1
I0905 23:19:50.280217   549 net.cpp:329] pool1 <- conv1
I0905 23:19:50.280225   549 net.cpp:290] pool1 -> pool1
I0905 23:19:50.280236   549 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:19:50.280241   549 net.cpp:125] pool1 needs backward computation.
I0905 23:19:50.280248   549 net.cpp:66] Creating Layer norm1
I0905 23:19:50.280254   549 net.cpp:329] norm1 <- pool1
I0905 23:19:50.280261   549 net.cpp:290] norm1 -> norm1
I0905 23:19:50.280272   549 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:19:50.280277   549 net.cpp:125] norm1 needs backward computation.
I0905 23:19:50.280284   549 net.cpp:66] Creating Layer conv2
I0905 23:19:50.280290   549 net.cpp:329] conv2 <- norm1
I0905 23:19:50.280297   549 net.cpp:290] conv2 -> conv2
I0905 23:19:50.289436   549 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:19:50.289451   549 net.cpp:125] conv2 needs backward computation.
I0905 23:19:50.289459   549 net.cpp:66] Creating Layer relu2
I0905 23:19:50.289465   549 net.cpp:329] relu2 <- conv2
I0905 23:19:50.289471   549 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:19:50.289479   549 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:19:50.289485   549 net.cpp:125] relu2 needs backward computation.
I0905 23:19:50.289491   549 net.cpp:66] Creating Layer pool2
I0905 23:19:50.289497   549 net.cpp:329] pool2 <- conv2
I0905 23:19:50.289505   549 net.cpp:290] pool2 -> pool2
I0905 23:19:50.289512   549 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:19:50.289517   549 net.cpp:125] pool2 needs backward computation.
I0905 23:19:50.289527   549 net.cpp:66] Creating Layer fc7
I0905 23:19:50.289533   549 net.cpp:329] fc7 <- pool2
I0905 23:19:50.289541   549 net.cpp:290] fc7 -> fc7
I0905 23:19:50.939445   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.939488   549 net.cpp:125] fc7 needs backward computation.
I0905 23:19:50.939501   549 net.cpp:66] Creating Layer relu7
I0905 23:19:50.939508   549 net.cpp:329] relu7 <- fc7
I0905 23:19:50.939517   549 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:19:50.939527   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.939533   549 net.cpp:125] relu7 needs backward computation.
I0905 23:19:50.939540   549 net.cpp:66] Creating Layer drop7
I0905 23:19:50.939546   549 net.cpp:329] drop7 <- fc7
I0905 23:19:50.939553   549 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:19:50.939573   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.939579   549 net.cpp:125] drop7 needs backward computation.
I0905 23:19:50.939587   549 net.cpp:66] Creating Layer fc8
I0905 23:19:50.939594   549 net.cpp:329] fc8 <- fc7
I0905 23:19:50.939602   549 net.cpp:290] fc8 -> fc8
I0905 23:19:50.947407   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.947420   549 net.cpp:125] fc8 needs backward computation.
I0905 23:19:50.947427   549 net.cpp:66] Creating Layer relu8
I0905 23:19:50.947433   549 net.cpp:329] relu8 <- fc8
I0905 23:19:50.947442   549 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:19:50.947449   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.947454   549 net.cpp:125] relu8 needs backward computation.
I0905 23:19:50.947461   549 net.cpp:66] Creating Layer drop8
I0905 23:19:50.947468   549 net.cpp:329] drop8 <- fc8
I0905 23:19:50.947474   549 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:19:50.947480   549 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:19:50.947486   549 net.cpp:125] drop8 needs backward computation.
I0905 23:19:50.947495   549 net.cpp:66] Creating Layer fc9
I0905 23:19:50.947501   549 net.cpp:329] fc9 <- fc8
I0905 23:19:50.947509   549 net.cpp:290] fc9 -> fc9
I0905 23:19:50.947882   549 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:19:50.947895   549 net.cpp:125] fc9 needs backward computation.
I0905 23:19:50.947902   549 net.cpp:66] Creating Layer fc10
I0905 23:19:50.947908   549 net.cpp:329] fc10 <- fc9
I0905 23:19:50.947916   549 net.cpp:290] fc10 -> fc10
I0905 23:19:50.947929   549 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:19:50.947937   549 net.cpp:125] fc10 needs backward computation.
I0905 23:19:50.947943   549 net.cpp:66] Creating Layer prob
I0905 23:19:50.947949   549 net.cpp:329] prob <- fc10
I0905 23:19:50.947957   549 net.cpp:290] prob -> prob
I0905 23:19:50.947967   549 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:19:50.947973   549 net.cpp:125] prob needs backward computation.
I0905 23:19:50.947978   549 net.cpp:156] This network produces output prob
I0905 23:19:50.947990   549 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:19:50.947999   549 net.cpp:167] Network initialization done.
I0905 23:19:50.948004   549 net.cpp:168] Memory required for data: 6183480
Classifying 15 inputs.
Done in 9.53 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:20:01.508533   553 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:20:01.508671   553 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:20:01.508680   553 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:20:01.508826   553 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:20:01.508889   553 net.cpp:292] Input 0 -> data
I0905 23:20:01.508916   553 net.cpp:66] Creating Layer conv1
I0905 23:20:01.508924   553 net.cpp:329] conv1 <- data
I0905 23:20:01.508931   553 net.cpp:290] conv1 -> conv1
I0905 23:20:01.510306   553 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:20:01.510325   553 net.cpp:125] conv1 needs backward computation.
I0905 23:20:01.510334   553 net.cpp:66] Creating Layer relu1
I0905 23:20:01.510340   553 net.cpp:329] relu1 <- conv1
I0905 23:20:01.510347   553 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:20:01.510355   553 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:20:01.510361   553 net.cpp:125] relu1 needs backward computation.
I0905 23:20:01.510368   553 net.cpp:66] Creating Layer pool1
I0905 23:20:01.510375   553 net.cpp:329] pool1 <- conv1
I0905 23:20:01.510380   553 net.cpp:290] pool1 -> pool1
I0905 23:20:01.510391   553 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:20:01.510397   553 net.cpp:125] pool1 needs backward computation.
I0905 23:20:01.510404   553 net.cpp:66] Creating Layer norm1
I0905 23:20:01.510409   553 net.cpp:329] norm1 <- pool1
I0905 23:20:01.510416   553 net.cpp:290] norm1 -> norm1
I0905 23:20:01.510426   553 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:20:01.510432   553 net.cpp:125] norm1 needs backward computation.
I0905 23:20:01.510438   553 net.cpp:66] Creating Layer conv2
I0905 23:20:01.510444   553 net.cpp:329] conv2 <- norm1
I0905 23:20:01.510452   553 net.cpp:290] conv2 -> conv2
I0905 23:20:01.519582   553 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:20:01.519597   553 net.cpp:125] conv2 needs backward computation.
I0905 23:20:01.519603   553 net.cpp:66] Creating Layer relu2
I0905 23:20:01.519609   553 net.cpp:329] relu2 <- conv2
I0905 23:20:01.519616   553 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:20:01.519623   553 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:20:01.519629   553 net.cpp:125] relu2 needs backward computation.
I0905 23:20:01.519635   553 net.cpp:66] Creating Layer pool2
I0905 23:20:01.519645   553 net.cpp:329] pool2 <- conv2
I0905 23:20:01.519652   553 net.cpp:290] pool2 -> pool2
I0905 23:20:01.519661   553 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:20:01.519666   553 net.cpp:125] pool2 needs backward computation.
I0905 23:20:01.519676   553 net.cpp:66] Creating Layer fc7
I0905 23:20:01.519681   553 net.cpp:329] fc7 <- pool2
I0905 23:20:01.519690   553 net.cpp:290] fc7 -> fc7
I0905 23:20:02.172219   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.172265   553 net.cpp:125] fc7 needs backward computation.
I0905 23:20:02.172276   553 net.cpp:66] Creating Layer relu7
I0905 23:20:02.172283   553 net.cpp:329] relu7 <- fc7
I0905 23:20:02.172292   553 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:20:02.172303   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.172308   553 net.cpp:125] relu7 needs backward computation.
I0905 23:20:02.172317   553 net.cpp:66] Creating Layer drop7
I0905 23:20:02.172322   553 net.cpp:329] drop7 <- fc7
I0905 23:20:02.172328   553 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:20:02.172339   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.172344   553 net.cpp:125] drop7 needs backward computation.
I0905 23:20:02.172353   553 net.cpp:66] Creating Layer fc8
I0905 23:20:02.172358   553 net.cpp:329] fc8 <- fc7
I0905 23:20:02.172368   553 net.cpp:290] fc8 -> fc8
I0905 23:20:02.180166   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.180178   553 net.cpp:125] fc8 needs backward computation.
I0905 23:20:02.180186   553 net.cpp:66] Creating Layer relu8
I0905 23:20:02.180191   553 net.cpp:329] relu8 <- fc8
I0905 23:20:02.180202   553 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:20:02.180208   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.180213   553 net.cpp:125] relu8 needs backward computation.
I0905 23:20:02.180220   553 net.cpp:66] Creating Layer drop8
I0905 23:20:02.180225   553 net.cpp:329] drop8 <- fc8
I0905 23:20:02.180232   553 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:20:02.180238   553 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:20:02.180244   553 net.cpp:125] drop8 needs backward computation.
I0905 23:20:02.180253   553 net.cpp:66] Creating Layer fc9
I0905 23:20:02.180258   553 net.cpp:329] fc9 <- fc8
I0905 23:20:02.180265   553 net.cpp:290] fc9 -> fc9
I0905 23:20:02.180639   553 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:20:02.180649   553 net.cpp:125] fc9 needs backward computation.
I0905 23:20:02.180658   553 net.cpp:66] Creating Layer fc10
I0905 23:20:02.180663   553 net.cpp:329] fc10 <- fc9
I0905 23:20:02.180671   553 net.cpp:290] fc10 -> fc10
I0905 23:20:02.180683   553 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:20:02.180691   553 net.cpp:125] fc10 needs backward computation.
I0905 23:20:02.180698   553 net.cpp:66] Creating Layer prob
I0905 23:20:02.180703   553 net.cpp:329] prob <- fc10
I0905 23:20:02.180711   553 net.cpp:290] prob -> prob
I0905 23:20:02.180721   553 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:20:02.180726   553 net.cpp:125] prob needs backward computation.
I0905 23:20:02.180730   553 net.cpp:156] This network produces output prob
I0905 23:20:02.180743   553 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:20:02.180752   553 net.cpp:167] Network initialization done.
I0905 23:20:02.180757   553 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:22:57.583324   562 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:22:57.583524   562 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:22:57.583534   562 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:22:57.583762   562 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:22:57.583824   562 net.cpp:292] Input 0 -> data
I0905 23:22:57.583869   562 net.cpp:66] Creating Layer conv1
I0905 23:22:57.583878   562 net.cpp:329] conv1 <- data
I0905 23:22:57.583886   562 net.cpp:290] conv1 -> conv1
I0905 23:22:57.616081   562 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:22:57.616113   562 net.cpp:125] conv1 needs backward computation.
I0905 23:22:57.616123   562 net.cpp:66] Creating Layer relu1
I0905 23:22:57.616130   562 net.cpp:329] relu1 <- conv1
I0905 23:22:57.616137   562 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:22:57.616145   562 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:22:57.616152   562 net.cpp:125] relu1 needs backward computation.
I0905 23:22:57.616158   562 net.cpp:66] Creating Layer pool1
I0905 23:22:57.616164   562 net.cpp:329] pool1 <- conv1
I0905 23:22:57.616171   562 net.cpp:290] pool1 -> pool1
I0905 23:22:57.616183   562 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:22:57.616189   562 net.cpp:125] pool1 needs backward computation.
I0905 23:22:57.616195   562 net.cpp:66] Creating Layer norm1
I0905 23:22:57.616206   562 net.cpp:329] norm1 <- pool1
I0905 23:22:57.616214   562 net.cpp:290] norm1 -> norm1
I0905 23:22:57.616225   562 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:22:57.616230   562 net.cpp:125] norm1 needs backward computation.
I0905 23:22:57.616237   562 net.cpp:66] Creating Layer conv2
I0905 23:22:57.616243   562 net.cpp:329] conv2 <- norm1
I0905 23:22:57.616250   562 net.cpp:290] conv2 -> conv2
I0905 23:22:57.625408   562 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:22:57.625424   562 net.cpp:125] conv2 needs backward computation.
I0905 23:22:57.625432   562 net.cpp:66] Creating Layer relu2
I0905 23:22:57.625437   562 net.cpp:329] relu2 <- conv2
I0905 23:22:57.625444   562 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:22:57.625452   562 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:22:57.625458   562 net.cpp:125] relu2 needs backward computation.
I0905 23:22:57.625464   562 net.cpp:66] Creating Layer pool2
I0905 23:22:57.625469   562 net.cpp:329] pool2 <- conv2
I0905 23:22:57.625476   562 net.cpp:290] pool2 -> pool2
I0905 23:22:57.625484   562 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:22:57.625490   562 net.cpp:125] pool2 needs backward computation.
I0905 23:22:57.625499   562 net.cpp:66] Creating Layer fc7
I0905 23:22:57.625505   562 net.cpp:329] fc7 <- pool2
I0905 23:22:57.625514   562 net.cpp:290] fc7 -> fc7
I0905 23:22:58.274008   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.274055   562 net.cpp:125] fc7 needs backward computation.
I0905 23:22:58.274067   562 net.cpp:66] Creating Layer relu7
I0905 23:22:58.274075   562 net.cpp:329] relu7 <- fc7
I0905 23:22:58.274085   562 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:22:58.274094   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.274101   562 net.cpp:125] relu7 needs backward computation.
I0905 23:22:58.274107   562 net.cpp:66] Creating Layer drop7
I0905 23:22:58.274113   562 net.cpp:329] drop7 <- fc7
I0905 23:22:58.274119   562 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:22:58.274130   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.274137   562 net.cpp:125] drop7 needs backward computation.
I0905 23:22:58.274145   562 net.cpp:66] Creating Layer fc8
I0905 23:22:58.274150   562 net.cpp:329] fc8 <- fc7
I0905 23:22:58.274159   562 net.cpp:290] fc8 -> fc8
I0905 23:22:58.281940   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.281952   562 net.cpp:125] fc8 needs backward computation.
I0905 23:22:58.281960   562 net.cpp:66] Creating Layer relu8
I0905 23:22:58.281965   562 net.cpp:329] relu8 <- fc8
I0905 23:22:58.281973   562 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:22:58.281980   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.281986   562 net.cpp:125] relu8 needs backward computation.
I0905 23:22:58.281992   562 net.cpp:66] Creating Layer drop8
I0905 23:22:58.281998   562 net.cpp:329] drop8 <- fc8
I0905 23:22:58.282004   562 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:22:58.282011   562 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:22:58.282017   562 net.cpp:125] drop8 needs backward computation.
I0905 23:22:58.282026   562 net.cpp:66] Creating Layer fc9
I0905 23:22:58.282032   562 net.cpp:329] fc9 <- fc8
I0905 23:22:58.282038   562 net.cpp:290] fc9 -> fc9
I0905 23:22:58.282412   562 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:22:58.282423   562 net.cpp:125] fc9 needs backward computation.
I0905 23:22:58.282431   562 net.cpp:66] Creating Layer fc10
I0905 23:22:58.282438   562 net.cpp:329] fc10 <- fc9
I0905 23:22:58.282446   562 net.cpp:290] fc10 -> fc10
I0905 23:22:58.282457   562 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:22:58.282465   562 net.cpp:125] fc10 needs backward computation.
I0905 23:22:58.282472   562 net.cpp:66] Creating Layer prob
I0905 23:22:58.282479   562 net.cpp:329] prob <- fc10
I0905 23:22:58.282485   562 net.cpp:290] prob -> prob
I0905 23:22:58.282495   562 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:22:58.282501   562 net.cpp:125] prob needs backward computation.
I0905 23:22:58.282516   562 net.cpp:156] This network produces output prob
I0905 23:22:58.282531   562 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:22:58.282538   562 net.cpp:167] Network initialization done.
I0905 23:22:58.282544   562 net.cpp:168] Memory required for data: 6183480
Classifying 29 inputs.
Done in 18.37 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:23:20.589509   566 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:23:20.589669   566 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:23:20.589680   566 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:23:20.589831   566 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:23:20.589885   566 net.cpp:292] Input 0 -> data
I0905 23:23:20.589921   566 net.cpp:66] Creating Layer conv1
I0905 23:23:20.589929   566 net.cpp:329] conv1 <- data
I0905 23:23:20.589938   566 net.cpp:290] conv1 -> conv1
I0905 23:23:20.591342   566 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:23:20.591361   566 net.cpp:125] conv1 needs backward computation.
I0905 23:23:20.591370   566 net.cpp:66] Creating Layer relu1
I0905 23:23:20.591377   566 net.cpp:329] relu1 <- conv1
I0905 23:23:20.591384   566 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:23:20.591393   566 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:23:20.591399   566 net.cpp:125] relu1 needs backward computation.
I0905 23:23:20.591406   566 net.cpp:66] Creating Layer pool1
I0905 23:23:20.591413   566 net.cpp:329] pool1 <- conv1
I0905 23:23:20.591419   566 net.cpp:290] pool1 -> pool1
I0905 23:23:20.591430   566 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:23:20.591436   566 net.cpp:125] pool1 needs backward computation.
I0905 23:23:20.591444   566 net.cpp:66] Creating Layer norm1
I0905 23:23:20.591449   566 net.cpp:329] norm1 <- pool1
I0905 23:23:20.591456   566 net.cpp:290] norm1 -> norm1
I0905 23:23:20.591467   566 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:23:20.591473   566 net.cpp:125] norm1 needs backward computation.
I0905 23:23:20.591480   566 net.cpp:66] Creating Layer conv2
I0905 23:23:20.591486   566 net.cpp:329] conv2 <- norm1
I0905 23:23:20.591495   566 net.cpp:290] conv2 -> conv2
I0905 23:23:20.600842   566 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:23:20.600857   566 net.cpp:125] conv2 needs backward computation.
I0905 23:23:20.600864   566 net.cpp:66] Creating Layer relu2
I0905 23:23:20.600870   566 net.cpp:329] relu2 <- conv2
I0905 23:23:20.600878   566 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:23:20.600884   566 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:23:20.600890   566 net.cpp:125] relu2 needs backward computation.
I0905 23:23:20.600896   566 net.cpp:66] Creating Layer pool2
I0905 23:23:20.600903   566 net.cpp:329] pool2 <- conv2
I0905 23:23:20.600908   566 net.cpp:290] pool2 -> pool2
I0905 23:23:20.600916   566 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:23:20.600922   566 net.cpp:125] pool2 needs backward computation.
I0905 23:23:20.600931   566 net.cpp:66] Creating Layer fc7
I0905 23:23:20.600937   566 net.cpp:329] fc7 <- pool2
I0905 23:23:20.600945   566 net.cpp:290] fc7 -> fc7
I0905 23:23:21.248211   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.248255   566 net.cpp:125] fc7 needs backward computation.
I0905 23:23:21.248268   566 net.cpp:66] Creating Layer relu7
I0905 23:23:21.248275   566 net.cpp:329] relu7 <- fc7
I0905 23:23:21.248286   566 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:23:21.248296   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.248302   566 net.cpp:125] relu7 needs backward computation.
I0905 23:23:21.248309   566 net.cpp:66] Creating Layer drop7
I0905 23:23:21.248314   566 net.cpp:329] drop7 <- fc7
I0905 23:23:21.248322   566 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:23:21.248332   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.248338   566 net.cpp:125] drop7 needs backward computation.
I0905 23:23:21.248347   566 net.cpp:66] Creating Layer fc8
I0905 23:23:21.248353   566 net.cpp:329] fc8 <- fc7
I0905 23:23:21.248361   566 net.cpp:290] fc8 -> fc8
I0905 23:23:21.256122   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.256134   566 net.cpp:125] fc8 needs backward computation.
I0905 23:23:21.256142   566 net.cpp:66] Creating Layer relu8
I0905 23:23:21.256147   566 net.cpp:329] relu8 <- fc8
I0905 23:23:21.256157   566 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:23:21.256165   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.256171   566 net.cpp:125] relu8 needs backward computation.
I0905 23:23:21.256177   566 net.cpp:66] Creating Layer drop8
I0905 23:23:21.256183   566 net.cpp:329] drop8 <- fc8
I0905 23:23:21.256189   566 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:23:21.256196   566 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:21.256217   566 net.cpp:125] drop8 needs backward computation.
I0905 23:23:21.256227   566 net.cpp:66] Creating Layer fc9
I0905 23:23:21.256232   566 net.cpp:329] fc9 <- fc8
I0905 23:23:21.256240   566 net.cpp:290] fc9 -> fc9
I0905 23:23:21.256613   566 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:23:21.256624   566 net.cpp:125] fc9 needs backward computation.
I0905 23:23:21.256633   566 net.cpp:66] Creating Layer fc10
I0905 23:23:21.256639   566 net.cpp:329] fc10 <- fc9
I0905 23:23:21.256647   566 net.cpp:290] fc10 -> fc10
I0905 23:23:21.256666   566 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:23:21.256675   566 net.cpp:125] fc10 needs backward computation.
I0905 23:23:21.256682   566 net.cpp:66] Creating Layer prob
I0905 23:23:21.256688   566 net.cpp:329] prob <- fc10
I0905 23:23:21.256696   566 net.cpp:290] prob -> prob
I0905 23:23:21.256706   566 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:23:21.256712   566 net.cpp:125] prob needs backward computation.
I0905 23:23:21.256717   566 net.cpp:156] This network produces output prob
I0905 23:23:21.256731   566 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:23:21.256739   566 net.cpp:167] Network initialization done.
I0905 23:23:21.256746   566 net.cpp:168] Memory required for data: 6183480
Classifying 100 inputs.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 132, in main
    predictions = classifier.predict(inputs[:reminder], not args.center_only)
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/classifier.py", line 64, in predict
    self.image_dims[0], self.image_dims[1], inputs[0].shape[2]),
IndexError: list index out of range
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:23:24.399255   570 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:23:24.399392   570 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:23:24.399401   570 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:23:24.399548   570 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:23:24.399610   570 net.cpp:292] Input 0 -> data
I0905 23:23:24.399636   570 net.cpp:66] Creating Layer conv1
I0905 23:23:24.399643   570 net.cpp:329] conv1 <- data
I0905 23:23:24.399652   570 net.cpp:290] conv1 -> conv1
I0905 23:23:24.401012   570 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:23:24.401031   570 net.cpp:125] conv1 needs backward computation.
I0905 23:23:24.401039   570 net.cpp:66] Creating Layer relu1
I0905 23:23:24.401046   570 net.cpp:329] relu1 <- conv1
I0905 23:23:24.401052   570 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:23:24.401060   570 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:23:24.401067   570 net.cpp:125] relu1 needs backward computation.
I0905 23:23:24.401073   570 net.cpp:66] Creating Layer pool1
I0905 23:23:24.401078   570 net.cpp:329] pool1 <- conv1
I0905 23:23:24.401085   570 net.cpp:290] pool1 -> pool1
I0905 23:23:24.401098   570 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:23:24.401103   570 net.cpp:125] pool1 needs backward computation.
I0905 23:23:24.401109   570 net.cpp:66] Creating Layer norm1
I0905 23:23:24.401115   570 net.cpp:329] norm1 <- pool1
I0905 23:23:24.401121   570 net.cpp:290] norm1 -> norm1
I0905 23:23:24.401131   570 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:23:24.401137   570 net.cpp:125] norm1 needs backward computation.
I0905 23:23:24.401145   570 net.cpp:66] Creating Layer conv2
I0905 23:23:24.401150   570 net.cpp:329] conv2 <- norm1
I0905 23:23:24.401157   570 net.cpp:290] conv2 -> conv2
I0905 23:23:24.410351   570 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:23:24.410367   570 net.cpp:125] conv2 needs backward computation.
I0905 23:23:24.410374   570 net.cpp:66] Creating Layer relu2
I0905 23:23:24.410380   570 net.cpp:329] relu2 <- conv2
I0905 23:23:24.410387   570 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:23:24.410394   570 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:23:24.410400   570 net.cpp:125] relu2 needs backward computation.
I0905 23:23:24.410406   570 net.cpp:66] Creating Layer pool2
I0905 23:23:24.410411   570 net.cpp:329] pool2 <- conv2
I0905 23:23:24.410418   570 net.cpp:290] pool2 -> pool2
I0905 23:23:24.410426   570 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:23:24.410433   570 net.cpp:125] pool2 needs backward computation.
I0905 23:23:24.410441   570 net.cpp:66] Creating Layer fc7
I0905 23:23:24.410447   570 net.cpp:329] fc7 <- pool2
I0905 23:23:24.410454   570 net.cpp:290] fc7 -> fc7
I0905 23:23:25.060206   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.060266   570 net.cpp:125] fc7 needs backward computation.
I0905 23:23:25.060288   570 net.cpp:66] Creating Layer relu7
I0905 23:23:25.060297   570 net.cpp:329] relu7 <- fc7
I0905 23:23:25.060307   570 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:23:25.060317   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.060322   570 net.cpp:125] relu7 needs backward computation.
I0905 23:23:25.060330   570 net.cpp:66] Creating Layer drop7
I0905 23:23:25.060335   570 net.cpp:329] drop7 <- fc7
I0905 23:23:25.060343   570 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:23:25.060353   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.060359   570 net.cpp:125] drop7 needs backward computation.
I0905 23:23:25.060367   570 net.cpp:66] Creating Layer fc8
I0905 23:23:25.060374   570 net.cpp:329] fc8 <- fc7
I0905 23:23:25.060382   570 net.cpp:290] fc8 -> fc8
I0905 23:23:25.068419   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.068433   570 net.cpp:125] fc8 needs backward computation.
I0905 23:23:25.068440   570 net.cpp:66] Creating Layer relu8
I0905 23:23:25.068445   570 net.cpp:329] relu8 <- fc8
I0905 23:23:25.068454   570 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:23:25.068461   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.068467   570 net.cpp:125] relu8 needs backward computation.
I0905 23:23:25.068475   570 net.cpp:66] Creating Layer drop8
I0905 23:23:25.068480   570 net.cpp:329] drop8 <- fc8
I0905 23:23:25.068486   570 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:23:25.068493   570 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:23:25.068498   570 net.cpp:125] drop8 needs backward computation.
I0905 23:23:25.068507   570 net.cpp:66] Creating Layer fc9
I0905 23:23:25.068513   570 net.cpp:329] fc9 <- fc8
I0905 23:23:25.068521   570 net.cpp:290] fc9 -> fc9
I0905 23:23:25.068904   570 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:23:25.068917   570 net.cpp:125] fc9 needs backward computation.
I0905 23:23:25.068925   570 net.cpp:66] Creating Layer fc10
I0905 23:23:25.068931   570 net.cpp:329] fc10 <- fc9
I0905 23:23:25.068940   570 net.cpp:290] fc10 -> fc10
I0905 23:23:25.068953   570 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:23:25.068960   570 net.cpp:125] fc10 needs backward computation.
I0905 23:23:25.068967   570 net.cpp:66] Creating Layer prob
I0905 23:23:25.068972   570 net.cpp:329] prob <- fc10
I0905 23:23:25.068980   570 net.cpp:290] prob -> prob
I0905 23:23:25.068990   570 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:23:25.068996   570 net.cpp:125] prob needs backward computation.
I0905 23:23:25.069001   570 net.cpp:156] This network produces output prob
I0905 23:23:25.069015   570 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:23:25.069023   570 net.cpp:167] Network initialization done.
I0905 23:23:25.069028   570 net.cpp:168] Memory required for data: 6183480
Classifying 155 inputs.
Done in 98.70 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 55 is out of bounds for axis 0 with size 55
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:25:07.148195   576 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:25:07.148336   576 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:25:07.148346   576 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:25:07.148496   576 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:25:07.148561   576 net.cpp:292] Input 0 -> data
I0905 23:25:07.148587   576 net.cpp:66] Creating Layer conv1
I0905 23:25:07.148596   576 net.cpp:329] conv1 <- data
I0905 23:25:07.148603   576 net.cpp:290] conv1 -> conv1
I0905 23:25:07.150018   576 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:25:07.150038   576 net.cpp:125] conv1 needs backward computation.
I0905 23:25:07.150048   576 net.cpp:66] Creating Layer relu1
I0905 23:25:07.150055   576 net.cpp:329] relu1 <- conv1
I0905 23:25:07.150063   576 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:25:07.150071   576 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:25:07.150079   576 net.cpp:125] relu1 needs backward computation.
I0905 23:25:07.150085   576 net.cpp:66] Creating Layer pool1
I0905 23:25:07.150091   576 net.cpp:329] pool1 <- conv1
I0905 23:25:07.150099   576 net.cpp:290] pool1 -> pool1
I0905 23:25:07.150110   576 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:25:07.150116   576 net.cpp:125] pool1 needs backward computation.
I0905 23:25:07.150123   576 net.cpp:66] Creating Layer norm1
I0905 23:25:07.150130   576 net.cpp:329] norm1 <- pool1
I0905 23:25:07.150136   576 net.cpp:290] norm1 -> norm1
I0905 23:25:07.150146   576 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:25:07.150153   576 net.cpp:125] norm1 needs backward computation.
I0905 23:25:07.150166   576 net.cpp:66] Creating Layer conv2
I0905 23:25:07.150172   576 net.cpp:329] conv2 <- norm1
I0905 23:25:07.150179   576 net.cpp:290] conv2 -> conv2
I0905 23:25:07.159315   576 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:25:07.159332   576 net.cpp:125] conv2 needs backward computation.
I0905 23:25:07.159338   576 net.cpp:66] Creating Layer relu2
I0905 23:25:07.159344   576 net.cpp:329] relu2 <- conv2
I0905 23:25:07.159351   576 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:25:07.159358   576 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:25:07.159364   576 net.cpp:125] relu2 needs backward computation.
I0905 23:25:07.159371   576 net.cpp:66] Creating Layer pool2
I0905 23:25:07.159376   576 net.cpp:329] pool2 <- conv2
I0905 23:25:07.159384   576 net.cpp:290] pool2 -> pool2
I0905 23:25:07.159391   576 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:25:07.159397   576 net.cpp:125] pool2 needs backward computation.
I0905 23:25:07.159406   576 net.cpp:66] Creating Layer fc7
I0905 23:25:07.159412   576 net.cpp:329] fc7 <- pool2
I0905 23:25:07.159420   576 net.cpp:290] fc7 -> fc7
I0905 23:25:07.807327   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.807373   576 net.cpp:125] fc7 needs backward computation.
I0905 23:25:07.807385   576 net.cpp:66] Creating Layer relu7
I0905 23:25:07.807394   576 net.cpp:329] relu7 <- fc7
I0905 23:25:07.807404   576 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:25:07.807415   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.807420   576 net.cpp:125] relu7 needs backward computation.
I0905 23:25:07.807428   576 net.cpp:66] Creating Layer drop7
I0905 23:25:07.807433   576 net.cpp:329] drop7 <- fc7
I0905 23:25:07.807440   576 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:25:07.807451   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.807457   576 net.cpp:125] drop7 needs backward computation.
I0905 23:25:07.807466   576 net.cpp:66] Creating Layer fc8
I0905 23:25:07.807471   576 net.cpp:329] fc8 <- fc7
I0905 23:25:07.807482   576 net.cpp:290] fc8 -> fc8
I0905 23:25:07.815479   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.815491   576 net.cpp:125] fc8 needs backward computation.
I0905 23:25:07.815500   576 net.cpp:66] Creating Layer relu8
I0905 23:25:07.815505   576 net.cpp:329] relu8 <- fc8
I0905 23:25:07.815513   576 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:25:07.815521   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.815527   576 net.cpp:125] relu8 needs backward computation.
I0905 23:25:07.815534   576 net.cpp:66] Creating Layer drop8
I0905 23:25:07.815539   576 net.cpp:329] drop8 <- fc8
I0905 23:25:07.815546   576 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:25:07.815553   576 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:25:07.815559   576 net.cpp:125] drop8 needs backward computation.
I0905 23:25:07.815568   576 net.cpp:66] Creating Layer fc9
I0905 23:25:07.815574   576 net.cpp:329] fc9 <- fc8
I0905 23:25:07.815582   576 net.cpp:290] fc9 -> fc9
I0905 23:25:07.815965   576 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:25:07.815978   576 net.cpp:125] fc9 needs backward computation.
I0905 23:25:07.815986   576 net.cpp:66] Creating Layer fc10
I0905 23:25:07.815992   576 net.cpp:329] fc10 <- fc9
I0905 23:25:07.816001   576 net.cpp:290] fc10 -> fc10
I0905 23:25:07.816014   576 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:25:07.816021   576 net.cpp:125] fc10 needs backward computation.
I0905 23:25:07.816028   576 net.cpp:66] Creating Layer prob
I0905 23:25:07.816035   576 net.cpp:329] prob <- fc10
I0905 23:25:07.816042   576 net.cpp:290] prob -> prob
I0905 23:25:07.816052   576 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:25:07.816058   576 net.cpp:125] prob needs backward computation.
I0905 23:25:07.816064   576 net.cpp:156] This network produces output prob
I0905 23:25:07.816076   576 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:25:07.816086   576 net.cpp:167] Network initialization done.
I0905 23:25:07.816092   576 net.cpp:168] Memory required for data: 6183480
Classifying 192 inputs.
Done in 121.10 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 92 is out of bounds for axis 0 with size 92
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:27:13.533041   582 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:27:13.533179   582 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:27:13.533187   582 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:27:13.533334   582 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:27:13.533397   582 net.cpp:292] Input 0 -> data
I0905 23:27:13.533424   582 net.cpp:66] Creating Layer conv1
I0905 23:27:13.533432   582 net.cpp:329] conv1 <- data
I0905 23:27:13.533439   582 net.cpp:290] conv1 -> conv1
I0905 23:27:13.534811   582 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:27:13.534832   582 net.cpp:125] conv1 needs backward computation.
I0905 23:27:13.534840   582 net.cpp:66] Creating Layer relu1
I0905 23:27:13.534847   582 net.cpp:329] relu1 <- conv1
I0905 23:27:13.534853   582 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:27:13.534862   582 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:27:13.534868   582 net.cpp:125] relu1 needs backward computation.
I0905 23:27:13.534874   582 net.cpp:66] Creating Layer pool1
I0905 23:27:13.534880   582 net.cpp:329] pool1 <- conv1
I0905 23:27:13.534888   582 net.cpp:290] pool1 -> pool1
I0905 23:27:13.534898   582 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:27:13.534904   582 net.cpp:125] pool1 needs backward computation.
I0905 23:27:13.534911   582 net.cpp:66] Creating Layer norm1
I0905 23:27:13.534916   582 net.cpp:329] norm1 <- pool1
I0905 23:27:13.534924   582 net.cpp:290] norm1 -> norm1
I0905 23:27:13.534934   582 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:27:13.534940   582 net.cpp:125] norm1 needs backward computation.
I0905 23:27:13.534946   582 net.cpp:66] Creating Layer conv2
I0905 23:27:13.534952   582 net.cpp:329] conv2 <- norm1
I0905 23:27:13.534960   582 net.cpp:290] conv2 -> conv2
I0905 23:27:13.544092   582 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:27:13.544107   582 net.cpp:125] conv2 needs backward computation.
I0905 23:27:13.544116   582 net.cpp:66] Creating Layer relu2
I0905 23:27:13.544121   582 net.cpp:329] relu2 <- conv2
I0905 23:27:13.544127   582 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:27:13.544134   582 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:27:13.544141   582 net.cpp:125] relu2 needs backward computation.
I0905 23:27:13.544147   582 net.cpp:66] Creating Layer pool2
I0905 23:27:13.544152   582 net.cpp:329] pool2 <- conv2
I0905 23:27:13.544158   582 net.cpp:290] pool2 -> pool2
I0905 23:27:13.544167   582 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:27:13.544173   582 net.cpp:125] pool2 needs backward computation.
I0905 23:27:13.544181   582 net.cpp:66] Creating Layer fc7
I0905 23:27:13.544188   582 net.cpp:329] fc7 <- pool2
I0905 23:27:13.544194   582 net.cpp:290] fc7 -> fc7
I0905 23:27:14.193603   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.193649   582 net.cpp:125] fc7 needs backward computation.
I0905 23:27:14.193661   582 net.cpp:66] Creating Layer relu7
I0905 23:27:14.193668   582 net.cpp:329] relu7 <- fc7
I0905 23:27:14.193677   582 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:27:14.193688   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.193693   582 net.cpp:125] relu7 needs backward computation.
I0905 23:27:14.193701   582 net.cpp:66] Creating Layer drop7
I0905 23:27:14.193706   582 net.cpp:329] drop7 <- fc7
I0905 23:27:14.193713   582 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:27:14.193724   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.193730   582 net.cpp:125] drop7 needs backward computation.
I0905 23:27:14.193738   582 net.cpp:66] Creating Layer fc8
I0905 23:27:14.193744   582 net.cpp:329] fc8 <- fc7
I0905 23:27:14.193753   582 net.cpp:290] fc8 -> fc8
I0905 23:27:14.201529   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.201541   582 net.cpp:125] fc8 needs backward computation.
I0905 23:27:14.201549   582 net.cpp:66] Creating Layer relu8
I0905 23:27:14.201555   582 net.cpp:329] relu8 <- fc8
I0905 23:27:14.201562   582 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:27:14.201570   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.201575   582 net.cpp:125] relu8 needs backward computation.
I0905 23:27:14.201589   582 net.cpp:66] Creating Layer drop8
I0905 23:27:14.201602   582 net.cpp:329] drop8 <- fc8
I0905 23:27:14.201609   582 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:27:14.201627   582 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:27:14.201634   582 net.cpp:125] drop8 needs backward computation.
I0905 23:27:14.201643   582 net.cpp:66] Creating Layer fc9
I0905 23:27:14.201649   582 net.cpp:329] fc9 <- fc8
I0905 23:27:14.201656   582 net.cpp:290] fc9 -> fc9
I0905 23:27:14.202040   582 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:27:14.202052   582 net.cpp:125] fc9 needs backward computation.
I0905 23:27:14.202061   582 net.cpp:66] Creating Layer fc10
I0905 23:27:14.202066   582 net.cpp:329] fc10 <- fc9
I0905 23:27:14.202075   582 net.cpp:290] fc10 -> fc10
I0905 23:27:14.202087   582 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:27:14.202095   582 net.cpp:125] fc10 needs backward computation.
I0905 23:27:14.202102   582 net.cpp:66] Creating Layer prob
I0905 23:27:14.202108   582 net.cpp:329] prob <- fc10
I0905 23:27:14.202117   582 net.cpp:290] prob -> prob
I0905 23:27:14.202127   582 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:27:14.202133   582 net.cpp:125] prob needs backward computation.
I0905 23:27:14.202138   582 net.cpp:156] This network produces output prob
I0905 23:27:14.202152   582 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:27:14.202160   582 net.cpp:167] Network initialization done.
I0905 23:27:14.202167   582 net.cpp:168] Memory required for data: 6183480
Classifying 105 inputs.
Done in 67.92 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 5 is out of bounds for axis 0 with size 5
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:28:24.693717   587 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:28:24.693856   587 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:28:24.693866   587 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:28:24.694012   587 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:28:24.694075   587 net.cpp:292] Input 0 -> data
I0905 23:28:24.694100   587 net.cpp:66] Creating Layer conv1
I0905 23:28:24.694108   587 net.cpp:329] conv1 <- data
I0905 23:28:24.694115   587 net.cpp:290] conv1 -> conv1
I0905 23:28:24.695494   587 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:28:24.695513   587 net.cpp:125] conv1 needs backward computation.
I0905 23:28:24.695521   587 net.cpp:66] Creating Layer relu1
I0905 23:28:24.695528   587 net.cpp:329] relu1 <- conv1
I0905 23:28:24.695533   587 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:28:24.695543   587 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:28:24.695549   587 net.cpp:125] relu1 needs backward computation.
I0905 23:28:24.695555   587 net.cpp:66] Creating Layer pool1
I0905 23:28:24.695560   587 net.cpp:329] pool1 <- conv1
I0905 23:28:24.695567   587 net.cpp:290] pool1 -> pool1
I0905 23:28:24.695577   587 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:28:24.695583   587 net.cpp:125] pool1 needs backward computation.
I0905 23:28:24.695590   587 net.cpp:66] Creating Layer norm1
I0905 23:28:24.695595   587 net.cpp:329] norm1 <- pool1
I0905 23:28:24.695602   587 net.cpp:290] norm1 -> norm1
I0905 23:28:24.695611   587 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:28:24.695617   587 net.cpp:125] norm1 needs backward computation.
I0905 23:28:24.695624   587 net.cpp:66] Creating Layer conv2
I0905 23:28:24.695631   587 net.cpp:329] conv2 <- norm1
I0905 23:28:24.695637   587 net.cpp:290] conv2 -> conv2
I0905 23:28:24.704743   587 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:28:24.704758   587 net.cpp:125] conv2 needs backward computation.
I0905 23:28:24.704766   587 net.cpp:66] Creating Layer relu2
I0905 23:28:24.704771   587 net.cpp:329] relu2 <- conv2
I0905 23:28:24.704777   587 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:28:24.704784   587 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:28:24.704790   587 net.cpp:125] relu2 needs backward computation.
I0905 23:28:24.704797   587 net.cpp:66] Creating Layer pool2
I0905 23:28:24.704802   587 net.cpp:329] pool2 <- conv2
I0905 23:28:24.704808   587 net.cpp:290] pool2 -> pool2
I0905 23:28:24.704816   587 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:28:24.704821   587 net.cpp:125] pool2 needs backward computation.
I0905 23:28:24.704830   587 net.cpp:66] Creating Layer fc7
I0905 23:28:24.704836   587 net.cpp:329] fc7 <- pool2
I0905 23:28:24.704843   587 net.cpp:290] fc7 -> fc7
I0905 23:28:25.380391   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.380444   587 net.cpp:125] fc7 needs backward computation.
I0905 23:28:25.380456   587 net.cpp:66] Creating Layer relu7
I0905 23:28:25.380465   587 net.cpp:329] relu7 <- fc7
I0905 23:28:25.380484   587 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:28:25.380496   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.380501   587 net.cpp:125] relu7 needs backward computation.
I0905 23:28:25.380509   587 net.cpp:66] Creating Layer drop7
I0905 23:28:25.380514   587 net.cpp:329] drop7 <- fc7
I0905 23:28:25.380520   587 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:28:25.380532   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.380537   587 net.cpp:125] drop7 needs backward computation.
I0905 23:28:25.380545   587 net.cpp:66] Creating Layer fc8
I0905 23:28:25.380550   587 net.cpp:329] fc8 <- fc7
I0905 23:28:25.380559   587 net.cpp:290] fc8 -> fc8
I0905 23:28:25.388339   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.388352   587 net.cpp:125] fc8 needs backward computation.
I0905 23:28:25.388360   587 net.cpp:66] Creating Layer relu8
I0905 23:28:25.388365   587 net.cpp:329] relu8 <- fc8
I0905 23:28:25.388373   587 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:28:25.388381   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.388386   587 net.cpp:125] relu8 needs backward computation.
I0905 23:28:25.388392   587 net.cpp:66] Creating Layer drop8
I0905 23:28:25.388397   587 net.cpp:329] drop8 <- fc8
I0905 23:28:25.388403   587 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:28:25.388411   587 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:28:25.388417   587 net.cpp:125] drop8 needs backward computation.
I0905 23:28:25.388424   587 net.cpp:66] Creating Layer fc9
I0905 23:28:25.388430   587 net.cpp:329] fc9 <- fc8
I0905 23:28:25.388437   587 net.cpp:290] fc9 -> fc9
I0905 23:28:25.388808   587 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:28:25.388820   587 net.cpp:125] fc9 needs backward computation.
I0905 23:28:25.388828   587 net.cpp:66] Creating Layer fc10
I0905 23:28:25.388834   587 net.cpp:329] fc10 <- fc9
I0905 23:28:25.388844   587 net.cpp:290] fc10 -> fc10
I0905 23:28:25.388855   587 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:28:25.388864   587 net.cpp:125] fc10 needs backward computation.
I0905 23:28:25.388870   587 net.cpp:66] Creating Layer prob
I0905 23:28:25.388875   587 net.cpp:329] prob <- fc10
I0905 23:28:25.388885   587 net.cpp:290] prob -> prob
I0905 23:28:25.388893   587 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:28:25.388900   587 net.cpp:125] prob needs backward computation.
I0905 23:28:25.388905   587 net.cpp:156] This network produces output prob
I0905 23:28:25.388916   587 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:28:25.388926   587 net.cpp:167] Network initialization done.
I0905 23:28:25.388931   587 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:33:59.323346   602 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:33:59.323626   602 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:33:59.323642   602 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:33:59.323973   602 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:33:59.324071   602 net.cpp:292] Input 0 -> data
I0905 23:33:59.324142   602 net.cpp:66] Creating Layer conv1
I0905 23:33:59.324156   602 net.cpp:329] conv1 <- data
I0905 23:33:59.324170   602 net.cpp:290] conv1 -> conv1
I0905 23:33:59.355938   602 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:33:59.355963   602 net.cpp:125] conv1 needs backward computation.
I0905 23:33:59.355974   602 net.cpp:66] Creating Layer relu1
I0905 23:33:59.355981   602 net.cpp:329] relu1 <- conv1
I0905 23:33:59.355989   602 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:33:59.355998   602 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:33:59.356004   602 net.cpp:125] relu1 needs backward computation.
I0905 23:33:59.356012   602 net.cpp:66] Creating Layer pool1
I0905 23:33:59.356017   602 net.cpp:329] pool1 <- conv1
I0905 23:33:59.356025   602 net.cpp:290] pool1 -> pool1
I0905 23:33:59.356036   602 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:33:59.356044   602 net.cpp:125] pool1 needs backward computation.
I0905 23:33:59.356050   602 net.cpp:66] Creating Layer norm1
I0905 23:33:59.356056   602 net.cpp:329] norm1 <- pool1
I0905 23:33:59.356063   602 net.cpp:290] norm1 -> norm1
I0905 23:33:59.356073   602 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:33:59.356081   602 net.cpp:125] norm1 needs backward computation.
I0905 23:33:59.356087   602 net.cpp:66] Creating Layer conv2
I0905 23:33:59.356093   602 net.cpp:329] conv2 <- norm1
I0905 23:33:59.356101   602 net.cpp:290] conv2 -> conv2
I0905 23:33:59.365474   602 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:33:59.365490   602 net.cpp:125] conv2 needs backward computation.
I0905 23:33:59.365499   602 net.cpp:66] Creating Layer relu2
I0905 23:33:59.365505   602 net.cpp:329] relu2 <- conv2
I0905 23:33:59.365517   602 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:33:59.365525   602 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:33:59.365531   602 net.cpp:125] relu2 needs backward computation.
I0905 23:33:59.365540   602 net.cpp:66] Creating Layer pool2
I0905 23:33:59.365546   602 net.cpp:329] pool2 <- conv2
I0905 23:33:59.365553   602 net.cpp:290] pool2 -> pool2
I0905 23:33:59.365562   602 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:33:59.365568   602 net.cpp:125] pool2 needs backward computation.
I0905 23:33:59.365576   602 net.cpp:66] Creating Layer fc7
I0905 23:33:59.365586   602 net.cpp:329] fc7 <- pool2
I0905 23:33:59.365593   602 net.cpp:290] fc7 -> fc7
I0905 23:34:00.014689   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.014734   602 net.cpp:125] fc7 needs backward computation.
I0905 23:34:00.014747   602 net.cpp:66] Creating Layer relu7
I0905 23:34:00.014755   602 net.cpp:329] relu7 <- fc7
I0905 23:34:00.014765   602 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:34:00.014775   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.014781   602 net.cpp:125] relu7 needs backward computation.
I0905 23:34:00.014788   602 net.cpp:66] Creating Layer drop7
I0905 23:34:00.014794   602 net.cpp:329] drop7 <- fc7
I0905 23:34:00.014801   602 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:34:00.014811   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.014817   602 net.cpp:125] drop7 needs backward computation.
I0905 23:34:00.014827   602 net.cpp:66] Creating Layer fc8
I0905 23:34:00.014832   602 net.cpp:329] fc8 <- fc7
I0905 23:34:00.014840   602 net.cpp:290] fc8 -> fc8
I0905 23:34:00.022917   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.022930   602 net.cpp:125] fc8 needs backward computation.
I0905 23:34:00.022938   602 net.cpp:66] Creating Layer relu8
I0905 23:34:00.022944   602 net.cpp:329] relu8 <- fc8
I0905 23:34:00.022953   602 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:34:00.022960   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.022968   602 net.cpp:125] relu8 needs backward computation.
I0905 23:34:00.022974   602 net.cpp:66] Creating Layer drop8
I0905 23:34:00.022979   602 net.cpp:329] drop8 <- fc8
I0905 23:34:00.022986   602 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:34:00.022994   602 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:34:00.023000   602 net.cpp:125] drop8 needs backward computation.
I0905 23:34:00.023008   602 net.cpp:66] Creating Layer fc9
I0905 23:34:00.023015   602 net.cpp:329] fc9 <- fc8
I0905 23:34:00.023022   602 net.cpp:290] fc9 -> fc9
I0905 23:34:00.023407   602 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:34:00.023419   602 net.cpp:125] fc9 needs backward computation.
I0905 23:34:00.023428   602 net.cpp:66] Creating Layer fc10
I0905 23:34:00.023434   602 net.cpp:329] fc10 <- fc9
I0905 23:34:00.023443   602 net.cpp:290] fc10 -> fc10
I0905 23:34:00.023455   602 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:34:00.023463   602 net.cpp:125] fc10 needs backward computation.
I0905 23:34:00.023470   602 net.cpp:66] Creating Layer prob
I0905 23:34:00.023476   602 net.cpp:329] prob <- fc10
I0905 23:34:00.023484   602 net.cpp:290] prob -> prob
I0905 23:34:00.023495   602 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:34:00.023501   602 net.cpp:125] prob needs backward computation.
I0905 23:34:00.023506   602 net.cpp:156] This network produces output prob
I0905 23:34:00.023520   602 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:34:00.023530   602 net.cpp:167] Network initialization done.
I0905 23:34:00.023535   602 net.cpp:168] Memory required for data: 6183480
Classifying 212 inputs.
Done in 141.52 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 12 is out of bounds for axis 0 with size 12
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:36:31.249016   610 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:36:31.249166   610 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:36:31.249173   610 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:36:31.249321   610 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:36:31.249372   610 net.cpp:292] Input 0 -> data
I0905 23:36:31.249398   610 net.cpp:66] Creating Layer conv1
I0905 23:36:31.249405   610 net.cpp:329] conv1 <- data
I0905 23:36:31.249414   610 net.cpp:290] conv1 -> conv1
I0905 23:36:31.250792   610 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:36:31.250812   610 net.cpp:125] conv1 needs backward computation.
I0905 23:36:31.250821   610 net.cpp:66] Creating Layer relu1
I0905 23:36:31.250831   610 net.cpp:329] relu1 <- conv1
I0905 23:36:31.250839   610 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:36:31.250848   610 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:36:31.250854   610 net.cpp:125] relu1 needs backward computation.
I0905 23:36:31.250860   610 net.cpp:66] Creating Layer pool1
I0905 23:36:31.250866   610 net.cpp:329] pool1 <- conv1
I0905 23:36:31.250874   610 net.cpp:290] pool1 -> pool1
I0905 23:36:31.250885   610 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:36:31.250890   610 net.cpp:125] pool1 needs backward computation.
I0905 23:36:31.250896   610 net.cpp:66] Creating Layer norm1
I0905 23:36:31.250902   610 net.cpp:329] norm1 <- pool1
I0905 23:36:31.250910   610 net.cpp:290] norm1 -> norm1
I0905 23:36:31.250918   610 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:36:31.250924   610 net.cpp:125] norm1 needs backward computation.
I0905 23:36:31.250932   610 net.cpp:66] Creating Layer conv2
I0905 23:36:31.250937   610 net.cpp:329] conv2 <- norm1
I0905 23:36:31.250944   610 net.cpp:290] conv2 -> conv2
I0905 23:36:31.260084   610 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:36:31.260099   610 net.cpp:125] conv2 needs backward computation.
I0905 23:36:31.260107   610 net.cpp:66] Creating Layer relu2
I0905 23:36:31.260113   610 net.cpp:329] relu2 <- conv2
I0905 23:36:31.260119   610 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:36:31.260126   610 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:36:31.260133   610 net.cpp:125] relu2 needs backward computation.
I0905 23:36:31.260138   610 net.cpp:66] Creating Layer pool2
I0905 23:36:31.260143   610 net.cpp:329] pool2 <- conv2
I0905 23:36:31.260150   610 net.cpp:290] pool2 -> pool2
I0905 23:36:31.260159   610 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:36:31.260164   610 net.cpp:125] pool2 needs backward computation.
I0905 23:36:31.260174   610 net.cpp:66] Creating Layer fc7
I0905 23:36:31.260179   610 net.cpp:329] fc7 <- pool2
I0905 23:36:31.260186   610 net.cpp:290] fc7 -> fc7
I0905 23:36:31.936645   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.936703   610 net.cpp:125] fc7 needs backward computation.
I0905 23:36:31.936723   610 net.cpp:66] Creating Layer relu7
I0905 23:36:31.936732   610 net.cpp:329] relu7 <- fc7
I0905 23:36:31.936741   610 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:36:31.936753   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.936758   610 net.cpp:125] relu7 needs backward computation.
I0905 23:36:31.936766   610 net.cpp:66] Creating Layer drop7
I0905 23:36:31.936771   610 net.cpp:329] drop7 <- fc7
I0905 23:36:31.936779   610 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:36:31.936789   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.936795   610 net.cpp:125] drop7 needs backward computation.
I0905 23:36:31.936805   610 net.cpp:66] Creating Layer fc8
I0905 23:36:31.936810   610 net.cpp:329] fc8 <- fc7
I0905 23:36:31.936818   610 net.cpp:290] fc8 -> fc8
I0905 23:36:31.945179   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.945194   610 net.cpp:125] fc8 needs backward computation.
I0905 23:36:31.945200   610 net.cpp:66] Creating Layer relu8
I0905 23:36:31.945206   610 net.cpp:329] relu8 <- fc8
I0905 23:36:31.945214   610 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:36:31.945222   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.945229   610 net.cpp:125] relu8 needs backward computation.
I0905 23:36:31.945235   610 net.cpp:66] Creating Layer drop8
I0905 23:36:31.945240   610 net.cpp:329] drop8 <- fc8
I0905 23:36:31.945247   610 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:36:31.945255   610 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:31.945261   610 net.cpp:125] drop8 needs backward computation.
I0905 23:36:31.945269   610 net.cpp:66] Creating Layer fc9
I0905 23:36:31.945276   610 net.cpp:329] fc9 <- fc8
I0905 23:36:31.945282   610 net.cpp:290] fc9 -> fc9
I0905 23:36:31.945684   610 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:36:31.945698   610 net.cpp:125] fc9 needs backward computation.
I0905 23:36:31.945713   610 net.cpp:66] Creating Layer fc10
I0905 23:36:31.945720   610 net.cpp:329] fc10 <- fc9
I0905 23:36:31.945729   610 net.cpp:290] fc10 -> fc10
I0905 23:36:31.945741   610 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:36:31.945749   610 net.cpp:125] fc10 needs backward computation.
I0905 23:36:31.945756   610 net.cpp:66] Creating Layer prob
I0905 23:36:31.945762   610 net.cpp:329] prob <- fc10
I0905 23:36:31.945770   610 net.cpp:290] prob -> prob
I0905 23:36:31.945780   610 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:36:31.945786   610 net.cpp:125] prob needs backward computation.
I0905 23:36:31.945791   610 net.cpp:156] This network produces output prob
I0905 23:36:31.945804   610 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:36:31.945813   610 net.cpp:167] Network initialization done.
I0905 23:36:31.945818   610 net.cpp:168] Memory required for data: 6183480
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 119, in main
    inputs.append(caffe.io.load_image(im_f))
  File "/home/jack/Documents/caffe/testmine/distribute/python/caffe/io.py", line 23, in load_image
    img = skimage.img_as_float(skimage.io.imread(filename)).astype(np.float32)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 287, in img_as_float
    return convert(image, np.float64, force_copy)
  File "/home/jack/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py", line 99, in convert
    raise ValueError("can not convert %s to %s." % (dtypeobj_in, dtypeobj))
ValueError: can not convert object to float64.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:36:41.019315   614 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:36:41.019454   614 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:36:41.019462   614 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:36:41.019610   614 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:36:41.019673   614 net.cpp:292] Input 0 -> data
I0905 23:36:41.019701   614 net.cpp:66] Creating Layer conv1
I0905 23:36:41.019707   614 net.cpp:329] conv1 <- data
I0905 23:36:41.019716   614 net.cpp:290] conv1 -> conv1
I0905 23:36:41.021096   614 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:36:41.021114   614 net.cpp:125] conv1 needs backward computation.
I0905 23:36:41.021123   614 net.cpp:66] Creating Layer relu1
I0905 23:36:41.021131   614 net.cpp:329] relu1 <- conv1
I0905 23:36:41.021136   614 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:36:41.021145   614 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:36:41.021152   614 net.cpp:125] relu1 needs backward computation.
I0905 23:36:41.021157   614 net.cpp:66] Creating Layer pool1
I0905 23:36:41.021163   614 net.cpp:329] pool1 <- conv1
I0905 23:36:41.021170   614 net.cpp:290] pool1 -> pool1
I0905 23:36:41.021180   614 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:36:41.021186   614 net.cpp:125] pool1 needs backward computation.
I0905 23:36:41.021193   614 net.cpp:66] Creating Layer norm1
I0905 23:36:41.021198   614 net.cpp:329] norm1 <- pool1
I0905 23:36:41.021205   614 net.cpp:290] norm1 -> norm1
I0905 23:36:41.021215   614 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:36:41.021221   614 net.cpp:125] norm1 needs backward computation.
I0905 23:36:41.021229   614 net.cpp:66] Creating Layer conv2
I0905 23:36:41.021234   614 net.cpp:329] conv2 <- norm1
I0905 23:36:41.021241   614 net.cpp:290] conv2 -> conv2
I0905 23:36:41.030378   614 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:36:41.030393   614 net.cpp:125] conv2 needs backward computation.
I0905 23:36:41.030400   614 net.cpp:66] Creating Layer relu2
I0905 23:36:41.030406   614 net.cpp:329] relu2 <- conv2
I0905 23:36:41.030412   614 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:36:41.030421   614 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:36:41.030426   614 net.cpp:125] relu2 needs backward computation.
I0905 23:36:41.030432   614 net.cpp:66] Creating Layer pool2
I0905 23:36:41.030437   614 net.cpp:329] pool2 <- conv2
I0905 23:36:41.030444   614 net.cpp:290] pool2 -> pool2
I0905 23:36:41.030452   614 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:36:41.030457   614 net.cpp:125] pool2 needs backward computation.
I0905 23:36:41.030467   614 net.cpp:66] Creating Layer fc7
I0905 23:36:41.030473   614 net.cpp:329] fc7 <- pool2
I0905 23:36:41.030480   614 net.cpp:290] fc7 -> fc7
I0905 23:36:41.679587   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.679631   614 net.cpp:125] fc7 needs backward computation.
I0905 23:36:41.679644   614 net.cpp:66] Creating Layer relu7
I0905 23:36:41.679651   614 net.cpp:329] relu7 <- fc7
I0905 23:36:41.679672   614 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:36:41.679683   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.679689   614 net.cpp:125] relu7 needs backward computation.
I0905 23:36:41.679697   614 net.cpp:66] Creating Layer drop7
I0905 23:36:41.679702   614 net.cpp:329] drop7 <- fc7
I0905 23:36:41.679708   614 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:36:41.679719   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.679725   614 net.cpp:125] drop7 needs backward computation.
I0905 23:36:41.679733   614 net.cpp:66] Creating Layer fc8
I0905 23:36:41.679738   614 net.cpp:329] fc8 <- fc7
I0905 23:36:41.679747   614 net.cpp:290] fc8 -> fc8
I0905 23:36:41.687513   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.687526   614 net.cpp:125] fc8 needs backward computation.
I0905 23:36:41.687533   614 net.cpp:66] Creating Layer relu8
I0905 23:36:41.687540   614 net.cpp:329] relu8 <- fc8
I0905 23:36:41.687547   614 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:36:41.687554   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.687561   614 net.cpp:125] relu8 needs backward computation.
I0905 23:36:41.687566   614 net.cpp:66] Creating Layer drop8
I0905 23:36:41.687572   614 net.cpp:329] drop8 <- fc8
I0905 23:36:41.687578   614 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:36:41.687585   614 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:36:41.687590   614 net.cpp:125] drop8 needs backward computation.
I0905 23:36:41.687599   614 net.cpp:66] Creating Layer fc9
I0905 23:36:41.687605   614 net.cpp:329] fc9 <- fc8
I0905 23:36:41.687613   614 net.cpp:290] fc9 -> fc9
I0905 23:36:41.688004   614 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:36:41.688016   614 net.cpp:125] fc9 needs backward computation.
I0905 23:36:41.688025   614 net.cpp:66] Creating Layer fc10
I0905 23:36:41.688030   614 net.cpp:329] fc10 <- fc9
I0905 23:36:41.688038   614 net.cpp:290] fc10 -> fc10
I0905 23:36:41.688051   614 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:36:41.688058   614 net.cpp:125] fc10 needs backward computation.
I0905 23:36:41.688066   614 net.cpp:66] Creating Layer prob
I0905 23:36:41.688071   614 net.cpp:329] prob <- fc10
I0905 23:36:41.688078   614 net.cpp:290] prob -> prob
I0905 23:36:41.688088   614 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:36:41.688094   614 net.cpp:125] prob needs backward computation.
I0905 23:36:41.688099   614 net.cpp:156] This network produces output prob
I0905 23:36:41.688112   614 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:36:41.688120   614 net.cpp:167] Network initialization done.
I0905 23:36:41.688125   614 net.cpp:168] Memory required for data: 6183480
Classifying 46 inputs.
Done in 29.25 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:37:13.109874   619 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:37:13.110016   619 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:37:13.110026   619 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:37:13.110178   619 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:37:13.110244   619 net.cpp:292] Input 0 -> data
I0905 23:37:13.110271   619 net.cpp:66] Creating Layer conv1
I0905 23:37:13.110280   619 net.cpp:329] conv1 <- data
I0905 23:37:13.110287   619 net.cpp:290] conv1 -> conv1
I0905 23:37:13.111688   619 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:37:13.111707   619 net.cpp:125] conv1 needs backward computation.
I0905 23:37:13.111717   619 net.cpp:66] Creating Layer relu1
I0905 23:37:13.111723   619 net.cpp:329] relu1 <- conv1
I0905 23:37:13.111731   619 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:37:13.111739   619 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:37:13.111747   619 net.cpp:125] relu1 needs backward computation.
I0905 23:37:13.111753   619 net.cpp:66] Creating Layer pool1
I0905 23:37:13.111759   619 net.cpp:329] pool1 <- conv1
I0905 23:37:13.111767   619 net.cpp:290] pool1 -> pool1
I0905 23:37:13.111778   619 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:37:13.111784   619 net.cpp:125] pool1 needs backward computation.
I0905 23:37:13.111791   619 net.cpp:66] Creating Layer norm1
I0905 23:37:13.111798   619 net.cpp:329] norm1 <- pool1
I0905 23:37:13.111804   619 net.cpp:290] norm1 -> norm1
I0905 23:37:13.111815   619 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:37:13.111821   619 net.cpp:125] norm1 needs backward computation.
I0905 23:37:13.111829   619 net.cpp:66] Creating Layer conv2
I0905 23:37:13.111835   619 net.cpp:329] conv2 <- norm1
I0905 23:37:13.111843   619 net.cpp:290] conv2 -> conv2
I0905 23:37:13.121001   619 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:37:13.121016   619 net.cpp:125] conv2 needs backward computation.
I0905 23:37:13.121023   619 net.cpp:66] Creating Layer relu2
I0905 23:37:13.121034   619 net.cpp:329] relu2 <- conv2
I0905 23:37:13.121042   619 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:37:13.121049   619 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:37:13.121055   619 net.cpp:125] relu2 needs backward computation.
I0905 23:37:13.121062   619 net.cpp:66] Creating Layer pool2
I0905 23:37:13.121068   619 net.cpp:329] pool2 <- conv2
I0905 23:37:13.121074   619 net.cpp:290] pool2 -> pool2
I0905 23:37:13.121083   619 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:37:13.121088   619 net.cpp:125] pool2 needs backward computation.
I0905 23:37:13.121098   619 net.cpp:66] Creating Layer fc7
I0905 23:37:13.121104   619 net.cpp:329] fc7 <- pool2
I0905 23:37:13.121111   619 net.cpp:290] fc7 -> fc7
I0905 23:37:13.770285   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.770330   619 net.cpp:125] fc7 needs backward computation.
I0905 23:37:13.770344   619 net.cpp:66] Creating Layer relu7
I0905 23:37:13.770351   619 net.cpp:329] relu7 <- fc7
I0905 23:37:13.770360   619 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:37:13.770370   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.770377   619 net.cpp:125] relu7 needs backward computation.
I0905 23:37:13.770385   619 net.cpp:66] Creating Layer drop7
I0905 23:37:13.770390   619 net.cpp:329] drop7 <- fc7
I0905 23:37:13.770396   619 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:37:13.770407   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.770413   619 net.cpp:125] drop7 needs backward computation.
I0905 23:37:13.770422   619 net.cpp:66] Creating Layer fc8
I0905 23:37:13.770427   619 net.cpp:329] fc8 <- fc7
I0905 23:37:13.770437   619 net.cpp:290] fc8 -> fc8
I0905 23:37:13.778220   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.778234   619 net.cpp:125] fc8 needs backward computation.
I0905 23:37:13.778240   619 net.cpp:66] Creating Layer relu8
I0905 23:37:13.778246   619 net.cpp:329] relu8 <- fc8
I0905 23:37:13.778254   619 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:37:13.778261   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.778267   619 net.cpp:125] relu8 needs backward computation.
I0905 23:37:13.778275   619 net.cpp:66] Creating Layer drop8
I0905 23:37:13.778280   619 net.cpp:329] drop8 <- fc8
I0905 23:37:13.778286   619 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:37:13.778293   619 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:37:13.778300   619 net.cpp:125] drop8 needs backward computation.
I0905 23:37:13.778308   619 net.cpp:66] Creating Layer fc9
I0905 23:37:13.778314   619 net.cpp:329] fc9 <- fc8
I0905 23:37:13.778321   619 net.cpp:290] fc9 -> fc9
I0905 23:37:13.778695   619 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:37:13.778707   619 net.cpp:125] fc9 needs backward computation.
I0905 23:37:13.778715   619 net.cpp:66] Creating Layer fc10
I0905 23:37:13.778722   619 net.cpp:329] fc10 <- fc9
I0905 23:37:13.778729   619 net.cpp:290] fc10 -> fc10
I0905 23:37:13.778741   619 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:37:13.778749   619 net.cpp:125] fc10 needs backward computation.
I0905 23:37:13.778756   619 net.cpp:66] Creating Layer prob
I0905 23:37:13.778762   619 net.cpp:329] prob <- fc10
I0905 23:37:13.778770   619 net.cpp:290] prob -> prob
I0905 23:37:13.778780   619 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:37:13.778786   619 net.cpp:125] prob needs backward computation.
I0905 23:37:13.778791   619 net.cpp:156] This network produces output prob
I0905 23:37:13.778805   619 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:37:13.778812   619 net.cpp:167] Network initialization done.
I0905 23:37:13.778818   619 net.cpp:168] Memory required for data: 6183480
Classifying 208 inputs.
Done in 136.28 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 8 is out of bounds for axis 0 with size 8
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:39:35.153525   639 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:39:35.153694   639 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:39:35.153705   639 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:39:35.153852   639 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:39:35.153905   639 net.cpp:292] Input 0 -> data
I0905 23:39:35.153930   639 net.cpp:66] Creating Layer conv1
I0905 23:39:35.153938   639 net.cpp:329] conv1 <- data
I0905 23:39:35.153945   639 net.cpp:290] conv1 -> conv1
I0905 23:39:35.155308   639 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:39:35.155325   639 net.cpp:125] conv1 needs backward computation.
I0905 23:39:35.155339   639 net.cpp:66] Creating Layer relu1
I0905 23:39:35.155345   639 net.cpp:329] relu1 <- conv1
I0905 23:39:35.155352   639 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:39:35.155361   639 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:39:35.155367   639 net.cpp:125] relu1 needs backward computation.
I0905 23:39:35.155375   639 net.cpp:66] Creating Layer pool1
I0905 23:39:35.155380   639 net.cpp:329] pool1 <- conv1
I0905 23:39:35.155386   639 net.cpp:290] pool1 -> pool1
I0905 23:39:35.155397   639 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:39:35.155403   639 net.cpp:125] pool1 needs backward computation.
I0905 23:39:35.155411   639 net.cpp:66] Creating Layer norm1
I0905 23:39:35.155416   639 net.cpp:329] norm1 <- pool1
I0905 23:39:35.155422   639 net.cpp:290] norm1 -> norm1
I0905 23:39:35.155432   639 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:39:35.155437   639 net.cpp:125] norm1 needs backward computation.
I0905 23:39:35.155446   639 net.cpp:66] Creating Layer conv2
I0905 23:39:35.155452   639 net.cpp:329] conv2 <- norm1
I0905 23:39:35.155458   639 net.cpp:290] conv2 -> conv2
I0905 23:39:35.164598   639 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:39:35.164614   639 net.cpp:125] conv2 needs backward computation.
I0905 23:39:35.164621   639 net.cpp:66] Creating Layer relu2
I0905 23:39:35.164626   639 net.cpp:329] relu2 <- conv2
I0905 23:39:35.164633   639 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:39:35.164640   639 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:39:35.164646   639 net.cpp:125] relu2 needs backward computation.
I0905 23:39:35.164655   639 net.cpp:66] Creating Layer pool2
I0905 23:39:35.164660   639 net.cpp:329] pool2 <- conv2
I0905 23:39:35.164667   639 net.cpp:290] pool2 -> pool2
I0905 23:39:35.164675   639 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:39:35.164681   639 net.cpp:125] pool2 needs backward computation.
I0905 23:39:35.164688   639 net.cpp:66] Creating Layer fc7
I0905 23:39:35.164693   639 net.cpp:329] fc7 <- pool2
I0905 23:39:35.164700   639 net.cpp:290] fc7 -> fc7
I0905 23:39:35.815639   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.815683   639 net.cpp:125] fc7 needs backward computation.
I0905 23:39:35.815696   639 net.cpp:66] Creating Layer relu7
I0905 23:39:35.815703   639 net.cpp:329] relu7 <- fc7
I0905 23:39:35.815713   639 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:39:35.815722   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.815728   639 net.cpp:125] relu7 needs backward computation.
I0905 23:39:35.815737   639 net.cpp:66] Creating Layer drop7
I0905 23:39:35.815742   639 net.cpp:329] drop7 <- fc7
I0905 23:39:35.815748   639 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:39:35.815759   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.815764   639 net.cpp:125] drop7 needs backward computation.
I0905 23:39:35.815773   639 net.cpp:66] Creating Layer fc8
I0905 23:39:35.815779   639 net.cpp:329] fc8 <- fc7
I0905 23:39:35.815790   639 net.cpp:290] fc8 -> fc8
I0905 23:39:35.823573   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.823586   639 net.cpp:125] fc8 needs backward computation.
I0905 23:39:35.823593   639 net.cpp:66] Creating Layer relu8
I0905 23:39:35.823598   639 net.cpp:329] relu8 <- fc8
I0905 23:39:35.823606   639 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:39:35.823614   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.823619   639 net.cpp:125] relu8 needs backward computation.
I0905 23:39:35.823626   639 net.cpp:66] Creating Layer drop8
I0905 23:39:35.823632   639 net.cpp:329] drop8 <- fc8
I0905 23:39:35.823638   639 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:39:35.823645   639 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:39:35.823650   639 net.cpp:125] drop8 needs backward computation.
I0905 23:39:35.823658   639 net.cpp:66] Creating Layer fc9
I0905 23:39:35.823664   639 net.cpp:329] fc9 <- fc8
I0905 23:39:35.823671   639 net.cpp:290] fc9 -> fc9
I0905 23:39:35.824044   639 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:39:35.824066   639 net.cpp:125] fc9 needs backward computation.
I0905 23:39:35.824075   639 net.cpp:66] Creating Layer fc10
I0905 23:39:35.824080   639 net.cpp:329] fc10 <- fc9
I0905 23:39:35.824090   639 net.cpp:290] fc10 -> fc10
I0905 23:39:35.824100   639 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:39:35.824108   639 net.cpp:125] fc10 needs backward computation.
I0905 23:39:35.824115   639 net.cpp:66] Creating Layer prob
I0905 23:39:35.824121   639 net.cpp:329] prob <- fc10
I0905 23:39:35.824128   639 net.cpp:290] prob -> prob
I0905 23:39:35.824138   639 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:39:35.824144   639 net.cpp:125] prob needs backward computation.
I0905 23:39:35.824149   639 net.cpp:156] This network produces output prob
I0905 23:39:35.824162   639 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:39:35.824170   639 net.cpp:167] Network initialization done.
I0905 23:39:35.824175   639 net.cpp:168] Memory required for data: 6183480
Killed
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:42:43.558931   647 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:42:43.559136   647 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:42:43.559146   647 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:42:43.559315   647 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:42:43.559378   647 net.cpp:292] Input 0 -> data
I0905 23:42:43.559425   647 net.cpp:66] Creating Layer conv1
I0905 23:42:43.559434   647 net.cpp:329] conv1 <- data
I0905 23:42:43.559443   647 net.cpp:290] conv1 -> conv1
I0905 23:42:43.592586   647 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:42:43.592624   647 net.cpp:125] conv1 needs backward computation.
I0905 23:42:43.592641   647 net.cpp:66] Creating Layer relu1
I0905 23:42:43.592651   647 net.cpp:329] relu1 <- conv1
I0905 23:42:43.592664   647 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:42:43.592677   647 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:42:43.592687   647 net.cpp:125] relu1 needs backward computation.
I0905 23:42:43.592700   647 net.cpp:66] Creating Layer pool1
I0905 23:42:43.592708   647 net.cpp:329] pool1 <- conv1
I0905 23:42:43.592720   647 net.cpp:290] pool1 -> pool1
I0905 23:42:43.592737   647 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:42:43.592747   647 net.cpp:125] pool1 needs backward computation.
I0905 23:42:43.592758   647 net.cpp:66] Creating Layer norm1
I0905 23:42:43.592767   647 net.cpp:329] norm1 <- pool1
I0905 23:42:43.592778   647 net.cpp:290] norm1 -> norm1
I0905 23:42:43.592794   647 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:42:43.592804   647 net.cpp:125] norm1 needs backward computation.
I0905 23:42:43.592816   647 net.cpp:66] Creating Layer conv2
I0905 23:42:43.592826   647 net.cpp:329] conv2 <- norm1
I0905 23:42:43.592838   647 net.cpp:290] conv2 -> conv2
I0905 23:42:43.603457   647 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:42:43.603472   647 net.cpp:125] conv2 needs backward computation.
I0905 23:42:43.603481   647 net.cpp:66] Creating Layer relu2
I0905 23:42:43.603485   647 net.cpp:329] relu2 <- conv2
I0905 23:42:43.603492   647 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:42:43.603499   647 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:42:43.603505   647 net.cpp:125] relu2 needs backward computation.
I0905 23:42:43.603513   647 net.cpp:66] Creating Layer pool2
I0905 23:42:43.603518   647 net.cpp:329] pool2 <- conv2
I0905 23:42:43.603524   647 net.cpp:290] pool2 -> pool2
I0905 23:42:43.603533   647 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:42:43.603538   647 net.cpp:125] pool2 needs backward computation.
I0905 23:42:43.603546   647 net.cpp:66] Creating Layer fc7
I0905 23:42:43.603552   647 net.cpp:329] fc7 <- pool2
I0905 23:42:43.603560   647 net.cpp:290] fc7 -> fc7
I0905 23:42:44.260845   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.260890   647 net.cpp:125] fc7 needs backward computation.
I0905 23:42:44.260903   647 net.cpp:66] Creating Layer relu7
I0905 23:42:44.260910   647 net.cpp:329] relu7 <- fc7
I0905 23:42:44.260921   647 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:42:44.260931   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.260936   647 net.cpp:125] relu7 needs backward computation.
I0905 23:42:44.260943   647 net.cpp:66] Creating Layer drop7
I0905 23:42:44.260949   647 net.cpp:329] drop7 <- fc7
I0905 23:42:44.260956   647 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:42:44.260967   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.260972   647 net.cpp:125] drop7 needs backward computation.
I0905 23:42:44.260980   647 net.cpp:66] Creating Layer fc8
I0905 23:42:44.260987   647 net.cpp:329] fc8 <- fc7
I0905 23:42:44.260995   647 net.cpp:290] fc8 -> fc8
I0905 23:42:44.268781   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.268795   647 net.cpp:125] fc8 needs backward computation.
I0905 23:42:44.268801   647 net.cpp:66] Creating Layer relu8
I0905 23:42:44.268807   647 net.cpp:329] relu8 <- fc8
I0905 23:42:44.268815   647 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:42:44.268822   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.268828   647 net.cpp:125] relu8 needs backward computation.
I0905 23:42:44.268836   647 net.cpp:66] Creating Layer drop8
I0905 23:42:44.268841   647 net.cpp:329] drop8 <- fc8
I0905 23:42:44.268846   647 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:42:44.268853   647 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:42:44.268859   647 net.cpp:125] drop8 needs backward computation.
I0905 23:42:44.268868   647 net.cpp:66] Creating Layer fc9
I0905 23:42:44.268873   647 net.cpp:329] fc9 <- fc8
I0905 23:42:44.268880   647 net.cpp:290] fc9 -> fc9
I0905 23:42:44.269255   647 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:42:44.269268   647 net.cpp:125] fc9 needs backward computation.
I0905 23:42:44.269276   647 net.cpp:66] Creating Layer fc10
I0905 23:42:44.269281   647 net.cpp:329] fc10 <- fc9
I0905 23:42:44.269290   647 net.cpp:290] fc10 -> fc10
I0905 23:42:44.269302   647 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:42:44.269310   647 net.cpp:125] fc10 needs backward computation.
I0905 23:42:44.269316   647 net.cpp:66] Creating Layer prob
I0905 23:42:44.269322   647 net.cpp:329] prob <- fc10
I0905 23:42:44.269330   647 net.cpp:290] prob -> prob
I0905 23:42:44.269340   647 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:42:44.269345   647 net.cpp:125] prob needs backward computation.
I0905 23:42:44.269351   647 net.cpp:156] This network produces output prob
I0905 23:42:44.269363   647 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:42:44.269372   647 net.cpp:167] Network initialization done.
I0905 23:42:44.269377   647 net.cpp:168] Memory required for data: 6183480
Classifying 55 inputs.
Done in 35.37 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:43:24.062026   653 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:43:24.062175   653 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:43:24.062186   653 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:43:24.062343   653 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:43:24.062407   653 net.cpp:292] Input 0 -> data
I0905 23:43:24.062433   653 net.cpp:66] Creating Layer conv1
I0905 23:43:24.062439   653 net.cpp:329] conv1 <- data
I0905 23:43:24.062448   653 net.cpp:290] conv1 -> conv1
I0905 23:43:24.063807   653 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:43:24.063824   653 net.cpp:125] conv1 needs backward computation.
I0905 23:43:24.063834   653 net.cpp:66] Creating Layer relu1
I0905 23:43:24.063840   653 net.cpp:329] relu1 <- conv1
I0905 23:43:24.063848   653 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:43:24.063855   653 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:43:24.063861   653 net.cpp:125] relu1 needs backward computation.
I0905 23:43:24.063868   653 net.cpp:66] Creating Layer pool1
I0905 23:43:24.063874   653 net.cpp:329] pool1 <- conv1
I0905 23:43:24.063881   653 net.cpp:290] pool1 -> pool1
I0905 23:43:24.063892   653 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:43:24.063899   653 net.cpp:125] pool1 needs backward computation.
I0905 23:43:24.063906   653 net.cpp:66] Creating Layer norm1
I0905 23:43:24.063911   653 net.cpp:329] norm1 <- pool1
I0905 23:43:24.063918   653 net.cpp:290] norm1 -> norm1
I0905 23:43:24.063928   653 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:43:24.063935   653 net.cpp:125] norm1 needs backward computation.
I0905 23:43:24.063942   653 net.cpp:66] Creating Layer conv2
I0905 23:43:24.063948   653 net.cpp:329] conv2 <- norm1
I0905 23:43:24.063956   653 net.cpp:290] conv2 -> conv2
I0905 23:43:24.073086   653 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:43:24.073101   653 net.cpp:125] conv2 needs backward computation.
I0905 23:43:24.073109   653 net.cpp:66] Creating Layer relu2
I0905 23:43:24.073114   653 net.cpp:329] relu2 <- conv2
I0905 23:43:24.073122   653 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:43:24.073128   653 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:43:24.073134   653 net.cpp:125] relu2 needs backward computation.
I0905 23:43:24.073140   653 net.cpp:66] Creating Layer pool2
I0905 23:43:24.073145   653 net.cpp:329] pool2 <- conv2
I0905 23:43:24.073153   653 net.cpp:290] pool2 -> pool2
I0905 23:43:24.073160   653 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:43:24.073166   653 net.cpp:125] pool2 needs backward computation.
I0905 23:43:24.073175   653 net.cpp:66] Creating Layer fc7
I0905 23:43:24.073181   653 net.cpp:329] fc7 <- pool2
I0905 23:43:24.073194   653 net.cpp:290] fc7 -> fc7
I0905 23:43:24.720826   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.720872   653 net.cpp:125] fc7 needs backward computation.
I0905 23:43:24.720885   653 net.cpp:66] Creating Layer relu7
I0905 23:43:24.720892   653 net.cpp:329] relu7 <- fc7
I0905 23:43:24.720901   653 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:43:24.720911   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.720917   653 net.cpp:125] relu7 needs backward computation.
I0905 23:43:24.720924   653 net.cpp:66] Creating Layer drop7
I0905 23:43:24.720931   653 net.cpp:329] drop7 <- fc7
I0905 23:43:24.720937   653 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:43:24.720948   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.720954   653 net.cpp:125] drop7 needs backward computation.
I0905 23:43:24.720963   653 net.cpp:66] Creating Layer fc8
I0905 23:43:24.720968   653 net.cpp:329] fc8 <- fc7
I0905 23:43:24.720978   653 net.cpp:290] fc8 -> fc8
I0905 23:43:24.728750   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.728762   653 net.cpp:125] fc8 needs backward computation.
I0905 23:43:24.728770   653 net.cpp:66] Creating Layer relu8
I0905 23:43:24.728775   653 net.cpp:329] relu8 <- fc8
I0905 23:43:24.728783   653 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:43:24.728790   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.728796   653 net.cpp:125] relu8 needs backward computation.
I0905 23:43:24.728802   653 net.cpp:66] Creating Layer drop8
I0905 23:43:24.728808   653 net.cpp:329] drop8 <- fc8
I0905 23:43:24.728814   653 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:43:24.728821   653 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:43:24.728827   653 net.cpp:125] drop8 needs backward computation.
I0905 23:43:24.728835   653 net.cpp:66] Creating Layer fc9
I0905 23:43:24.728842   653 net.cpp:329] fc9 <- fc8
I0905 23:43:24.728848   653 net.cpp:290] fc9 -> fc9
I0905 23:43:24.729221   653 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:43:24.729233   653 net.cpp:125] fc9 needs backward computation.
I0905 23:43:24.729241   653 net.cpp:66] Creating Layer fc10
I0905 23:43:24.729248   653 net.cpp:329] fc10 <- fc9
I0905 23:43:24.729257   653 net.cpp:290] fc10 -> fc10
I0905 23:43:24.729269   653 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:43:24.729277   653 net.cpp:125] fc10 needs backward computation.
I0905 23:43:24.729285   653 net.cpp:66] Creating Layer prob
I0905 23:43:24.729290   653 net.cpp:329] prob <- fc10
I0905 23:43:24.729297   653 net.cpp:290] prob -> prob
I0905 23:43:24.729307   653 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:43:24.729312   653 net.cpp:125] prob needs backward computation.
I0905 23:43:24.729317   653 net.cpp:156] This network produces output prob
I0905 23:43:24.729331   653 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:43:24.729338   653 net.cpp:167] Network initialization done.
I0905 23:43:24.729343   653 net.cpp:168] Memory required for data: 6183480
Classifying 95 inputs.
Done in 57.90 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:44:25.392319   658 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:44:25.392460   658 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:44:25.392469   658 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:44:25.392617   658 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:44:25.392683   658 net.cpp:292] Input 0 -> data
I0905 23:44:25.392709   658 net.cpp:66] Creating Layer conv1
I0905 23:44:25.392715   658 net.cpp:329] conv1 <- data
I0905 23:44:25.392724   658 net.cpp:290] conv1 -> conv1
I0905 23:44:25.394105   658 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:44:25.394125   658 net.cpp:125] conv1 needs backward computation.
I0905 23:44:25.394135   658 net.cpp:66] Creating Layer relu1
I0905 23:44:25.394141   658 net.cpp:329] relu1 <- conv1
I0905 23:44:25.394150   658 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:44:25.394158   658 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:44:25.394165   658 net.cpp:125] relu1 needs backward computation.
I0905 23:44:25.394171   658 net.cpp:66] Creating Layer pool1
I0905 23:44:25.394177   658 net.cpp:329] pool1 <- conv1
I0905 23:44:25.394184   658 net.cpp:290] pool1 -> pool1
I0905 23:44:25.394196   658 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:44:25.394203   658 net.cpp:125] pool1 needs backward computation.
I0905 23:44:25.394211   658 net.cpp:66] Creating Layer norm1
I0905 23:44:25.394217   658 net.cpp:329] norm1 <- pool1
I0905 23:44:25.394223   658 net.cpp:290] norm1 -> norm1
I0905 23:44:25.394233   658 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:44:25.394239   658 net.cpp:125] norm1 needs backward computation.
I0905 23:44:25.394254   658 net.cpp:66] Creating Layer conv2
I0905 23:44:25.394264   658 net.cpp:329] conv2 <- norm1
I0905 23:44:25.394271   658 net.cpp:290] conv2 -> conv2
I0905 23:44:25.403412   658 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:44:25.403427   658 net.cpp:125] conv2 needs backward computation.
I0905 23:44:25.403435   658 net.cpp:66] Creating Layer relu2
I0905 23:44:25.403441   658 net.cpp:329] relu2 <- conv2
I0905 23:44:25.403448   658 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:44:25.403455   658 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:44:25.403461   658 net.cpp:125] relu2 needs backward computation.
I0905 23:44:25.403470   658 net.cpp:66] Creating Layer pool2
I0905 23:44:25.403475   658 net.cpp:329] pool2 <- conv2
I0905 23:44:25.403482   658 net.cpp:290] pool2 -> pool2
I0905 23:44:25.403491   658 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:44:25.403496   658 net.cpp:125] pool2 needs backward computation.
I0905 23:44:25.403504   658 net.cpp:66] Creating Layer fc7
I0905 23:44:25.403509   658 net.cpp:329] fc7 <- pool2
I0905 23:44:25.403517   658 net.cpp:290] fc7 -> fc7
I0905 23:44:26.053817   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.053861   658 net.cpp:125] fc7 needs backward computation.
I0905 23:44:26.053874   658 net.cpp:66] Creating Layer relu7
I0905 23:44:26.053881   658 net.cpp:329] relu7 <- fc7
I0905 23:44:26.053891   658 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:44:26.053901   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.053907   658 net.cpp:125] relu7 needs backward computation.
I0905 23:44:26.053915   658 net.cpp:66] Creating Layer drop7
I0905 23:44:26.053920   658 net.cpp:329] drop7 <- fc7
I0905 23:44:26.053926   658 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:44:26.053937   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.053943   658 net.cpp:125] drop7 needs backward computation.
I0905 23:44:26.053952   658 net.cpp:66] Creating Layer fc8
I0905 23:44:26.053957   658 net.cpp:329] fc8 <- fc7
I0905 23:44:26.053967   658 net.cpp:290] fc8 -> fc8
I0905 23:44:26.061744   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.061758   658 net.cpp:125] fc8 needs backward computation.
I0905 23:44:26.061765   658 net.cpp:66] Creating Layer relu8
I0905 23:44:26.061770   658 net.cpp:329] relu8 <- fc8
I0905 23:44:26.061779   658 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:44:26.061786   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.061792   658 net.cpp:125] relu8 needs backward computation.
I0905 23:44:26.061799   658 net.cpp:66] Creating Layer drop8
I0905 23:44:26.061805   658 net.cpp:329] drop8 <- fc8
I0905 23:44:26.061810   658 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:44:26.061817   658 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:44:26.061823   658 net.cpp:125] drop8 needs backward computation.
I0905 23:44:26.061832   658 net.cpp:66] Creating Layer fc9
I0905 23:44:26.061838   658 net.cpp:329] fc9 <- fc8
I0905 23:44:26.061846   658 net.cpp:290] fc9 -> fc9
I0905 23:44:26.062239   658 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:44:26.062252   658 net.cpp:125] fc9 needs backward computation.
I0905 23:44:26.062259   658 net.cpp:66] Creating Layer fc10
I0905 23:44:26.062265   658 net.cpp:329] fc10 <- fc9
I0905 23:44:26.062274   658 net.cpp:290] fc10 -> fc10
I0905 23:44:26.062286   658 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:44:26.062294   658 net.cpp:125] fc10 needs backward computation.
I0905 23:44:26.062300   658 net.cpp:66] Creating Layer prob
I0905 23:44:26.062306   658 net.cpp:329] prob <- fc10
I0905 23:44:26.062314   658 net.cpp:290] prob -> prob
I0905 23:44:26.062324   658 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:44:26.062330   658 net.cpp:125] prob needs backward computation.
I0905 23:44:26.062335   658 net.cpp:156] This network produces output prob
I0905 23:44:26.062347   658 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:44:26.062356   658 net.cpp:167] Network initialization done.
I0905 23:44:26.062361   658 net.cpp:168] Memory required for data: 6183480
Classifying 183 inputs.
Done in 112.99 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 83 is out of bounds for axis 0 with size 83
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:46:22.530347   665 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:46:22.530485   665 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:46:22.530495   665 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:46:22.530643   665 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:46:22.530697   665 net.cpp:292] Input 0 -> data
I0905 23:46:22.530733   665 net.cpp:66] Creating Layer conv1
I0905 23:46:22.530740   665 net.cpp:329] conv1 <- data
I0905 23:46:22.530750   665 net.cpp:290] conv1 -> conv1
I0905 23:46:22.532109   665 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:46:22.532126   665 net.cpp:125] conv1 needs backward computation.
I0905 23:46:22.532135   665 net.cpp:66] Creating Layer relu1
I0905 23:46:22.532141   665 net.cpp:329] relu1 <- conv1
I0905 23:46:22.532148   665 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:46:22.532157   665 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:46:22.532163   665 net.cpp:125] relu1 needs backward computation.
I0905 23:46:22.532169   665 net.cpp:66] Creating Layer pool1
I0905 23:46:22.532176   665 net.cpp:329] pool1 <- conv1
I0905 23:46:22.532181   665 net.cpp:290] pool1 -> pool1
I0905 23:46:22.532193   665 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:46:22.532199   665 net.cpp:125] pool1 needs backward computation.
I0905 23:46:22.532205   665 net.cpp:66] Creating Layer norm1
I0905 23:46:22.532212   665 net.cpp:329] norm1 <- pool1
I0905 23:46:22.532218   665 net.cpp:290] norm1 -> norm1
I0905 23:46:22.532227   665 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:46:22.532233   665 net.cpp:125] norm1 needs backward computation.
I0905 23:46:22.532240   665 net.cpp:66] Creating Layer conv2
I0905 23:46:22.532246   665 net.cpp:329] conv2 <- norm1
I0905 23:46:22.532253   665 net.cpp:290] conv2 -> conv2
I0905 23:46:22.541396   665 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:46:22.541412   665 net.cpp:125] conv2 needs backward computation.
I0905 23:46:22.541419   665 net.cpp:66] Creating Layer relu2
I0905 23:46:22.541425   665 net.cpp:329] relu2 <- conv2
I0905 23:46:22.541432   665 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:46:22.541440   665 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:46:22.541445   665 net.cpp:125] relu2 needs backward computation.
I0905 23:46:22.541453   665 net.cpp:66] Creating Layer pool2
I0905 23:46:22.541460   665 net.cpp:329] pool2 <- conv2
I0905 23:46:22.541465   665 net.cpp:290] pool2 -> pool2
I0905 23:46:22.541473   665 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:46:22.541479   665 net.cpp:125] pool2 needs backward computation.
I0905 23:46:22.541486   665 net.cpp:66] Creating Layer fc7
I0905 23:46:22.541491   665 net.cpp:329] fc7 <- pool2
I0905 23:46:22.541498   665 net.cpp:290] fc7 -> fc7
I0905 23:46:23.192026   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.192073   665 net.cpp:125] fc7 needs backward computation.
I0905 23:46:23.192086   665 net.cpp:66] Creating Layer relu7
I0905 23:46:23.192093   665 net.cpp:329] relu7 <- fc7
I0905 23:46:23.192102   665 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:46:23.192112   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.192118   665 net.cpp:125] relu7 needs backward computation.
I0905 23:46:23.192126   665 net.cpp:66] Creating Layer drop7
I0905 23:46:23.192131   665 net.cpp:329] drop7 <- fc7
I0905 23:46:23.192138   665 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:46:23.192148   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.192154   665 net.cpp:125] drop7 needs backward computation.
I0905 23:46:23.192163   665 net.cpp:66] Creating Layer fc8
I0905 23:46:23.192168   665 net.cpp:329] fc8 <- fc7
I0905 23:46:23.192178   665 net.cpp:290] fc8 -> fc8
I0905 23:46:23.199975   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.199987   665 net.cpp:125] fc8 needs backward computation.
I0905 23:46:23.199995   665 net.cpp:66] Creating Layer relu8
I0905 23:46:23.200000   665 net.cpp:329] relu8 <- fc8
I0905 23:46:23.200007   665 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:46:23.200016   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.200021   665 net.cpp:125] relu8 needs backward computation.
I0905 23:46:23.200026   665 net.cpp:66] Creating Layer drop8
I0905 23:46:23.200032   665 net.cpp:329] drop8 <- fc8
I0905 23:46:23.200038   665 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:46:23.200045   665 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:46:23.200062   665 net.cpp:125] drop8 needs backward computation.
I0905 23:46:23.200070   665 net.cpp:66] Creating Layer fc9
I0905 23:46:23.200076   665 net.cpp:329] fc9 <- fc8
I0905 23:46:23.200083   665 net.cpp:290] fc9 -> fc9
I0905 23:46:23.200459   665 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:46:23.200470   665 net.cpp:125] fc9 needs backward computation.
I0905 23:46:23.200479   665 net.cpp:66] Creating Layer fc10
I0905 23:46:23.200484   665 net.cpp:329] fc10 <- fc9
I0905 23:46:23.200492   665 net.cpp:290] fc10 -> fc10
I0905 23:46:23.200503   665 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:46:23.200511   665 net.cpp:125] fc10 needs backward computation.
I0905 23:46:23.200518   665 net.cpp:66] Creating Layer prob
I0905 23:46:23.200523   665 net.cpp:329] prob <- fc10
I0905 23:46:23.200531   665 net.cpp:290] prob -> prob
I0905 23:46:23.200541   665 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:46:23.200546   665 net.cpp:125] prob needs backward computation.
I0905 23:46:23.200551   665 net.cpp:156] This network produces output prob
I0905 23:46:23.200564   665 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:46:23.200572   665 net.cpp:167] Network initialization done.
I0905 23:46:23.200577   665 net.cpp:168] Memory required for data: 6183480
Classifying 225 inputs.
Done in 148.24 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 25 is out of bounds for axis 0 with size 25
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0905 23:48:58.478178   673 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0905 23:48:58.478318   673 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0905 23:48:58.478327   673 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0905 23:48:58.478474   673 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0905 23:48:58.478539   673 net.cpp:292] Input 0 -> data
I0905 23:48:58.478565   673 net.cpp:66] Creating Layer conv1
I0905 23:48:58.478572   673 net.cpp:329] conv1 <- data
I0905 23:48:58.478580   673 net.cpp:290] conv1 -> conv1
I0905 23:48:58.479939   673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:48:58.479957   673 net.cpp:125] conv1 needs backward computation.
I0905 23:48:58.479966   673 net.cpp:66] Creating Layer relu1
I0905 23:48:58.479972   673 net.cpp:329] relu1 <- conv1
I0905 23:48:58.479979   673 net.cpp:280] relu1 -> conv1 (in-place)
I0905 23:48:58.479989   673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0905 23:48:58.479993   673 net.cpp:125] relu1 needs backward computation.
I0905 23:48:58.480000   673 net.cpp:66] Creating Layer pool1
I0905 23:48:58.480006   673 net.cpp:329] pool1 <- conv1
I0905 23:48:58.480012   673 net.cpp:290] pool1 -> pool1
I0905 23:48:58.480023   673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:48:58.480029   673 net.cpp:125] pool1 needs backward computation.
I0905 23:48:58.480036   673 net.cpp:66] Creating Layer norm1
I0905 23:48:58.480042   673 net.cpp:329] norm1 <- pool1
I0905 23:48:58.480048   673 net.cpp:290] norm1 -> norm1
I0905 23:48:58.480058   673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0905 23:48:58.480064   673 net.cpp:125] norm1 needs backward computation.
I0905 23:48:58.480072   673 net.cpp:66] Creating Layer conv2
I0905 23:48:58.480077   673 net.cpp:329] conv2 <- norm1
I0905 23:48:58.480084   673 net.cpp:290] conv2 -> conv2
I0905 23:48:58.489332   673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:48:58.489349   673 net.cpp:125] conv2 needs backward computation.
I0905 23:48:58.489356   673 net.cpp:66] Creating Layer relu2
I0905 23:48:58.489362   673 net.cpp:329] relu2 <- conv2
I0905 23:48:58.489368   673 net.cpp:280] relu2 -> conv2 (in-place)
I0905 23:48:58.489377   673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0905 23:48:58.489382   673 net.cpp:125] relu2 needs backward computation.
I0905 23:48:58.489390   673 net.cpp:66] Creating Layer pool2
I0905 23:48:58.489397   673 net.cpp:329] pool2 <- conv2
I0905 23:48:58.489403   673 net.cpp:290] pool2 -> pool2
I0905 23:48:58.489411   673 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0905 23:48:58.489418   673 net.cpp:125] pool2 needs backward computation.
I0905 23:48:58.489424   673 net.cpp:66] Creating Layer fc7
I0905 23:48:58.489429   673 net.cpp:329] fc7 <- pool2
I0905 23:48:58.489436   673 net.cpp:290] fc7 -> fc7
I0905 23:48:59.181674   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.181720   673 net.cpp:125] fc7 needs backward computation.
I0905 23:48:59.181732   673 net.cpp:66] Creating Layer relu7
I0905 23:48:59.181740   673 net.cpp:329] relu7 <- fc7
I0905 23:48:59.181748   673 net.cpp:280] relu7 -> fc7 (in-place)
I0905 23:48:59.181769   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.181776   673 net.cpp:125] relu7 needs backward computation.
I0905 23:48:59.181783   673 net.cpp:66] Creating Layer drop7
I0905 23:48:59.181788   673 net.cpp:329] drop7 <- fc7
I0905 23:48:59.181795   673 net.cpp:280] drop7 -> fc7 (in-place)
I0905 23:48:59.181805   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.181812   673 net.cpp:125] drop7 needs backward computation.
I0905 23:48:59.181820   673 net.cpp:66] Creating Layer fc8
I0905 23:48:59.181825   673 net.cpp:329] fc8 <- fc7
I0905 23:48:59.181834   673 net.cpp:290] fc8 -> fc8
I0905 23:48:59.190145   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.190157   673 net.cpp:125] fc8 needs backward computation.
I0905 23:48:59.190165   673 net.cpp:66] Creating Layer relu8
I0905 23:48:59.190171   673 net.cpp:329] relu8 <- fc8
I0905 23:48:59.190178   673 net.cpp:280] relu8 -> fc8 (in-place)
I0905 23:48:59.190186   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.190191   673 net.cpp:125] relu8 needs backward computation.
I0905 23:48:59.190196   673 net.cpp:66] Creating Layer drop8
I0905 23:48:59.190202   673 net.cpp:329] drop8 <- fc8
I0905 23:48:59.190208   673 net.cpp:280] drop8 -> fc8 (in-place)
I0905 23:48:59.190215   673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0905 23:48:59.190220   673 net.cpp:125] drop8 needs backward computation.
I0905 23:48:59.190229   673 net.cpp:66] Creating Layer fc9
I0905 23:48:59.190234   673 net.cpp:329] fc9 <- fc8
I0905 23:48:59.190242   673 net.cpp:290] fc9 -> fc9
I0905 23:48:59.190639   673 net.cpp:83] Top shape: 10 24 1 1 (240)
I0905 23:48:59.190651   673 net.cpp:125] fc9 needs backward computation.
I0905 23:48:59.190660   673 net.cpp:66] Creating Layer fc10
I0905 23:48:59.190665   673 net.cpp:329] fc10 <- fc9
I0905 23:48:59.190673   673 net.cpp:290] fc10 -> fc10
I0905 23:48:59.190685   673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:48:59.190693   673 net.cpp:125] fc10 needs backward computation.
I0905 23:48:59.190701   673 net.cpp:66] Creating Layer prob
I0905 23:48:59.190706   673 net.cpp:329] prob <- fc10
I0905 23:48:59.190713   673 net.cpp:290] prob -> prob
I0905 23:48:59.190722   673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0905 23:48:59.190728   673 net.cpp:125] prob needs backward computation.
I0905 23:48:59.190733   673 net.cpp:156] This network produces output prob
I0905 23:48:59.190747   673 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0905 23:48:59.190754   673 net.cpp:167] Network initialization done.
I0905 23:48:59.190759   673 net.cpp:168] Memory required for data: 6183480
Classifying 354 inputs.
Done in 218.68 s.
Traceback (most recent call last):
  File "distribute/python/classify_m.py", line 156, in <module>
    main(sys.argv)
  File "distribute/python/classify_m.py", line 145, in main
    prediction = predictions[i]
IndexError: index 54 is out of bounds for axis 0 with size 54

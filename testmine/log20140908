nohup: ignoring input
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:16:16.145210 25586 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:16:16.145431 25586 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:16:16.145441 25586 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:16:16.145614 25586 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:16:16.145664 25586 net.cpp:292] Input 0 -> data
I0908 23:16:16.145689 25586 net.cpp:66] Creating Layer conv1
I0908 23:16:16.145696 25586 net.cpp:329] conv1 <- data
I0908 23:16:16.145704 25586 net.cpp:290] conv1 -> conv1
I0908 23:16:16.147045 25586 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:16:16.147063 25586 net.cpp:125] conv1 needs backward computation.
I0908 23:16:16.147078 25586 net.cpp:66] Creating Layer relu1
I0908 23:16:16.147085 25586 net.cpp:329] relu1 <- conv1
I0908 23:16:16.147091 25586 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:16:16.147100 25586 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:16:16.147105 25586 net.cpp:125] relu1 needs backward computation.
I0908 23:16:16.147112 25586 net.cpp:66] Creating Layer pool1
I0908 23:16:16.147117 25586 net.cpp:329] pool1 <- conv1
I0908 23:16:16.147125 25586 net.cpp:290] pool1 -> pool1
I0908 23:16:16.147135 25586 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:16:16.147140 25586 net.cpp:125] pool1 needs backward computation.
I0908 23:16:16.147147 25586 net.cpp:66] Creating Layer norm1
I0908 23:16:16.147152 25586 net.cpp:329] norm1 <- pool1
I0908 23:16:16.147158 25586 net.cpp:290] norm1 -> norm1
I0908 23:16:16.160267 25586 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:16:16.160302 25586 net.cpp:125] norm1 needs backward computation.
I0908 23:16:16.160327 25586 net.cpp:66] Creating Layer conv2
I0908 23:16:16.160333 25586 net.cpp:329] conv2 <- norm1
I0908 23:16:16.160341 25586 net.cpp:290] conv2 -> conv2
I0908 23:16:16.169234 25586 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:16:16.169250 25586 net.cpp:125] conv2 needs backward computation.
I0908 23:16:16.169256 25586 net.cpp:66] Creating Layer relu2
I0908 23:16:16.169262 25586 net.cpp:329] relu2 <- conv2
I0908 23:16:16.169268 25586 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:16:16.169275 25586 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:16:16.169281 25586 net.cpp:125] relu2 needs backward computation.
I0908 23:16:16.169289 25586 net.cpp:66] Creating Layer pool2
I0908 23:16:16.169294 25586 net.cpp:329] pool2 <- conv2
I0908 23:16:16.169301 25586 net.cpp:290] pool2 -> pool2
I0908 23:16:16.169309 25586 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:16:16.169314 25586 net.cpp:125] pool2 needs backward computation.
I0908 23:16:16.169322 25586 net.cpp:66] Creating Layer fc7
I0908 23:16:16.169327 25586 net.cpp:329] fc7 <- pool2
I0908 23:16:16.169333 25586 net.cpp:290] fc7 -> fc7
I0908 23:16:16.799526 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.799573 25586 net.cpp:125] fc7 needs backward computation.
I0908 23:16:16.799587 25586 net.cpp:66] Creating Layer relu7
I0908 23:16:16.799593 25586 net.cpp:329] relu7 <- fc7
I0908 23:16:16.799602 25586 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:16:16.799612 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.799618 25586 net.cpp:125] relu7 needs backward computation.
I0908 23:16:16.799625 25586 net.cpp:66] Creating Layer drop7
I0908 23:16:16.799630 25586 net.cpp:329] drop7 <- fc7
I0908 23:16:16.799636 25586 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:16:16.799648 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.799653 25586 net.cpp:125] drop7 needs backward computation.
I0908 23:16:16.799660 25586 net.cpp:66] Creating Layer fc8
I0908 23:16:16.799666 25586 net.cpp:329] fc8 <- fc7
I0908 23:16:16.799676 25586 net.cpp:290] fc8 -> fc8
I0908 23:16:16.807256 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.807268 25586 net.cpp:125] fc8 needs backward computation.
I0908 23:16:16.807276 25586 net.cpp:66] Creating Layer relu8
I0908 23:16:16.807281 25586 net.cpp:329] relu8 <- fc8
I0908 23:16:16.807288 25586 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:16:16.807296 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.807301 25586 net.cpp:125] relu8 needs backward computation.
I0908 23:16:16.807307 25586 net.cpp:66] Creating Layer drop8
I0908 23:16:16.807312 25586 net.cpp:329] drop8 <- fc8
I0908 23:16:16.807318 25586 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:16:16.807325 25586 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:16:16.807330 25586 net.cpp:125] drop8 needs backward computation.
I0908 23:16:16.807338 25586 net.cpp:66] Creating Layer fc9
I0908 23:16:16.807344 25586 net.cpp:329] fc9 <- fc8
I0908 23:16:16.807351 25586 net.cpp:290] fc9 -> fc9
I0908 23:16:16.807725 25586 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:16:16.807747 25586 net.cpp:125] fc9 needs backward computation.
I0908 23:16:16.807755 25586 net.cpp:66] Creating Layer fc10
I0908 23:16:16.807761 25586 net.cpp:329] fc10 <- fc9
I0908 23:16:16.807770 25586 net.cpp:290] fc10 -> fc10
I0908 23:16:16.807781 25586 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:16:16.807790 25586 net.cpp:125] fc10 needs backward computation.
I0908 23:16:16.807796 25586 net.cpp:66] Creating Layer prob
I0908 23:16:16.807801 25586 net.cpp:329] prob <- fc10
I0908 23:16:16.807809 25586 net.cpp:290] prob -> prob
I0908 23:16:16.807819 25586 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:16:16.807826 25586 net.cpp:125] prob needs backward computation.
I0908 23:16:16.807831 25586 net.cpp:156] This network produces output prob
I0908 23:16:16.807842 25586 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:16:16.807852 25586 net.cpp:167] Network initialization done.
I0908 23:16:16.807857 25586 net.cpp:168] Memory required for data: 6183480
Classifying 919 inputs.
Done in 544.85 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:25:35.764122 25657 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:25:35.764257 25657 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:25:35.764266 25657 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:25:35.764411 25657 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:25:35.764472 25657 net.cpp:292] Input 0 -> data
I0908 23:25:35.764497 25657 net.cpp:66] Creating Layer conv1
I0908 23:25:35.764504 25657 net.cpp:329] conv1 <- data
I0908 23:25:35.764513 25657 net.cpp:290] conv1 -> conv1
I0908 23:25:35.765877 25657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:25:35.765897 25657 net.cpp:125] conv1 needs backward computation.
I0908 23:25:35.765905 25657 net.cpp:66] Creating Layer relu1
I0908 23:25:35.765911 25657 net.cpp:329] relu1 <- conv1
I0908 23:25:35.765918 25657 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:25:35.765926 25657 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:25:35.765933 25657 net.cpp:125] relu1 needs backward computation.
I0908 23:25:35.765939 25657 net.cpp:66] Creating Layer pool1
I0908 23:25:35.765944 25657 net.cpp:329] pool1 <- conv1
I0908 23:25:35.765951 25657 net.cpp:290] pool1 -> pool1
I0908 23:25:35.765962 25657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:25:35.765969 25657 net.cpp:125] pool1 needs backward computation.
I0908 23:25:35.765975 25657 net.cpp:66] Creating Layer norm1
I0908 23:25:35.765981 25657 net.cpp:329] norm1 <- pool1
I0908 23:25:35.765987 25657 net.cpp:290] norm1 -> norm1
I0908 23:25:35.765997 25657 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:25:35.766003 25657 net.cpp:125] norm1 needs backward computation.
I0908 23:25:35.766010 25657 net.cpp:66] Creating Layer conv2
I0908 23:25:35.766016 25657 net.cpp:329] conv2 <- norm1
I0908 23:25:35.766022 25657 net.cpp:290] conv2 -> conv2
I0908 23:25:35.774870 25657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:25:35.774884 25657 net.cpp:125] conv2 needs backward computation.
I0908 23:25:35.774891 25657 net.cpp:66] Creating Layer relu2
I0908 23:25:35.774896 25657 net.cpp:329] relu2 <- conv2
I0908 23:25:35.774904 25657 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:25:35.774910 25657 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:25:35.774915 25657 net.cpp:125] relu2 needs backward computation.
I0908 23:25:35.774922 25657 net.cpp:66] Creating Layer pool2
I0908 23:25:35.774927 25657 net.cpp:329] pool2 <- conv2
I0908 23:25:35.774934 25657 net.cpp:290] pool2 -> pool2
I0908 23:25:35.774941 25657 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:25:35.774947 25657 net.cpp:125] pool2 needs backward computation.
I0908 23:25:35.774956 25657 net.cpp:66] Creating Layer fc7
I0908 23:25:35.774962 25657 net.cpp:329] fc7 <- pool2
I0908 23:25:35.774970 25657 net.cpp:290] fc7 -> fc7
I0908 23:25:36.410609 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.410655 25657 net.cpp:125] fc7 needs backward computation.
I0908 23:25:36.410666 25657 net.cpp:66] Creating Layer relu7
I0908 23:25:36.410676 25657 net.cpp:329] relu7 <- fc7
I0908 23:25:36.410683 25657 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:25:36.410693 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.410698 25657 net.cpp:125] relu7 needs backward computation.
I0908 23:25:36.410706 25657 net.cpp:66] Creating Layer drop7
I0908 23:25:36.410712 25657 net.cpp:329] drop7 <- fc7
I0908 23:25:36.410717 25657 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:25:36.410728 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.410734 25657 net.cpp:125] drop7 needs backward computation.
I0908 23:25:36.410742 25657 net.cpp:66] Creating Layer fc8
I0908 23:25:36.410748 25657 net.cpp:329] fc8 <- fc7
I0908 23:25:36.410766 25657 net.cpp:290] fc8 -> fc8
I0908 23:25:36.418313 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.418324 25657 net.cpp:125] fc8 needs backward computation.
I0908 23:25:36.418331 25657 net.cpp:66] Creating Layer relu8
I0908 23:25:36.418337 25657 net.cpp:329] relu8 <- fc8
I0908 23:25:36.418345 25657 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:25:36.418352 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.418359 25657 net.cpp:125] relu8 needs backward computation.
I0908 23:25:36.418365 25657 net.cpp:66] Creating Layer drop8
I0908 23:25:36.418370 25657 net.cpp:329] drop8 <- fc8
I0908 23:25:36.418376 25657 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:25:36.418385 25657 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:25:36.418390 25657 net.cpp:125] drop8 needs backward computation.
I0908 23:25:36.418398 25657 net.cpp:66] Creating Layer fc9
I0908 23:25:36.418403 25657 net.cpp:329] fc9 <- fc8
I0908 23:25:36.418411 25657 net.cpp:290] fc9 -> fc9
I0908 23:25:36.418776 25657 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:25:36.418786 25657 net.cpp:125] fc9 needs backward computation.
I0908 23:25:36.418794 25657 net.cpp:66] Creating Layer fc10
I0908 23:25:36.418799 25657 net.cpp:329] fc10 <- fc9
I0908 23:25:36.418808 25657 net.cpp:290] fc10 -> fc10
I0908 23:25:36.418819 25657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:25:36.418828 25657 net.cpp:125] fc10 needs backward computation.
I0908 23:25:36.418834 25657 net.cpp:66] Creating Layer prob
I0908 23:25:36.418840 25657 net.cpp:329] prob <- fc10
I0908 23:25:36.418848 25657 net.cpp:290] prob -> prob
I0908 23:25:36.418858 25657 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:25:36.418864 25657 net.cpp:125] prob needs backward computation.
I0908 23:25:36.418867 25657 net.cpp:156] This network produces output prob
I0908 23:25:36.418880 25657 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:25:36.418889 25657 net.cpp:167] Network initialization done.
I0908 23:25:36.418895 25657 net.cpp:168] Memory required for data: 6183480
Classifying 544 inputs.
Done in 350.31 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:31:53.518332 25668 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:31:53.518481 25668 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:31:53.518491 25668 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:31:53.518636 25668 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:31:53.518695 25668 net.cpp:292] Input 0 -> data
I0908 23:31:53.518721 25668 net.cpp:66] Creating Layer conv1
I0908 23:31:53.518728 25668 net.cpp:329] conv1 <- data
I0908 23:31:53.518738 25668 net.cpp:290] conv1 -> conv1
I0908 23:31:53.543658 25668 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:31:53.543681 25668 net.cpp:125] conv1 needs backward computation.
I0908 23:31:53.543691 25668 net.cpp:66] Creating Layer relu1
I0908 23:31:53.543699 25668 net.cpp:329] relu1 <- conv1
I0908 23:31:53.543705 25668 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:31:53.543715 25668 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:31:53.543720 25668 net.cpp:125] relu1 needs backward computation.
I0908 23:31:53.543727 25668 net.cpp:66] Creating Layer pool1
I0908 23:31:53.543732 25668 net.cpp:329] pool1 <- conv1
I0908 23:31:53.543740 25668 net.cpp:290] pool1 -> pool1
I0908 23:31:53.543751 25668 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:31:53.543756 25668 net.cpp:125] pool1 needs backward computation.
I0908 23:31:53.543763 25668 net.cpp:66] Creating Layer norm1
I0908 23:31:53.543768 25668 net.cpp:329] norm1 <- pool1
I0908 23:31:53.543776 25668 net.cpp:290] norm1 -> norm1
I0908 23:31:53.543784 25668 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:31:53.543790 25668 net.cpp:125] norm1 needs backward computation.
I0908 23:31:53.543797 25668 net.cpp:66] Creating Layer conv2
I0908 23:31:53.543803 25668 net.cpp:329] conv2 <- norm1
I0908 23:31:53.543809 25668 net.cpp:290] conv2 -> conv2
I0908 23:31:53.552645 25668 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:31:53.552659 25668 net.cpp:125] conv2 needs backward computation.
I0908 23:31:53.552666 25668 net.cpp:66] Creating Layer relu2
I0908 23:31:53.552671 25668 net.cpp:329] relu2 <- conv2
I0908 23:31:53.552678 25668 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:31:53.552685 25668 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:31:53.552690 25668 net.cpp:125] relu2 needs backward computation.
I0908 23:31:53.552696 25668 net.cpp:66] Creating Layer pool2
I0908 23:31:53.552702 25668 net.cpp:329] pool2 <- conv2
I0908 23:31:53.552708 25668 net.cpp:290] pool2 -> pool2
I0908 23:31:53.552716 25668 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:31:53.552722 25668 net.cpp:125] pool2 needs backward computation.
I0908 23:31:53.552731 25668 net.cpp:66] Creating Layer fc7
I0908 23:31:53.552742 25668 net.cpp:329] fc7 <- pool2
I0908 23:31:53.552749 25668 net.cpp:290] fc7 -> fc7
I0908 23:31:54.177075 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.177124 25668 net.cpp:125] fc7 needs backward computation.
I0908 23:31:54.177136 25668 net.cpp:66] Creating Layer relu7
I0908 23:31:54.177145 25668 net.cpp:329] relu7 <- fc7
I0908 23:31:54.177153 25668 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:31:54.177162 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.177168 25668 net.cpp:125] relu7 needs backward computation.
I0908 23:31:54.177175 25668 net.cpp:66] Creating Layer drop7
I0908 23:31:54.177181 25668 net.cpp:329] drop7 <- fc7
I0908 23:31:54.177187 25668 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:31:54.177198 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.177203 25668 net.cpp:125] drop7 needs backward computation.
I0908 23:31:54.177211 25668 net.cpp:66] Creating Layer fc8
I0908 23:31:54.177217 25668 net.cpp:329] fc8 <- fc7
I0908 23:31:54.177225 25668 net.cpp:290] fc8 -> fc8
I0908 23:31:54.184778 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.184790 25668 net.cpp:125] fc8 needs backward computation.
I0908 23:31:54.184798 25668 net.cpp:66] Creating Layer relu8
I0908 23:31:54.184803 25668 net.cpp:329] relu8 <- fc8
I0908 23:31:54.184810 25668 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:31:54.184818 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.184823 25668 net.cpp:125] relu8 needs backward computation.
I0908 23:31:54.184829 25668 net.cpp:66] Creating Layer drop8
I0908 23:31:54.184835 25668 net.cpp:329] drop8 <- fc8
I0908 23:31:54.184841 25668 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:31:54.184849 25668 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:31:54.184855 25668 net.cpp:125] drop8 needs backward computation.
I0908 23:31:54.184862 25668 net.cpp:66] Creating Layer fc9
I0908 23:31:54.184869 25668 net.cpp:329] fc9 <- fc8
I0908 23:31:54.184875 25668 net.cpp:290] fc9 -> fc9
I0908 23:31:54.185237 25668 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:31:54.185247 25668 net.cpp:125] fc9 needs backward computation.
I0908 23:31:54.185256 25668 net.cpp:66] Creating Layer fc10
I0908 23:31:54.185261 25668 net.cpp:329] fc10 <- fc9
I0908 23:31:54.185269 25668 net.cpp:290] fc10 -> fc10
I0908 23:31:54.185281 25668 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:31:54.185288 25668 net.cpp:125] fc10 needs backward computation.
I0908 23:31:54.185295 25668 net.cpp:66] Creating Layer prob
I0908 23:31:54.185300 25668 net.cpp:329] prob <- fc10
I0908 23:31:54.185308 25668 net.cpp:290] prob -> prob
I0908 23:31:54.185317 25668 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:31:54.185323 25668 net.cpp:125] prob needs backward computation.
I0908 23:31:54.185328 25668 net.cpp:156] This network produces output prob
I0908 23:31:54.185341 25668 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:31:54.185349 25668 net.cpp:167] Network initialization done.
I0908 23:31:54.185354 25668 net.cpp:168] Memory required for data: 6183480
Classifying 78 inputs.
Done in 47.59 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:32:47.583238 25673 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:32:47.583375 25673 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:32:47.583385 25673 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:32:47.583528 25673 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:32:47.583588 25673 net.cpp:292] Input 0 -> data
I0908 23:32:47.583614 25673 net.cpp:66] Creating Layer conv1
I0908 23:32:47.583621 25673 net.cpp:329] conv1 <- data
I0908 23:32:47.583629 25673 net.cpp:290] conv1 -> conv1
I0908 23:32:47.584950 25673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:32:47.584969 25673 net.cpp:125] conv1 needs backward computation.
I0908 23:32:47.584977 25673 net.cpp:66] Creating Layer relu1
I0908 23:32:47.584983 25673 net.cpp:329] relu1 <- conv1
I0908 23:32:47.584990 25673 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:32:47.585000 25673 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:32:47.585005 25673 net.cpp:125] relu1 needs backward computation.
I0908 23:32:47.585011 25673 net.cpp:66] Creating Layer pool1
I0908 23:32:47.585017 25673 net.cpp:329] pool1 <- conv1
I0908 23:32:47.585023 25673 net.cpp:290] pool1 -> pool1
I0908 23:32:47.585034 25673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:32:47.585041 25673 net.cpp:125] pool1 needs backward computation.
I0908 23:32:47.585047 25673 net.cpp:66] Creating Layer norm1
I0908 23:32:47.585052 25673 net.cpp:329] norm1 <- pool1
I0908 23:32:47.585058 25673 net.cpp:290] norm1 -> norm1
I0908 23:32:47.585067 25673 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:32:47.585073 25673 net.cpp:125] norm1 needs backward computation.
I0908 23:32:47.585085 25673 net.cpp:66] Creating Layer conv2
I0908 23:32:47.585091 25673 net.cpp:329] conv2 <- norm1
I0908 23:32:47.585098 25673 net.cpp:290] conv2 -> conv2
I0908 23:32:47.593997 25673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:32:47.594012 25673 net.cpp:125] conv2 needs backward computation.
I0908 23:32:47.594018 25673 net.cpp:66] Creating Layer relu2
I0908 23:32:47.594023 25673 net.cpp:329] relu2 <- conv2
I0908 23:32:47.594030 25673 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:32:47.594038 25673 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:32:47.594043 25673 net.cpp:125] relu2 needs backward computation.
I0908 23:32:47.594051 25673 net.cpp:66] Creating Layer pool2
I0908 23:32:47.594058 25673 net.cpp:329] pool2 <- conv2
I0908 23:32:47.594064 25673 net.cpp:290] pool2 -> pool2
I0908 23:32:47.594071 25673 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:32:47.594077 25673 net.cpp:125] pool2 needs backward computation.
I0908 23:32:47.594084 25673 net.cpp:66] Creating Layer fc7
I0908 23:32:47.594089 25673 net.cpp:329] fc7 <- pool2
I0908 23:32:47.594096 25673 net.cpp:290] fc7 -> fc7
I0908 23:32:48.225610 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.225658 25673 net.cpp:125] fc7 needs backward computation.
I0908 23:32:48.225672 25673 net.cpp:66] Creating Layer relu7
I0908 23:32:48.225678 25673 net.cpp:329] relu7 <- fc7
I0908 23:32:48.225687 25673 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:32:48.225698 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.225704 25673 net.cpp:125] relu7 needs backward computation.
I0908 23:32:48.225713 25673 net.cpp:66] Creating Layer drop7
I0908 23:32:48.225718 25673 net.cpp:329] drop7 <- fc7
I0908 23:32:48.225723 25673 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:32:48.225743 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.225749 25673 net.cpp:125] drop7 needs backward computation.
I0908 23:32:48.225756 25673 net.cpp:66] Creating Layer fc8
I0908 23:32:48.225761 25673 net.cpp:329] fc8 <- fc7
I0908 23:32:48.225770 25673 net.cpp:290] fc8 -> fc8
I0908 23:32:48.233320 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.233332 25673 net.cpp:125] fc8 needs backward computation.
I0908 23:32:48.233340 25673 net.cpp:66] Creating Layer relu8
I0908 23:32:48.233345 25673 net.cpp:329] relu8 <- fc8
I0908 23:32:48.233355 25673 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:32:48.233361 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.233367 25673 net.cpp:125] relu8 needs backward computation.
I0908 23:32:48.233373 25673 net.cpp:66] Creating Layer drop8
I0908 23:32:48.233378 25673 net.cpp:329] drop8 <- fc8
I0908 23:32:48.233384 25673 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:32:48.233392 25673 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:32:48.233397 25673 net.cpp:125] drop8 needs backward computation.
I0908 23:32:48.233405 25673 net.cpp:66] Creating Layer fc9
I0908 23:32:48.233410 25673 net.cpp:329] fc9 <- fc8
I0908 23:32:48.233417 25673 net.cpp:290] fc9 -> fc9
I0908 23:32:48.233788 25673 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:32:48.233803 25673 net.cpp:125] fc9 needs backward computation.
I0908 23:32:48.233810 25673 net.cpp:66] Creating Layer fc10
I0908 23:32:48.233815 25673 net.cpp:329] fc10 <- fc9
I0908 23:32:48.233824 25673 net.cpp:290] fc10 -> fc10
I0908 23:32:48.233836 25673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:32:48.233844 25673 net.cpp:125] fc10 needs backward computation.
I0908 23:32:48.233850 25673 net.cpp:66] Creating Layer prob
I0908 23:32:48.233855 25673 net.cpp:329] prob <- fc10
I0908 23:32:48.233863 25673 net.cpp:290] prob -> prob
I0908 23:32:48.233872 25673 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:32:48.233878 25673 net.cpp:125] prob needs backward computation.
I0908 23:32:48.233883 25673 net.cpp:156] This network produces output prob
I0908 23:32:48.233896 25673 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:32:48.233904 25673 net.cpp:167] Network initialization done.
I0908 23:32:48.233909 25673 net.cpp:168] Memory required for data: 6183480
Classifying 240 inputs.
Done in 138.49 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:35:10.892082 25678 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:35:10.892220 25678 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:35:10.892230 25678 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:35:10.892372 25678 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:35:10.892423 25678 net.cpp:292] Input 0 -> data
I0908 23:35:10.892448 25678 net.cpp:66] Creating Layer conv1
I0908 23:35:10.892456 25678 net.cpp:329] conv1 <- data
I0908 23:35:10.892463 25678 net.cpp:290] conv1 -> conv1
I0908 23:35:10.893823 25678 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:35:10.893851 25678 net.cpp:125] conv1 needs backward computation.
I0908 23:35:10.893861 25678 net.cpp:66] Creating Layer relu1
I0908 23:35:10.893867 25678 net.cpp:329] relu1 <- conv1
I0908 23:35:10.893873 25678 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:35:10.893882 25678 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:35:10.893887 25678 net.cpp:125] relu1 needs backward computation.
I0908 23:35:10.893894 25678 net.cpp:66] Creating Layer pool1
I0908 23:35:10.893899 25678 net.cpp:329] pool1 <- conv1
I0908 23:35:10.893906 25678 net.cpp:290] pool1 -> pool1
I0908 23:35:10.893918 25678 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:35:10.893923 25678 net.cpp:125] pool1 needs backward computation.
I0908 23:35:10.893929 25678 net.cpp:66] Creating Layer norm1
I0908 23:35:10.893935 25678 net.cpp:329] norm1 <- pool1
I0908 23:35:10.893941 25678 net.cpp:290] norm1 -> norm1
I0908 23:35:10.893951 25678 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:35:10.893956 25678 net.cpp:125] norm1 needs backward computation.
I0908 23:35:10.893965 25678 net.cpp:66] Creating Layer conv2
I0908 23:35:10.893970 25678 net.cpp:329] conv2 <- norm1
I0908 23:35:10.893976 25678 net.cpp:290] conv2 -> conv2
I0908 23:35:10.902812 25678 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:35:10.902827 25678 net.cpp:125] conv2 needs backward computation.
I0908 23:35:10.902833 25678 net.cpp:66] Creating Layer relu2
I0908 23:35:10.902838 25678 net.cpp:329] relu2 <- conv2
I0908 23:35:10.902845 25678 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:35:10.902853 25678 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:35:10.902858 25678 net.cpp:125] relu2 needs backward computation.
I0908 23:35:10.902866 25678 net.cpp:66] Creating Layer pool2
I0908 23:35:10.902871 25678 net.cpp:329] pool2 <- conv2
I0908 23:35:10.902878 25678 net.cpp:290] pool2 -> pool2
I0908 23:35:10.902886 25678 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:35:10.902891 25678 net.cpp:125] pool2 needs backward computation.
I0908 23:35:10.902899 25678 net.cpp:66] Creating Layer fc7
I0908 23:35:10.902904 25678 net.cpp:329] fc7 <- pool2
I0908 23:35:10.902910 25678 net.cpp:290] fc7 -> fc7
I0908 23:35:11.527153 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.527200 25678 net.cpp:125] fc7 needs backward computation.
I0908 23:35:11.527214 25678 net.cpp:66] Creating Layer relu7
I0908 23:35:11.527220 25678 net.cpp:329] relu7 <- fc7
I0908 23:35:11.527230 25678 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:35:11.527240 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.527245 25678 net.cpp:125] relu7 needs backward computation.
I0908 23:35:11.527252 25678 net.cpp:66] Creating Layer drop7
I0908 23:35:11.527258 25678 net.cpp:329] drop7 <- fc7
I0908 23:35:11.527264 25678 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:35:11.527274 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.527281 25678 net.cpp:125] drop7 needs backward computation.
I0908 23:35:11.527288 25678 net.cpp:66] Creating Layer fc8
I0908 23:35:11.527293 25678 net.cpp:329] fc8 <- fc7
I0908 23:35:11.527302 25678 net.cpp:290] fc8 -> fc8
I0908 23:35:11.534849 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.534860 25678 net.cpp:125] fc8 needs backward computation.
I0908 23:35:11.534868 25678 net.cpp:66] Creating Layer relu8
I0908 23:35:11.534873 25678 net.cpp:329] relu8 <- fc8
I0908 23:35:11.534881 25678 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:35:11.534888 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.534893 25678 net.cpp:125] relu8 needs backward computation.
I0908 23:35:11.534899 25678 net.cpp:66] Creating Layer drop8
I0908 23:35:11.534905 25678 net.cpp:329] drop8 <- fc8
I0908 23:35:11.534911 25678 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:35:11.534917 25678 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:35:11.534924 25678 net.cpp:125] drop8 needs backward computation.
I0908 23:35:11.534931 25678 net.cpp:66] Creating Layer fc9
I0908 23:35:11.534937 25678 net.cpp:329] fc9 <- fc8
I0908 23:35:11.534955 25678 net.cpp:290] fc9 -> fc9
I0908 23:35:11.535318 25678 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:35:11.535331 25678 net.cpp:125] fc9 needs backward computation.
I0908 23:35:11.535338 25678 net.cpp:66] Creating Layer fc10
I0908 23:35:11.535344 25678 net.cpp:329] fc10 <- fc9
I0908 23:35:11.535352 25678 net.cpp:290] fc10 -> fc10
I0908 23:35:11.535363 25678 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:35:11.535372 25678 net.cpp:125] fc10 needs backward computation.
I0908 23:35:11.535377 25678 net.cpp:66] Creating Layer prob
I0908 23:35:11.535382 25678 net.cpp:329] prob <- fc10
I0908 23:35:11.535390 25678 net.cpp:290] prob -> prob
I0908 23:35:11.535399 25678 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:35:11.535405 25678 net.cpp:125] prob needs backward computation.
I0908 23:35:11.535410 25678 net.cpp:156] This network produces output prob
I0908 23:35:11.535423 25678 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:35:11.535430 25678 net.cpp:167] Network initialization done.
I0908 23:35:11.535435 25678 net.cpp:168] Memory required for data: 6183480
Classifying 366 inputs.
Done in 255.29 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:39:54.708307 25698 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:39:54.708457 25698 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:39:54.708466 25698 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:39:54.708611 25698 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:39:54.708667 25698 net.cpp:292] Input 0 -> data
I0908 23:39:54.708693 25698 net.cpp:66] Creating Layer conv1
I0908 23:39:54.708699 25698 net.cpp:329] conv1 <- data
I0908 23:39:54.708708 25698 net.cpp:290] conv1 -> conv1
I0908 23:39:54.733597 25698 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:39:54.733628 25698 net.cpp:125] conv1 needs backward computation.
I0908 23:39:54.733638 25698 net.cpp:66] Creating Layer relu1
I0908 23:39:54.733644 25698 net.cpp:329] relu1 <- conv1
I0908 23:39:54.733650 25698 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:39:54.733659 25698 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:39:54.733664 25698 net.cpp:125] relu1 needs backward computation.
I0908 23:39:54.733671 25698 net.cpp:66] Creating Layer pool1
I0908 23:39:54.733676 25698 net.cpp:329] pool1 <- conv1
I0908 23:39:54.733683 25698 net.cpp:290] pool1 -> pool1
I0908 23:39:54.733695 25698 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:39:54.733700 25698 net.cpp:125] pool1 needs backward computation.
I0908 23:39:54.733706 25698 net.cpp:66] Creating Layer norm1
I0908 23:39:54.733711 25698 net.cpp:329] norm1 <- pool1
I0908 23:39:54.733718 25698 net.cpp:290] norm1 -> norm1
I0908 23:39:54.733727 25698 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:39:54.733733 25698 net.cpp:125] norm1 needs backward computation.
I0908 23:39:54.733741 25698 net.cpp:66] Creating Layer conv2
I0908 23:39:54.733747 25698 net.cpp:329] conv2 <- norm1
I0908 23:39:54.733753 25698 net.cpp:290] conv2 -> conv2
I0908 23:39:54.742571 25698 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:39:54.742585 25698 net.cpp:125] conv2 needs backward computation.
I0908 23:39:54.742593 25698 net.cpp:66] Creating Layer relu2
I0908 23:39:54.742599 25698 net.cpp:329] relu2 <- conv2
I0908 23:39:54.742604 25698 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:39:54.742611 25698 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:39:54.742617 25698 net.cpp:125] relu2 needs backward computation.
I0908 23:39:54.742624 25698 net.cpp:66] Creating Layer pool2
I0908 23:39:54.742629 25698 net.cpp:329] pool2 <- conv2
I0908 23:39:54.742635 25698 net.cpp:290] pool2 -> pool2
I0908 23:39:54.742642 25698 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:39:54.742647 25698 net.cpp:125] pool2 needs backward computation.
I0908 23:39:54.742656 25698 net.cpp:66] Creating Layer fc7
I0908 23:39:54.742662 25698 net.cpp:329] fc7 <- pool2
I0908 23:39:54.742669 25698 net.cpp:290] fc7 -> fc7
I0908 23:39:55.367473 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.367519 25698 net.cpp:125] fc7 needs backward computation.
I0908 23:39:55.367532 25698 net.cpp:66] Creating Layer relu7
I0908 23:39:55.367542 25698 net.cpp:329] relu7 <- fc7
I0908 23:39:55.367549 25698 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:39:55.367558 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.367564 25698 net.cpp:125] relu7 needs backward computation.
I0908 23:39:55.367571 25698 net.cpp:66] Creating Layer drop7
I0908 23:39:55.367576 25698 net.cpp:329] drop7 <- fc7
I0908 23:39:55.367583 25698 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:39:55.367594 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.367599 25698 net.cpp:125] drop7 needs backward computation.
I0908 23:39:55.367607 25698 net.cpp:66] Creating Layer fc8
I0908 23:39:55.367624 25698 net.cpp:329] fc8 <- fc7
I0908 23:39:55.367635 25698 net.cpp:290] fc8 -> fc8
I0908 23:39:55.375180 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.375191 25698 net.cpp:125] fc8 needs backward computation.
I0908 23:39:55.375198 25698 net.cpp:66] Creating Layer relu8
I0908 23:39:55.375205 25698 net.cpp:329] relu8 <- fc8
I0908 23:39:55.375211 25698 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:39:55.375218 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.375224 25698 net.cpp:125] relu8 needs backward computation.
I0908 23:39:55.375231 25698 net.cpp:66] Creating Layer drop8
I0908 23:39:55.375236 25698 net.cpp:329] drop8 <- fc8
I0908 23:39:55.375242 25698 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:39:55.375251 25698 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:39:55.375255 25698 net.cpp:125] drop8 needs backward computation.
I0908 23:39:55.375262 25698 net.cpp:66] Creating Layer fc9
I0908 23:39:55.375268 25698 net.cpp:329] fc9 <- fc8
I0908 23:39:55.375274 25698 net.cpp:290] fc9 -> fc9
I0908 23:39:55.375639 25698 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:39:55.375649 25698 net.cpp:125] fc9 needs backward computation.
I0908 23:39:55.375658 25698 net.cpp:66] Creating Layer fc10
I0908 23:39:55.375663 25698 net.cpp:329] fc10 <- fc9
I0908 23:39:55.375670 25698 net.cpp:290] fc10 -> fc10
I0908 23:39:55.375681 25698 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:39:55.375689 25698 net.cpp:125] fc10 needs backward computation.
I0908 23:39:55.375695 25698 net.cpp:66] Creating Layer prob
I0908 23:39:55.375701 25698 net.cpp:329] prob <- fc10
I0908 23:39:55.375708 25698 net.cpp:290] prob -> prob
I0908 23:39:55.375718 25698 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:39:55.375723 25698 net.cpp:125] prob needs backward computation.
I0908 23:39:55.375728 25698 net.cpp:156] This network produces output prob
I0908 23:39:55.375741 25698 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:39:55.375749 25698 net.cpp:167] Network initialization done.
I0908 23:39:55.375754 25698 net.cpp:168] Memory required for data: 6183480
Classifying 22 inputs.
Done in 13.89 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:40:12.594161 25701 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:40:12.594297 25701 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:40:12.594305 25701 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:40:12.594450 25701 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:40:12.594511 25701 net.cpp:292] Input 0 -> data
I0908 23:40:12.594537 25701 net.cpp:66] Creating Layer conv1
I0908 23:40:12.594543 25701 net.cpp:329] conv1 <- data
I0908 23:40:12.594552 25701 net.cpp:290] conv1 -> conv1
I0908 23:40:12.595871 25701 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:40:12.595890 25701 net.cpp:125] conv1 needs backward computation.
I0908 23:40:12.595898 25701 net.cpp:66] Creating Layer relu1
I0908 23:40:12.595903 25701 net.cpp:329] relu1 <- conv1
I0908 23:40:12.595911 25701 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:40:12.595918 25701 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:40:12.595924 25701 net.cpp:125] relu1 needs backward computation.
I0908 23:40:12.595932 25701 net.cpp:66] Creating Layer pool1
I0908 23:40:12.595937 25701 net.cpp:329] pool1 <- conv1
I0908 23:40:12.595943 25701 net.cpp:290] pool1 -> pool1
I0908 23:40:12.595954 25701 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:40:12.595959 25701 net.cpp:125] pool1 needs backward computation.
I0908 23:40:12.595967 25701 net.cpp:66] Creating Layer norm1
I0908 23:40:12.595971 25701 net.cpp:329] norm1 <- pool1
I0908 23:40:12.595978 25701 net.cpp:290] norm1 -> norm1
I0908 23:40:12.595988 25701 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:40:12.595993 25701 net.cpp:125] norm1 needs backward computation.
I0908 23:40:12.596000 25701 net.cpp:66] Creating Layer conv2
I0908 23:40:12.596006 25701 net.cpp:329] conv2 <- norm1
I0908 23:40:12.596014 25701 net.cpp:290] conv2 -> conv2
I0908 23:40:12.604874 25701 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:40:12.604888 25701 net.cpp:125] conv2 needs backward computation.
I0908 23:40:12.604894 25701 net.cpp:66] Creating Layer relu2
I0908 23:40:12.604900 25701 net.cpp:329] relu2 <- conv2
I0908 23:40:12.604907 25701 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:40:12.604913 25701 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:40:12.604919 25701 net.cpp:125] relu2 needs backward computation.
I0908 23:40:12.604925 25701 net.cpp:66] Creating Layer pool2
I0908 23:40:12.604930 25701 net.cpp:329] pool2 <- conv2
I0908 23:40:12.604938 25701 net.cpp:290] pool2 -> pool2
I0908 23:40:12.604944 25701 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:40:12.604954 25701 net.cpp:125] pool2 needs backward computation.
I0908 23:40:12.604964 25701 net.cpp:66] Creating Layer fc7
I0908 23:40:12.604969 25701 net.cpp:329] fc7 <- pool2
I0908 23:40:12.604976 25701 net.cpp:290] fc7 -> fc7
I0908 23:40:13.229436 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.229482 25701 net.cpp:125] fc7 needs backward computation.
I0908 23:40:13.229495 25701 net.cpp:66] Creating Layer relu7
I0908 23:40:13.229503 25701 net.cpp:329] relu7 <- fc7
I0908 23:40:13.229511 25701 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:40:13.229528 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.229535 25701 net.cpp:125] relu7 needs backward computation.
I0908 23:40:13.229542 25701 net.cpp:66] Creating Layer drop7
I0908 23:40:13.229547 25701 net.cpp:329] drop7 <- fc7
I0908 23:40:13.229553 25701 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:40:13.229564 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.229570 25701 net.cpp:125] drop7 needs backward computation.
I0908 23:40:13.229578 25701 net.cpp:66] Creating Layer fc8
I0908 23:40:13.229583 25701 net.cpp:329] fc8 <- fc7
I0908 23:40:13.229593 25701 net.cpp:290] fc8 -> fc8
I0908 23:40:13.237155 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.237167 25701 net.cpp:125] fc8 needs backward computation.
I0908 23:40:13.237174 25701 net.cpp:66] Creating Layer relu8
I0908 23:40:13.237180 25701 net.cpp:329] relu8 <- fc8
I0908 23:40:13.237187 25701 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:40:13.237195 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.237200 25701 net.cpp:125] relu8 needs backward computation.
I0908 23:40:13.237206 25701 net.cpp:66] Creating Layer drop8
I0908 23:40:13.237212 25701 net.cpp:329] drop8 <- fc8
I0908 23:40:13.237218 25701 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:40:13.237226 25701 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:40:13.237231 25701 net.cpp:125] drop8 needs backward computation.
I0908 23:40:13.237238 25701 net.cpp:66] Creating Layer fc9
I0908 23:40:13.237244 25701 net.cpp:329] fc9 <- fc8
I0908 23:40:13.237251 25701 net.cpp:290] fc9 -> fc9
I0908 23:40:13.237630 25701 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:40:13.237643 25701 net.cpp:125] fc9 needs backward computation.
I0908 23:40:13.237651 25701 net.cpp:66] Creating Layer fc10
I0908 23:40:13.237656 25701 net.cpp:329] fc10 <- fc9
I0908 23:40:13.237665 25701 net.cpp:290] fc10 -> fc10
I0908 23:40:13.237684 25701 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:40:13.237691 25701 net.cpp:125] fc10 needs backward computation.
I0908 23:40:13.237699 25701 net.cpp:66] Creating Layer prob
I0908 23:40:13.237704 25701 net.cpp:329] prob <- fc10
I0908 23:40:13.237711 25701 net.cpp:290] prob -> prob
I0908 23:40:13.237720 25701 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:40:13.237726 25701 net.cpp:125] prob needs backward computation.
I0908 23:40:13.237731 25701 net.cpp:156] This network produces output prob
I0908 23:40:13.237743 25701 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:40:13.237751 25701 net.cpp:167] Network initialization done.
I0908 23:40:13.237756 25701 net.cpp:168] Memory required for data: 6183480
Classifying 308 inputs.
Done in 190.75 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:43:45.954881 25709 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:43:45.955032 25709 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:43:45.955041 25709 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:43:45.955185 25709 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:43:45.955245 25709 net.cpp:292] Input 0 -> data
I0908 23:43:45.955271 25709 net.cpp:66] Creating Layer conv1
I0908 23:43:45.955278 25709 net.cpp:329] conv1 <- data
I0908 23:43:45.955286 25709 net.cpp:290] conv1 -> conv1
I0908 23:43:45.963701 25709 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:43:45.963726 25709 net.cpp:125] conv1 needs backward computation.
I0908 23:43:45.963735 25709 net.cpp:66] Creating Layer relu1
I0908 23:43:45.963742 25709 net.cpp:329] relu1 <- conv1
I0908 23:43:45.963748 25709 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:43:45.963757 25709 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:43:45.963762 25709 net.cpp:125] relu1 needs backward computation.
I0908 23:43:45.963769 25709 net.cpp:66] Creating Layer pool1
I0908 23:43:45.963775 25709 net.cpp:329] pool1 <- conv1
I0908 23:43:45.963781 25709 net.cpp:290] pool1 -> pool1
I0908 23:43:45.963793 25709 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:43:45.963798 25709 net.cpp:125] pool1 needs backward computation.
I0908 23:43:45.963805 25709 net.cpp:66] Creating Layer norm1
I0908 23:43:45.963810 25709 net.cpp:329] norm1 <- pool1
I0908 23:43:45.963817 25709 net.cpp:290] norm1 -> norm1
I0908 23:43:45.963831 25709 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:43:45.963837 25709 net.cpp:125] norm1 needs backward computation.
I0908 23:43:45.963845 25709 net.cpp:66] Creating Layer conv2
I0908 23:43:45.963850 25709 net.cpp:329] conv2 <- norm1
I0908 23:43:45.963857 25709 net.cpp:290] conv2 -> conv2
I0908 23:43:45.972764 25709 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:43:45.972779 25709 net.cpp:125] conv2 needs backward computation.
I0908 23:43:45.972786 25709 net.cpp:66] Creating Layer relu2
I0908 23:43:45.972792 25709 net.cpp:329] relu2 <- conv2
I0908 23:43:45.972800 25709 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:43:45.972806 25709 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:43:45.972812 25709 net.cpp:125] relu2 needs backward computation.
I0908 23:43:45.972818 25709 net.cpp:66] Creating Layer pool2
I0908 23:43:45.972823 25709 net.cpp:329] pool2 <- conv2
I0908 23:43:45.972831 25709 net.cpp:290] pool2 -> pool2
I0908 23:43:45.972838 25709 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:43:45.972843 25709 net.cpp:125] pool2 needs backward computation.
I0908 23:43:45.972853 25709 net.cpp:66] Creating Layer fc7
I0908 23:43:45.972859 25709 net.cpp:329] fc7 <- pool2
I0908 23:43:45.972867 25709 net.cpp:290] fc7 -> fc7
I0908 23:43:46.597131 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.597180 25709 net.cpp:125] fc7 needs backward computation.
I0908 23:43:46.597193 25709 net.cpp:66] Creating Layer relu7
I0908 23:43:46.597203 25709 net.cpp:329] relu7 <- fc7
I0908 23:43:46.597210 25709 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:43:46.597219 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.597225 25709 net.cpp:125] relu7 needs backward computation.
I0908 23:43:46.597232 25709 net.cpp:66] Creating Layer drop7
I0908 23:43:46.597237 25709 net.cpp:329] drop7 <- fc7
I0908 23:43:46.597244 25709 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:43:46.597254 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.597260 25709 net.cpp:125] drop7 needs backward computation.
I0908 23:43:46.597268 25709 net.cpp:66] Creating Layer fc8
I0908 23:43:46.597275 25709 net.cpp:329] fc8 <- fc7
I0908 23:43:46.597283 25709 net.cpp:290] fc8 -> fc8
I0908 23:43:46.604832 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.604845 25709 net.cpp:125] fc8 needs backward computation.
I0908 23:43:46.604851 25709 net.cpp:66] Creating Layer relu8
I0908 23:43:46.604857 25709 net.cpp:329] relu8 <- fc8
I0908 23:43:46.604866 25709 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:43:46.604872 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.604877 25709 net.cpp:125] relu8 needs backward computation.
I0908 23:43:46.604884 25709 net.cpp:66] Creating Layer drop8
I0908 23:43:46.604889 25709 net.cpp:329] drop8 <- fc8
I0908 23:43:46.604895 25709 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:43:46.604903 25709 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:46.604909 25709 net.cpp:125] drop8 needs backward computation.
I0908 23:43:46.604917 25709 net.cpp:66] Creating Layer fc9
I0908 23:43:46.604923 25709 net.cpp:329] fc9 <- fc8
I0908 23:43:46.604929 25709 net.cpp:290] fc9 -> fc9
I0908 23:43:46.605291 25709 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:43:46.605303 25709 net.cpp:125] fc9 needs backward computation.
I0908 23:43:46.605309 25709 net.cpp:66] Creating Layer fc10
I0908 23:43:46.605315 25709 net.cpp:329] fc10 <- fc9
I0908 23:43:46.605324 25709 net.cpp:290] fc10 -> fc10
I0908 23:43:46.605335 25709 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:43:46.605342 25709 net.cpp:125] fc10 needs backward computation.
I0908 23:43:46.605350 25709 net.cpp:66] Creating Layer prob
I0908 23:43:46.605355 25709 net.cpp:329] prob <- fc10
I0908 23:43:46.605362 25709 net.cpp:290] prob -> prob
I0908 23:43:46.605371 25709 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:43:46.605377 25709 net.cpp:125] prob needs backward computation.
I0908 23:43:46.605382 25709 net.cpp:156] This network produces output prob
I0908 23:43:46.605394 25709 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:43:46.605413 25709 net.cpp:167] Network initialization done.
I0908 23:43:46.605418 25709 net.cpp:168] Memory required for data: 6183480
Classifying 16 inputs.
Done in 9.64 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:43:58.557977 25713 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:43:58.558114 25713 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:43:58.558122 25713 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:43:58.558265 25713 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:43:58.558316 25713 net.cpp:292] Input 0 -> data
I0908 23:43:58.558341 25713 net.cpp:66] Creating Layer conv1
I0908 23:43:58.558348 25713 net.cpp:329] conv1 <- data
I0908 23:43:58.558367 25713 net.cpp:290] conv1 -> conv1
I0908 23:43:58.559697 25713 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:43:58.559715 25713 net.cpp:125] conv1 needs backward computation.
I0908 23:43:58.559725 25713 net.cpp:66] Creating Layer relu1
I0908 23:43:58.559731 25713 net.cpp:329] relu1 <- conv1
I0908 23:43:58.559736 25713 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:43:58.559746 25713 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:43:58.559751 25713 net.cpp:125] relu1 needs backward computation.
I0908 23:43:58.559757 25713 net.cpp:66] Creating Layer pool1
I0908 23:43:58.559762 25713 net.cpp:329] pool1 <- conv1
I0908 23:43:58.559769 25713 net.cpp:290] pool1 -> pool1
I0908 23:43:58.559780 25713 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:43:58.559787 25713 net.cpp:125] pool1 needs backward computation.
I0908 23:43:58.559792 25713 net.cpp:66] Creating Layer norm1
I0908 23:43:58.559798 25713 net.cpp:329] norm1 <- pool1
I0908 23:43:58.559804 25713 net.cpp:290] norm1 -> norm1
I0908 23:43:58.559813 25713 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:43:58.559819 25713 net.cpp:125] norm1 needs backward computation.
I0908 23:43:58.559826 25713 net.cpp:66] Creating Layer conv2
I0908 23:43:58.559831 25713 net.cpp:329] conv2 <- norm1
I0908 23:43:58.559839 25713 net.cpp:290] conv2 -> conv2
I0908 23:43:58.568688 25713 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:43:58.568702 25713 net.cpp:125] conv2 needs backward computation.
I0908 23:43:58.568709 25713 net.cpp:66] Creating Layer relu2
I0908 23:43:58.568716 25713 net.cpp:329] relu2 <- conv2
I0908 23:43:58.568722 25713 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:43:58.568728 25713 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:43:58.568733 25713 net.cpp:125] relu2 needs backward computation.
I0908 23:43:58.568739 25713 net.cpp:66] Creating Layer pool2
I0908 23:43:58.568745 25713 net.cpp:329] pool2 <- conv2
I0908 23:43:58.568752 25713 net.cpp:290] pool2 -> pool2
I0908 23:43:58.568759 25713 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:43:58.568764 25713 net.cpp:125] pool2 needs backward computation.
I0908 23:43:58.568773 25713 net.cpp:66] Creating Layer fc7
I0908 23:43:58.568779 25713 net.cpp:329] fc7 <- pool2
I0908 23:43:58.568786 25713 net.cpp:290] fc7 -> fc7
I0908 23:43:59.193289 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.193341 25713 net.cpp:125] fc7 needs backward computation.
I0908 23:43:59.193353 25713 net.cpp:66] Creating Layer relu7
I0908 23:43:59.193362 25713 net.cpp:329] relu7 <- fc7
I0908 23:43:59.193370 25713 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:43:59.193380 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.193385 25713 net.cpp:125] relu7 needs backward computation.
I0908 23:43:59.193393 25713 net.cpp:66] Creating Layer drop7
I0908 23:43:59.193398 25713 net.cpp:329] drop7 <- fc7
I0908 23:43:59.193404 25713 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:43:59.193415 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.193420 25713 net.cpp:125] drop7 needs backward computation.
I0908 23:43:59.193428 25713 net.cpp:66] Creating Layer fc8
I0908 23:43:59.193434 25713 net.cpp:329] fc8 <- fc7
I0908 23:43:59.193442 25713 net.cpp:290] fc8 -> fc8
I0908 23:43:59.200978 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.200991 25713 net.cpp:125] fc8 needs backward computation.
I0908 23:43:59.200999 25713 net.cpp:66] Creating Layer relu8
I0908 23:43:59.201004 25713 net.cpp:329] relu8 <- fc8
I0908 23:43:59.201011 25713 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:43:59.201019 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.201023 25713 net.cpp:125] relu8 needs backward computation.
I0908 23:43:59.201030 25713 net.cpp:66] Creating Layer drop8
I0908 23:43:59.201035 25713 net.cpp:329] drop8 <- fc8
I0908 23:43:59.201040 25713 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:43:59.201050 25713 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:43:59.201055 25713 net.cpp:125] drop8 needs backward computation.
I0908 23:43:59.201061 25713 net.cpp:66] Creating Layer fc9
I0908 23:43:59.201077 25713 net.cpp:329] fc9 <- fc8
I0908 23:43:59.201084 25713 net.cpp:290] fc9 -> fc9
I0908 23:43:59.201447 25713 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:43:59.201458 25713 net.cpp:125] fc9 needs backward computation.
I0908 23:43:59.201465 25713 net.cpp:66] Creating Layer fc10
I0908 23:43:59.201472 25713 net.cpp:329] fc10 <- fc9
I0908 23:43:59.201479 25713 net.cpp:290] fc10 -> fc10
I0908 23:43:59.201490 25713 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:43:59.201498 25713 net.cpp:125] fc10 needs backward computation.
I0908 23:43:59.201504 25713 net.cpp:66] Creating Layer prob
I0908 23:43:59.201509 25713 net.cpp:329] prob <- fc10
I0908 23:43:59.201526 25713 net.cpp:290] prob -> prob
I0908 23:43:59.201537 25713 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:43:59.201544 25713 net.cpp:125] prob needs backward computation.
I0908 23:43:59.201549 25713 net.cpp:156] This network produces output prob
I0908 23:43:59.201560 25713 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:43:59.201570 25713 net.cpp:167] Network initialization done.
I0908 23:43:59.201575 25713 net.cpp:168] Memory required for data: 6183480
Classifying 305 inputs.
Done in 181.38 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:47:08.464982 25719 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:47:08.465117 25719 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:47:08.465127 25719 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:47:08.465270 25719 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:47:08.465332 25719 net.cpp:292] Input 0 -> data
I0908 23:47:08.465359 25719 net.cpp:66] Creating Layer conv1
I0908 23:47:08.465366 25719 net.cpp:329] conv1 <- data
I0908 23:47:08.465374 25719 net.cpp:290] conv1 -> conv1
I0908 23:47:08.466755 25719 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:47:08.466775 25719 net.cpp:125] conv1 needs backward computation.
I0908 23:47:08.466784 25719 net.cpp:66] Creating Layer relu1
I0908 23:47:08.466790 25719 net.cpp:329] relu1 <- conv1
I0908 23:47:08.466796 25719 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:47:08.466805 25719 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:47:08.466810 25719 net.cpp:125] relu1 needs backward computation.
I0908 23:47:08.466817 25719 net.cpp:66] Creating Layer pool1
I0908 23:47:08.466824 25719 net.cpp:329] pool1 <- conv1
I0908 23:47:08.466830 25719 net.cpp:290] pool1 -> pool1
I0908 23:47:08.466840 25719 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:47:08.466846 25719 net.cpp:125] pool1 needs backward computation.
I0908 23:47:08.466852 25719 net.cpp:66] Creating Layer norm1
I0908 23:47:08.466858 25719 net.cpp:329] norm1 <- pool1
I0908 23:47:08.466864 25719 net.cpp:290] norm1 -> norm1
I0908 23:47:08.466874 25719 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:47:08.466881 25719 net.cpp:125] norm1 needs backward computation.
I0908 23:47:08.466887 25719 net.cpp:66] Creating Layer conv2
I0908 23:47:08.466892 25719 net.cpp:329] conv2 <- norm1
I0908 23:47:08.466899 25719 net.cpp:290] conv2 -> conv2
I0908 23:47:08.475752 25719 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:47:08.475766 25719 net.cpp:125] conv2 needs backward computation.
I0908 23:47:08.475774 25719 net.cpp:66] Creating Layer relu2
I0908 23:47:08.475780 25719 net.cpp:329] relu2 <- conv2
I0908 23:47:08.475785 25719 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:47:08.475793 25719 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:47:08.475798 25719 net.cpp:125] relu2 needs backward computation.
I0908 23:47:08.475805 25719 net.cpp:66] Creating Layer pool2
I0908 23:47:08.475810 25719 net.cpp:329] pool2 <- conv2
I0908 23:47:08.475816 25719 net.cpp:290] pool2 -> pool2
I0908 23:47:08.475823 25719 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:47:08.475829 25719 net.cpp:125] pool2 needs backward computation.
I0908 23:47:08.475838 25719 net.cpp:66] Creating Layer fc7
I0908 23:47:08.475844 25719 net.cpp:329] fc7 <- pool2
I0908 23:47:08.475852 25719 net.cpp:290] fc7 -> fc7
I0908 23:47:09.101141 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.101187 25719 net.cpp:125] fc7 needs backward computation.
I0908 23:47:09.101200 25719 net.cpp:66] Creating Layer relu7
I0908 23:47:09.101208 25719 net.cpp:329] relu7 <- fc7
I0908 23:47:09.101217 25719 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:47:09.101227 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.101233 25719 net.cpp:125] relu7 needs backward computation.
I0908 23:47:09.101239 25719 net.cpp:66] Creating Layer drop7
I0908 23:47:09.101245 25719 net.cpp:329] drop7 <- fc7
I0908 23:47:09.101251 25719 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:47:09.101261 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.101279 25719 net.cpp:125] drop7 needs backward computation.
I0908 23:47:09.101286 25719 net.cpp:66] Creating Layer fc8
I0908 23:47:09.101292 25719 net.cpp:329] fc8 <- fc7
I0908 23:47:09.101300 25719 net.cpp:290] fc8 -> fc8
I0908 23:47:09.108844 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.108856 25719 net.cpp:125] fc8 needs backward computation.
I0908 23:47:09.108863 25719 net.cpp:66] Creating Layer relu8
I0908 23:47:09.108870 25719 net.cpp:329] relu8 <- fc8
I0908 23:47:09.108877 25719 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:47:09.108885 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.108891 25719 net.cpp:125] relu8 needs backward computation.
I0908 23:47:09.108896 25719 net.cpp:66] Creating Layer drop8
I0908 23:47:09.108901 25719 net.cpp:329] drop8 <- fc8
I0908 23:47:09.108908 25719 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:47:09.108916 25719 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:47:09.108922 25719 net.cpp:125] drop8 needs backward computation.
I0908 23:47:09.108929 25719 net.cpp:66] Creating Layer fc9
I0908 23:47:09.108934 25719 net.cpp:329] fc9 <- fc8
I0908 23:47:09.108942 25719 net.cpp:290] fc9 -> fc9
I0908 23:47:09.109307 25719 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:47:09.109318 25719 net.cpp:125] fc9 needs backward computation.
I0908 23:47:09.109324 25719 net.cpp:66] Creating Layer fc10
I0908 23:47:09.109330 25719 net.cpp:329] fc10 <- fc9
I0908 23:47:09.109338 25719 net.cpp:290] fc10 -> fc10
I0908 23:47:09.109350 25719 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:47:09.109359 25719 net.cpp:125] fc10 needs backward computation.
I0908 23:47:09.109364 25719 net.cpp:66] Creating Layer prob
I0908 23:47:09.109370 25719 net.cpp:329] prob <- fc10
I0908 23:47:09.109379 25719 net.cpp:290] prob -> prob
I0908 23:47:09.109387 25719 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:47:09.109393 25719 net.cpp:125] prob needs backward computation.
I0908 23:47:09.109398 25719 net.cpp:156] This network produces output prob
I0908 23:47:09.109411 25719 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:47:09.109419 25719 net.cpp:167] Network initialization done.
I0908 23:47:09.109424 25719 net.cpp:168] Memory required for data: 6183480
Classifying 139 inputs.
Done in 92.84 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:48:45.313464 25723 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:48:45.313644 25723 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:48:45.313654 25723 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:48:45.313799 25723 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:48:45.313861 25723 net.cpp:292] Input 0 -> data
I0908 23:48:45.313889 25723 net.cpp:66] Creating Layer conv1
I0908 23:48:45.313895 25723 net.cpp:329] conv1 <- data
I0908 23:48:45.313904 25723 net.cpp:290] conv1 -> conv1
I0908 23:48:45.315224 25723 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:48:45.315243 25723 net.cpp:125] conv1 needs backward computation.
I0908 23:48:45.315251 25723 net.cpp:66] Creating Layer relu1
I0908 23:48:45.315258 25723 net.cpp:329] relu1 <- conv1
I0908 23:48:45.315263 25723 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:48:45.315271 25723 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:48:45.315278 25723 net.cpp:125] relu1 needs backward computation.
I0908 23:48:45.315284 25723 net.cpp:66] Creating Layer pool1
I0908 23:48:45.315289 25723 net.cpp:329] pool1 <- conv1
I0908 23:48:45.315296 25723 net.cpp:290] pool1 -> pool1
I0908 23:48:45.315306 25723 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:48:45.315312 25723 net.cpp:125] pool1 needs backward computation.
I0908 23:48:45.315318 25723 net.cpp:66] Creating Layer norm1
I0908 23:48:45.315323 25723 net.cpp:329] norm1 <- pool1
I0908 23:48:45.315330 25723 net.cpp:290] norm1 -> norm1
I0908 23:48:45.315340 25723 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:48:45.315346 25723 net.cpp:125] norm1 needs backward computation.
I0908 23:48:45.315352 25723 net.cpp:66] Creating Layer conv2
I0908 23:48:45.315357 25723 net.cpp:329] conv2 <- norm1
I0908 23:48:45.315364 25723 net.cpp:290] conv2 -> conv2
I0908 23:48:45.324236 25723 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:48:45.324252 25723 net.cpp:125] conv2 needs backward computation.
I0908 23:48:45.324259 25723 net.cpp:66] Creating Layer relu2
I0908 23:48:45.324265 25723 net.cpp:329] relu2 <- conv2
I0908 23:48:45.324271 25723 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:48:45.324278 25723 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:48:45.324285 25723 net.cpp:125] relu2 needs backward computation.
I0908 23:48:45.324290 25723 net.cpp:66] Creating Layer pool2
I0908 23:48:45.324295 25723 net.cpp:329] pool2 <- conv2
I0908 23:48:45.324302 25723 net.cpp:290] pool2 -> pool2
I0908 23:48:45.324316 25723 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:48:45.324321 25723 net.cpp:125] pool2 needs backward computation.
I0908 23:48:45.324331 25723 net.cpp:66] Creating Layer fc7
I0908 23:48:45.324337 25723 net.cpp:329] fc7 <- pool2
I0908 23:48:45.324343 25723 net.cpp:290] fc7 -> fc7
I0908 23:48:45.948639 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.948688 25723 net.cpp:125] fc7 needs backward computation.
I0908 23:48:45.948699 25723 net.cpp:66] Creating Layer relu7
I0908 23:48:45.948709 25723 net.cpp:329] relu7 <- fc7
I0908 23:48:45.948716 25723 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:48:45.948725 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.948731 25723 net.cpp:125] relu7 needs backward computation.
I0908 23:48:45.948739 25723 net.cpp:66] Creating Layer drop7
I0908 23:48:45.948743 25723 net.cpp:329] drop7 <- fc7
I0908 23:48:45.948750 25723 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:48:45.948760 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.948765 25723 net.cpp:125] drop7 needs backward computation.
I0908 23:48:45.948773 25723 net.cpp:66] Creating Layer fc8
I0908 23:48:45.948779 25723 net.cpp:329] fc8 <- fc7
I0908 23:48:45.948788 25723 net.cpp:290] fc8 -> fc8
I0908 23:48:45.956331 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.956342 25723 net.cpp:125] fc8 needs backward computation.
I0908 23:48:45.956349 25723 net.cpp:66] Creating Layer relu8
I0908 23:48:45.956356 25723 net.cpp:329] relu8 <- fc8
I0908 23:48:45.956363 25723 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:48:45.956370 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.956375 25723 net.cpp:125] relu8 needs backward computation.
I0908 23:48:45.956382 25723 net.cpp:66] Creating Layer drop8
I0908 23:48:45.956387 25723 net.cpp:329] drop8 <- fc8
I0908 23:48:45.956393 25723 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:48:45.956401 25723 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:48:45.956408 25723 net.cpp:125] drop8 needs backward computation.
I0908 23:48:45.956414 25723 net.cpp:66] Creating Layer fc9
I0908 23:48:45.956419 25723 net.cpp:329] fc9 <- fc8
I0908 23:48:45.956426 25723 net.cpp:290] fc9 -> fc9
I0908 23:48:45.956789 25723 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:48:45.956799 25723 net.cpp:125] fc9 needs backward computation.
I0908 23:48:45.956807 25723 net.cpp:66] Creating Layer fc10
I0908 23:48:45.956814 25723 net.cpp:329] fc10 <- fc9
I0908 23:48:45.956821 25723 net.cpp:290] fc10 -> fc10
I0908 23:48:45.956833 25723 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:48:45.956840 25723 net.cpp:125] fc10 needs backward computation.
I0908 23:48:45.956847 25723 net.cpp:66] Creating Layer prob
I0908 23:48:45.956852 25723 net.cpp:329] prob <- fc10
I0908 23:48:45.956861 25723 net.cpp:290] prob -> prob
I0908 23:48:45.956869 25723 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:48:45.956876 25723 net.cpp:125] prob needs backward computation.
I0908 23:48:45.956879 25723 net.cpp:156] This network produces output prob
I0908 23:48:45.956892 25723 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:48:45.956900 25723 net.cpp:167] Network initialization done.
I0908 23:48:45.956904 25723 net.cpp:168] Memory required for data: 6183480
Classifying 362 inputs.
Done in 216.16 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:52:35.040770 25731 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:52:35.040906 25731 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:52:35.040915 25731 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:52:35.041060 25731 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:52:35.041121 25731 net.cpp:292] Input 0 -> data
I0908 23:52:35.041146 25731 net.cpp:66] Creating Layer conv1
I0908 23:52:35.041152 25731 net.cpp:329] conv1 <- data
I0908 23:52:35.041160 25731 net.cpp:290] conv1 -> conv1
I0908 23:52:35.042551 25731 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:52:35.042570 25731 net.cpp:125] conv1 needs backward computation.
I0908 23:52:35.042579 25731 net.cpp:66] Creating Layer relu1
I0908 23:52:35.042585 25731 net.cpp:329] relu1 <- conv1
I0908 23:52:35.042593 25731 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:52:35.042600 25731 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:52:35.042606 25731 net.cpp:125] relu1 needs backward computation.
I0908 23:52:35.042613 25731 net.cpp:66] Creating Layer pool1
I0908 23:52:35.042618 25731 net.cpp:329] pool1 <- conv1
I0908 23:52:35.042624 25731 net.cpp:290] pool1 -> pool1
I0908 23:52:35.042636 25731 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:52:35.042641 25731 net.cpp:125] pool1 needs backward computation.
I0908 23:52:35.042649 25731 net.cpp:66] Creating Layer norm1
I0908 23:52:35.042654 25731 net.cpp:329] norm1 <- pool1
I0908 23:52:35.042665 25731 net.cpp:290] norm1 -> norm1
I0908 23:52:35.042675 25731 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:52:35.042681 25731 net.cpp:125] norm1 needs backward computation.
I0908 23:52:35.042688 25731 net.cpp:66] Creating Layer conv2
I0908 23:52:35.042695 25731 net.cpp:329] conv2 <- norm1
I0908 23:52:35.042701 25731 net.cpp:290] conv2 -> conv2
I0908 23:52:35.051837 25731 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:52:35.051852 25731 net.cpp:125] conv2 needs backward computation.
I0908 23:52:35.051859 25731 net.cpp:66] Creating Layer relu2
I0908 23:52:35.051864 25731 net.cpp:329] relu2 <- conv2
I0908 23:52:35.051872 25731 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:52:35.051878 25731 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:52:35.051883 25731 net.cpp:125] relu2 needs backward computation.
I0908 23:52:35.051892 25731 net.cpp:66] Creating Layer pool2
I0908 23:52:35.051898 25731 net.cpp:329] pool2 <- conv2
I0908 23:52:35.051904 25731 net.cpp:290] pool2 -> pool2
I0908 23:52:35.051913 25731 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:52:35.051918 25731 net.cpp:125] pool2 needs backward computation.
I0908 23:52:35.051924 25731 net.cpp:66] Creating Layer fc7
I0908 23:52:35.051930 25731 net.cpp:329] fc7 <- pool2
I0908 23:52:35.051937 25731 net.cpp:290] fc7 -> fc7
I0908 23:52:35.676338 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.676384 25731 net.cpp:125] fc7 needs backward computation.
I0908 23:52:35.676396 25731 net.cpp:66] Creating Layer relu7
I0908 23:52:35.676403 25731 net.cpp:329] relu7 <- fc7
I0908 23:52:35.676412 25731 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:52:35.676422 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.676427 25731 net.cpp:125] relu7 needs backward computation.
I0908 23:52:35.676435 25731 net.cpp:66] Creating Layer drop7
I0908 23:52:35.676440 25731 net.cpp:329] drop7 <- fc7
I0908 23:52:35.676446 25731 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:52:35.676457 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.676463 25731 net.cpp:125] drop7 needs backward computation.
I0908 23:52:35.676472 25731 net.cpp:66] Creating Layer fc8
I0908 23:52:35.676477 25731 net.cpp:329] fc8 <- fc7
I0908 23:52:35.676486 25731 net.cpp:290] fc8 -> fc8
I0908 23:52:35.684057 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.684069 25731 net.cpp:125] fc8 needs backward computation.
I0908 23:52:35.684077 25731 net.cpp:66] Creating Layer relu8
I0908 23:52:35.684082 25731 net.cpp:329] relu8 <- fc8
I0908 23:52:35.684090 25731 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:52:35.684098 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.684103 25731 net.cpp:125] relu8 needs backward computation.
I0908 23:52:35.684109 25731 net.cpp:66] Creating Layer drop8
I0908 23:52:35.684114 25731 net.cpp:329] drop8 <- fc8
I0908 23:52:35.684120 25731 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:52:35.684128 25731 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:52:35.684133 25731 net.cpp:125] drop8 needs backward computation.
I0908 23:52:35.684141 25731 net.cpp:66] Creating Layer fc9
I0908 23:52:35.684147 25731 net.cpp:329] fc9 <- fc8
I0908 23:52:35.684154 25731 net.cpp:290] fc9 -> fc9
I0908 23:52:35.684516 25731 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:52:35.684527 25731 net.cpp:125] fc9 needs backward computation.
I0908 23:52:35.684535 25731 net.cpp:66] Creating Layer fc10
I0908 23:52:35.684541 25731 net.cpp:329] fc10 <- fc9
I0908 23:52:35.684550 25731 net.cpp:290] fc10 -> fc10
I0908 23:52:35.684561 25731 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:52:35.684568 25731 net.cpp:125] fc10 needs backward computation.
I0908 23:52:35.684576 25731 net.cpp:66] Creating Layer prob
I0908 23:52:35.684581 25731 net.cpp:329] prob <- fc10
I0908 23:52:35.684588 25731 net.cpp:290] prob -> prob
I0908 23:52:35.684598 25731 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:52:35.684603 25731 net.cpp:125] prob needs backward computation.
I0908 23:52:35.684608 25731 net.cpp:156] This network produces output prob
I0908 23:52:35.684630 25731 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:52:35.684639 25731 net.cpp:167] Network initialization done.
I0908 23:52:35.684644 25731 net.cpp:168] Memory required for data: 6183480
Classifying 423 inputs.
Done in 259.94 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:57:06.844257 25740 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:57:06.844393 25740 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:57:06.844401 25740 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:57:06.844547 25740 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:57:06.844599 25740 net.cpp:292] Input 0 -> data
I0908 23:57:06.844633 25740 net.cpp:66] Creating Layer conv1
I0908 23:57:06.844640 25740 net.cpp:329] conv1 <- data
I0908 23:57:06.844650 25740 net.cpp:290] conv1 -> conv1
I0908 23:57:06.846014 25740 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:57:06.846032 25740 net.cpp:125] conv1 needs backward computation.
I0908 23:57:06.846041 25740 net.cpp:66] Creating Layer relu1
I0908 23:57:06.846047 25740 net.cpp:329] relu1 <- conv1
I0908 23:57:06.846053 25740 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:57:06.846062 25740 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:57:06.846068 25740 net.cpp:125] relu1 needs backward computation.
I0908 23:57:06.846074 25740 net.cpp:66] Creating Layer pool1
I0908 23:57:06.846081 25740 net.cpp:329] pool1 <- conv1
I0908 23:57:06.846087 25740 net.cpp:290] pool1 -> pool1
I0908 23:57:06.846097 25740 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:57:06.846103 25740 net.cpp:125] pool1 needs backward computation.
I0908 23:57:06.846110 25740 net.cpp:66] Creating Layer norm1
I0908 23:57:06.846115 25740 net.cpp:329] norm1 <- pool1
I0908 23:57:06.846122 25740 net.cpp:290] norm1 -> norm1
I0908 23:57:06.846132 25740 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:57:06.846138 25740 net.cpp:125] norm1 needs backward computation.
I0908 23:57:06.846144 25740 net.cpp:66] Creating Layer conv2
I0908 23:57:06.846150 25740 net.cpp:329] conv2 <- norm1
I0908 23:57:06.846158 25740 net.cpp:290] conv2 -> conv2
I0908 23:57:06.855005 25740 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:57:06.855020 25740 net.cpp:125] conv2 needs backward computation.
I0908 23:57:06.855026 25740 net.cpp:66] Creating Layer relu2
I0908 23:57:06.855031 25740 net.cpp:329] relu2 <- conv2
I0908 23:57:06.855038 25740 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:57:06.855046 25740 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:57:06.855051 25740 net.cpp:125] relu2 needs backward computation.
I0908 23:57:06.855057 25740 net.cpp:66] Creating Layer pool2
I0908 23:57:06.855062 25740 net.cpp:329] pool2 <- conv2
I0908 23:57:06.855068 25740 net.cpp:290] pool2 -> pool2
I0908 23:57:06.855077 25740 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:57:06.855082 25740 net.cpp:125] pool2 needs backward computation.
I0908 23:57:06.855090 25740 net.cpp:66] Creating Layer fc7
I0908 23:57:06.855096 25740 net.cpp:329] fc7 <- pool2
I0908 23:57:06.855103 25740 net.cpp:290] fc7 -> fc7
I0908 23:57:07.507092 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.507138 25740 net.cpp:125] fc7 needs backward computation.
I0908 23:57:07.507149 25740 net.cpp:66] Creating Layer relu7
I0908 23:57:07.507160 25740 net.cpp:329] relu7 <- fc7
I0908 23:57:07.507168 25740 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:57:07.507177 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.507182 25740 net.cpp:125] relu7 needs backward computation.
I0908 23:57:07.507190 25740 net.cpp:66] Creating Layer drop7
I0908 23:57:07.507195 25740 net.cpp:329] drop7 <- fc7
I0908 23:57:07.507201 25740 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:57:07.507212 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.507218 25740 net.cpp:125] drop7 needs backward computation.
I0908 23:57:07.507226 25740 net.cpp:66] Creating Layer fc8
I0908 23:57:07.507231 25740 net.cpp:329] fc8 <- fc7
I0908 23:57:07.507241 25740 net.cpp:290] fc8 -> fc8
I0908 23:57:07.515120 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.515132 25740 net.cpp:125] fc8 needs backward computation.
I0908 23:57:07.515139 25740 net.cpp:66] Creating Layer relu8
I0908 23:57:07.515144 25740 net.cpp:329] relu8 <- fc8
I0908 23:57:07.515152 25740 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:57:07.515161 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.515166 25740 net.cpp:125] relu8 needs backward computation.
I0908 23:57:07.515172 25740 net.cpp:66] Creating Layer drop8
I0908 23:57:07.515177 25740 net.cpp:329] drop8 <- fc8
I0908 23:57:07.515183 25740 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:57:07.515192 25740 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:07.515208 25740 net.cpp:125] drop8 needs backward computation.
I0908 23:57:07.515216 25740 net.cpp:66] Creating Layer fc9
I0908 23:57:07.515221 25740 net.cpp:329] fc9 <- fc8
I0908 23:57:07.515228 25740 net.cpp:290] fc9 -> fc9
I0908 23:57:07.515607 25740 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:57:07.515617 25740 net.cpp:125] fc9 needs backward computation.
I0908 23:57:07.515626 25740 net.cpp:66] Creating Layer fc10
I0908 23:57:07.515631 25740 net.cpp:329] fc10 <- fc9
I0908 23:57:07.515640 25740 net.cpp:290] fc10 -> fc10
I0908 23:57:07.515651 25740 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:57:07.515660 25740 net.cpp:125] fc10 needs backward computation.
I0908 23:57:07.515666 25740 net.cpp:66] Creating Layer prob
I0908 23:57:07.515671 25740 net.cpp:329] prob <- fc10
I0908 23:57:07.515678 25740 net.cpp:290] prob -> prob
I0908 23:57:07.515688 25740 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:57:07.515694 25740 net.cpp:125] prob needs backward computation.
I0908 23:57:07.515699 25740 net.cpp:156] This network produces output prob
I0908 23:57:07.515712 25740 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:57:07.515719 25740 net.cpp:167] Network initialization done.
I0908 23:57:07.515724 25740 net.cpp:168] Memory required for data: 6183480
Classifying 18 inputs.
Done in 11.17 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:57:20.485875 25743 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:57:20.486011 25743 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:57:20.486021 25743 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:57:20.486162 25743 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:57:20.486223 25743 net.cpp:292] Input 0 -> data
I0908 23:57:20.486249 25743 net.cpp:66] Creating Layer conv1
I0908 23:57:20.486256 25743 net.cpp:329] conv1 <- data
I0908 23:57:20.486264 25743 net.cpp:290] conv1 -> conv1
I0908 23:57:20.487601 25743 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:57:20.487617 25743 net.cpp:125] conv1 needs backward computation.
I0908 23:57:20.487627 25743 net.cpp:66] Creating Layer relu1
I0908 23:57:20.487632 25743 net.cpp:329] relu1 <- conv1
I0908 23:57:20.487638 25743 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:57:20.487648 25743 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:57:20.487653 25743 net.cpp:125] relu1 needs backward computation.
I0908 23:57:20.487659 25743 net.cpp:66] Creating Layer pool1
I0908 23:57:20.487664 25743 net.cpp:329] pool1 <- conv1
I0908 23:57:20.487671 25743 net.cpp:290] pool1 -> pool1
I0908 23:57:20.487681 25743 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:57:20.487687 25743 net.cpp:125] pool1 needs backward computation.
I0908 23:57:20.487694 25743 net.cpp:66] Creating Layer norm1
I0908 23:57:20.487699 25743 net.cpp:329] norm1 <- pool1
I0908 23:57:20.487705 25743 net.cpp:290] norm1 -> norm1
I0908 23:57:20.487715 25743 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:57:20.487720 25743 net.cpp:125] norm1 needs backward computation.
I0908 23:57:20.487727 25743 net.cpp:66] Creating Layer conv2
I0908 23:57:20.487733 25743 net.cpp:329] conv2 <- norm1
I0908 23:57:20.487740 25743 net.cpp:290] conv2 -> conv2
I0908 23:57:20.496603 25743 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:57:20.496618 25743 net.cpp:125] conv2 needs backward computation.
I0908 23:57:20.496624 25743 net.cpp:66] Creating Layer relu2
I0908 23:57:20.496630 25743 net.cpp:329] relu2 <- conv2
I0908 23:57:20.496636 25743 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:57:20.496644 25743 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:57:20.496649 25743 net.cpp:125] relu2 needs backward computation.
I0908 23:57:20.496654 25743 net.cpp:66] Creating Layer pool2
I0908 23:57:20.496660 25743 net.cpp:329] pool2 <- conv2
I0908 23:57:20.496666 25743 net.cpp:290] pool2 -> pool2
I0908 23:57:20.496675 25743 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:57:20.496680 25743 net.cpp:125] pool2 needs backward computation.
I0908 23:57:20.496688 25743 net.cpp:66] Creating Layer fc7
I0908 23:57:20.496695 25743 net.cpp:329] fc7 <- pool2
I0908 23:57:20.496701 25743 net.cpp:290] fc7 -> fc7
I0908 23:57:21.121068 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.121114 25743 net.cpp:125] fc7 needs backward computation.
I0908 23:57:21.121124 25743 net.cpp:66] Creating Layer relu7
I0908 23:57:21.121134 25743 net.cpp:329] relu7 <- fc7
I0908 23:57:21.121141 25743 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:57:21.121150 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.121156 25743 net.cpp:125] relu7 needs backward computation.
I0908 23:57:21.121163 25743 net.cpp:66] Creating Layer drop7
I0908 23:57:21.121168 25743 net.cpp:329] drop7 <- fc7
I0908 23:57:21.121175 25743 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:57:21.121196 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.121202 25743 net.cpp:125] drop7 needs backward computation.
I0908 23:57:21.121211 25743 net.cpp:66] Creating Layer fc8
I0908 23:57:21.121217 25743 net.cpp:329] fc8 <- fc7
I0908 23:57:21.121224 25743 net.cpp:290] fc8 -> fc8
I0908 23:57:21.128779 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.128792 25743 net.cpp:125] fc8 needs backward computation.
I0908 23:57:21.128798 25743 net.cpp:66] Creating Layer relu8
I0908 23:57:21.128804 25743 net.cpp:329] relu8 <- fc8
I0908 23:57:21.128813 25743 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:57:21.128819 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.128824 25743 net.cpp:125] relu8 needs backward computation.
I0908 23:57:21.128830 25743 net.cpp:66] Creating Layer drop8
I0908 23:57:21.128836 25743 net.cpp:329] drop8 <- fc8
I0908 23:57:21.128842 25743 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:57:21.128850 25743 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:57:21.128855 25743 net.cpp:125] drop8 needs backward computation.
I0908 23:57:21.128864 25743 net.cpp:66] Creating Layer fc9
I0908 23:57:21.128868 25743 net.cpp:329] fc9 <- fc8
I0908 23:57:21.128875 25743 net.cpp:290] fc9 -> fc9
I0908 23:57:21.129240 25743 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:57:21.129250 25743 net.cpp:125] fc9 needs backward computation.
I0908 23:57:21.129257 25743 net.cpp:66] Creating Layer fc10
I0908 23:57:21.129262 25743 net.cpp:329] fc10 <- fc9
I0908 23:57:21.129271 25743 net.cpp:290] fc10 -> fc10
I0908 23:57:21.129281 25743 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:57:21.129289 25743 net.cpp:125] fc10 needs backward computation.
I0908 23:57:21.129295 25743 net.cpp:66] Creating Layer prob
I0908 23:57:21.129302 25743 net.cpp:329] prob <- fc10
I0908 23:57:21.129308 25743 net.cpp:290] prob -> prob
I0908 23:57:21.129317 25743 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:57:21.129323 25743 net.cpp:125] prob needs backward computation.
I0908 23:57:21.129328 25743 net.cpp:156] This network produces output prob
I0908 23:57:21.129341 25743 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:57:21.129349 25743 net.cpp:167] Network initialization done.
I0908 23:57:21.129354 25743 net.cpp:168] Memory required for data: 6183480
Classifying 204 inputs.
Done in 120.06 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0908 23:59:28.327927 25748 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0908 23:59:28.328063 25748 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0908 23:59:28.328073 25748 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0908 23:59:28.328217 25748 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0908 23:59:28.328277 25748 net.cpp:292] Input 0 -> data
I0908 23:59:28.328304 25748 net.cpp:66] Creating Layer conv1
I0908 23:59:28.328310 25748 net.cpp:329] conv1 <- data
I0908 23:59:28.328317 25748 net.cpp:290] conv1 -> conv1
I0908 23:59:28.329675 25748 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:59:28.329694 25748 net.cpp:125] conv1 needs backward computation.
I0908 23:59:28.329702 25748 net.cpp:66] Creating Layer relu1
I0908 23:59:28.329709 25748 net.cpp:329] relu1 <- conv1
I0908 23:59:28.329715 25748 net.cpp:280] relu1 -> conv1 (in-place)
I0908 23:59:28.329723 25748 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0908 23:59:28.329730 25748 net.cpp:125] relu1 needs backward computation.
I0908 23:59:28.329735 25748 net.cpp:66] Creating Layer pool1
I0908 23:59:28.329741 25748 net.cpp:329] pool1 <- conv1
I0908 23:59:28.329747 25748 net.cpp:290] pool1 -> pool1
I0908 23:59:28.329758 25748 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:59:28.329763 25748 net.cpp:125] pool1 needs backward computation.
I0908 23:59:28.329771 25748 net.cpp:66] Creating Layer norm1
I0908 23:59:28.329776 25748 net.cpp:329] norm1 <- pool1
I0908 23:59:28.329782 25748 net.cpp:290] norm1 -> norm1
I0908 23:59:28.329792 25748 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0908 23:59:28.329797 25748 net.cpp:125] norm1 needs backward computation.
I0908 23:59:28.329804 25748 net.cpp:66] Creating Layer conv2
I0908 23:59:28.329810 25748 net.cpp:329] conv2 <- norm1
I0908 23:59:28.329816 25748 net.cpp:290] conv2 -> conv2
I0908 23:59:28.338657 25748 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:59:28.338671 25748 net.cpp:125] conv2 needs backward computation.
I0908 23:59:28.338678 25748 net.cpp:66] Creating Layer relu2
I0908 23:59:28.338685 25748 net.cpp:329] relu2 <- conv2
I0908 23:59:28.338690 25748 net.cpp:280] relu2 -> conv2 (in-place)
I0908 23:59:28.338697 25748 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0908 23:59:28.338702 25748 net.cpp:125] relu2 needs backward computation.
I0908 23:59:28.338711 25748 net.cpp:66] Creating Layer pool2
I0908 23:59:28.338721 25748 net.cpp:329] pool2 <- conv2
I0908 23:59:28.338728 25748 net.cpp:290] pool2 -> pool2
I0908 23:59:28.338737 25748 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0908 23:59:28.338742 25748 net.cpp:125] pool2 needs backward computation.
I0908 23:59:28.338748 25748 net.cpp:66] Creating Layer fc7
I0908 23:59:28.338753 25748 net.cpp:329] fc7 <- pool2
I0908 23:59:28.338760 25748 net.cpp:290] fc7 -> fc7
I0908 23:59:28.963184 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.963232 25748 net.cpp:125] fc7 needs backward computation.
I0908 23:59:28.963244 25748 net.cpp:66] Creating Layer relu7
I0908 23:59:28.963251 25748 net.cpp:329] relu7 <- fc7
I0908 23:59:28.963260 25748 net.cpp:280] relu7 -> fc7 (in-place)
I0908 23:59:28.963269 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.963275 25748 net.cpp:125] relu7 needs backward computation.
I0908 23:59:28.963282 25748 net.cpp:66] Creating Layer drop7
I0908 23:59:28.963287 25748 net.cpp:329] drop7 <- fc7
I0908 23:59:28.963294 25748 net.cpp:280] drop7 -> fc7 (in-place)
I0908 23:59:28.963305 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.963310 25748 net.cpp:125] drop7 needs backward computation.
I0908 23:59:28.963320 25748 net.cpp:66] Creating Layer fc8
I0908 23:59:28.963325 25748 net.cpp:329] fc8 <- fc7
I0908 23:59:28.963333 25748 net.cpp:290] fc8 -> fc8
I0908 23:59:28.970896 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.970908 25748 net.cpp:125] fc8 needs backward computation.
I0908 23:59:28.970916 25748 net.cpp:66] Creating Layer relu8
I0908 23:59:28.970921 25748 net.cpp:329] relu8 <- fc8
I0908 23:59:28.970928 25748 net.cpp:280] relu8 -> fc8 (in-place)
I0908 23:59:28.970935 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.970940 25748 net.cpp:125] relu8 needs backward computation.
I0908 23:59:28.970947 25748 net.cpp:66] Creating Layer drop8
I0908 23:59:28.970952 25748 net.cpp:329] drop8 <- fc8
I0908 23:59:28.970958 25748 net.cpp:280] drop8 -> fc8 (in-place)
I0908 23:59:28.970965 25748 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0908 23:59:28.970970 25748 net.cpp:125] drop8 needs backward computation.
I0908 23:59:28.970978 25748 net.cpp:66] Creating Layer fc9
I0908 23:59:28.970984 25748 net.cpp:329] fc9 <- fc8
I0908 23:59:28.970990 25748 net.cpp:290] fc9 -> fc9
I0908 23:59:28.971354 25748 net.cpp:83] Top shape: 10 24 1 1 (240)
I0908 23:59:28.971365 25748 net.cpp:125] fc9 needs backward computation.
I0908 23:59:28.971374 25748 net.cpp:66] Creating Layer fc10
I0908 23:59:28.971379 25748 net.cpp:329] fc10 <- fc9
I0908 23:59:28.971386 25748 net.cpp:290] fc10 -> fc10
I0908 23:59:28.971397 25748 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:59:28.971405 25748 net.cpp:125] fc10 needs backward computation.
I0908 23:59:28.971412 25748 net.cpp:66] Creating Layer prob
I0908 23:59:28.971417 25748 net.cpp:329] prob <- fc10
I0908 23:59:28.971424 25748 net.cpp:290] prob -> prob
I0908 23:59:28.971434 25748 net.cpp:83] Top shape: 10 2 1 1 (20)
I0908 23:59:28.971439 25748 net.cpp:125] prob needs backward computation.
I0908 23:59:28.971444 25748 net.cpp:156] This network produces output prob
I0908 23:59:28.971457 25748 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0908 23:59:28.971465 25748 net.cpp:167] Network initialization done.
I0908 23:59:28.971469 25748 net.cpp:168] Memory required for data: 6183480
Classifying 85 inputs.
Done in 53.26 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:00:24.412811 25758 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:00:24.412946 25758 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:00:24.412955 25758 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:00:24.413100 25758 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:00:24.413161 25758 net.cpp:292] Input 0 -> data
I0909 00:00:24.413187 25758 net.cpp:66] Creating Layer conv1
I0909 00:00:24.413192 25758 net.cpp:329] conv1 <- data
I0909 00:00:24.413202 25758 net.cpp:290] conv1 -> conv1
I0909 00:00:24.414567 25758 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:00:24.414587 25758 net.cpp:125] conv1 needs backward computation.
I0909 00:00:24.414595 25758 net.cpp:66] Creating Layer relu1
I0909 00:00:24.414600 25758 net.cpp:329] relu1 <- conv1
I0909 00:00:24.414608 25758 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:00:24.414615 25758 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:00:24.414621 25758 net.cpp:125] relu1 needs backward computation.
I0909 00:00:24.414628 25758 net.cpp:66] Creating Layer pool1
I0909 00:00:24.414633 25758 net.cpp:329] pool1 <- conv1
I0909 00:00:24.414639 25758 net.cpp:290] pool1 -> pool1
I0909 00:00:24.414650 25758 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:00:24.414655 25758 net.cpp:125] pool1 needs backward computation.
I0909 00:00:24.414667 25758 net.cpp:66] Creating Layer norm1
I0909 00:00:24.414672 25758 net.cpp:329] norm1 <- pool1
I0909 00:00:24.414680 25758 net.cpp:290] norm1 -> norm1
I0909 00:00:24.414690 25758 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:00:24.414695 25758 net.cpp:125] norm1 needs backward computation.
I0909 00:00:24.414701 25758 net.cpp:66] Creating Layer conv2
I0909 00:00:24.414707 25758 net.cpp:329] conv2 <- norm1
I0909 00:00:24.414715 25758 net.cpp:290] conv2 -> conv2
I0909 00:00:24.423547 25758 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:00:24.423562 25758 net.cpp:125] conv2 needs backward computation.
I0909 00:00:24.423568 25758 net.cpp:66] Creating Layer relu2
I0909 00:00:24.423573 25758 net.cpp:329] relu2 <- conv2
I0909 00:00:24.423580 25758 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:00:24.423586 25758 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:00:24.423593 25758 net.cpp:125] relu2 needs backward computation.
I0909 00:00:24.423599 25758 net.cpp:66] Creating Layer pool2
I0909 00:00:24.423604 25758 net.cpp:329] pool2 <- conv2
I0909 00:00:24.423610 25758 net.cpp:290] pool2 -> pool2
I0909 00:00:24.423617 25758 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:00:24.423624 25758 net.cpp:125] pool2 needs backward computation.
I0909 00:00:24.423631 25758 net.cpp:66] Creating Layer fc7
I0909 00:00:24.423637 25758 net.cpp:329] fc7 <- pool2
I0909 00:00:24.423645 25758 net.cpp:290] fc7 -> fc7
I0909 00:00:25.048301 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.048348 25758 net.cpp:125] fc7 needs backward computation.
I0909 00:00:25.048360 25758 net.cpp:66] Creating Layer relu7
I0909 00:00:25.048369 25758 net.cpp:329] relu7 <- fc7
I0909 00:00:25.048377 25758 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:00:25.048388 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.048393 25758 net.cpp:125] relu7 needs backward computation.
I0909 00:00:25.048399 25758 net.cpp:66] Creating Layer drop7
I0909 00:00:25.048404 25758 net.cpp:329] drop7 <- fc7
I0909 00:00:25.048411 25758 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:00:25.048421 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.048426 25758 net.cpp:125] drop7 needs backward computation.
I0909 00:00:25.048435 25758 net.cpp:66] Creating Layer fc8
I0909 00:00:25.048440 25758 net.cpp:329] fc8 <- fc7
I0909 00:00:25.048449 25758 net.cpp:290] fc8 -> fc8
I0909 00:00:25.056004 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.056015 25758 net.cpp:125] fc8 needs backward computation.
I0909 00:00:25.056022 25758 net.cpp:66] Creating Layer relu8
I0909 00:00:25.056027 25758 net.cpp:329] relu8 <- fc8
I0909 00:00:25.056035 25758 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:00:25.056042 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.056048 25758 net.cpp:125] relu8 needs backward computation.
I0909 00:00:25.056054 25758 net.cpp:66] Creating Layer drop8
I0909 00:00:25.056059 25758 net.cpp:329] drop8 <- fc8
I0909 00:00:25.056066 25758 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:00:25.056074 25758 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:25.056079 25758 net.cpp:125] drop8 needs backward computation.
I0909 00:00:25.056087 25758 net.cpp:66] Creating Layer fc9
I0909 00:00:25.056092 25758 net.cpp:329] fc9 <- fc8
I0909 00:00:25.056098 25758 net.cpp:290] fc9 -> fc9
I0909 00:00:25.056462 25758 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:00:25.056473 25758 net.cpp:125] fc9 needs backward computation.
I0909 00:00:25.056480 25758 net.cpp:66] Creating Layer fc10
I0909 00:00:25.056485 25758 net.cpp:329] fc10 <- fc9
I0909 00:00:25.056493 25758 net.cpp:290] fc10 -> fc10
I0909 00:00:25.056505 25758 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:00:25.056514 25758 net.cpp:125] fc10 needs backward computation.
I0909 00:00:25.056519 25758 net.cpp:66] Creating Layer prob
I0909 00:00:25.056525 25758 net.cpp:329] prob <- fc10
I0909 00:00:25.056532 25758 net.cpp:290] prob -> prob
I0909 00:00:25.056542 25758 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:00:25.056547 25758 net.cpp:125] prob needs backward computation.
I0909 00:00:25.056562 25758 net.cpp:156] This network produces output prob
I0909 00:00:25.056576 25758 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:00:25.056583 25758 net.cpp:167] Network initialization done.
I0909 00:00:25.056588 25758 net.cpp:168] Memory required for data: 6183480
Classifying 50 inputs.
Done in 31.45 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:00:58.307324 25762 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:00:58.307461 25762 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:00:58.307471 25762 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:00:58.307612 25762 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:00:58.307673 25762 net.cpp:292] Input 0 -> data
I0909 00:00:58.307699 25762 net.cpp:66] Creating Layer conv1
I0909 00:00:58.307706 25762 net.cpp:329] conv1 <- data
I0909 00:00:58.307714 25762 net.cpp:290] conv1 -> conv1
I0909 00:00:58.309061 25762 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:00:58.309077 25762 net.cpp:125] conv1 needs backward computation.
I0909 00:00:58.309087 25762 net.cpp:66] Creating Layer relu1
I0909 00:00:58.309092 25762 net.cpp:329] relu1 <- conv1
I0909 00:00:58.309098 25762 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:00:58.309108 25762 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:00:58.309113 25762 net.cpp:125] relu1 needs backward computation.
I0909 00:00:58.309119 25762 net.cpp:66] Creating Layer pool1
I0909 00:00:58.309124 25762 net.cpp:329] pool1 <- conv1
I0909 00:00:58.309131 25762 net.cpp:290] pool1 -> pool1
I0909 00:00:58.309141 25762 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:00:58.309147 25762 net.cpp:125] pool1 needs backward computation.
I0909 00:00:58.309154 25762 net.cpp:66] Creating Layer norm1
I0909 00:00:58.309159 25762 net.cpp:329] norm1 <- pool1
I0909 00:00:58.309165 25762 net.cpp:290] norm1 -> norm1
I0909 00:00:58.309175 25762 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:00:58.309180 25762 net.cpp:125] norm1 needs backward computation.
I0909 00:00:58.309187 25762 net.cpp:66] Creating Layer conv2
I0909 00:00:58.309193 25762 net.cpp:329] conv2 <- norm1
I0909 00:00:58.309201 25762 net.cpp:290] conv2 -> conv2
I0909 00:00:58.318306 25762 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:00:58.318320 25762 net.cpp:125] conv2 needs backward computation.
I0909 00:00:58.318327 25762 net.cpp:66] Creating Layer relu2
I0909 00:00:58.318333 25762 net.cpp:329] relu2 <- conv2
I0909 00:00:58.318339 25762 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:00:58.318346 25762 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:00:58.318351 25762 net.cpp:125] relu2 needs backward computation.
I0909 00:00:58.318359 25762 net.cpp:66] Creating Layer pool2
I0909 00:00:58.318366 25762 net.cpp:329] pool2 <- conv2
I0909 00:00:58.318372 25762 net.cpp:290] pool2 -> pool2
I0909 00:00:58.318379 25762 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:00:58.318385 25762 net.cpp:125] pool2 needs backward computation.
I0909 00:00:58.318392 25762 net.cpp:66] Creating Layer fc7
I0909 00:00:58.318397 25762 net.cpp:329] fc7 <- pool2
I0909 00:00:58.318403 25762 net.cpp:290] fc7 -> fc7
I0909 00:00:58.943035 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.943083 25762 net.cpp:125] fc7 needs backward computation.
I0909 00:00:58.943095 25762 net.cpp:66] Creating Layer relu7
I0909 00:00:58.943102 25762 net.cpp:329] relu7 <- fc7
I0909 00:00:58.943111 25762 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:00:58.943121 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.943127 25762 net.cpp:125] relu7 needs backward computation.
I0909 00:00:58.943135 25762 net.cpp:66] Creating Layer drop7
I0909 00:00:58.943140 25762 net.cpp:329] drop7 <- fc7
I0909 00:00:58.943145 25762 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:00:58.943156 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.943162 25762 net.cpp:125] drop7 needs backward computation.
I0909 00:00:58.943171 25762 net.cpp:66] Creating Layer fc8
I0909 00:00:58.943176 25762 net.cpp:329] fc8 <- fc7
I0909 00:00:58.943184 25762 net.cpp:290] fc8 -> fc8
I0909 00:00:58.950747 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.950759 25762 net.cpp:125] fc8 needs backward computation.
I0909 00:00:58.950767 25762 net.cpp:66] Creating Layer relu8
I0909 00:00:58.950772 25762 net.cpp:329] relu8 <- fc8
I0909 00:00:58.950779 25762 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:00:58.950786 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.950793 25762 net.cpp:125] relu8 needs backward computation.
I0909 00:00:58.950798 25762 net.cpp:66] Creating Layer drop8
I0909 00:00:58.950803 25762 net.cpp:329] drop8 <- fc8
I0909 00:00:58.950809 25762 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:00:58.950827 25762 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:00:58.950834 25762 net.cpp:125] drop8 needs backward computation.
I0909 00:00:58.950842 25762 net.cpp:66] Creating Layer fc9
I0909 00:00:58.950847 25762 net.cpp:329] fc9 <- fc8
I0909 00:00:58.950855 25762 net.cpp:290] fc9 -> fc9
I0909 00:00:58.951216 25762 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:00:58.951227 25762 net.cpp:125] fc9 needs backward computation.
I0909 00:00:58.951236 25762 net.cpp:66] Creating Layer fc10
I0909 00:00:58.951241 25762 net.cpp:329] fc10 <- fc9
I0909 00:00:58.951249 25762 net.cpp:290] fc10 -> fc10
I0909 00:00:58.951261 25762 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:00:58.951267 25762 net.cpp:125] fc10 needs backward computation.
I0909 00:00:58.951274 25762 net.cpp:66] Creating Layer prob
I0909 00:00:58.951279 25762 net.cpp:329] prob <- fc10
I0909 00:00:58.951287 25762 net.cpp:290] prob -> prob
I0909 00:00:58.951297 25762 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:00:58.951303 25762 net.cpp:125] prob needs backward computation.
I0909 00:00:58.951308 25762 net.cpp:156] This network produces output prob
I0909 00:00:58.951319 25762 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:00:58.951328 25762 net.cpp:167] Network initialization done.
I0909 00:00:58.951333 25762 net.cpp:168] Memory required for data: 6183480
Classifying 159 inputs.
Done in 101.13 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:02:43.717138 25767 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:02:43.717272 25767 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:02:43.717283 25767 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:02:43.717425 25767 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:02:43.717486 25767 net.cpp:292] Input 0 -> data
I0909 00:02:43.717512 25767 net.cpp:66] Creating Layer conv1
I0909 00:02:43.717537 25767 net.cpp:329] conv1 <- data
I0909 00:02:43.717555 25767 net.cpp:290] conv1 -> conv1
I0909 00:02:43.718888 25767 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:02:43.718905 25767 net.cpp:125] conv1 needs backward computation.
I0909 00:02:43.718914 25767 net.cpp:66] Creating Layer relu1
I0909 00:02:43.718920 25767 net.cpp:329] relu1 <- conv1
I0909 00:02:43.718926 25767 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:02:43.718935 25767 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:02:43.718940 25767 net.cpp:125] relu1 needs backward computation.
I0909 00:02:43.718947 25767 net.cpp:66] Creating Layer pool1
I0909 00:02:43.718952 25767 net.cpp:329] pool1 <- conv1
I0909 00:02:43.718958 25767 net.cpp:290] pool1 -> pool1
I0909 00:02:43.718969 25767 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:02:43.718974 25767 net.cpp:125] pool1 needs backward computation.
I0909 00:02:43.718981 25767 net.cpp:66] Creating Layer norm1
I0909 00:02:43.718987 25767 net.cpp:329] norm1 <- pool1
I0909 00:02:43.718993 25767 net.cpp:290] norm1 -> norm1
I0909 00:02:43.719002 25767 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:02:43.719008 25767 net.cpp:125] norm1 needs backward computation.
I0909 00:02:43.719015 25767 net.cpp:66] Creating Layer conv2
I0909 00:02:43.719020 25767 net.cpp:329] conv2 <- norm1
I0909 00:02:43.719027 25767 net.cpp:290] conv2 -> conv2
I0909 00:02:43.727902 25767 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:02:43.727916 25767 net.cpp:125] conv2 needs backward computation.
I0909 00:02:43.727923 25767 net.cpp:66] Creating Layer relu2
I0909 00:02:43.727929 25767 net.cpp:329] relu2 <- conv2
I0909 00:02:43.727936 25767 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:02:43.727942 25767 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:02:43.727948 25767 net.cpp:125] relu2 needs backward computation.
I0909 00:02:43.727954 25767 net.cpp:66] Creating Layer pool2
I0909 00:02:43.727959 25767 net.cpp:329] pool2 <- conv2
I0909 00:02:43.727965 25767 net.cpp:290] pool2 -> pool2
I0909 00:02:43.727973 25767 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:02:43.727978 25767 net.cpp:125] pool2 needs backward computation.
I0909 00:02:43.727988 25767 net.cpp:66] Creating Layer fc7
I0909 00:02:43.727993 25767 net.cpp:329] fc7 <- pool2
I0909 00:02:43.727999 25767 net.cpp:290] fc7 -> fc7
I0909 00:02:44.352579 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.352627 25767 net.cpp:125] fc7 needs backward computation.
I0909 00:02:44.352640 25767 net.cpp:66] Creating Layer relu7
I0909 00:02:44.352648 25767 net.cpp:329] relu7 <- fc7
I0909 00:02:44.352656 25767 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:02:44.352666 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.352671 25767 net.cpp:125] relu7 needs backward computation.
I0909 00:02:44.352679 25767 net.cpp:66] Creating Layer drop7
I0909 00:02:44.352694 25767 net.cpp:329] drop7 <- fc7
I0909 00:02:44.352701 25767 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:02:44.352712 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.352717 25767 net.cpp:125] drop7 needs backward computation.
I0909 00:02:44.352726 25767 net.cpp:66] Creating Layer fc8
I0909 00:02:44.352731 25767 net.cpp:329] fc8 <- fc7
I0909 00:02:44.352740 25767 net.cpp:290] fc8 -> fc8
I0909 00:02:44.360282 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.360294 25767 net.cpp:125] fc8 needs backward computation.
I0909 00:02:44.360301 25767 net.cpp:66] Creating Layer relu8
I0909 00:02:44.360306 25767 net.cpp:329] relu8 <- fc8
I0909 00:02:44.360314 25767 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:02:44.360321 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.360327 25767 net.cpp:125] relu8 needs backward computation.
I0909 00:02:44.360333 25767 net.cpp:66] Creating Layer drop8
I0909 00:02:44.360338 25767 net.cpp:329] drop8 <- fc8
I0909 00:02:44.360344 25767 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:02:44.360352 25767 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:02:44.360358 25767 net.cpp:125] drop8 needs backward computation.
I0909 00:02:44.360365 25767 net.cpp:66] Creating Layer fc9
I0909 00:02:44.360370 25767 net.cpp:329] fc9 <- fc8
I0909 00:02:44.360378 25767 net.cpp:290] fc9 -> fc9
I0909 00:02:44.360739 25767 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:02:44.360750 25767 net.cpp:125] fc9 needs backward computation.
I0909 00:02:44.360759 25767 net.cpp:66] Creating Layer fc10
I0909 00:02:44.360764 25767 net.cpp:329] fc10 <- fc9
I0909 00:02:44.360771 25767 net.cpp:290] fc10 -> fc10
I0909 00:02:44.360784 25767 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:02:44.360790 25767 net.cpp:125] fc10 needs backward computation.
I0909 00:02:44.360797 25767 net.cpp:66] Creating Layer prob
I0909 00:02:44.360802 25767 net.cpp:329] prob <- fc10
I0909 00:02:44.360810 25767 net.cpp:290] prob -> prob
I0909 00:02:44.360820 25767 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:02:44.360826 25767 net.cpp:125] prob needs backward computation.
I0909 00:02:44.360829 25767 net.cpp:156] This network produces output prob
I0909 00:02:44.360842 25767 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:02:44.360851 25767 net.cpp:167] Network initialization done.
I0909 00:02:44.360855 25767 net.cpp:168] Memory required for data: 6183480
Classifying 24 inputs.
Done in 14.63 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:03:00.201185 25771 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:03:00.201325 25771 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:03:00.201334 25771 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:03:00.201483 25771 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:03:00.201565 25771 net.cpp:292] Input 0 -> data
I0909 00:03:00.201592 25771 net.cpp:66] Creating Layer conv1
I0909 00:03:00.201599 25771 net.cpp:329] conv1 <- data
I0909 00:03:00.201608 25771 net.cpp:290] conv1 -> conv1
I0909 00:03:00.202941 25771 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:03:00.202960 25771 net.cpp:125] conv1 needs backward computation.
I0909 00:03:00.202968 25771 net.cpp:66] Creating Layer relu1
I0909 00:03:00.202973 25771 net.cpp:329] relu1 <- conv1
I0909 00:03:00.202980 25771 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:03:00.202988 25771 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:03:00.202994 25771 net.cpp:125] relu1 needs backward computation.
I0909 00:03:00.203001 25771 net.cpp:66] Creating Layer pool1
I0909 00:03:00.203006 25771 net.cpp:329] pool1 <- conv1
I0909 00:03:00.203013 25771 net.cpp:290] pool1 -> pool1
I0909 00:03:00.203023 25771 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:03:00.203029 25771 net.cpp:125] pool1 needs backward computation.
I0909 00:03:00.203035 25771 net.cpp:66] Creating Layer norm1
I0909 00:03:00.203042 25771 net.cpp:329] norm1 <- pool1
I0909 00:03:00.203047 25771 net.cpp:290] norm1 -> norm1
I0909 00:03:00.203057 25771 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:03:00.203063 25771 net.cpp:125] norm1 needs backward computation.
I0909 00:03:00.203069 25771 net.cpp:66] Creating Layer conv2
I0909 00:03:00.203075 25771 net.cpp:329] conv2 <- norm1
I0909 00:03:00.203081 25771 net.cpp:290] conv2 -> conv2
I0909 00:03:00.211989 25771 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:03:00.212007 25771 net.cpp:125] conv2 needs backward computation.
I0909 00:03:00.212014 25771 net.cpp:66] Creating Layer relu2
I0909 00:03:00.212020 25771 net.cpp:329] relu2 <- conv2
I0909 00:03:00.212028 25771 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:03:00.212034 25771 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:03:00.212039 25771 net.cpp:125] relu2 needs backward computation.
I0909 00:03:00.212051 25771 net.cpp:66] Creating Layer pool2
I0909 00:03:00.212057 25771 net.cpp:329] pool2 <- conv2
I0909 00:03:00.212064 25771 net.cpp:290] pool2 -> pool2
I0909 00:03:00.212072 25771 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:03:00.212079 25771 net.cpp:125] pool2 needs backward computation.
I0909 00:03:00.212087 25771 net.cpp:66] Creating Layer fc7
I0909 00:03:00.212093 25771 net.cpp:329] fc7 <- pool2
I0909 00:03:00.212101 25771 net.cpp:290] fc7 -> fc7
I0909 00:03:00.837298 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.837335 25771 net.cpp:125] fc7 needs backward computation.
I0909 00:03:00.837348 25771 net.cpp:66] Creating Layer relu7
I0909 00:03:00.837357 25771 net.cpp:329] relu7 <- fc7
I0909 00:03:00.837365 25771 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:03:00.837374 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.837379 25771 net.cpp:125] relu7 needs backward computation.
I0909 00:03:00.837386 25771 net.cpp:66] Creating Layer drop7
I0909 00:03:00.837391 25771 net.cpp:329] drop7 <- fc7
I0909 00:03:00.837399 25771 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:03:00.837409 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.837414 25771 net.cpp:125] drop7 needs backward computation.
I0909 00:03:00.837422 25771 net.cpp:66] Creating Layer fc8
I0909 00:03:00.837427 25771 net.cpp:329] fc8 <- fc7
I0909 00:03:00.837436 25771 net.cpp:290] fc8 -> fc8
I0909 00:03:00.844980 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.844991 25771 net.cpp:125] fc8 needs backward computation.
I0909 00:03:00.844997 25771 net.cpp:66] Creating Layer relu8
I0909 00:03:00.845003 25771 net.cpp:329] relu8 <- fc8
I0909 00:03:00.845010 25771 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:03:00.845017 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.845023 25771 net.cpp:125] relu8 needs backward computation.
I0909 00:03:00.845029 25771 net.cpp:66] Creating Layer drop8
I0909 00:03:00.845034 25771 net.cpp:329] drop8 <- fc8
I0909 00:03:00.845041 25771 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:03:00.845048 25771 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:03:00.845054 25771 net.cpp:125] drop8 needs backward computation.
I0909 00:03:00.845062 25771 net.cpp:66] Creating Layer fc9
I0909 00:03:00.845067 25771 net.cpp:329] fc9 <- fc8
I0909 00:03:00.845073 25771 net.cpp:290] fc9 -> fc9
I0909 00:03:00.845438 25771 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:03:00.845448 25771 net.cpp:125] fc9 needs backward computation.
I0909 00:03:00.845455 25771 net.cpp:66] Creating Layer fc10
I0909 00:03:00.845461 25771 net.cpp:329] fc10 <- fc9
I0909 00:03:00.845469 25771 net.cpp:290] fc10 -> fc10
I0909 00:03:00.845480 25771 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:03:00.845489 25771 net.cpp:125] fc10 needs backward computation.
I0909 00:03:00.845494 25771 net.cpp:66] Creating Layer prob
I0909 00:03:00.845499 25771 net.cpp:329] prob <- fc10
I0909 00:03:00.845507 25771 net.cpp:290] prob -> prob
I0909 00:03:00.845525 25771 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:03:00.845532 25771 net.cpp:125] prob needs backward computation.
I0909 00:03:00.845536 25771 net.cpp:156] This network produces output prob
I0909 00:03:00.845549 25771 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:03:00.845557 25771 net.cpp:167] Network initialization done.
I0909 00:03:00.845562 25771 net.cpp:168] Memory required for data: 6183480
Classifying 101 inputs.
Done in 62.18 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:04:07.139123 25776 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:04:07.139260 25776 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:04:07.139268 25776 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:04:07.139423 25776 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:04:07.139474 25776 net.cpp:292] Input 0 -> data
I0909 00:04:07.139500 25776 net.cpp:66] Creating Layer conv1
I0909 00:04:07.139507 25776 net.cpp:329] conv1 <- data
I0909 00:04:07.139514 25776 net.cpp:290] conv1 -> conv1
I0909 00:04:07.140836 25776 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:04:07.140854 25776 net.cpp:125] conv1 needs backward computation.
I0909 00:04:07.140863 25776 net.cpp:66] Creating Layer relu1
I0909 00:04:07.140869 25776 net.cpp:329] relu1 <- conv1
I0909 00:04:07.140877 25776 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:04:07.140884 25776 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:04:07.140890 25776 net.cpp:125] relu1 needs backward computation.
I0909 00:04:07.140897 25776 net.cpp:66] Creating Layer pool1
I0909 00:04:07.140902 25776 net.cpp:329] pool1 <- conv1
I0909 00:04:07.140908 25776 net.cpp:290] pool1 -> pool1
I0909 00:04:07.140919 25776 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:04:07.140929 25776 net.cpp:125] pool1 needs backward computation.
I0909 00:04:07.140936 25776 net.cpp:66] Creating Layer norm1
I0909 00:04:07.140943 25776 net.cpp:329] norm1 <- pool1
I0909 00:04:07.140949 25776 net.cpp:290] norm1 -> norm1
I0909 00:04:07.140959 25776 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:04:07.140964 25776 net.cpp:125] norm1 needs backward computation.
I0909 00:04:07.140970 25776 net.cpp:66] Creating Layer conv2
I0909 00:04:07.140976 25776 net.cpp:329] conv2 <- norm1
I0909 00:04:07.140983 25776 net.cpp:290] conv2 -> conv2
I0909 00:04:07.149855 25776 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:04:07.149869 25776 net.cpp:125] conv2 needs backward computation.
I0909 00:04:07.149876 25776 net.cpp:66] Creating Layer relu2
I0909 00:04:07.149883 25776 net.cpp:329] relu2 <- conv2
I0909 00:04:07.149888 25776 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:04:07.149895 25776 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:04:07.149901 25776 net.cpp:125] relu2 needs backward computation.
I0909 00:04:07.149907 25776 net.cpp:66] Creating Layer pool2
I0909 00:04:07.149912 25776 net.cpp:329] pool2 <- conv2
I0909 00:04:07.149919 25776 net.cpp:290] pool2 -> pool2
I0909 00:04:07.149927 25776 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:04:07.149932 25776 net.cpp:125] pool2 needs backward computation.
I0909 00:04:07.149941 25776 net.cpp:66] Creating Layer fc7
I0909 00:04:07.149947 25776 net.cpp:329] fc7 <- pool2
I0909 00:04:07.149955 25776 net.cpp:290] fc7 -> fc7
I0909 00:04:07.774216 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.774262 25776 net.cpp:125] fc7 needs backward computation.
I0909 00:04:07.774276 25776 net.cpp:66] Creating Layer relu7
I0909 00:04:07.774286 25776 net.cpp:329] relu7 <- fc7
I0909 00:04:07.774294 25776 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:04:07.774303 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.774309 25776 net.cpp:125] relu7 needs backward computation.
I0909 00:04:07.774317 25776 net.cpp:66] Creating Layer drop7
I0909 00:04:07.774322 25776 net.cpp:329] drop7 <- fc7
I0909 00:04:07.774328 25776 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:04:07.774338 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.774344 25776 net.cpp:125] drop7 needs backward computation.
I0909 00:04:07.774353 25776 net.cpp:66] Creating Layer fc8
I0909 00:04:07.774358 25776 net.cpp:329] fc8 <- fc7
I0909 00:04:07.774366 25776 net.cpp:290] fc8 -> fc8
I0909 00:04:07.781908 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.781920 25776 net.cpp:125] fc8 needs backward computation.
I0909 00:04:07.781927 25776 net.cpp:66] Creating Layer relu8
I0909 00:04:07.781932 25776 net.cpp:329] relu8 <- fc8
I0909 00:04:07.781940 25776 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:04:07.781947 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.781954 25776 net.cpp:125] relu8 needs backward computation.
I0909 00:04:07.781960 25776 net.cpp:66] Creating Layer drop8
I0909 00:04:07.781965 25776 net.cpp:329] drop8 <- fc8
I0909 00:04:07.781970 25776 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:04:07.781978 25776 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:04:07.781985 25776 net.cpp:125] drop8 needs backward computation.
I0909 00:04:07.781991 25776 net.cpp:66] Creating Layer fc9
I0909 00:04:07.781997 25776 net.cpp:329] fc9 <- fc8
I0909 00:04:07.782004 25776 net.cpp:290] fc9 -> fc9
I0909 00:04:07.782367 25776 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:04:07.782377 25776 net.cpp:125] fc9 needs backward computation.
I0909 00:04:07.782385 25776 net.cpp:66] Creating Layer fc10
I0909 00:04:07.782392 25776 net.cpp:329] fc10 <- fc9
I0909 00:04:07.782399 25776 net.cpp:290] fc10 -> fc10
I0909 00:04:07.782410 25776 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:04:07.782418 25776 net.cpp:125] fc10 needs backward computation.
I0909 00:04:07.782425 25776 net.cpp:66] Creating Layer prob
I0909 00:04:07.782430 25776 net.cpp:329] prob <- fc10
I0909 00:04:07.782438 25776 net.cpp:290] prob -> prob
I0909 00:04:07.782448 25776 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:04:07.782462 25776 net.cpp:125] prob needs backward computation.
I0909 00:04:07.782469 25776 net.cpp:156] This network produces output prob
I0909 00:04:07.782480 25776 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:04:07.782488 25776 net.cpp:167] Network initialization done.
I0909 00:04:07.782493 25776 net.cpp:168] Memory required for data: 6183480
Classifying 218 inputs.
Done in 138.25 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:06:30.695143 25781 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:06:30.695296 25781 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:06:30.695812 25781 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:06:30.695981 25781 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:06:30.696043 25781 net.cpp:292] Input 0 -> data
I0909 00:06:30.696069 25781 net.cpp:66] Creating Layer conv1
I0909 00:06:30.696075 25781 net.cpp:329] conv1 <- data
I0909 00:06:30.696084 25781 net.cpp:290] conv1 -> conv1
I0909 00:06:30.697454 25781 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:06:30.697473 25781 net.cpp:125] conv1 needs backward computation.
I0909 00:06:30.697481 25781 net.cpp:66] Creating Layer relu1
I0909 00:06:30.697487 25781 net.cpp:329] relu1 <- conv1
I0909 00:06:30.697494 25781 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:06:30.697502 25781 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:06:30.697509 25781 net.cpp:125] relu1 needs backward computation.
I0909 00:06:30.697535 25781 net.cpp:66] Creating Layer pool1
I0909 00:06:30.697543 25781 net.cpp:329] pool1 <- conv1
I0909 00:06:30.697551 25781 net.cpp:290] pool1 -> pool1
I0909 00:06:30.697562 25781 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:06:30.697568 25781 net.cpp:125] pool1 needs backward computation.
I0909 00:06:30.697576 25781 net.cpp:66] Creating Layer norm1
I0909 00:06:30.697582 25781 net.cpp:329] norm1 <- pool1
I0909 00:06:30.697587 25781 net.cpp:290] norm1 -> norm1
I0909 00:06:30.697597 25781 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:06:30.697603 25781 net.cpp:125] norm1 needs backward computation.
I0909 00:06:30.697612 25781 net.cpp:66] Creating Layer conv2
I0909 00:06:30.697617 25781 net.cpp:329] conv2 <- norm1
I0909 00:06:30.697624 25781 net.cpp:290] conv2 -> conv2
I0909 00:06:30.706603 25781 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:06:30.706616 25781 net.cpp:125] conv2 needs backward computation.
I0909 00:06:30.706624 25781 net.cpp:66] Creating Layer relu2
I0909 00:06:30.706629 25781 net.cpp:329] relu2 <- conv2
I0909 00:06:30.706635 25781 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:06:30.706642 25781 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:06:30.706648 25781 net.cpp:125] relu2 needs backward computation.
I0909 00:06:30.706655 25781 net.cpp:66] Creating Layer pool2
I0909 00:06:30.706660 25781 net.cpp:329] pool2 <- conv2
I0909 00:06:30.706666 25781 net.cpp:290] pool2 -> pool2
I0909 00:06:30.706675 25781 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:06:30.706679 25781 net.cpp:125] pool2 needs backward computation.
I0909 00:06:30.706688 25781 net.cpp:66] Creating Layer fc7
I0909 00:06:30.706694 25781 net.cpp:329] fc7 <- pool2
I0909 00:06:30.706701 25781 net.cpp:290] fc7 -> fc7
I0909 00:06:31.331733 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.331781 25781 net.cpp:125] fc7 needs backward computation.
I0909 00:06:31.331794 25781 net.cpp:66] Creating Layer relu7
I0909 00:06:31.331802 25781 net.cpp:329] relu7 <- fc7
I0909 00:06:31.331810 25781 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:06:31.331820 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.331825 25781 net.cpp:125] relu7 needs backward computation.
I0909 00:06:31.331833 25781 net.cpp:66] Creating Layer drop7
I0909 00:06:31.331838 25781 net.cpp:329] drop7 <- fc7
I0909 00:06:31.331845 25781 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:06:31.331856 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.331861 25781 net.cpp:125] drop7 needs backward computation.
I0909 00:06:31.331869 25781 net.cpp:66] Creating Layer fc8
I0909 00:06:31.331876 25781 net.cpp:329] fc8 <- fc7
I0909 00:06:31.331884 25781 net.cpp:290] fc8 -> fc8
I0909 00:06:31.339422 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.339434 25781 net.cpp:125] fc8 needs backward computation.
I0909 00:06:31.339442 25781 net.cpp:66] Creating Layer relu8
I0909 00:06:31.339447 25781 net.cpp:329] relu8 <- fc8
I0909 00:06:31.339454 25781 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:06:31.339462 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.339468 25781 net.cpp:125] relu8 needs backward computation.
I0909 00:06:31.339473 25781 net.cpp:66] Creating Layer drop8
I0909 00:06:31.339489 25781 net.cpp:329] drop8 <- fc8
I0909 00:06:31.339496 25781 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:06:31.339505 25781 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:06:31.339510 25781 net.cpp:125] drop8 needs backward computation.
I0909 00:06:31.339519 25781 net.cpp:66] Creating Layer fc9
I0909 00:06:31.339524 25781 net.cpp:329] fc9 <- fc8
I0909 00:06:31.339530 25781 net.cpp:290] fc9 -> fc9
I0909 00:06:31.339908 25781 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:06:31.339920 25781 net.cpp:125] fc9 needs backward computation.
I0909 00:06:31.339927 25781 net.cpp:66] Creating Layer fc10
I0909 00:06:31.339932 25781 net.cpp:329] fc10 <- fc9
I0909 00:06:31.339941 25781 net.cpp:290] fc10 -> fc10
I0909 00:06:31.339952 25781 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:06:31.339959 25781 net.cpp:125] fc10 needs backward computation.
I0909 00:06:31.339967 25781 net.cpp:66] Creating Layer prob
I0909 00:06:31.339972 25781 net.cpp:329] prob <- fc10
I0909 00:06:31.339979 25781 net.cpp:290] prob -> prob
I0909 00:06:31.339988 25781 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:06:31.339994 25781 net.cpp:125] prob needs backward computation.
I0909 00:06:31.339999 25781 net.cpp:156] This network produces output prob
I0909 00:06:31.340011 25781 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:06:31.340019 25781 net.cpp:167] Network initialization done.
I0909 00:06:31.340024 25781 net.cpp:168] Memory required for data: 6183480
Classifying 152 inputs.
Done in 93.98 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:08:10.673686 25786 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:08:10.673823 25786 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:08:10.673832 25786 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:08:10.673976 25786 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:08:10.674039 25786 net.cpp:292] Input 0 -> data
I0909 00:08:10.674065 25786 net.cpp:66] Creating Layer conv1
I0909 00:08:10.674072 25786 net.cpp:329] conv1 <- data
I0909 00:08:10.674079 25786 net.cpp:290] conv1 -> conv1
I0909 00:08:10.675401 25786 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:08:10.675420 25786 net.cpp:125] conv1 needs backward computation.
I0909 00:08:10.675427 25786 net.cpp:66] Creating Layer relu1
I0909 00:08:10.675433 25786 net.cpp:329] relu1 <- conv1
I0909 00:08:10.675441 25786 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:08:10.675448 25786 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:08:10.675453 25786 net.cpp:125] relu1 needs backward computation.
I0909 00:08:10.675461 25786 net.cpp:66] Creating Layer pool1
I0909 00:08:10.675465 25786 net.cpp:329] pool1 <- conv1
I0909 00:08:10.675472 25786 net.cpp:290] pool1 -> pool1
I0909 00:08:10.675482 25786 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:08:10.675488 25786 net.cpp:125] pool1 needs backward computation.
I0909 00:08:10.675495 25786 net.cpp:66] Creating Layer norm1
I0909 00:08:10.675500 25786 net.cpp:329] norm1 <- pool1
I0909 00:08:10.675506 25786 net.cpp:290] norm1 -> norm1
I0909 00:08:10.675516 25786 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:08:10.675523 25786 net.cpp:125] norm1 needs backward computation.
I0909 00:08:10.675529 25786 net.cpp:66] Creating Layer conv2
I0909 00:08:10.675534 25786 net.cpp:329] conv2 <- norm1
I0909 00:08:10.675541 25786 net.cpp:290] conv2 -> conv2
I0909 00:08:10.684424 25786 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:08:10.684438 25786 net.cpp:125] conv2 needs backward computation.
I0909 00:08:10.684445 25786 net.cpp:66] Creating Layer relu2
I0909 00:08:10.684450 25786 net.cpp:329] relu2 <- conv2
I0909 00:08:10.684456 25786 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:08:10.684463 25786 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:08:10.684468 25786 net.cpp:125] relu2 needs backward computation.
I0909 00:08:10.684474 25786 net.cpp:66] Creating Layer pool2
I0909 00:08:10.684479 25786 net.cpp:329] pool2 <- conv2
I0909 00:08:10.684486 25786 net.cpp:290] pool2 -> pool2
I0909 00:08:10.684494 25786 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:08:10.684499 25786 net.cpp:125] pool2 needs backward computation.
I0909 00:08:10.684509 25786 net.cpp:66] Creating Layer fc7
I0909 00:08:10.684514 25786 net.cpp:329] fc7 <- pool2
I0909 00:08:10.684520 25786 net.cpp:290] fc7 -> fc7
I0909 00:08:11.310330 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.310379 25786 net.cpp:125] fc7 needs backward computation.
I0909 00:08:11.310391 25786 net.cpp:66] Creating Layer relu7
I0909 00:08:11.310400 25786 net.cpp:329] relu7 <- fc7
I0909 00:08:11.310408 25786 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:08:11.310417 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.310423 25786 net.cpp:125] relu7 needs backward computation.
I0909 00:08:11.310442 25786 net.cpp:66] Creating Layer drop7
I0909 00:08:11.310448 25786 net.cpp:329] drop7 <- fc7
I0909 00:08:11.310454 25786 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:08:11.310466 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.310470 25786 net.cpp:125] drop7 needs backward computation.
I0909 00:08:11.310478 25786 net.cpp:66] Creating Layer fc8
I0909 00:08:11.310483 25786 net.cpp:329] fc8 <- fc7
I0909 00:08:11.310492 25786 net.cpp:290] fc8 -> fc8
I0909 00:08:11.318053 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.318064 25786 net.cpp:125] fc8 needs backward computation.
I0909 00:08:11.318071 25786 net.cpp:66] Creating Layer relu8
I0909 00:08:11.318078 25786 net.cpp:329] relu8 <- fc8
I0909 00:08:11.318084 25786 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:08:11.318091 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.318097 25786 net.cpp:125] relu8 needs backward computation.
I0909 00:08:11.318104 25786 net.cpp:66] Creating Layer drop8
I0909 00:08:11.318109 25786 net.cpp:329] drop8 <- fc8
I0909 00:08:11.318114 25786 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:08:11.318122 25786 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:11.318128 25786 net.cpp:125] drop8 needs backward computation.
I0909 00:08:11.318135 25786 net.cpp:66] Creating Layer fc9
I0909 00:08:11.318141 25786 net.cpp:329] fc9 <- fc8
I0909 00:08:11.318147 25786 net.cpp:290] fc9 -> fc9
I0909 00:08:11.318512 25786 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:08:11.318522 25786 net.cpp:125] fc9 needs backward computation.
I0909 00:08:11.318531 25786 net.cpp:66] Creating Layer fc10
I0909 00:08:11.318536 25786 net.cpp:329] fc10 <- fc9
I0909 00:08:11.318544 25786 net.cpp:290] fc10 -> fc10
I0909 00:08:11.318555 25786 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:08:11.318563 25786 net.cpp:125] fc10 needs backward computation.
I0909 00:08:11.318569 25786 net.cpp:66] Creating Layer prob
I0909 00:08:11.318575 25786 net.cpp:329] prob <- fc10
I0909 00:08:11.318583 25786 net.cpp:290] prob -> prob
I0909 00:08:11.318591 25786 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:08:11.318598 25786 net.cpp:125] prob needs backward computation.
I0909 00:08:11.318603 25786 net.cpp:156] This network produces output prob
I0909 00:08:11.318614 25786 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:08:11.318622 25786 net.cpp:167] Network initialization done.
I0909 00:08:11.318627 25786 net.cpp:168] Memory required for data: 6183480
Classifying 61 inputs.
Done in 36.60 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:08:49.561336 25790 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:08:49.561475 25790 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:08:49.561483 25790 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:08:49.561668 25790 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:08:49.561732 25790 net.cpp:292] Input 0 -> data
I0909 00:08:49.561758 25790 net.cpp:66] Creating Layer conv1
I0909 00:08:49.561765 25790 net.cpp:329] conv1 <- data
I0909 00:08:49.561774 25790 net.cpp:290] conv1 -> conv1
I0909 00:08:49.563098 25790 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:08:49.563117 25790 net.cpp:125] conv1 needs backward computation.
I0909 00:08:49.563125 25790 net.cpp:66] Creating Layer relu1
I0909 00:08:49.563130 25790 net.cpp:329] relu1 <- conv1
I0909 00:08:49.563138 25790 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:08:49.563145 25790 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:08:49.563151 25790 net.cpp:125] relu1 needs backward computation.
I0909 00:08:49.563158 25790 net.cpp:66] Creating Layer pool1
I0909 00:08:49.563163 25790 net.cpp:329] pool1 <- conv1
I0909 00:08:49.563169 25790 net.cpp:290] pool1 -> pool1
I0909 00:08:49.563180 25790 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:08:49.563185 25790 net.cpp:125] pool1 needs backward computation.
I0909 00:08:49.563192 25790 net.cpp:66] Creating Layer norm1
I0909 00:08:49.563197 25790 net.cpp:329] norm1 <- pool1
I0909 00:08:49.563204 25790 net.cpp:290] norm1 -> norm1
I0909 00:08:49.563213 25790 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:08:49.563218 25790 net.cpp:125] norm1 needs backward computation.
I0909 00:08:49.563225 25790 net.cpp:66] Creating Layer conv2
I0909 00:08:49.563231 25790 net.cpp:329] conv2 <- norm1
I0909 00:08:49.563237 25790 net.cpp:290] conv2 -> conv2
I0909 00:08:49.572134 25790 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:08:49.572147 25790 net.cpp:125] conv2 needs backward computation.
I0909 00:08:49.572154 25790 net.cpp:66] Creating Layer relu2
I0909 00:08:49.572160 25790 net.cpp:329] relu2 <- conv2
I0909 00:08:49.572166 25790 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:08:49.572172 25790 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:08:49.572182 25790 net.cpp:125] relu2 needs backward computation.
I0909 00:08:49.572190 25790 net.cpp:66] Creating Layer pool2
I0909 00:08:49.572195 25790 net.cpp:329] pool2 <- conv2
I0909 00:08:49.572201 25790 net.cpp:290] pool2 -> pool2
I0909 00:08:49.572209 25790 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:08:49.572214 25790 net.cpp:125] pool2 needs backward computation.
I0909 00:08:49.572223 25790 net.cpp:66] Creating Layer fc7
I0909 00:08:49.572229 25790 net.cpp:329] fc7 <- pool2
I0909 00:08:49.572237 25790 net.cpp:290] fc7 -> fc7
I0909 00:08:50.201897 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.201953 25790 net.cpp:125] fc7 needs backward computation.
I0909 00:08:50.201972 25790 net.cpp:66] Creating Layer relu7
I0909 00:08:50.201985 25790 net.cpp:329] relu7 <- fc7
I0909 00:08:50.201994 25790 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:08:50.202004 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.202010 25790 net.cpp:125] relu7 needs backward computation.
I0909 00:08:50.202018 25790 net.cpp:66] Creating Layer drop7
I0909 00:08:50.202023 25790 net.cpp:329] drop7 <- fc7
I0909 00:08:50.202030 25790 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:08:50.202041 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.202047 25790 net.cpp:125] drop7 needs backward computation.
I0909 00:08:50.202055 25790 net.cpp:66] Creating Layer fc8
I0909 00:08:50.202060 25790 net.cpp:329] fc8 <- fc7
I0909 00:08:50.202069 25790 net.cpp:290] fc8 -> fc8
I0909 00:08:50.209681 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.209692 25790 net.cpp:125] fc8 needs backward computation.
I0909 00:08:50.209698 25790 net.cpp:66] Creating Layer relu8
I0909 00:08:50.209704 25790 net.cpp:329] relu8 <- fc8
I0909 00:08:50.209712 25790 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:08:50.209718 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.209724 25790 net.cpp:125] relu8 needs backward computation.
I0909 00:08:50.209730 25790 net.cpp:66] Creating Layer drop8
I0909 00:08:50.209735 25790 net.cpp:329] drop8 <- fc8
I0909 00:08:50.209741 25790 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:08:50.209750 25790 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:08:50.209755 25790 net.cpp:125] drop8 needs backward computation.
I0909 00:08:50.209764 25790 net.cpp:66] Creating Layer fc9
I0909 00:08:50.209769 25790 net.cpp:329] fc9 <- fc8
I0909 00:08:50.209775 25790 net.cpp:290] fc9 -> fc9
I0909 00:08:50.210139 25790 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:08:50.210150 25790 net.cpp:125] fc9 needs backward computation.
I0909 00:08:50.210157 25790 net.cpp:66] Creating Layer fc10
I0909 00:08:50.210162 25790 net.cpp:329] fc10 <- fc9
I0909 00:08:50.210171 25790 net.cpp:290] fc10 -> fc10
I0909 00:08:50.210182 25790 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:08:50.210191 25790 net.cpp:125] fc10 needs backward computation.
I0909 00:08:50.210196 25790 net.cpp:66] Creating Layer prob
I0909 00:08:50.210201 25790 net.cpp:329] prob <- fc10
I0909 00:08:50.210209 25790 net.cpp:290] prob -> prob
I0909 00:08:50.210219 25790 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:08:50.210224 25790 net.cpp:125] prob needs backward computation.
I0909 00:08:50.210229 25790 net.cpp:156] This network produces output prob
I0909 00:08:50.210242 25790 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:08:50.210250 25790 net.cpp:167] Network initialization done.
I0909 00:08:50.210255 25790 net.cpp:168] Memory required for data: 6183480
Classifying 134 inputs.
Done in 83.24 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:10:17.022269 25807 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:10:17.022404 25807 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:10:17.022413 25807 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:10:17.022569 25807 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:10:17.022620 25807 net.cpp:292] Input 0 -> data
I0909 00:10:17.022645 25807 net.cpp:66] Creating Layer conv1
I0909 00:10:17.022652 25807 net.cpp:329] conv1 <- data
I0909 00:10:17.022660 25807 net.cpp:290] conv1 -> conv1
I0909 00:10:17.023983 25807 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:10:17.023999 25807 net.cpp:125] conv1 needs backward computation.
I0909 00:10:17.024008 25807 net.cpp:66] Creating Layer relu1
I0909 00:10:17.024014 25807 net.cpp:329] relu1 <- conv1
I0909 00:10:17.024021 25807 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:10:17.024029 25807 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:10:17.024035 25807 net.cpp:125] relu1 needs backward computation.
I0909 00:10:17.024042 25807 net.cpp:66] Creating Layer pool1
I0909 00:10:17.024047 25807 net.cpp:329] pool1 <- conv1
I0909 00:10:17.024060 25807 net.cpp:290] pool1 -> pool1
I0909 00:10:17.024070 25807 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:10:17.024076 25807 net.cpp:125] pool1 needs backward computation.
I0909 00:10:17.024083 25807 net.cpp:66] Creating Layer norm1
I0909 00:10:17.024088 25807 net.cpp:329] norm1 <- pool1
I0909 00:10:17.024096 25807 net.cpp:290] norm1 -> norm1
I0909 00:10:17.024104 25807 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:10:17.024111 25807 net.cpp:125] norm1 needs backward computation.
I0909 00:10:17.024117 25807 net.cpp:66] Creating Layer conv2
I0909 00:10:17.024123 25807 net.cpp:329] conv2 <- norm1
I0909 00:10:17.024130 25807 net.cpp:290] conv2 -> conv2
I0909 00:10:17.033018 25807 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:10:17.033032 25807 net.cpp:125] conv2 needs backward computation.
I0909 00:10:17.033041 25807 net.cpp:66] Creating Layer relu2
I0909 00:10:17.033046 25807 net.cpp:329] relu2 <- conv2
I0909 00:10:17.033052 25807 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:10:17.033059 25807 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:10:17.033064 25807 net.cpp:125] relu2 needs backward computation.
I0909 00:10:17.033071 25807 net.cpp:66] Creating Layer pool2
I0909 00:10:17.033077 25807 net.cpp:329] pool2 <- conv2
I0909 00:10:17.033082 25807 net.cpp:290] pool2 -> pool2
I0909 00:10:17.033090 25807 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:10:17.033095 25807 net.cpp:125] pool2 needs backward computation.
I0909 00:10:17.033104 25807 net.cpp:66] Creating Layer fc7
I0909 00:10:17.033110 25807 net.cpp:329] fc7 <- pool2
I0909 00:10:17.033118 25807 net.cpp:290] fc7 -> fc7
I0909 00:10:17.701462 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.701509 25807 net.cpp:125] fc7 needs backward computation.
I0909 00:10:17.701524 25807 net.cpp:66] Creating Layer relu7
I0909 00:10:17.701534 25807 net.cpp:329] relu7 <- fc7
I0909 00:10:17.701544 25807 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:10:17.701552 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.701558 25807 net.cpp:125] relu7 needs backward computation.
I0909 00:10:17.701565 25807 net.cpp:66] Creating Layer drop7
I0909 00:10:17.701570 25807 net.cpp:329] drop7 <- fc7
I0909 00:10:17.701576 25807 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:10:17.701587 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.701592 25807 net.cpp:125] drop7 needs backward computation.
I0909 00:10:17.701601 25807 net.cpp:66] Creating Layer fc8
I0909 00:10:17.701606 25807 net.cpp:329] fc8 <- fc7
I0909 00:10:17.701616 25807 net.cpp:290] fc8 -> fc8
I0909 00:10:17.709672 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.709684 25807 net.cpp:125] fc8 needs backward computation.
I0909 00:10:17.709692 25807 net.cpp:66] Creating Layer relu8
I0909 00:10:17.709697 25807 net.cpp:329] relu8 <- fc8
I0909 00:10:17.709704 25807 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:10:17.709712 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.709717 25807 net.cpp:125] relu8 needs backward computation.
I0909 00:10:17.709723 25807 net.cpp:66] Creating Layer drop8
I0909 00:10:17.709729 25807 net.cpp:329] drop8 <- fc8
I0909 00:10:17.709735 25807 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:10:17.709743 25807 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:10:17.709749 25807 net.cpp:125] drop8 needs backward computation.
I0909 00:10:17.709756 25807 net.cpp:66] Creating Layer fc9
I0909 00:10:17.709761 25807 net.cpp:329] fc9 <- fc8
I0909 00:10:17.709769 25807 net.cpp:290] fc9 -> fc9
I0909 00:10:17.710155 25807 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:10:17.710165 25807 net.cpp:125] fc9 needs backward computation.
I0909 00:10:17.710173 25807 net.cpp:66] Creating Layer fc10
I0909 00:10:17.710180 25807 net.cpp:329] fc10 <- fc9
I0909 00:10:17.710187 25807 net.cpp:290] fc10 -> fc10
I0909 00:10:17.710199 25807 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:10:17.710207 25807 net.cpp:125] fc10 needs backward computation.
I0909 00:10:17.710213 25807 net.cpp:66] Creating Layer prob
I0909 00:10:17.710218 25807 net.cpp:329] prob <- fc10
I0909 00:10:17.710238 25807 net.cpp:290] prob -> prob
I0909 00:10:17.710248 25807 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:10:17.710254 25807 net.cpp:125] prob needs backward computation.
I0909 00:10:17.710259 25807 net.cpp:156] This network produces output prob
I0909 00:10:17.710271 25807 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:10:17.710280 25807 net.cpp:167] Network initialization done.
I0909 00:10:17.710285 25807 net.cpp:168] Memory required for data: 6183480
Classifying 268 inputs.
Done in 161.97 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:13:06.983772 25814 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:13:06.983908 25814 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:13:06.983917 25814 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:13:06.984061 25814 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:13:06.984122 25814 net.cpp:292] Input 0 -> data
I0909 00:13:06.984148 25814 net.cpp:66] Creating Layer conv1
I0909 00:13:06.984153 25814 net.cpp:329] conv1 <- data
I0909 00:13:06.984163 25814 net.cpp:290] conv1 -> conv1
I0909 00:13:06.985482 25814 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:13:06.985499 25814 net.cpp:125] conv1 needs backward computation.
I0909 00:13:06.985508 25814 net.cpp:66] Creating Layer relu1
I0909 00:13:06.985514 25814 net.cpp:329] relu1 <- conv1
I0909 00:13:06.985541 25814 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:13:06.985558 25814 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:13:06.985564 25814 net.cpp:125] relu1 needs backward computation.
I0909 00:13:06.985571 25814 net.cpp:66] Creating Layer pool1
I0909 00:13:06.985577 25814 net.cpp:329] pool1 <- conv1
I0909 00:13:06.985589 25814 net.cpp:290] pool1 -> pool1
I0909 00:13:06.985606 25814 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:13:06.985612 25814 net.cpp:125] pool1 needs backward computation.
I0909 00:13:06.985620 25814 net.cpp:66] Creating Layer norm1
I0909 00:13:06.985625 25814 net.cpp:329] norm1 <- pool1
I0909 00:13:06.985631 25814 net.cpp:290] norm1 -> norm1
I0909 00:13:06.985641 25814 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:13:06.985646 25814 net.cpp:125] norm1 needs backward computation.
I0909 00:13:06.985654 25814 net.cpp:66] Creating Layer conv2
I0909 00:13:06.985659 25814 net.cpp:329] conv2 <- norm1
I0909 00:13:06.985666 25814 net.cpp:290] conv2 -> conv2
I0909 00:13:06.994534 25814 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:13:06.994549 25814 net.cpp:125] conv2 needs backward computation.
I0909 00:13:06.994555 25814 net.cpp:66] Creating Layer relu2
I0909 00:13:06.994560 25814 net.cpp:329] relu2 <- conv2
I0909 00:13:06.994567 25814 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:13:06.994575 25814 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:13:06.994580 25814 net.cpp:125] relu2 needs backward computation.
I0909 00:13:06.994586 25814 net.cpp:66] Creating Layer pool2
I0909 00:13:06.994591 25814 net.cpp:329] pool2 <- conv2
I0909 00:13:06.994598 25814 net.cpp:290] pool2 -> pool2
I0909 00:13:06.994606 25814 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:13:06.994611 25814 net.cpp:125] pool2 needs backward computation.
I0909 00:13:06.994621 25814 net.cpp:66] Creating Layer fc7
I0909 00:13:06.994626 25814 net.cpp:329] fc7 <- pool2
I0909 00:13:06.994633 25814 net.cpp:290] fc7 -> fc7
I0909 00:13:07.635078 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.635124 25814 net.cpp:125] fc7 needs backward computation.
I0909 00:13:07.635136 25814 net.cpp:66] Creating Layer relu7
I0909 00:13:07.635145 25814 net.cpp:329] relu7 <- fc7
I0909 00:13:07.635154 25814 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:13:07.635164 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.635169 25814 net.cpp:125] relu7 needs backward computation.
I0909 00:13:07.635176 25814 net.cpp:66] Creating Layer drop7
I0909 00:13:07.635181 25814 net.cpp:329] drop7 <- fc7
I0909 00:13:07.635188 25814 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:13:07.635198 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.635205 25814 net.cpp:125] drop7 needs backward computation.
I0909 00:13:07.635212 25814 net.cpp:66] Creating Layer fc8
I0909 00:13:07.635217 25814 net.cpp:329] fc8 <- fc7
I0909 00:13:07.635226 25814 net.cpp:290] fc8 -> fc8
I0909 00:13:07.642985 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.642997 25814 net.cpp:125] fc8 needs backward computation.
I0909 00:13:07.643004 25814 net.cpp:66] Creating Layer relu8
I0909 00:13:07.643009 25814 net.cpp:329] relu8 <- fc8
I0909 00:13:07.643018 25814 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:13:07.643025 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.643030 25814 net.cpp:125] relu8 needs backward computation.
I0909 00:13:07.643049 25814 net.cpp:66] Creating Layer drop8
I0909 00:13:07.643054 25814 net.cpp:329] drop8 <- fc8
I0909 00:13:07.643060 25814 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:13:07.643069 25814 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:07.643074 25814 net.cpp:125] drop8 needs backward computation.
I0909 00:13:07.643082 25814 net.cpp:66] Creating Layer fc9
I0909 00:13:07.643087 25814 net.cpp:329] fc9 <- fc8
I0909 00:13:07.643095 25814 net.cpp:290] fc9 -> fc9
I0909 00:13:07.643466 25814 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:13:07.643476 25814 net.cpp:125] fc9 needs backward computation.
I0909 00:13:07.643484 25814 net.cpp:66] Creating Layer fc10
I0909 00:13:07.643491 25814 net.cpp:329] fc10 <- fc9
I0909 00:13:07.643498 25814 net.cpp:290] fc10 -> fc10
I0909 00:13:07.643510 25814 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:13:07.643517 25814 net.cpp:125] fc10 needs backward computation.
I0909 00:13:07.643524 25814 net.cpp:66] Creating Layer prob
I0909 00:13:07.643529 25814 net.cpp:329] prob <- fc10
I0909 00:13:07.643537 25814 net.cpp:290] prob -> prob
I0909 00:13:07.643548 25814 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:13:07.643553 25814 net.cpp:125] prob needs backward computation.
I0909 00:13:07.643558 25814 net.cpp:156] This network produces output prob
I0909 00:13:07.643569 25814 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:13:07.643579 25814 net.cpp:167] Network initialization done.
I0909 00:13:07.643584 25814 net.cpp:168] Memory required for data: 6183480
Classifying 2 inputs.
Done in 1.28 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:13:09.652284 25817 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:13:09.652420 25817 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:13:09.652428 25817 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:13:09.652571 25817 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:13:09.652632 25817 net.cpp:292] Input 0 -> data
I0909 00:13:09.652659 25817 net.cpp:66] Creating Layer conv1
I0909 00:13:09.652667 25817 net.cpp:329] conv1 <- data
I0909 00:13:09.652674 25817 net.cpp:290] conv1 -> conv1
I0909 00:13:09.654054 25817 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:13:09.654073 25817 net.cpp:125] conv1 needs backward computation.
I0909 00:13:09.654083 25817 net.cpp:66] Creating Layer relu1
I0909 00:13:09.654088 25817 net.cpp:329] relu1 <- conv1
I0909 00:13:09.654094 25817 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:13:09.654103 25817 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:13:09.654109 25817 net.cpp:125] relu1 needs backward computation.
I0909 00:13:09.654115 25817 net.cpp:66] Creating Layer pool1
I0909 00:13:09.654120 25817 net.cpp:329] pool1 <- conv1
I0909 00:13:09.654127 25817 net.cpp:290] pool1 -> pool1
I0909 00:13:09.654137 25817 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:13:09.654144 25817 net.cpp:125] pool1 needs backward computation.
I0909 00:13:09.654150 25817 net.cpp:66] Creating Layer norm1
I0909 00:13:09.654155 25817 net.cpp:329] norm1 <- pool1
I0909 00:13:09.654161 25817 net.cpp:290] norm1 -> norm1
I0909 00:13:09.654171 25817 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:13:09.654176 25817 net.cpp:125] norm1 needs backward computation.
I0909 00:13:09.654183 25817 net.cpp:66] Creating Layer conv2
I0909 00:13:09.654189 25817 net.cpp:329] conv2 <- norm1
I0909 00:13:09.654196 25817 net.cpp:290] conv2 -> conv2
I0909 00:13:09.663033 25817 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:13:09.663048 25817 net.cpp:125] conv2 needs backward computation.
I0909 00:13:09.663054 25817 net.cpp:66] Creating Layer relu2
I0909 00:13:09.663059 25817 net.cpp:329] relu2 <- conv2
I0909 00:13:09.663065 25817 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:13:09.663072 25817 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:13:09.663079 25817 net.cpp:125] relu2 needs backward computation.
I0909 00:13:09.663084 25817 net.cpp:66] Creating Layer pool2
I0909 00:13:09.663089 25817 net.cpp:329] pool2 <- conv2
I0909 00:13:09.663095 25817 net.cpp:290] pool2 -> pool2
I0909 00:13:09.663103 25817 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:13:09.663108 25817 net.cpp:125] pool2 needs backward computation.
I0909 00:13:09.663117 25817 net.cpp:66] Creating Layer fc7
I0909 00:13:09.663123 25817 net.cpp:329] fc7 <- pool2
I0909 00:13:09.663130 25817 net.cpp:290] fc7 -> fc7
I0909 00:13:10.287905 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.287955 25817 net.cpp:125] fc7 needs backward computation.
I0909 00:13:10.287967 25817 net.cpp:66] Creating Layer relu7
I0909 00:13:10.287976 25817 net.cpp:329] relu7 <- fc7
I0909 00:13:10.287983 25817 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:13:10.287993 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.288009 25817 net.cpp:125] relu7 needs backward computation.
I0909 00:13:10.288017 25817 net.cpp:66] Creating Layer drop7
I0909 00:13:10.288022 25817 net.cpp:329] drop7 <- fc7
I0909 00:13:10.288028 25817 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:13:10.288039 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.288045 25817 net.cpp:125] drop7 needs backward computation.
I0909 00:13:10.288053 25817 net.cpp:66] Creating Layer fc8
I0909 00:13:10.288058 25817 net.cpp:329] fc8 <- fc7
I0909 00:13:10.288067 25817 net.cpp:290] fc8 -> fc8
I0909 00:13:10.295617 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.295630 25817 net.cpp:125] fc8 needs backward computation.
I0909 00:13:10.295636 25817 net.cpp:66] Creating Layer relu8
I0909 00:13:10.295642 25817 net.cpp:329] relu8 <- fc8
I0909 00:13:10.295650 25817 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:13:10.295656 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.295662 25817 net.cpp:125] relu8 needs backward computation.
I0909 00:13:10.295668 25817 net.cpp:66] Creating Layer drop8
I0909 00:13:10.295673 25817 net.cpp:329] drop8 <- fc8
I0909 00:13:10.295680 25817 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:13:10.295688 25817 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:13:10.295693 25817 net.cpp:125] drop8 needs backward computation.
I0909 00:13:10.295701 25817 net.cpp:66] Creating Layer fc9
I0909 00:13:10.295706 25817 net.cpp:329] fc9 <- fc8
I0909 00:13:10.295712 25817 net.cpp:290] fc9 -> fc9
I0909 00:13:10.296077 25817 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:13:10.296087 25817 net.cpp:125] fc9 needs backward computation.
I0909 00:13:10.296095 25817 net.cpp:66] Creating Layer fc10
I0909 00:13:10.296100 25817 net.cpp:329] fc10 <- fc9
I0909 00:13:10.296108 25817 net.cpp:290] fc10 -> fc10
I0909 00:13:10.296120 25817 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:13:10.296128 25817 net.cpp:125] fc10 needs backward computation.
I0909 00:13:10.296134 25817 net.cpp:66] Creating Layer prob
I0909 00:13:10.296139 25817 net.cpp:329] prob <- fc10
I0909 00:13:10.296147 25817 net.cpp:290] prob -> prob
I0909 00:13:10.296156 25817 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:13:10.296162 25817 net.cpp:125] prob needs backward computation.
I0909 00:13:10.296167 25817 net.cpp:156] This network produces output prob
I0909 00:13:10.296180 25817 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:13:10.296187 25817 net.cpp:167] Network initialization done.
I0909 00:13:10.296192 25817 net.cpp:168] Memory required for data: 6183480
Classifying 88 inputs.
Done in 53.54 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:14:06.498672 25821 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:14:06.498807 25821 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:14:06.498816 25821 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:14:06.498958 25821 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:14:06.499022 25821 net.cpp:292] Input 0 -> data
I0909 00:14:06.499047 25821 net.cpp:66] Creating Layer conv1
I0909 00:14:06.499054 25821 net.cpp:329] conv1 <- data
I0909 00:14:06.499063 25821 net.cpp:290] conv1 -> conv1
I0909 00:14:06.500382 25821 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:14:06.500399 25821 net.cpp:125] conv1 needs backward computation.
I0909 00:14:06.500408 25821 net.cpp:66] Creating Layer relu1
I0909 00:14:06.500414 25821 net.cpp:329] relu1 <- conv1
I0909 00:14:06.500421 25821 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:14:06.500429 25821 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:14:06.500434 25821 net.cpp:125] relu1 needs backward computation.
I0909 00:14:06.500442 25821 net.cpp:66] Creating Layer pool1
I0909 00:14:06.500447 25821 net.cpp:329] pool1 <- conv1
I0909 00:14:06.500453 25821 net.cpp:290] pool1 -> pool1
I0909 00:14:06.500463 25821 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:14:06.500469 25821 net.cpp:125] pool1 needs backward computation.
I0909 00:14:06.500476 25821 net.cpp:66] Creating Layer norm1
I0909 00:14:06.500481 25821 net.cpp:329] norm1 <- pool1
I0909 00:14:06.500488 25821 net.cpp:290] norm1 -> norm1
I0909 00:14:06.500497 25821 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:14:06.500504 25821 net.cpp:125] norm1 needs backward computation.
I0909 00:14:06.500510 25821 net.cpp:66] Creating Layer conv2
I0909 00:14:06.500515 25821 net.cpp:329] conv2 <- norm1
I0909 00:14:06.500522 25821 net.cpp:290] conv2 -> conv2
I0909 00:14:06.509393 25821 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:14:06.509407 25821 net.cpp:125] conv2 needs backward computation.
I0909 00:14:06.509414 25821 net.cpp:66] Creating Layer relu2
I0909 00:14:06.509420 25821 net.cpp:329] relu2 <- conv2
I0909 00:14:06.509431 25821 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:14:06.509438 25821 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:14:06.509444 25821 net.cpp:125] relu2 needs backward computation.
I0909 00:14:06.509450 25821 net.cpp:66] Creating Layer pool2
I0909 00:14:06.509455 25821 net.cpp:329] pool2 <- conv2
I0909 00:14:06.509462 25821 net.cpp:290] pool2 -> pool2
I0909 00:14:06.509470 25821 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:14:06.509475 25821 net.cpp:125] pool2 needs backward computation.
I0909 00:14:06.509485 25821 net.cpp:66] Creating Layer fc7
I0909 00:14:06.509490 25821 net.cpp:329] fc7 <- pool2
I0909 00:14:06.509497 25821 net.cpp:290] fc7 -> fc7
I0909 00:14:07.133999 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.134048 25821 net.cpp:125] fc7 needs backward computation.
I0909 00:14:07.134062 25821 net.cpp:66] Creating Layer relu7
I0909 00:14:07.134070 25821 net.cpp:329] relu7 <- fc7
I0909 00:14:07.134078 25821 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:14:07.134088 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.134094 25821 net.cpp:125] relu7 needs backward computation.
I0909 00:14:07.134101 25821 net.cpp:66] Creating Layer drop7
I0909 00:14:07.134106 25821 net.cpp:329] drop7 <- fc7
I0909 00:14:07.134112 25821 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:14:07.134124 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.134130 25821 net.cpp:125] drop7 needs backward computation.
I0909 00:14:07.134137 25821 net.cpp:66] Creating Layer fc8
I0909 00:14:07.134142 25821 net.cpp:329] fc8 <- fc7
I0909 00:14:07.134151 25821 net.cpp:290] fc8 -> fc8
I0909 00:14:07.141698 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.141710 25821 net.cpp:125] fc8 needs backward computation.
I0909 00:14:07.141717 25821 net.cpp:66] Creating Layer relu8
I0909 00:14:07.141723 25821 net.cpp:329] relu8 <- fc8
I0909 00:14:07.141731 25821 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:14:07.141737 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.141743 25821 net.cpp:125] relu8 needs backward computation.
I0909 00:14:07.141749 25821 net.cpp:66] Creating Layer drop8
I0909 00:14:07.141754 25821 net.cpp:329] drop8 <- fc8
I0909 00:14:07.141762 25821 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:14:07.141769 25821 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:14:07.141775 25821 net.cpp:125] drop8 needs backward computation.
I0909 00:14:07.141782 25821 net.cpp:66] Creating Layer fc9
I0909 00:14:07.141788 25821 net.cpp:329] fc9 <- fc8
I0909 00:14:07.141794 25821 net.cpp:290] fc9 -> fc9
I0909 00:14:07.142160 25821 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:14:07.142170 25821 net.cpp:125] fc9 needs backward computation.
I0909 00:14:07.142179 25821 net.cpp:66] Creating Layer fc10
I0909 00:14:07.142184 25821 net.cpp:329] fc10 <- fc9
I0909 00:14:07.142192 25821 net.cpp:290] fc10 -> fc10
I0909 00:14:07.142204 25821 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:14:07.142212 25821 net.cpp:125] fc10 needs backward computation.
I0909 00:14:07.142218 25821 net.cpp:66] Creating Layer prob
I0909 00:14:07.142225 25821 net.cpp:329] prob <- fc10
I0909 00:14:07.142231 25821 net.cpp:290] prob -> prob
I0909 00:14:07.142241 25821 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:14:07.142247 25821 net.cpp:125] prob needs backward computation.
I0909 00:14:07.142251 25821 net.cpp:156] This network produces output prob
I0909 00:14:07.142264 25821 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:14:07.142272 25821 net.cpp:167] Network initialization done.
I0909 00:14:07.142277 25821 net.cpp:168] Memory required for data: 6183480
Classifying 103 inputs.
Done in 63.85 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:15:14.014844 25825 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:15:14.014986 25825 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:15:14.014994 25825 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:15:14.015152 25825 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:15:14.015203 25825 net.cpp:292] Input 0 -> data
I0909 00:15:14.015229 25825 net.cpp:66] Creating Layer conv1
I0909 00:15:14.015236 25825 net.cpp:329] conv1 <- data
I0909 00:15:14.015244 25825 net.cpp:290] conv1 -> conv1
I0909 00:15:14.016603 25825 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:14.016621 25825 net.cpp:125] conv1 needs backward computation.
I0909 00:15:14.016630 25825 net.cpp:66] Creating Layer relu1
I0909 00:15:14.016636 25825 net.cpp:329] relu1 <- conv1
I0909 00:15:14.016643 25825 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:15:14.016651 25825 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:14.016657 25825 net.cpp:125] relu1 needs backward computation.
I0909 00:15:14.016664 25825 net.cpp:66] Creating Layer pool1
I0909 00:15:14.016674 25825 net.cpp:329] pool1 <- conv1
I0909 00:15:14.016682 25825 net.cpp:290] pool1 -> pool1
I0909 00:15:14.016693 25825 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:14.016700 25825 net.cpp:125] pool1 needs backward computation.
I0909 00:15:14.016706 25825 net.cpp:66] Creating Layer norm1
I0909 00:15:14.016712 25825 net.cpp:329] norm1 <- pool1
I0909 00:15:14.016718 25825 net.cpp:290] norm1 -> norm1
I0909 00:15:14.016728 25825 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:14.016734 25825 net.cpp:125] norm1 needs backward computation.
I0909 00:15:14.016741 25825 net.cpp:66] Creating Layer conv2
I0909 00:15:14.016747 25825 net.cpp:329] conv2 <- norm1
I0909 00:15:14.016754 25825 net.cpp:290] conv2 -> conv2
I0909 00:15:14.025833 25825 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:14.025848 25825 net.cpp:125] conv2 needs backward computation.
I0909 00:15:14.025856 25825 net.cpp:66] Creating Layer relu2
I0909 00:15:14.025861 25825 net.cpp:329] relu2 <- conv2
I0909 00:15:14.025867 25825 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:15:14.025874 25825 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:14.025879 25825 net.cpp:125] relu2 needs backward computation.
I0909 00:15:14.025888 25825 net.cpp:66] Creating Layer pool2
I0909 00:15:14.025894 25825 net.cpp:329] pool2 <- conv2
I0909 00:15:14.025900 25825 net.cpp:290] pool2 -> pool2
I0909 00:15:14.025908 25825 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:15:14.025914 25825 net.cpp:125] pool2 needs backward computation.
I0909 00:15:14.025920 25825 net.cpp:66] Creating Layer fc7
I0909 00:15:14.025926 25825 net.cpp:329] fc7 <- pool2
I0909 00:15:14.025933 25825 net.cpp:290] fc7 -> fc7
I0909 00:15:14.650457 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.650502 25825 net.cpp:125] fc7 needs backward computation.
I0909 00:15:14.650514 25825 net.cpp:66] Creating Layer relu7
I0909 00:15:14.650521 25825 net.cpp:329] relu7 <- fc7
I0909 00:15:14.650531 25825 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:15:14.650542 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.650547 25825 net.cpp:125] relu7 needs backward computation.
I0909 00:15:14.650554 25825 net.cpp:66] Creating Layer drop7
I0909 00:15:14.650559 25825 net.cpp:329] drop7 <- fc7
I0909 00:15:14.650565 25825 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:15:14.650576 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.650583 25825 net.cpp:125] drop7 needs backward computation.
I0909 00:15:14.650590 25825 net.cpp:66] Creating Layer fc8
I0909 00:15:14.650595 25825 net.cpp:329] fc8 <- fc7
I0909 00:15:14.650604 25825 net.cpp:290] fc8 -> fc8
I0909 00:15:14.658166 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.658179 25825 net.cpp:125] fc8 needs backward computation.
I0909 00:15:14.658185 25825 net.cpp:66] Creating Layer relu8
I0909 00:15:14.658190 25825 net.cpp:329] relu8 <- fc8
I0909 00:15:14.658198 25825 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:15:14.658205 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.658211 25825 net.cpp:125] relu8 needs backward computation.
I0909 00:15:14.658217 25825 net.cpp:66] Creating Layer drop8
I0909 00:15:14.658222 25825 net.cpp:329] drop8 <- fc8
I0909 00:15:14.658228 25825 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:15:14.658236 25825 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:14.658241 25825 net.cpp:125] drop8 needs backward computation.
I0909 00:15:14.658249 25825 net.cpp:66] Creating Layer fc9
I0909 00:15:14.658256 25825 net.cpp:329] fc9 <- fc8
I0909 00:15:14.658262 25825 net.cpp:290] fc9 -> fc9
I0909 00:15:14.658627 25825 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:15:14.658638 25825 net.cpp:125] fc9 needs backward computation.
I0909 00:15:14.658645 25825 net.cpp:66] Creating Layer fc10
I0909 00:15:14.658651 25825 net.cpp:329] fc10 <- fc9
I0909 00:15:14.658659 25825 net.cpp:290] fc10 -> fc10
I0909 00:15:14.658671 25825 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:14.658679 25825 net.cpp:125] fc10 needs backward computation.
I0909 00:15:14.658695 25825 net.cpp:66] Creating Layer prob
I0909 00:15:14.658701 25825 net.cpp:329] prob <- fc10
I0909 00:15:14.658710 25825 net.cpp:290] prob -> prob
I0909 00:15:14.658718 25825 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:14.658725 25825 net.cpp:125] prob needs backward computation.
I0909 00:15:14.658730 25825 net.cpp:156] This network produces output prob
I0909 00:15:14.658741 25825 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:15:14.658751 25825 net.cpp:167] Network initialization done.
I0909 00:15:14.658756 25825 net.cpp:168] Memory required for data: 6183480
Classifying 6 inputs.
Done in 3.84 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:15:19.300113 25828 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:15:19.300248 25828 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:15:19.300257 25828 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:15:19.300401 25828 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:15:19.300464 25828 net.cpp:292] Input 0 -> data
I0909 00:15:19.300490 25828 net.cpp:66] Creating Layer conv1
I0909 00:15:19.300498 25828 net.cpp:329] conv1 <- data
I0909 00:15:19.300505 25828 net.cpp:290] conv1 -> conv1
I0909 00:15:19.301869 25828 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:19.301888 25828 net.cpp:125] conv1 needs backward computation.
I0909 00:15:19.301898 25828 net.cpp:66] Creating Layer relu1
I0909 00:15:19.301903 25828 net.cpp:329] relu1 <- conv1
I0909 00:15:19.301909 25828 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:15:19.301918 25828 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:19.301923 25828 net.cpp:125] relu1 needs backward computation.
I0909 00:15:19.301930 25828 net.cpp:66] Creating Layer pool1
I0909 00:15:19.301935 25828 net.cpp:329] pool1 <- conv1
I0909 00:15:19.301942 25828 net.cpp:290] pool1 -> pool1
I0909 00:15:19.301952 25828 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:19.301959 25828 net.cpp:125] pool1 needs backward computation.
I0909 00:15:19.301965 25828 net.cpp:66] Creating Layer norm1
I0909 00:15:19.301970 25828 net.cpp:329] norm1 <- pool1
I0909 00:15:19.301977 25828 net.cpp:290] norm1 -> norm1
I0909 00:15:19.301986 25828 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:19.301992 25828 net.cpp:125] norm1 needs backward computation.
I0909 00:15:19.302000 25828 net.cpp:66] Creating Layer conv2
I0909 00:15:19.302006 25828 net.cpp:329] conv2 <- norm1
I0909 00:15:19.302012 25828 net.cpp:290] conv2 -> conv2
I0909 00:15:19.310863 25828 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:19.310875 25828 net.cpp:125] conv2 needs backward computation.
I0909 00:15:19.310883 25828 net.cpp:66] Creating Layer relu2
I0909 00:15:19.310889 25828 net.cpp:329] relu2 <- conv2
I0909 00:15:19.310894 25828 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:15:19.310901 25828 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:19.310906 25828 net.cpp:125] relu2 needs backward computation.
I0909 00:15:19.310912 25828 net.cpp:66] Creating Layer pool2
I0909 00:15:19.310917 25828 net.cpp:329] pool2 <- conv2
I0909 00:15:19.310924 25828 net.cpp:290] pool2 -> pool2
I0909 00:15:19.310931 25828 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:15:19.310937 25828 net.cpp:125] pool2 needs backward computation.
I0909 00:15:19.310945 25828 net.cpp:66] Creating Layer fc7
I0909 00:15:19.310951 25828 net.cpp:329] fc7 <- pool2
I0909 00:15:19.310958 25828 net.cpp:290] fc7 -> fc7
I0909 00:15:19.935515 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.935561 25828 net.cpp:125] fc7 needs backward computation.
I0909 00:15:19.935575 25828 net.cpp:66] Creating Layer relu7
I0909 00:15:19.935582 25828 net.cpp:329] relu7 <- fc7
I0909 00:15:19.935590 25828 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:15:19.935600 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.935606 25828 net.cpp:125] relu7 needs backward computation.
I0909 00:15:19.935612 25828 net.cpp:66] Creating Layer drop7
I0909 00:15:19.935617 25828 net.cpp:329] drop7 <- fc7
I0909 00:15:19.935623 25828 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:15:19.935633 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.935639 25828 net.cpp:125] drop7 needs backward computation.
I0909 00:15:19.935647 25828 net.cpp:66] Creating Layer fc8
I0909 00:15:19.935652 25828 net.cpp:329] fc8 <- fc7
I0909 00:15:19.935662 25828 net.cpp:290] fc8 -> fc8
I0909 00:15:19.943234 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.943246 25828 net.cpp:125] fc8 needs backward computation.
I0909 00:15:19.943253 25828 net.cpp:66] Creating Layer relu8
I0909 00:15:19.943259 25828 net.cpp:329] relu8 <- fc8
I0909 00:15:19.943266 25828 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:15:19.943274 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.943290 25828 net.cpp:125] relu8 needs backward computation.
I0909 00:15:19.943297 25828 net.cpp:66] Creating Layer drop8
I0909 00:15:19.943302 25828 net.cpp:329] drop8 <- fc8
I0909 00:15:19.943308 25828 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:15:19.943316 25828 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:19.943322 25828 net.cpp:125] drop8 needs backward computation.
I0909 00:15:19.943330 25828 net.cpp:66] Creating Layer fc9
I0909 00:15:19.943336 25828 net.cpp:329] fc9 <- fc8
I0909 00:15:19.943342 25828 net.cpp:290] fc9 -> fc9
I0909 00:15:19.943724 25828 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:15:19.943735 25828 net.cpp:125] fc9 needs backward computation.
I0909 00:15:19.943744 25828 net.cpp:66] Creating Layer fc10
I0909 00:15:19.943749 25828 net.cpp:329] fc10 <- fc9
I0909 00:15:19.943758 25828 net.cpp:290] fc10 -> fc10
I0909 00:15:19.943778 25828 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:19.943786 25828 net.cpp:125] fc10 needs backward computation.
I0909 00:15:19.943792 25828 net.cpp:66] Creating Layer prob
I0909 00:15:19.943805 25828 net.cpp:329] prob <- fc10
I0909 00:15:19.943814 25828 net.cpp:290] prob -> prob
I0909 00:15:19.943822 25828 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:19.943828 25828 net.cpp:125] prob needs backward computation.
I0909 00:15:19.943832 25828 net.cpp:156] This network produces output prob
I0909 00:15:19.943845 25828 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:15:19.943853 25828 net.cpp:167] Network initialization done.
I0909 00:15:19.943858 25828 net.cpp:168] Memory required for data: 6183480
Classifying 51 inputs.
Done in 30.96 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:15:52.767499 25832 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:15:52.767637 25832 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:15:52.767645 25832 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:15:52.767788 25832 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:15:52.767849 25832 net.cpp:292] Input 0 -> data
I0909 00:15:52.767875 25832 net.cpp:66] Creating Layer conv1
I0909 00:15:52.767882 25832 net.cpp:329] conv1 <- data
I0909 00:15:52.767890 25832 net.cpp:290] conv1 -> conv1
I0909 00:15:52.769208 25832 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:52.769227 25832 net.cpp:125] conv1 needs backward computation.
I0909 00:15:52.769235 25832 net.cpp:66] Creating Layer relu1
I0909 00:15:52.769242 25832 net.cpp:329] relu1 <- conv1
I0909 00:15:52.769248 25832 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:15:52.769255 25832 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:15:52.769261 25832 net.cpp:125] relu1 needs backward computation.
I0909 00:15:52.769268 25832 net.cpp:66] Creating Layer pool1
I0909 00:15:52.769273 25832 net.cpp:329] pool1 <- conv1
I0909 00:15:52.769279 25832 net.cpp:290] pool1 -> pool1
I0909 00:15:52.769290 25832 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:52.769296 25832 net.cpp:125] pool1 needs backward computation.
I0909 00:15:52.769302 25832 net.cpp:66] Creating Layer norm1
I0909 00:15:52.769307 25832 net.cpp:329] norm1 <- pool1
I0909 00:15:52.769315 25832 net.cpp:290] norm1 -> norm1
I0909 00:15:52.769323 25832 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:15:52.769330 25832 net.cpp:125] norm1 needs backward computation.
I0909 00:15:52.769336 25832 net.cpp:66] Creating Layer conv2
I0909 00:15:52.769341 25832 net.cpp:329] conv2 <- norm1
I0909 00:15:52.769348 25832 net.cpp:290] conv2 -> conv2
I0909 00:15:52.778231 25832 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:52.778245 25832 net.cpp:125] conv2 needs backward computation.
I0909 00:15:52.778252 25832 net.cpp:66] Creating Layer relu2
I0909 00:15:52.778257 25832 net.cpp:329] relu2 <- conv2
I0909 00:15:52.778264 25832 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:15:52.778270 25832 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:15:52.778276 25832 net.cpp:125] relu2 needs backward computation.
I0909 00:15:52.778282 25832 net.cpp:66] Creating Layer pool2
I0909 00:15:52.778287 25832 net.cpp:329] pool2 <- conv2
I0909 00:15:52.778295 25832 net.cpp:290] pool2 -> pool2
I0909 00:15:52.778301 25832 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:15:52.778307 25832 net.cpp:125] pool2 needs backward computation.
I0909 00:15:52.778316 25832 net.cpp:66] Creating Layer fc7
I0909 00:15:52.778321 25832 net.cpp:329] fc7 <- pool2
I0909 00:15:52.778328 25832 net.cpp:290] fc7 -> fc7
I0909 00:15:53.402905 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.402952 25832 net.cpp:125] fc7 needs backward computation.
I0909 00:15:53.402964 25832 net.cpp:66] Creating Layer relu7
I0909 00:15:53.402973 25832 net.cpp:329] relu7 <- fc7
I0909 00:15:53.402992 25832 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:15:53.403002 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.403008 25832 net.cpp:125] relu7 needs backward computation.
I0909 00:15:53.403015 25832 net.cpp:66] Creating Layer drop7
I0909 00:15:53.403020 25832 net.cpp:329] drop7 <- fc7
I0909 00:15:53.403026 25832 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:15:53.403038 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.403043 25832 net.cpp:125] drop7 needs backward computation.
I0909 00:15:53.403051 25832 net.cpp:66] Creating Layer fc8
I0909 00:15:53.403056 25832 net.cpp:329] fc8 <- fc7
I0909 00:15:53.403065 25832 net.cpp:290] fc8 -> fc8
I0909 00:15:53.410626 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.410639 25832 net.cpp:125] fc8 needs backward computation.
I0909 00:15:53.410645 25832 net.cpp:66] Creating Layer relu8
I0909 00:15:53.410651 25832 net.cpp:329] relu8 <- fc8
I0909 00:15:53.410658 25832 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:15:53.410665 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.410671 25832 net.cpp:125] relu8 needs backward computation.
I0909 00:15:53.410677 25832 net.cpp:66] Creating Layer drop8
I0909 00:15:53.410682 25832 net.cpp:329] drop8 <- fc8
I0909 00:15:53.410688 25832 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:15:53.410697 25832 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:15:53.410702 25832 net.cpp:125] drop8 needs backward computation.
I0909 00:15:53.410709 25832 net.cpp:66] Creating Layer fc9
I0909 00:15:53.410714 25832 net.cpp:329] fc9 <- fc8
I0909 00:15:53.410722 25832 net.cpp:290] fc9 -> fc9
I0909 00:15:53.411084 25832 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:15:53.411094 25832 net.cpp:125] fc9 needs backward computation.
I0909 00:15:53.411103 25832 net.cpp:66] Creating Layer fc10
I0909 00:15:53.411108 25832 net.cpp:329] fc10 <- fc9
I0909 00:15:53.411115 25832 net.cpp:290] fc10 -> fc10
I0909 00:15:53.411128 25832 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:53.411134 25832 net.cpp:125] fc10 needs backward computation.
I0909 00:15:53.411141 25832 net.cpp:66] Creating Layer prob
I0909 00:15:53.411146 25832 net.cpp:329] prob <- fc10
I0909 00:15:53.411154 25832 net.cpp:290] prob -> prob
I0909 00:15:53.411164 25832 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:15:53.411169 25832 net.cpp:125] prob needs backward computation.
I0909 00:15:53.411173 25832 net.cpp:156] This network produces output prob
I0909 00:15:53.411185 25832 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:15:53.411193 25832 net.cpp:167] Network initialization done.
I0909 00:15:53.411198 25832 net.cpp:168] Memory required for data: 6183480
Classifying 25 inputs.
Done in 15.38 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:16:09.995429 25835 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:16:09.995564 25835 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:16:09.995573 25835 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:16:09.995718 25835 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:16:09.995777 25835 net.cpp:292] Input 0 -> data
I0909 00:16:09.995803 25835 net.cpp:66] Creating Layer conv1
I0909 00:16:09.995810 25835 net.cpp:329] conv1 <- data
I0909 00:16:09.995818 25835 net.cpp:290] conv1 -> conv1
I0909 00:16:09.997140 25835 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:16:09.997159 25835 net.cpp:125] conv1 needs backward computation.
I0909 00:16:09.997167 25835 net.cpp:66] Creating Layer relu1
I0909 00:16:09.997172 25835 net.cpp:329] relu1 <- conv1
I0909 00:16:09.997179 25835 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:16:09.997187 25835 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:16:09.997194 25835 net.cpp:125] relu1 needs backward computation.
I0909 00:16:09.997200 25835 net.cpp:66] Creating Layer pool1
I0909 00:16:09.997205 25835 net.cpp:329] pool1 <- conv1
I0909 00:16:09.997212 25835 net.cpp:290] pool1 -> pool1
I0909 00:16:09.997222 25835 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:16:09.997228 25835 net.cpp:125] pool1 needs backward computation.
I0909 00:16:09.997234 25835 net.cpp:66] Creating Layer norm1
I0909 00:16:09.997241 25835 net.cpp:329] norm1 <- pool1
I0909 00:16:09.997246 25835 net.cpp:290] norm1 -> norm1
I0909 00:16:09.997256 25835 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:16:09.997262 25835 net.cpp:125] norm1 needs backward computation.
I0909 00:16:09.997268 25835 net.cpp:66] Creating Layer conv2
I0909 00:16:09.997274 25835 net.cpp:329] conv2 <- norm1
I0909 00:16:09.997282 25835 net.cpp:290] conv2 -> conv2
I0909 00:16:10.006155 25835 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:16:10.006170 25835 net.cpp:125] conv2 needs backward computation.
I0909 00:16:10.006176 25835 net.cpp:66] Creating Layer relu2
I0909 00:16:10.006186 25835 net.cpp:329] relu2 <- conv2
I0909 00:16:10.006193 25835 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:16:10.006201 25835 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:16:10.006206 25835 net.cpp:125] relu2 needs backward computation.
I0909 00:16:10.006213 25835 net.cpp:66] Creating Layer pool2
I0909 00:16:10.006219 25835 net.cpp:329] pool2 <- conv2
I0909 00:16:10.006227 25835 net.cpp:290] pool2 -> pool2
I0909 00:16:10.006234 25835 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:16:10.006239 25835 net.cpp:125] pool2 needs backward computation.
I0909 00:16:10.006247 25835 net.cpp:66] Creating Layer fc7
I0909 00:16:10.006253 25835 net.cpp:329] fc7 <- pool2
I0909 00:16:10.006259 25835 net.cpp:290] fc7 -> fc7
I0909 00:16:10.630166 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.630215 25835 net.cpp:125] fc7 needs backward computation.
I0909 00:16:10.630228 25835 net.cpp:66] Creating Layer relu7
I0909 00:16:10.630234 25835 net.cpp:329] relu7 <- fc7
I0909 00:16:10.630242 25835 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:16:10.630252 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.630259 25835 net.cpp:125] relu7 needs backward computation.
I0909 00:16:10.630265 25835 net.cpp:66] Creating Layer drop7
I0909 00:16:10.630271 25835 net.cpp:329] drop7 <- fc7
I0909 00:16:10.630277 25835 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:16:10.630288 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.630293 25835 net.cpp:125] drop7 needs backward computation.
I0909 00:16:10.630302 25835 net.cpp:66] Creating Layer fc8
I0909 00:16:10.630307 25835 net.cpp:329] fc8 <- fc7
I0909 00:16:10.630316 25835 net.cpp:290] fc8 -> fc8
I0909 00:16:10.637871 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.637882 25835 net.cpp:125] fc8 needs backward computation.
I0909 00:16:10.637889 25835 net.cpp:66] Creating Layer relu8
I0909 00:16:10.637894 25835 net.cpp:329] relu8 <- fc8
I0909 00:16:10.637902 25835 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:16:10.637909 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.637914 25835 net.cpp:125] relu8 needs backward computation.
I0909 00:16:10.637922 25835 net.cpp:66] Creating Layer drop8
I0909 00:16:10.637927 25835 net.cpp:329] drop8 <- fc8
I0909 00:16:10.637933 25835 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:16:10.637939 25835 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:16:10.637944 25835 net.cpp:125] drop8 needs backward computation.
I0909 00:16:10.637953 25835 net.cpp:66] Creating Layer fc9
I0909 00:16:10.637959 25835 net.cpp:329] fc9 <- fc8
I0909 00:16:10.637965 25835 net.cpp:290] fc9 -> fc9
I0909 00:16:10.638326 25835 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:16:10.638339 25835 net.cpp:125] fc9 needs backward computation.
I0909 00:16:10.638346 25835 net.cpp:66] Creating Layer fc10
I0909 00:16:10.638351 25835 net.cpp:329] fc10 <- fc9
I0909 00:16:10.638360 25835 net.cpp:290] fc10 -> fc10
I0909 00:16:10.638371 25835 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:16:10.638378 25835 net.cpp:125] fc10 needs backward computation.
I0909 00:16:10.638386 25835 net.cpp:66] Creating Layer prob
I0909 00:16:10.638391 25835 net.cpp:329] prob <- fc10
I0909 00:16:10.638398 25835 net.cpp:290] prob -> prob
I0909 00:16:10.638407 25835 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:16:10.638413 25835 net.cpp:125] prob needs backward computation.
I0909 00:16:10.638418 25835 net.cpp:156] This network produces output prob
I0909 00:16:10.638430 25835 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:16:10.638438 25835 net.cpp:167] Network initialization done.
I0909 00:16:10.638443 25835 net.cpp:168] Memory required for data: 6183480
Classifying 163 inputs.
Done in 98.37 s.
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0909 00:17:52.295897 25842 upgrade_proto.cpp:575] Attempting to upgrade input file specified using deprecated V0LayerParameter: distribute/python/../../senti/model4_boost_deploy.prototxt
I0909 00:17:52.296033 25842 upgrade_proto.cpp:582] Successfully upgraded file specified using deprecated V0LayerParameter
E0909 00:17:52.296053 25842 upgrade_proto.cpp:585] Note that future Caffe releases will not support V0NetParameter; use ./build/tools/upgrade_net_proto_text.bin to upgrade this and any other network proto files to the new format.
I0909 00:17:52.296197 25842 net.cpp:38] Initializing net from parameters: 
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8"
  name: "fc8"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "relu8"
  type: RELU
}
layers {
  bottom: "fc8"
  top: "fc8"
  name: "drop8"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc8"
  top: "fc9"
  name: "fc9"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 24
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc9"
  top: "fc10"
  name: "fc10"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc10"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 10
input_dim: 3
input_dim: 227
input_dim: 227
I0909 00:17:52.296247 25842 net.cpp:292] Input 0 -> data
I0909 00:17:52.296272 25842 net.cpp:66] Creating Layer conv1
I0909 00:17:52.296280 25842 net.cpp:329] conv1 <- data
I0909 00:17:52.296288 25842 net.cpp:290] conv1 -> conv1
I0909 00:17:52.297662 25842 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:17:52.297680 25842 net.cpp:125] conv1 needs backward computation.
I0909 00:17:52.297689 25842 net.cpp:66] Creating Layer relu1
I0909 00:17:52.297696 25842 net.cpp:329] relu1 <- conv1
I0909 00:17:52.297703 25842 net.cpp:280] relu1 -> conv1 (in-place)
I0909 00:17:52.297711 25842 net.cpp:83] Top shape: 10 96 55 55 (2904000)
I0909 00:17:52.297719 25842 net.cpp:125] relu1 needs backward computation.
I0909 00:17:52.297729 25842 net.cpp:66] Creating Layer pool1
I0909 00:17:52.297736 25842 net.cpp:329] pool1 <- conv1
I0909 00:17:52.297744 25842 net.cpp:290] pool1 -> pool1
I0909 00:17:52.297755 25842 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:17:52.297760 25842 net.cpp:125] pool1 needs backward computation.
I0909 00:17:52.297768 25842 net.cpp:66] Creating Layer norm1
I0909 00:17:52.297773 25842 net.cpp:329] norm1 <- pool1
I0909 00:17:52.297780 25842 net.cpp:290] norm1 -> norm1
I0909 00:17:52.297791 25842 net.cpp:83] Top shape: 10 96 27 27 (699840)
I0909 00:17:52.297796 25842 net.cpp:125] norm1 needs backward computation.
I0909 00:17:52.297804 25842 net.cpp:66] Creating Layer conv2
I0909 00:17:52.297811 25842 net.cpp:329] conv2 <- norm1
I0909 00:17:52.297817 25842 net.cpp:290] conv2 -> conv2
I0909 00:17:52.306663 25842 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:17:52.306677 25842 net.cpp:125] conv2 needs backward computation.
I0909 00:17:52.306684 25842 net.cpp:66] Creating Layer relu2
I0909 00:17:52.306690 25842 net.cpp:329] relu2 <- conv2
I0909 00:17:52.306697 25842 net.cpp:280] relu2 -> conv2 (in-place)
I0909 00:17:52.306704 25842 net.cpp:83] Top shape: 10 256 27 27 (1866240)
I0909 00:17:52.306710 25842 net.cpp:125] relu2 needs backward computation.
I0909 00:17:52.306720 25842 net.cpp:66] Creating Layer pool2
I0909 00:17:52.306725 25842 net.cpp:329] pool2 <- conv2
I0909 00:17:52.306732 25842 net.cpp:290] pool2 -> pool2
I0909 00:17:52.306740 25842 net.cpp:83] Top shape: 10 256 13 13 (432640)
I0909 00:17:52.306746 25842 net.cpp:125] pool2 needs backward computation.
I0909 00:17:52.306753 25842 net.cpp:66] Creating Layer fc7
I0909 00:17:52.306759 25842 net.cpp:329] fc7 <- pool2
I0909 00:17:52.306766 25842 net.cpp:290] fc7 -> fc7
I0909 00:17:52.931287 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.931334 25842 net.cpp:125] fc7 needs backward computation.
I0909 00:17:52.931346 25842 net.cpp:66] Creating Layer relu7
I0909 00:17:52.931354 25842 net.cpp:329] relu7 <- fc7
I0909 00:17:52.931363 25842 net.cpp:280] relu7 -> fc7 (in-place)
I0909 00:17:52.931373 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.931380 25842 net.cpp:125] relu7 needs backward computation.
I0909 00:17:52.931387 25842 net.cpp:66] Creating Layer drop7
I0909 00:17:52.931393 25842 net.cpp:329] drop7 <- fc7
I0909 00:17:52.931399 25842 net.cpp:280] drop7 -> fc7 (in-place)
I0909 00:17:52.931411 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.931416 25842 net.cpp:125] drop7 needs backward computation.
I0909 00:17:52.931426 25842 net.cpp:66] Creating Layer fc8
I0909 00:17:52.931432 25842 net.cpp:329] fc8 <- fc7
I0909 00:17:52.931440 25842 net.cpp:290] fc8 -> fc8
I0909 00:17:52.938988 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.939002 25842 net.cpp:125] fc8 needs backward computation.
I0909 00:17:52.939008 25842 net.cpp:66] Creating Layer relu8
I0909 00:17:52.939014 25842 net.cpp:329] relu8 <- fc8
I0909 00:17:52.939023 25842 net.cpp:280] relu8 -> fc8 (in-place)
I0909 00:17:52.939029 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.939035 25842 net.cpp:125] relu8 needs backward computation.
I0909 00:17:52.939043 25842 net.cpp:66] Creating Layer drop8
I0909 00:17:52.939049 25842 net.cpp:329] drop8 <- fc8
I0909 00:17:52.939054 25842 net.cpp:280] drop8 -> fc8 (in-place)
I0909 00:17:52.939061 25842 net.cpp:83] Top shape: 10 512 1 1 (5120)
I0909 00:17:52.939067 25842 net.cpp:125] drop8 needs backward computation.
I0909 00:17:52.939076 25842 net.cpp:66] Creating Layer fc9
I0909 00:17:52.939082 25842 net.cpp:329] fc9 <- fc8
I0909 00:17:52.939090 25842 net.cpp:290] fc9 -> fc9
I0909 00:17:52.939453 25842 net.cpp:83] Top shape: 10 24 1 1 (240)
I0909 00:17:52.939465 25842 net.cpp:125] fc9 needs backward computation.
I0909 00:17:52.939473 25842 net.cpp:66] Creating Layer fc10
I0909 00:17:52.939479 25842 net.cpp:329] fc10 <- fc9
I0909 00:17:52.939488 25842 net.cpp:290] fc10 -> fc10
I0909 00:17:52.939501 25842 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:17:52.939519 25842 net.cpp:125] fc10 needs backward computation.
I0909 00:17:52.939527 25842 net.cpp:66] Creating Layer prob
I0909 00:17:52.939532 25842 net.cpp:329] prob <- fc10
I0909 00:17:52.939540 25842 net.cpp:290] prob -> prob
I0909 00:17:52.939550 25842 net.cpp:83] Top shape: 10 2 1 1 (20)
I0909 00:17:52.939556 25842 net.cpp:125] prob needs backward computation.
I0909 00:17:52.939561 25842 net.cpp:156] This network produces output prob
I0909 00:17:52.939574 25842 net.cpp:402] Collecting Learning Rate and Weight Decay.
I0909 00:17:52.939584 25842 net.cpp:167] Network initialization done.
I0909 00:17:52.939589 25842 net.cpp:168] Memory required for data: 6183480
